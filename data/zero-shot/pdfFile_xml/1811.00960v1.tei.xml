<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Vial</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
						</author>
						<title level="a" type="main">Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Word Sense Disambiguation (WSD), the predominant approach generally involves a supervised system trained on sense annotated corpora. The limited quantity of such corpora however restricts the coverage and the performance of these systems. In this article, we propose a new method that solves these issues by taking advantage of the knowledge present in WordNet, and especially the hypernymy and hyponymy relationships between synsets, in order to reduce the number of different sense tags that are necessary to disambiguate all words of the lexical database. Our method leads to state of the art results on most WSD evaluation tasks, while improving the coverage of supervised systems, reducing the training time and the size of the models, without additional training data. In addition, we exhibit results that significantly outperform the state of the art when our method is combined with an ensembling technique and the addition of the WordNet Gloss Tagged as training corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels, given a predefined sense inventory.</p><p>Various approaches have been proposed to achieve WSD, and they are generally ordered by the type and the quantity of resources they use:</p><p>? Knowledge-based methods rely on dictionaries, lexical databases, thesauri or knowledge graphs as main resources, and use algorithms such as lexical similarity measures <ref type="bibr" target="#b8">(Lesk, 1986)</ref> or graph-based measures <ref type="bibr" target="#b12">(Moro et al., 2014)</ref>.</p><p>? Supervised methods, on the other hand, exploit sense annotated corpora as training instances that can be used by a multiclass classifier such as SVM <ref type="bibr" target="#b0">(Chan et al., 2007;</ref><ref type="bibr" target="#b27">Zhong and Ng, 2010)</ref>, or more recently by a neural network <ref type="bibr" target="#b5">(K?geb?ck and Salomonsson, 2016)</ref>.</p><p>? Semi-supervised methods generally use unannotated data to artificially increase the quantity of sense annotated data and hence improve supervised methods <ref type="bibr" target="#b26">(Yuan et al., 2016)</ref>.</p><p>Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns (for instance <ref type="bibr" target="#b14">(Navigli et al., 2007)</ref>). State of the art classifiers used to combine a set of specific features such as the parts of speech tags of surrounding words, local collocations <ref type="bibr" target="#b27">(Zhong and Ng, 2010)</ref> and pretrained word embeddings <ref type="bibr" target="#b4">(Iacobacci et al., 2016)</ref>, but they are now replaced by recurrent neural networks which learn their own representation of words <ref type="bibr" target="#b20">(Raganato et al., 2017b;</ref><ref type="bibr" target="#b7">Le et al., 2018)</ref>.</p><p>One of the major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora. Indeed, while the lexical database WordNet <ref type="bibr" target="#b9">(Miller, 1995)</ref>, the sense inventory of reference used in most works on WSD, contains more than 200 000 different word-sense pairs 1 , the SemCor <ref type="bibr" target="#b10">(Miller et al., 1993)</ref>, the corpus which is used the most in the training of supervised systems, only represents approximately 34 000 of them.</p><p>Many works try to leverage this problem by creating new sense annotated corpora, either automatically <ref type="bibr" target="#b15">(Pasini and Navigli, 2017)</ref>, semi-automatically <ref type="bibr" target="#b24">(Taghipour and Ng, 2015)</ref>, or through crowdsourcing <ref type="bibr" target="#b26">(Yuan et al., 2016)</ref>, but in this work, the idea is to solve this issue by taking advantage of one of the multiple semantic relationships between senses included in WordNet: the hypernymy and hyponymy relationships. Our method is based on three observations: 1. A sense, its hypernym and its hyponyms share a common idea or concept, but on different levels of abstraction.</p><p>2. In general, a word can be disambiguated using the hypernyms of its senses, and not necessarily the senses themselves.</p><p>3. Consequently, we do not need to know every sense of WordNet to disambiguate all words of WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We propose a method for reducing the vocabulary of senses of WordNet by selecting the minimal set of senses required for differentiating the meaning of every word. By using this technique, and converting the sense tags present in sense annotated corpora to the most generalized sense possible, we are able to greatly improve the coverage and the generalization ability of supervised systems. We start by presenting the state of the art of supervised neural architectures for word sense disambiguation, then we describe our new method for sense vocabulary reduction. Our method is then evaluated by measuring its contribution to a state of the art neural WSD system evaluated on classic WSD evaluation campaigns.</p><p>The code for using our system or reproducing our results is available at the following URL: https://github.com/getalp/disambiguate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Word Sense Disambiguation</head><p>The neural approaches for WSD fall into two categories: approaches based on a neural model that learns to classify a sense directly, and approaches based on a neural language model that learns to predict a word, and is then used to find the closest sense to the predicted word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Model Based WSD</head><p>The core of these approaches is a powerful neural language model able to predict a word with consideration for the words surrounding it, thanks to a recurrent neural network trained on a massive quantity of unannotated data. The main works that implement these kind of model are <ref type="bibr" target="#b26">Yuan et al. (2016)</ref> and <ref type="bibr" target="#b7">Le et al. (2018)</ref>. Once the language model is trained, its predictions are used to produce sense vectors as the average of the word vectors predicted by the language model in the places where the words are sense annotated.</p><p>At test time, the language model is used to predict a vector according to the surrounding context, and the sense closest to the predicted vector is assigned to each word.</p><p>These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner. However, sense annotated corpora are still indispensable to construct the sense vectors, and the quantity of data needed for training the language model (100 billion tokens for <ref type="bibr" target="#b26">Yuan et al. (2016)</ref>, 2 billion tokens for <ref type="bibr" target="#b7">Le et al. (2018)</ref>) makes these systems more difficult to train than those relying on sense annotated data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification Based WSD</head><p>In these systems, the main neural network directly classifies and attributes a sense to each input word. Sense annotations are simply seen as tags put on every word, like a POS-tagging task for instance.</p><p>These models are more similar to classical supervised models such as <ref type="bibr" target="#b0">Chan et al. (2007)</ref>, except that the input features are not manually selected, but trained as part of the neural network (using pre-trained word embeddings or not).</p><p>In addition, we can distinguish two separate branches of these types of neural networks:</p><p>1. Those in which we have several distinct and small neural networks (or classifiers) for every different word in the dictionary <ref type="bibr" target="#b4">(Iacobacci et al., 2016;</ref><ref type="bibr" target="#b5">K?geb?ck and Salomonsson, 2016</ref>), each of them being able to manage a particular word and its particular senses. For instance, one of the classifiers is specialized into choosing between the four possible senses of the noun "mouse". This type of approaches is particularly fitted for the lexical sample tasks, where a small and finite set of very ambiguous words have to be sense annotated in several contexts, but it can also be used in all-words word sense disambiguation tasks.</p><p>2. Those in which we have a bigger and unique neural network that is able to manage all different words and assign a sense in the set of all existing sense in the dictionary used <ref type="bibr" target="#b20">(Raganato et al., 2017b)</ref>.</p><p>The advantage of the first branch of approaches is that in order to disambiguate a word, limiting our choice to one of its possible senses is computationally much easier than searching through all the senses of all words. To put things in perspective, the number of senses of each word in WordNet ranges from 1 to 59, whereas the total number of senses considering all words is 206 941.</p><p>The other approach however has an interesting property: all senses reside in the same vector space and hence share features in the hidden layers of the network. This allows the model to predict a common sense for two different words (i.e. synonyms), but it also offers the possibility to predict a sense for a word not present in the dictionary (e.g. neologism, spelling mistake...), and let the user or the underlying system to decide afterwards what to do with this prediction.</p><p>In practice, this ability of merging multiple sense tags together is especially useful when working with WordNet: indeed, this lexical database is based on the notion of synonym sets or "synsets", group of senses with the same meaning and definition. Disambiguating with synset tags instead of sense tags is a common practice <ref type="bibr" target="#b26">(Yuan et al., 2016;</ref><ref type="bibr" target="#b7">Le et al., 2018)</ref>, as it effectively decreases the output vocabulary of the classifier that considers all senses in WordNet from 206 941 to 117 659, and one can unambiguously retrieve the sense tag given a synset tag and the tagged word (because every sense of a word belong to a different synset).</p><p>In this work, we go further into this direction and we present a method based on the hy-pernymy and hyponymy relationships present in WordNet, in order to merge synset tags together and reduce even more the output vocabulary of such neural WSD systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sense Vocabulary Reduction</head><p>We can draw three issues of the current situation regarding supervised WSD systems:</p><p>1. The training of systems that directly predict a tag in the set of all WordNet senses becomes slower and take more memory the larger the output vocabulary is. This output layer size going up to 206 941 if we consider all word-senses, and 117 659 if we consider all synsets.</p><p>2. Due to the small number of manually sense annotated corpora available, a target word may never be observed during the training, and therefore the system would not be able to annotate it.</p><p>3. For the same reason, a word may have been observed, but not all of its senses. In this case the system is able to annotate the word, but if the expected sense has never been observed, the output will be wrong, regardless of the architecture of the supervised system.</p><p>In the SemCor <ref type="bibr" target="#b10">(Miller et al., 1993)</ref> for instance, the largest manually sense annotated corpus available, words are annotated with 33 760 different sense keys, which corresponds to approximately 16% of the sense inventory of WordNet 2 .</p><p>Grouping together multiple senses is hence a good way to overcome all these issues: by considering that multiple tags refer in fact to the same concept, the output vocabulary decreases, the ability of the trained system to generalize improves, and also its coverage.</p><p>Moreover, it reflects more accurately our intuition of what a sense is: clearly the notions of "tree" (with a trunk and leaves, not the mathematical graph) and "plant" (the living organism, not the industrial building) forms a group in our mind such that observing one sense in a context should help disambiguating the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From Senses to Synsets: A First Sense Vocabulary Reduction</head><p>WordNet is a lexical database organized in sets of synonyms called synsets. A synset is technically a group of one or more word-senses that have the same definition and consequently the same meaning. For instance, the first senses of "eye", "optic" and "oculus" all refer to a common synset which definition is "the organ of sight". Training a WSD supervised system to predict synset tags instead of word-sense tags is a common practice <ref type="bibr" target="#b26">(Yuan et al., 2016;</ref><ref type="bibr" target="#b7">Le et al., 2018)</ref>, and it can be seen as a form of output vocabulary reduction based on the knowledge that is present in WordNet.  <ref type="figure">Figure 1</ref>: Word-sense to synset vocabulary reduction applied on the first three senses of the words "help", "aid" and "assist" Illustrated in <ref type="figure">Figure 1</ref>, the word-sense to synset vocabulary reduction clearly helps to improve the coverage of supervised systems. Indeed, if the verb "help" is observed in the annotated data in its first sense, and consequently with the tag "v02553283", the context surrounding the target word can be used to later annotate the verb "assist" or "aid" with the same valid tag.</p><p>Once applied, the number of different labels needed to cover all senses of WordNet drops from 206 941 to 117 659 (approximately 43% of reduction), and considering the SemCor, the corpus contains 26 215 different synsets, which accounts now for 22% of this total. The vocab-ulary size was reduced and the coverage improved.</p><p>Going a little further, other information from WordNet can help the system to generalize. In the next section, we describe a new method for taking advantage of the hypernymy and hyponymy relationships in order to accomplish this same idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sense Vocabulary Reduction through Hypernymy and Hyponymy Relationships</head><p>According to <ref type="bibr" target="#b17">Polgu?re (2003)</ref>, hypernymy and hyponymy are two semantic relationships which correspond to a particular case of sense inclusion: the hyponym of a term is a specialization of this term, whereas its hypernym is a generalization. For instance, a "mouse" is a type of "rodent" which is in turn a type of "animal". In WordNet, these relationships bind nearly every nouns 3 together in a tree structure that goes from the generic root, the node "entity" to the most specific leaves, for instance the node "white-footed mouse". These relationships are also present on several verbs: so for instance "add" is a way of "compute" which in turn is a way of "reason" which is a way of "think".</p><p>For the sake of WSD, just like grouping together the senses of a same synset helps to better generalize, we hypothesize that grouping together the synsets of a same hypernymy relationship also helps in the same way.</p><p>The general idea of our method is that the most specialized concepts in WordNet are often superfluous in order to perform WSD.</p><p>Indeed, consider a small subset of Word-Net that only consists of the word "mouse", its first sense (the small rodent), its fourth sense (the electronic device), and all of their hypernyms. This is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We can see that every concept that is more specialized than the concepts "artifact" and "liv-ing_thing" could be removed, and we could map every tag of "mouse#1" to the tag of "liv-ing_thing#1" and we could still be able to disambiguate this word, but with a benefit: all other "living things" and animals in the sense annotated data could be tagged with the same sense, give examples of what is an animal and then show how to differentiate the small rodent to the hand-operated electronic device.</p><p>In order to achieve this goal of mapping every sense to its most generic sense still allowing to differentiate the meanings of the words, we have to consider certain difficulties that are not present with the word-sense to synset vocabulary reduction.</p><p>First, contrary to the synonymy relationship which is symmetric (i.e. if A is a synonym of B then B is a synonym of A), the hypernymy relationship is not. For instance, all mice are animals, but not all animals are mice.</p><p>In addition, two different senses of a word necessarily have two different synsets, but they may have the same direct hypernym, and they generally have the same inherited hypernym at a certain point. For instance, we can distinguish the sense 1 of "mouse" which is a type of "animal" from the sense 4 which is a type of "electronic device", but we cannot distinguish them if we go too far into the hypernymy hierarchy, because both of them are a type of "physical entity".</p><p>Finally, we could think of removing a synset from the vocabulary of WordNet because it is not useful locally (from the point of view of a specific word), but it could be necessary to diferentiate the meanings of another word.</p><p>Our method thus works in two steps:</p><p>1. We mark as "necessary" all synsets that are the lowest nodes of the hypernymy hierarchies of the senses of all word that can still allow to discriminate the different senses of the word.</p><p>2. We transform our sense vocabulary by mapping every synset to the lowest synset in its hypernymy hierarchy that is marked as "necessary".</p><p>The result of this method is that the most specific synsets of the tree that are not useful for discriminating are automatically removed from the vocabulary. In other words, the set of synsets that is left in the vocabulary is the smallest subset of all synsets that are necessary to distinguish every sense of every word of WordNet.</p><p>When applied on WordNet, the number of synset in the vocabulary now drops from 117 659 to 39 147 (approximately 66% of reduction), and applied on the SemCor, it now contains 12 779 different synsets, which counts for 32% of coverage. Again, the vocabulary size has drastically decreased, and the coverage really improved. Note that if we narrow down our computation to consider only polysemic words in WordNet, the full vocabulary of all reduced synsets of WordNet is 23 148, and the SemCor contains 9 461 of them represented, and that is a coverage of approximately 40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In order to evaluate our vocabulary reduction method, we applied it on a classification based neural network (subsection 2.2) capable of classifying a word in all possible synsets of WordNet. Our architecture is very similar to <ref type="bibr" target="#b20">Raganato et al. (2017b)</ref>'s BiLSTM model except for the input and output vocabulary used. Indeed, in their system, they have chosen to include their input vocabulary in their output vocabulary, so their network is able to predict both a sense tag when the target word has an entry in WordNet (nouns, verbs, adjectives and averbs), and a word tag for every other word <ref type="bibr">(pronouns, articles, etc.)</ref>. In our architecture, we chose to only predicts sense tags, in order to keep the output vocabulary the smallest possible.</p><p>Then we systematically trained two models:</p><p>1. A baseline model that predicts a tag belonging to all the synset tags seen during training (thus using the common wordsense to synset vocabulary reduction).</p><p>2. A second system trained under the same conditions, but with our vocabulary reduction through hypernyms algorithm applied on the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural Architecture</head><p>The architecture of our neural network relies on 3 layers :</p><p>1. The input layer, which takes directly the words in a vector form, from a pre-trained word embeddings model.</p><p>2. The hidden layer, composed of bidirectional LSTM units <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>3. The output layer, which represents for each word in input, a probability distribution over all senses in the output vocabulary used, thanks to a classical softmax function.</p><p>The cost function to minimize during the training is the cross entropy between the output layer and a one-hot vector, i.e. a vector for which all coordinates are set to 0 except for the coordinate at the index of the target sense which is 1. In consequence, the cost function is ? log q[s], where q[s] is the output of the network at the index s of the target sense.</p><p>Our model always predicts a sense in output, for every input word, even for words that do not convey directly a meaning (e.g. stopwords, articles, etc.) or words that were not annotated in the training set. However, we assign a special tag &lt;skip&gt; to these cases, allowing us to ignore the predictions made by the model and to not take it into account during the back-propagation step of the training.</p><p>This behavior is the main difference between our architecture and the one introduced by <ref type="bibr" target="#b20">(Raganato et al., 2017b)</ref>. In their model, the gradient is computed over all words of a sentence, and those that do not have a sense in WordNet are annotated with their surface form.</p><p>In input of our network, we used the GloVe vectors <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> pretrained on Wikipedia 2014 and Gigaword 5 4 . The dimension of the vectors is 300, the vocabulary size is 400 000 and all words are lowercased. These vectors are also used as input in the network described by <ref type="bibr" target="#b5">(K?geb?ck and Salomonsson, 2016)</ref>.</p><p>For the hidden layer of recurrent units, we chose LSTM cells of size 1000 for each direction <ref type="bibr">(so 2000 in total)</ref>. This is approximately the same size that was used in <ref type="bibr" target="#b20">(Raganato et al., 2017b</ref>) (1024 per direction) and <ref type="bibr" target="#b26">(Yuan et al., 2016</ref>) (a single layer of size 2048).</p><p>Finally, we applied the regularization method Dropout <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> between the hidden layer and the output layer, with a parameter set to 50%, in order to avoid overfitting during the training and to make the model more robust.</p><p>We implemented our neural network using PyTorch 5 , and our code is available at the following URL: https://github.com/getalp/disambiguate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We compared our sense vocabulary reduction method on two training sets: The first is the SemCor <ref type="bibr" target="#b10">(Miller et al., 1993)</ref>, the most popular corpus that is used for training most WSD supervised systems. The second is the concatenation of the SemCor and the WordNet Gloss Tagged 6 . The latter is a corpus distributed as part of WordNet since its version 3.0, and it consists of all the definitions (glosses) of every synset of WordNet, with every word manually or semi-automatically sense annotated. We used the version of these corpora given as part of the UFSAC 2.1 resource 7 <ref type="bibr" target="#b25">(Vial et al., 2018)</ref>, a set of gathered publicly available sense annotated corpora converted into a clean and unified format.</p><p>We performed every training for 20 epochs.  That is, the whole training set has been read 20 times. At the beginning of each epoch we shuffled the training set. We evaluated our model at the end of every epoch on a development set, and we kept only the one which obtained the best F1 WSD score. The development set was composed of 4 000 random sentences taken from the WordNet Gloss Tagged for the models trained on the SemCor, and 4 000 random sentences extracted from the training set for the other models. We trained with mini-batches of 100 sentences, truncated to 80 words, and padded with zero vectors from the end, and we used Adam (Kingma and Ba, 2014), with the same default parameters described in their article as the optimization method, except for the learning rate that we set to 0.0001 (10 times smaller than the default value).</p><p>All models have been trained on Nvidia's Titan X GPUs. The approximate training times of individual models, depending on the training corpus and if the vocabulary reduction method was applied, are displayed in the following </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Disambiguation</head><p>In order to disambiguate an input sequence of words using the trained model, we followed the following steps: First, each word of the sequence is lowercased and transformed into a vector using the pre-trained word embeddings model, then the sequence of vectors is given as input to our model.</p><p>Then, we annotate each word with the one among its possible senses which has the maximum probability. We first map each sense to its synset in the case of the baseline model, or map each sense to its reduced synset, in the case of the sense vocabulary reduced model, according to the method described in subsection 3.2, then we select the one which has the maximum value in the output layer of the model.</p><p>Finally, if no sense is assigned, because no instance of the word has been observed in the training data, a back-off is performed. We chose the most common one which is to assign the first sense in WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>We evaluated our models on all evaluation corpora commonly used in WSD, that is the WSD tasks of the evaluation campaigns Sen-sEval/SemEval. We used the fine-grained evaluation corpora from the evaluation framework of <ref type="bibr" target="#b19">Raganato et al. (2017a)</ref>, which consists of SensEval 2 <ref type="bibr" target="#b1">(Edmonds and Cotton, 2001)</ref>, SensEval 3 <ref type="bibr" target="#b21">(Snyder and Palmer, 2004)</ref>, <ref type="bibr">Se-mEval 2007</ref><ref type="bibr">task 17 (Pradhan et al., 2007</ref>, SemEval 2013 task 12 <ref type="bibr" target="#b13">(Navigli et al., 2013)</ref> and SemEval 2015 task 13 <ref type="bibr" target="#b11">(Moro and Navigli, 2015)</ref>, as well as their "ALL" corpus consisting of the concatenation of all previous ones. We also compared our result on the coarse-grained task 7 of SemEval 2007 <ref type="bibr" target="#b14">(Navigli et al., 2007)</ref> which is not present in the evaluation framework. We used the version of these corpora from the UFSAC 2.1 resource 8 , the sense inventory used for the sense annotations is Word-Net 3.0.   For each evaluation, we trained 20 separated models, and we give two scores: First, the mean of the F1 scores obtained by the models, along with its standard deviation. Then, the score obtained by an ensemble of the models. For the ensemble, we averaged the predictions of all individual models through a geometric mean, a common practice that is used for instance in machine translation <ref type="bibr" target="#b2">Gehring et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>The scores obtained by our systems using a single trained model compared to the state-ofthe-art systems <ref type="bibr" target="#b26">(Yuan et al., 2016;</ref><ref type="bibr" target="#b20">Raganato et al., 2017b;</ref><ref type="bibr" target="#b4">Iacobacci et al., 2016)</ref>, along with the first sense baseline are present in table 2. The scores obtained by our ensemble of models are given in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>In subsection 3.2, we showed that our vocabulary reduction method improves the coverage of supervised systems over all WordNet vocabulary. In <ref type="table" target="#tab_1">Table 1</ref>, we can see that this coverage improvement holds true on the evaluation tasks, for both training sets. On the total of 7 253 words to annotate for the corpus "ALL", the baseline system trained on the SemCor only is incapable of annotating 491 of them, and with the vocabulary reduction applied this number drops to 91. When adding the WordNet Gloss Tagged to the training set, this number is 126 for the baseline system, and with the vocabulary reduction, only 12 words cannot be annotated. Now if we look at the results in <ref type="table" target="#tab_4">Table 2</ref>, the difference of scores obtained by our system using the sense vocabulary reduction or not is overall not significant (regarding the "ALL" column). However we can notice a very large gap on the SemEval 2013 task, especially when the SemCor is used alone for training. This can be explained by the fact that this corpus is only composed of nouns, and our method for vocabulary reduction targets this part of speech principally. This is also the task where the coverage was improved the most by our method, as it can be seen in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>In comparison with the other works, our systems trained on the SemCor alone expose results comparable with the best system of <ref type="bibr" target="#b26">Yuan et al. (2016)</ref>, which is trained on the same corpus and augmented with a semi-supervised method. When we add the WordNet Gloss Tagged to the training data however, we obtain systematically state of the art results on all tasks except on SensEval 3. Once again, the sense reduction method does not consistently improves or decreases the score on every task, and in overall (task "ALL"), the result is roughly the same as without sense reduction applied.</p><p>Finally, in <ref type="table" target="#tab_5">Table 3</ref> we show the results of our system ensembling 20 models by averaging the output of their last layer. As we can see, ensembling is a very efficient method in WSD as it improves systematically all our results. Interestingly, with ensembles, the scores are significantly higher when applying the vocabulary reduction algorithm. One possible interpretation is that individual models might be more frequently "lost" in the sense that with the sense vocabulary reduction applied, a lot of words are annotated with the same tag, and it can make the trained model "unsure" about the decisions it make. Ensemble of models tends to prevent this problem by favoring the most probable decisions of the models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>or assistance" "improve the condition of" "act as an assistant"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sense reduction trough hypernymy hierarchy applied on the first and fourth sense of the word "mouse" (some nodes are missing for clarity)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Coverage of supervised systems based on the training corpus and if the vocabulary reduction algorithm is applied or not. Numbers are the percentage of words that are observed during training and hence can be annotated.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table :</head><label>:</label><figDesc></figDesc><table><row><cell>Model</cell><cell>SemCor</cell><cell>SemCor+WNGT</cell></row><row><cell>baseline</cell><cell>50mn</cell><cell>360mn (6h)</cell></row><row><cell>vocabulary reduced</cell><cell>30mn</cell><cell>160mn (2h40)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>F1 scores (%) obtained by our systems against the state of the art on the English WSD tasks of the evaluation campaigns SensEval 2 (SE2), SensEval 3 (SE3), SemEval 2007 (SE07) task 7 and 17, SemEval 2013 (SE13) task 12, SemEval 2015 (SE15) task 13 and the corpus composed of the concatenation of all previous ones (ALL) except SE07 task 7. Results in bold are the best results from using the sense vocabulary reduction or not. Results in red are to our knowledge the best results obtained on the task. Our results are the mean scores of 20 individual systems, with the standard deviation given in parenthesis. Results prefixed by a star (*) was obtained on the development corpus used during the training.</figDesc><table><row><cell>System (ensemble)</cell><cell>SE2</cell><cell>SE3</cell><cell>SE07 (17)</cell><cell>SE13 (12)</cell><cell>SE15 (13)</cell><cell>ALL</cell><cell>SE07 (07)</cell></row><row><cell>Train on SemCor, baseline</cell><cell>73.93</cell><cell>70.81</cell><cell>63.74</cell><cell>67.27</cell><cell>72.27</cell><cell>70.77</cell><cell>83.77</cell></row><row><cell>Train on SemCor, vocabulary reduced</cell><cell>73.05</cell><cell>70.59</cell><cell>61.32</cell><cell>71.23</cell><cell>71.60</cell><cell>71.18</cell><cell>84.65</cell></row><row><cell>Train on SemCor+WNGT, baseline</cell><cell>74.72</cell><cell>71.08</cell><cell>62.86</cell><cell>71.11</cell><cell>74.83</cell><cell>72.23</cell><cell>85.01</cell></row><row><cell>Train on SemCor+WNGT, vocabulary reduced</cell><cell>75.15</cell><cell>70.11</cell><cell>66.81</cell><cell>72.63</cell><cell>74.46</cell><cell>72.74</cell><cell>86.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>F1 scores (%) obtained by our system with an ensemble of 20 models trained separately. Results in bold are the best results from all our systems. Results in red are to our knowledge the best results obtained on the task.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://wordnet.princeton.edu/ documentation/wnstats7wn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Only 2% of the polysemous nouns of WordNet are not part of this hierarchy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://nlp.stanford.edu/projects/glove/ 5 https://pytorch.org 6 http://wordnetcode.princeton.edu/ glosstag-files/glosstag.shtml 7 https://github.com/getalp/UFSAC</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/getalp/UFSAC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a new method that improves the coverage and the capacity of generalization of most supervised systems, by narrowing down the number of different sense in WordNet in order to keep only the senses that are essential for differentiating the meaning of all words present in the lexical database.</p><p>By considering that a same sense tag may be applied to many different words, this method also captures in a intuitive way a better representation of what is a sense for the task of word sense disambiguation.</p><p>We implemented a state of the art neural network for WSD and we showed that this method really improves the overall results using the same training corpus, especially when making ensembles of models, and also especially when disambiguating nouns.</p><p>We trained two sets of systems, one relying on the SemCor alone, and one with the addition of the WordNet Gloss Tagged corpus, and in this last configuration we obtained our best results that significantly outperform the state of the art on most WSD tasks.</p><p>With the combination of our sense vocabulary reduction method through the hypernymy hierarchy of WordNet and the addition of the WordNet Gloss Tagged to the set of training corpora, the coverage of our supervised system is almost 100% on most WSD tasks, and so this provides a solid alternative to the automatic or semi-automatic creation of sense annotated corpora, and this nearly eliminates the need for a first sense backoff in WSD supervised systems.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nus-pt: Exploiting parallel texts for word sense disambiguation in the english all-words tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, SemEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Senseval-2: Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, SENSEVAL &apos;01</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<ptr target="Aus-tralia.PMLR" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Embeddings for word sense disambiguation: An evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="897" to="907" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word sense disambiguation using a bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>K?geb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Salomonsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Cognitive Aspects of the Lexicon (CogALex). Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A deep dive into word sense disambiguation with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Urbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="354" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using mrd: how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDOC &apos;86</title>
		<meeting>SIGDOC &apos;86<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">T</forename><surname>Bunker</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075671.1075742</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology, HLT &apos;93</title>
		<meeting>the workshop on Human Language Technology, HLT &apos;93<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 13: Multilingual all-words sense disambiguation and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SemEval-2013 Task 12: Multilingual Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 07: Coarsegrained english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2007</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traino-matic: Large-scale supervised word sense disambiguation in multiple languages without manual training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lexicologie et s?mantique lexicale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Polgu?re</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Les Presses de l&apos;Universit? de Montr?al</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 17: English lexical sample, srl and all words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, SemEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1167" to="1178" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</title>
		<meeting>SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One million sense-tagged instances for word sense disambiguation and induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="338" to="344" />
		</imprint>
	</monogr>
	<note>Beijing, China. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UFSAC: Unification of Sense Annotated Corpora and Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference (LREC)</title>
		<meeting><address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semisupervised word sense disambiguation with neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Altendorf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations, ACLDemos &apos;10</title>
		<meeting>the ACL 2010 System Demonstrations, ACLDemos &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
