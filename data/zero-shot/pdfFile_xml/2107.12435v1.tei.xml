<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pia</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>De Lange</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Halvorsen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
						</author>
						<title level="a" type="main">A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Colonoscopy</term>
					<term>polyp segmentation</term>
					<term>ResUNet++</term>
					<term>conditional random field</term>
					<term>test-time augmentation</term>
					<term>generalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colonoscopy is considered the gold standard for detection of colorectal cancer and its precursors. Existing examination methods are, however, hampered by high overall missrate, and many abnormalities are left undetected. Computer-Aided Diagnosis systems based on advanced machine learning algorithms are touted as a game-changer that can identify regions in the colon overlooked by the physicians during endoscopic examinations, and help detect and characterize lesions. In previous work, we have proposed the ResUNet++ architecture and demonstrated that it produces more efficient results compared with its counterparts U-Net and ResUNet. In this paper, we demonstrate that further improvements to the overall prediction performance of the ResUNet++ architecture can be achieved by using Conditional Random Field (CRF) and Test-Time Augmentation (TTA). We have performed extensive evaluations and validated the improvements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database, and CVC-VideoClinicDB. Moreover, we compare our proposed architecture and resulting model with other State-of-the-art methods. To explore the generalization capability of ResUNet++ on different publicly available polyp datasets, so that it could be used in a real-world setting, we performed an extensive cross-dataset evaluation. The experimental results show that applying CRF and TTA improves the performance on various polyp segmentation datasets both on the same dataset and cross-dataset. To check the model's performance on difficult to detect polyps, we selected, with the help of an expert gastroenterologist, 196 sessile or flat polyps that are less than ten millimeters in size. This additional data has been made available as a subset of Kvasir-SEG. Our approaches showed good results for flat or sessile and smaller polyps, which are known to be one of the major reasons for high polyp miss-rates. This is one of the significant strengths of our work and indicates that our methods should be investigated further for use in clinical practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Cancer is a primary health problem of contemporary society, with colorectal cancer (CRC) being the third most prevailing type in terms of cancer incidence and second in terms of mortality globally <ref type="bibr" target="#b1">[2]</ref>. Colorectal polyps are the precursors for the CRC. Early detection of polyps through high-quality colonoscopy and regular screening are cornerstones for the prevention of colorectal cancer <ref type="bibr" target="#b2">[3]</ref>, since adenomas can be found and resected before transforming to cancer and subsequently reducing CRC morbidity and mortality.</p><p>Regardless of the achievement of colonoscopy in lowering cancer burden, the estimated adenoma miss-rate is around 6-27% <ref type="bibr" target="#b4">[5]</ref>. In a recent pooled analysis of 8 randomized tandem colonoscopy studies, polyps smaller than 10 mm, sessile, and flat polyps <ref type="bibr" target="#b5">[6]</ref> are shown to most often be missed <ref type="bibr" target="#b6">[7]</ref>. Another reason why polyps are missed may be that the polyp either was not in the visual field or was not recognized despite being in the visual field due to fast withdrawal of the colonoscope <ref type="bibr" target="#b7">[8]</ref>. The adenoma miss-rate could be reduced by improving the quality of bowel preparation, applying optimal observation techniques, and ensuring a colonoscopy withdrawal time of at least six minutes <ref type="bibr" target="#b7">[8]</ref>. Moreover, adenoma detection rate can also be improved by using advanced techniques or devices, for example, auxiliary imaging devices, colonoscopes with increased field of view, add-on-devices, and colonoscopes with integrated inflatable, reusable balloon <ref type="bibr" target="#b2">[3]</ref>.</p><p>The structure and characteristics of a colorectal polyp changes over time at different development stages. Polyps have different shapes, sizes, colors, and appearances, which makes them challenging to analyze (see <ref type="figure" target="#fig_0">Figure 1</ref>). Moreover, there are challenges such as the presence of image artifacts like blurriness, surgical instruments, intestinal contents, flares, and low-quality images that can cause errors during segmentation.</p><p>Polyp segmentation is of crucial relevance in clinical applications to focus on the particular area of the potential lesion, extract detailed information, and possibly remove the polyp if necessary. A Computer-Aided Diagnosis (CADx) system for polyp segmentation can assist in monitoring and increasing the diagnostic ability by increasing the accuracy, precision, and reducing manual intervention. Moreover, it could lead to less segmentation errors than when conducted subjectively. Such systems could reduce doctor's workload and improve clinical workflow. Lumen segmentation helps clinicians navigate through the colon during screening, and it can be useful to establish a quality metric for the explored colon wall <ref type="bibr" target="#b8">[9]</ref>. Thus, an automated CADx system could be used as a supporting tool to reduce the miss-rate of the overlooked polyps.</p><p>A CADx system could be used in a clinical setting if it addresses two common challenges: (i) Robustness (i.e., the ability of the model to consistently perform well on both easy and challenging images), and (ii) Generalization (i.e., a model trained on specific intervention in a specific hospital should generalize across different hospitals) <ref type="bibr" target="#b9">[10]</ref>. Addressing these challenges is key to designing a powerful semantic segmentation system for medical images. Generalization capability checks the usefulness of the model across different available datasets coming from different hospitals and must finally be confirmed in multi-center randomized trials. A good generalizable model could be a significant step toward developing an acceptable clinical system. A cross-dataset evaluation is crucial to check the model on the unseen polyps from other sources and test the generalizability of it.</p><p>Toward developing a robust CADx system, we have previously proposed ResUNet++ <ref type="bibr" target="#b0">[1]</ref>: an initial encoder-decoder based deep-learning architecture for segmentation of medical images, which we trained, validated, and tested on the publicly available Kvasir-SEG <ref type="bibr" target="#b3">[4]</ref> and CVC-ClinicDB <ref type="bibr" target="#b10">[11]</ref> datasets. In this paper, we describe how the ResUNet++ architecture can be extended by applying Conditional Random Field (CRF) and Test-Time Augmentation (TTA) to further improve its prediction performance on segmented polyps. We have tested our approaches on six publicly available datasets, including both image datasets and video datasets. We have intentionally incorporated video datasets from colonoscopies to support the clinical significance. Usually, still-frames have at least one polyp sample. Videos have a situation where frames consist of both polyp and non-polyp. Therefore, we have tested the model on these video datasets and provided a new benchmark for the segmentation task. We have used extensive data augmentation to increase the training sample and used a comprehensive hyperparameter search to find optimal hyperparameters for the dataset. We have provided a more in-depth evaluation by including more evaluation metrics, and added justification for the ResUNet++, CRF, and TTA.</p><p>Additionally, we have performed extensive experiments on the cross-data evaluation, in-depth analysis of best performing and worst performing cases, and comparison of the proposed method with other recent works. Moreover, we have pointed out the necessity of solving tasks related to the miss-detection of flat and sessile polyps, and showed that our combining approach could detect the overlooked polyps with high effi-ciency, which could be of significant importance in the clinical settings. For this, we also released a dataset consisting sessile or flat polyps publicly. Furthermore, we have emphasized the use of cross-dataset evaluation by training and testing the model with images coming from various sources to achieve the generalizability goal.</p><p>In summary, the main contributions are as follows: 1) We have extended the ResUNet++ deep-learning architecture <ref type="bibr" target="#b0">[1]</ref> for automatic polyp segmentation with CRF and TTA to achieve better performance. The quantitative and qualitative results shows that applying CRF and TTA is effective. 2) We validate the extended architecture on a large range of datasets, i.e., Kvasir-SEG <ref type="bibr" target="#b3">[4]</ref>, CVC-ClinicDB <ref type="bibr" target="#b10">[11]</ref>, CVC-ColonDB <ref type="bibr" target="#b11">[12]</ref>, EITS-Larib <ref type="bibr" target="#b12">[13]</ref>, ASU-Mayo Clinic Colonoscopy Video Database <ref type="bibr" target="#b13">[14]</ref> and CVC-VideoClinicDB <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and we compare our proposed approaches with the recent State-of-the-art (SOTA) algorithm and set new a baseline. Moreover, we have compared our work with other recent works, which is often lacking in comparable studies. 3) We selected 196 flat or sessile polyps that are usually missed during colonoscopy examination <ref type="bibr" target="#b6">[7]</ref> from the Kvasir-SEG with the help of an expert gastroenterologist.</p><p>We have conducted experiments on this separate dataset to show how well our model performs on challenging polyps. Moreover, we release these polyp images and segmentation masks as a part of the Kvasir-SEG dataset so that researchers can build novel architectures and improve the results. 4) Our model has better detection of smaller and flat or sessile polyps, which are frequently missed during colonoscopy <ref type="bibr" target="#b6">[7]</ref>, which is a major strength compared to existing works. 5) In medical clinical practice, generalizable models are essential to target patient population. Our work is focused on generalizability, previously not much explored in the community. To promote generalizable Deep Learning (DL) models, we have trained our models on Kvasir-SEG and CVC-ClinicDB and tested and compared the results over five publicly available diverse unseen polyp dataset. Moreover, we have mixed two diverse datasets and conducted further experiments on other unseen datasets to show the behaviour of the model on the images captured using different devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Over the past decades, researchers have made several efforts at developing CADx prototypes for automated polyp segmentation. Most of the prior polyp segmentation approaches were based on analyzing either the polyp's edge or its texture. More recent approaches used Convolutional Neural Network (CNN) and pre-trained networks. Bernal et al. <ref type="bibr" target="#b10">[11]</ref> introduced a novel method for polyp localization that used WM-DOVA energy maps for accurately highlighting the polyps, irrespective of its type and size. Pozdeev et al. <ref type="bibr" target="#b16">[17]</ref> presented a fully automated polyp segmentation framework using pixel-wise prediction based upon the Fully Convolutional Network (FCN). Bernal et al. <ref type="bibr" target="#b17">[18]</ref> hosted the automatic polyp detection in colonoscopy videos sub-challenge, and later on, they presented a comparative validation of different methods for automatic polyp detection and concluded that the SOTA CNN based methods provide the most promising results.</p><p>Akbari et al. <ref type="bibr" target="#b18">[19]</ref> used the FCN-8S network and Otsu's thresholding method for automated colon polyp segmentation. Wang et al. <ref type="bibr" target="#b19">[20]</ref> used the SegNet <ref type="bibr" target="#b20">[21]</ref> architecture to detect polyps. They obtained high sensitivity, specificity, and receiver operating characteristic (ROC) curve value. Their algorithm could achieve a speed of 25 frames per second with some latency during real-time video analysis. Guo et al. <ref type="bibr" target="#b21">[22]</ref> used a Fully Convolutional Neural Network (FCNN) model for the Gastrointestinal Image ANAlysis (GIANA) polyp segmentation challenge. The proposed method won first place in the 2017 GIANA challenge for both standard definition (SD) and high definition image and won second place in the SD image segmentation task in the 2018 GIANA challenge. Yamada et al. <ref type="bibr" target="#b22">[23]</ref> developed a CADx support system that can be used for the real-time detection of polyps reducing the number of missed abnormalities during colonoscopy.</p><p>Poorneshwaran et al. <ref type="bibr" target="#b23">[24]</ref> used a Generative Adversarial Network (GAN) for polyp image segmentation. Kang et al. <ref type="bibr" target="#b24">[25]</ref> used Mask R-CNN, which relies on ResNet50 and ResNet101, as a backbone structure for automatic polyp detection and segmentation. Ali et al. <ref type="bibr" target="#b25">[26]</ref> presented various detection and segmentation methods that could classify, segment, and localize artifacts. Additionally, there are several recent really interesting studies on polyp segmentation <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>. They are useful steps toward building an automated polyp segmentation system. There are also some works which have hypothesized that coupling the existing architecture by applying careful post-processing technique could improve the model performance <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>From the presented related work, we observe that automatic CADx systems in the area of polyp segmentation are becoming mature. Researchers are conducting a variety of studies with different designs ranging from a retrospective study, prospective study, to post hoc examination of the prospectively obtained dataset. Some of the models achieve very high performance with smaller training and test datasets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The algorithms used for building the models are the ones that use handcrafted-, CNN-or pre-trained-features from ImageNet <ref type="bibr" target="#b32">[33]</ref>, where DL based algorithms are outperforming and gradually replacing the traditional handcrafted or machine learning (ML) approaches. Additionally, the performance of the models improves by the use of advance DL algorithms, especially designed for polyp segmentation task or any other similar biomedical image segmentation task. Moreover, there is interest for testing the proposed architectures with more than one dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>The main drawbacks in the field are the minimal effort applied towards testing the generalizability of the CADx system possible to achieve with the cross-dataset test. Additionally, there is almost no effort involved in designing an universal model that could accurately segment polyp coming from different sources, critical for the development of CADx for automated polyp segmentation. Besides, most of the current works have proposed algorithms that are tested on single, often small, imbalanced, and explicitly handpicked datasets. This renders conclusions regarding the performance of the algorithms almost useless (compared to other areas in ML like, for example, natural image classification or action recognition where the common practice is to test on more than one dataset and make source code and datasets publicly available). Additionally, the used datasets are often not public available (restricted and difficult to access), and the total number of images and videos used in the study are not sufficient to believe that the system is robust and generalizable for use in clinical trials. For instance, the model can produce output segmentation map with high sensitivity and precision on a particular dataset and completely fails on other modality images. Moreover, existing work often use small training and test datasets. These current limitations make it harder to develop a robust and generalizable systems.</p><p>Therefore, we aim to develop a CADx based support system that could achieve high performances irrespective of the datasets. To achieve the goal, we have done extensive experiments on various colonoscopy images and video datasets. Additionally, we have mixed the dataset from multiple centers and tested it on other diverse unseen datasets to achieve the goal of building a generalizable and robust CADx system that produces no segmentation errors. Moreover, we set a new benchmark for the publicly available datasets that can be improved in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE RESUNET++ ARCHITECTURE</head><p>ResUNet++ is a semantic segmentation deep neural network designed for medical image segmentation. The backbone for ResUNet++ architecture is ResUNet <ref type="bibr" target="#b33">[34]</ref>: an encoderdecoder network and based on U-Net <ref type="bibr" target="#b34">[35]</ref>. The proposed architecture takes the benefit of residual block, squeeze and excite block <ref type="bibr" target="#b35">[36]</ref>, atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b36">[37]</ref>, and attention block <ref type="bibr" target="#b37">[38]</ref>. What distinguishes ResUNet++ from ResUNet is the use of squeeze-and-excitation blocks (marked in dark gray) at the encoder, the ASPP block, (marked in the dark red) at bridge and decoder, and the attention block (marked in light green) at the decoder (see <ref type="figure">Figure 2</ref>). In the ResUNet++ model, we introduce the sequence of squeeze and excitation block to the encoder part of the network. Additionally, we replace the bridge of ResUNet with ASPP. In the decoder stage, we introduce a sequence of attention block, nearest-neighbor up-sampling, and concatenate it with the relevant feature map from the residual block of the encoder through skip connection. This process is followed by the residual unit with identity mapping, as shown in <ref type="figure">Figure 2</ref>.</p><p>We also introduce a series of additional skip connections from the residual unit of the encoder section to the attention block of the decoder section. We assign the number of filters <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>, along with the levels in the <ref type="bibr" target="#b0">[1]</ref> encoder section, which are the values in our ResUNet++ architecture. These filter combinations achieved the best results in our ResUNet++ experiment. In the decoder section, the number of the filter is reversed, and the sequence becomes [512, 256, 128, 64, 32]. As the semantic gap between the feature map of the encoder and decoder blocks are supposed to decrease, the number of filters in the convolution layers of the decoder block are also decreased to achieve better semantic coverage. Through this, we ensure that the overall quality of the feature maps is more alike to the ground truth mask. This is especially important as the loss in semantic space is likely to decrease, and therefore it will become more feasible to find a meaningful representation in semantic space. The overall ResUNet++ architecture consists of one stem block with three encoder blocks, an ASPP between the encoder and the decoder, and three decoder blocks. All the encoder and decoder blocks use the standard residual learning approach. Skip connections are introduced between encoder and decoder for the propagation of information. The output of the last decoder block is passed through the ASPP, followed by a 1 ? 1 convolution and a sigmoid activation function. All convolutional layers except for the output layer are batch normalized <ref type="bibr" target="#b38">[39]</ref> and are activated by a Rectified Linear Unit (ReLU) activation function <ref type="bibr" target="#b39">[40]</ref>. Finally, we get the output as binary segmentation maps. A brief explanation of each block is provided in the following sub-sections.</p><formula xml:id="formula_0">Sigmoid Conv2D (1x1) ASPP Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Addition Addition Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Batch Norm. &amp; ReLU Concatenate UpSampling Attention Squeeze &amp; Excite Addition Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Batch Norm. &amp; ReLU Concatenate UpSampling Attention Batch Norm. &amp; ReLU Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Addition Squeeze &amp; Excite Addition Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Batch Norm. &amp; ReLU Concatenate UpSampling Attention Batch Norm. &amp; ReLU Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Addition Squeeze &amp; Excite Batch Norm. &amp; ReLU Conv2D (3x3) Batch Norm. &amp; ReLU Conv2D (3x3) Addition ASPP INPUT OUTPUT ENCODING v DECODING BRIDGE Fig. 2. ResUNet++ architecture</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Residual Blocks</head><p>Training a deep neural network by expanding network depth can potentially improve overall performance. Nevertheless, simply stacking the CNN layer could also hamper the training process and cause exploding/vanishing gradient when backpropagation occurs <ref type="bibr" target="#b40">[41]</ref>. Residual connections facilitate the training process by directly routing the input information to the output and preserves the nobility of the gradient flow. The residual function simplifies the objective of optimization without any additional parameters and boosts the performance, which is the inspiration behind the deeper residual-based network <ref type="bibr" target="#b41">[42]</ref>. Equation (1) below shows the working principle.</p><formula xml:id="formula_1">y n = F (x n , W n ) + x n<label>(1)</label></formula><p>Here, x n is the input and F (?) is the residual function. The residual units consist of numerous combinations of Batch Normalization (BN), ReLU, and convolution layers. A detailed description of the combinations used and their impact can be found in the work of He et al. <ref type="bibr" target="#b42">[43]</ref>. We have employed the concept of a pre-activation residual unit in the ResUNet++ architecture from ResUNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Squeeze and Excitation block</head><p>The squeeze and excitation (SE) block is the building block for the CNN that re-calibrates channel-wise feature response by explicitly modeling interdependencies between the channels <ref type="bibr" target="#b35">[36]</ref>. The SE block learns the channel weights through global spatial information that increases the sensitivity of the effective feature maps, whereas it suppresses the irrelevant feature maps <ref type="bibr" target="#b0">[1]</ref>. The feature maps produced by the convolution have only access to the local information, meaning they have no access to the global information left by the local receptive field. To address this limitation, we perform a squeeze operation on the feature maps using the global average pooling to generate a global representation. We then use the global representation and perform sigmoid activation that helps us to learn a non-linear interaction between the channels, and capture the channel-wise dependencies. Here, the sigmoid activation output acts as a simple gating mechanism that ensures us to adaptively recalibrate the feature maps produced by the convolution. The adaptive recalibration or excitation operation explicitly models the interdependencies between the feature channels. The SE net has the capability of generalizing exceptionally well across various datasets <ref type="bibr" target="#b35">[36]</ref>. In the ResUNet++ architecture, we have stacked the SE block together with the residual block for improving the performance of the network, increasing the effective generalization across different medical datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Atrous Spatial Pyramidal Pooling</head><p>Since the introduction of Atrous convolution by Chen et al. <ref type="bibr" target="#b43">[44]</ref> to control the field-of-view to capture contextual information at multi-scale precisely, it has shown promising results for semantic image segmentation. Later, Chen et al. <ref type="bibr" target="#b44">[45]</ref> proposed ASPP, which is a parallel atrous convolution block to capture multiple-scale information simultaneously. ASPP captures the contextual information at different scales, and multiple parallel atrous convolutions with varying rates in the input feature map are fused <ref type="bibr" target="#b44">[45]</ref>. In ResUNet++, we use ASPP as a bridge between the encoder and the decoder sections, and after the final decoder block. We adopt ASPP in ResUNet++ to capture the useful multi-scale information between the encoder and the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention Units</head><p>Chen et al. <ref type="bibr" target="#b45">[46]</ref> proposed an attention model that can segment natural images by multi-scale input processing. Attention model is an improvement over average and max-pooling baseline and allows to visualize the features importance at different scales and positions <ref type="bibr" target="#b45">[46]</ref>. With the success of attention mechanisms, various medical image segmentation methods have integrated an attention mechanism into their architecture <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref>. The attention block gives importance to the subset of the network to highlight the most relevant information. We believe that the attention mechanism in our architecture will boost the effectiveness of the feature maps of the network by capturing the relevant semantic class and filtering out irrelevant information. Motivated by the recent achievement of attention mechanism in the field of medical image segmentation and computer vision in general, we have integrated an attention block at the decoder part of the ResUNet++ model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Conditional Random Field</head><p>Conditional Random Field (CRF) is a popular statistical modeling method used when the class labels for different inputs are not independent (e.g., image segmentation tasks). CRF can model useful geometric characteristics like shape, region connectivity, and contextual information <ref type="bibr" target="#b49">[50]</ref>. Therefore, the use of CRF can further improve the models capability to capture contextual information of the polyps and thus improve overall results. We have used CRF as a further step to produce more refined output to the test dataset for improving the segmentation results. we have used a dense CRF for our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Test Time Augmentation</head><p>Test-Time Augmentation (TTA) is a technique of performing reasonable modifications to the test dataset to improve the overall prediction performance. In TTA, augmentation is applied to each test image, and multiple augmented images are created. After that, we make predictions on these augmented images, and the average prediction of each augmented image is taken as the final output prediction. Inspired by the improvement of recent SOTA <ref type="bibr" target="#b21">[22]</ref>, we have used TTA in our work. In this paper, we utilize both horizontal and vertical flip for TTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We have used six different datasets of segmented polyps with ground truths in our experiments as shown in <ref type="table" target="#tab_0">Table I</ref>, i.e., Kvasir-SEG <ref type="bibr" target="#b3">[4]</ref>, CVC-ClinicDB <ref type="bibr" target="#b10">[11]</ref>, CVC-ColonDB <ref type="bibr" target="#b11">[12]</ref>, ETIS Larib Polyp DB <ref type="bibr" target="#b12">[13]</ref>, CVC-VideoClinicDB <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and ASU-Mayo Clinic dataset <ref type="bibr" target="#b13">[14]</ref>. They vary e.g., regarding number of images, image resolution, availability, devices used  for capturing and the accuracy of the segmentation masks. One example is given from the Kvasir-SEG in <ref type="figure" target="#fig_1">Figure 3</ref>. The Kvasir-SEG dataset includes 196 polyps smaller than 10 mm classified as Paris class 1 sessile or Paris class IIa. We have released this dataset seperately as subset of Kvasir-SEG. Note that for CVC-VideoClinicDB, we have only used the data from the CVC-VideoClinicDBtrainvalid folder since only these data have ground truth masks. Moreover, the ASU-Mayo Clinic dataset, which was made available at the "Automatic Polyp Detection in Colonoscopy Videos" sub-challenge at Endovis 2015 had ten normal videos (negative shots) and ten videos with polyps. However, the test subset is not available because of issues related to licensing. In our experiment, while training, validating, testing with 80:10:10 split on the ASU-Mayo, we used all 20 videos for experimentation. However, for the crossdataset test (i.e., Tables X and XI), we only tested on ten positive polyp videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Method</head><p>To evaluate polyp segmentation methods, where individual pixels should be identified and marked, we use metrics used in earlier research <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b50">[51]</ref> and in competitions like GIANA 1 , comparing the correctly and wrongly identified pixels of findings. The Dice coefficient (DSC) and the Intersection over Union (IoU) are the most commonly used metrics. We use the DSC to compare the similarity between the produced segmentation results and the original ground truth. Similarly, the IoU is used to compare the overlap between the output mask and original ground truth mask of the polyp. The mean Intersection over Union (mIoU) calculates IoU of each semantic class of the image and compute the mean over all the classes. There is a correlation between DSC and mIoU. However, we calculate both the metrics to provide a comprehensive results analysis that could lead to better understanding of the results.</p><p>Moreover, other often-used metrics for the binary classification are recall (true positive rate) and precision (positive predictive value). For the polyp segmentation, precision is the ratio of the number of correctly segmented pixels versus the total number of all the pixels. Similarly, recall is the ratio of correctly segmented pixel versus the total number of pixels present in the ground truth. In the polyp image segmentation, precision and recall are used to indicate over-segmentation and under-segmentation. For formal definitions and formulas, see the definitions in for example <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Finally, the receiver operating characteristic (ROC) curve analysis is also an important metric to characterize the performance of the binary classification system. In our study, we therefore calculate DSC, mIoU, recall, precision, and ROC when evaluating the segmentation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Augmentation</head><p>Data augmentation is a crucial step in increasing the number of polyp samples. This solves the data insufficiency problem, improves the performance of the model, and help to reduce over-fitting. We have used a large number of different data augmentation techniques to increase the training sample. We divide all the polyp datasets into training, validation, and testing sets using the ratio of 80:10:10 based on the random distribution except for the mixed datasets. After splitting the dataset, we apply data augmentation techniques such as center crop, random rotation, transpose, elastic transform, grid distortion, optical distortion, vertical flip, horizontal flip, grayscale, random brightness, random contrast, hue saturation value, RBG shift, course dropout, and different types of blur. For cropping the images, we have used a crop size of 256?256 pixels. For the experiment, we have resized the complete training, validation, and testing dataset to 256?256 pixels to reduce the computational complexity. We have only augmented the training dataset. The validation data is not augmented, and the test datasets were augmented while evaluation using TTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation and Hardware Details</head><p>We have implemented all the models using the Keras framework <ref type="bibr" target="#b51">[52]</ref> with Tensorflow <ref type="bibr" target="#b52">[53]</ref> as a backend. Source code of our implementation and information about our experimental setup are made publicly available on Github 2 . Our experiments were performed using a Volta 100 Tensor Core GPU on a Nvidia DGX-2 AI system capable of 2-petaFLOPS tensor performance. We used a Ubuntu 18.04.3LTS operating system with Cuda 10.1.243 version installed. We have performed different experiments with different sets of hyperparameters manually on the same dataset in order to select the optimal set of hyperparameters for the ResUNet++. Our model performed well with the batch size of 16, Nadam as an optimizer, binary cross-entropy as the loss function, and learning rate of 1e?5. The dice loss function was also competitive. These hyperparameters were chosen based on the empirical evaluation. All 2 https://github.com/DebeshJha/ResUNet-with-CRF-and-TTA the models were trained for 300 epochs. We have used early stopping to prevent the model from over-fitting. To further improve the results, we have used stochastic gradient descent with warm restarts (SGDR). All the hyperparameters were same except the learning rate, which was adjusted based on the requirement. We have also included the Tensorboard for the analysis and visualization of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In our previous work, we have showed that ResUNet++ outperforms the SOTA UNet <ref type="bibr" target="#b34">[35]</ref> and ResUNet <ref type="bibr" target="#b33">[34]</ref> models trained on Kvasir-SEG and CVC-ClinicDB dataset <ref type="bibr" target="#b0">[1]</ref>. In this work, we aim to improve the results of ResUNet++ by utilizing further hyperparameter optimization, CRF and TTA. In this section, we present and compare the results of ResUNet++ with CRF, TTA, and both approaches combined on the same dataset, mixed dataset, and cross-dataset. Although a direct comparison of approaches from the literature is difficult due to different testing mechanisms used by various authors, we nonetheless compare the results with the recent work for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results comparison on Kvasir-SEG dataset</head><p>Table II and <ref type="figure">Figure 4</ref> show the quantitative and qualitative results comparison. <ref type="figure">Figure 7</ref> shows the ROC curve for all the models. As seen in the quantitative results <ref type="table" target="#tab_0">(Table II)</ref>, qualitative results <ref type="figure">(Figure 4</ref>), and ROC curve <ref type="figure">(Figure 7)</ref>, our proposed methods outperform ResUNet++ on the Kvasir-SEG dataset. The improvement in results demonstrates the advantage of the use of the TTA, CRF and their combinations. <ref type="figure">Fig. 4</ref>. Qualitative results comparison of the proposed models with UNet, ResUNet, and ResUNet++. The figure shows the example of polyps that are usually missed-out during colonoscopy examination. We see that there is a high similarity between ground truth and predicted mask for the proposed models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results comparison on CVC-ClinicDB</head><p>CVC-ClinicDB is a commonly used dataset for polyp segmentation. Therefore, it becomes important that we bring different work from the literature together and compare the proposed algorithms with the existing works. We compare our algorithms with the SOTA algorithms. <ref type="table" target="#tab_0">Table III</ref> demonstrates that the combination of ResUNet++ and CRF achieves DSC of 0.9293 and mIoU of 0.8898, which is 2.23% improvement on PraNet <ref type="bibr" target="#b56">[57]</ref> in DSC and 4.98% improvement in mIoU, respectively, and the proposed methods shows the SOTA result on CVC-ClinicDB.</p><p>The ROC curve measures the performance for the classification problem provided a set threshold. We have set the probability threshold of 0.5. The combination of ResUNet++  <ref type="figure">Figure 8</ref>. Therefore, the results in <ref type="table" target="#tab_0">Table III</ref> and <ref type="figure">Figure 8</ref> show that applying TTA gives an improvement on CVC-ClinicDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results comparison on CVC-ColonDB dataset</head><p>Our results using the CVC-ColonDB dataset are presented in <ref type="table" target="#tab_0">Table IV</ref>. The table shows that proposed method of combining ResUNet++ and TTA achieved the highest DSC of 0.8474, which is 3.74% higher than SOTA <ref type="bibr" target="#b18">[19]</ref>, and mIoU of 0.8466 which is 20.66% higher than <ref type="bibr" target="#b56">[57]</ref>. The recall and precision of all three proposed methods are quite acceptable. When compared with ResUNet++, there is an improvement of     directly with ResUNet++ as it already showed superior performance on Kvasir-SEG and CVC-ClinicDB <ref type="bibr" target="#b0">[1]</ref>. Here, there are only marginal differences in the results of ResUNet++, "Re-sUNet++ + CRF", "ResUNet++ + TTA", and "ResUNet++ + CRF + TTA". However, ResUNet++ achieves maximum DSC of 0.6364, which is 0.84% improvement over SOTA <ref type="bibr" target="#b56">[57]</ref> and mIoU of 0.7534 which is 18.64% improvement over <ref type="bibr" target="#b56">[57]</ref>. The recall of ResUNet++ is 0.6346, which is slightly higher than the proposed methods. However, the precision of combining ResUNet++ and TTA is higher as compared to ResUNet++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results comparison on ETIS-Larib Polyp DB</head><p>From the results, we can say that the performance of architecture is data specific. Our proposed methods outperformed SOTA over five independent datasets, however, ResUNet++ shows better results than the combinational approaches on ETIS-Larib dataset. Still, the precision of combining Re-sUNet++ and TTA is slightly higher than ResUNet++. It is to be noted that ETIS-Larib contains only 196 images, out of which only 156 images are used for training. Even with the small training dataset, the models are performing satisfactory as compared to the SOTA <ref type="bibr" target="#b56">[57]</ref> with significant margin in mIoU, which can be considered as the strength of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on Kvasir-Sessile</head><p>As this is the first work on Kvasir-Sessile, we have compared the proposed methods with ResUNet++. <ref type="table" target="#tab_0">Table VI</ref> shows that combining ResUNet++ and TTA gives the DSC of 0.5042, mIoU of 0.6606 which can be considered a decent score on a smaller size dataset. The dataset contains small, diverse images, which are difficult to generalize with very few training samples. <ref type="table" target="#tab_0">Table VII</ref> shows the results of the proposed models on the CVC-VideoClinicDB. From the results, we can see that all models perform well on the dataset despite the fact that masks are not pixel perfect. One of the reasons for high performance is the presence of 11, 954 polyps and normal video frames that was used in training and testing. The combination of ResUNet++ and CRF obtained a DSC of 0.8811, mIoU of 0.8739, recall of 0.7743, and precision of 0.6706 which is quite acceptable for the segmentation task with this type of dataset. In CVC-VideoClinicDB, the ground-truth is marked with a oval or circle shape. However, it is understandable that pixel-precise annotations of this dataset will need great manual effort from expert endoscopists and engineers. <ref type="table" target="#tab_0">Table VIII</ref> shows the results of the proposed models on the ASU-Mayo ClinicDB. ASU-Mayo contains 18,781 frames, both polyp and non-polyp images. The combination of Re-sUNet++ and CRF obtained a DSC of 0.8850 and mIoU of 0.8635. As in the real clinical settings, the models trained on this type of dataset are more meaningful (as it contains both polyp and non-polyp frames). The capability to achieve good performance for these more challenging datasets is one of the strengths of the proposed method. This is supported by the fact that this dataset also contains a sufficient amount of images to enable sufficient training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results comparison on CVC-VideoClinicDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Results comparison on AUS-Mayo ClinicDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Results comparison on mixed dataset</head><p>To check the performance of the proposed approaches on the images captured using different devices, we have mixed the Kvasir-SEG and CVC-ClinicDB and used them for training. The model were tested on CVC-ColonDB and CVC-VideoClinicDB. <ref type="table" target="#tab_0">Table IX</ref> shows the result of the mixed dataset on both datasets. The combination of ResUNet++ and TTA obtains a DSC of 0.5084 and mIoU of 0.6859 with CVC-ColonDB. The combination of ResUNet++, CRF, and TTA obtained a DSC of 0.3603 and mIoU of 0.6468 with CVC-VideoClinicDB.</p><p>From the table, we can see that the combination of Re-sUNet++, CRF, and TTA performs better or very competitive in both still images and video frames. Here, it is also evident that the model trained on the smaller dataset (Kvasir-SEG and CVC-ClinicDB) which do not include non-polyp images is not performing well on larger and diverse datasets (CVC-VideoClinicDB) that contain both polyp and non-polyp frames. Additionally, for the CVC-VideoClinicDB datasets, the provided ground truth is not perfect (oval/circle) shaped. As the model trained on Kvasir-SEG and CVC-ClinicDB have perfect annotations, the model is good at predicting a perfect shaped mask. When we make predictions on the CVC-VideoClinicDB with imperfect masks, even if the predictions are good, the scores may not be high because of the difference in the provided ground truth and the predicted masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Cross-dataset result evaluation on Kvasir-SEG</head><p>For the cross-dataset evaluation, we trained the models on the Kvasir-SEG dataset and tested it on the other five independent datasets. <ref type="table" target="#tab_7">Table X</ref> shows the results of crossdata generalizability of ResUNet++ alone, and with the CRF and TTA techniques. The results of the models trained on Kvasir-SEG produces an average best mIoU of 0.6817 and an average best DSC of 0.4779 for both image and video datasets. From the above table, we can see that the proposed combinational approaches are performing competitive. For the image datasets, the combination of ResUNet++ and TTA is performing better, and for the video datasets, the combination of ResUNet++, CRF, and TTA is performing best. It is to be noted that we are training a model with 1000 Kvasir-SEG pixel segmented polyps and testing on (for example, 11,954 frames) oval-shaped polyp ground truth. Here, even if the predictions are correct, the evaluation scores will not be good because of the oval/circle shaped ground truth. Moreover, the datasets such as ASU-Mayo and CVC-VideoClinicDB are heavily imbalanced, but the model trained on Kvasir-SEG contains at least one polyp. This may also have caused the poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Cross-dataset evaluation on CVC-ClinicDB</head><p>To further test generaliziblity, we trained the models on CVC-CliniDB and tested it across five independent, diverse image and video datasets. Tables XI shows the results of cross-data generalizability. Like the previous test on Kvasir-SEG, the results follow the same pattern with the combination of ResUNet++ and TTA outperforming others on the image datasets and the combination of ResUNet++, CRF, and TTA outperforming its competitors on video datasets. ResUNet++ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Result summary</head><p>In summary, from all obtained results (i.e., qualitative, quantitative, and ROC curve), the following main observations can be drawn: (i) the proposed ResUNet++ is capable of segmenting the smaller, larger and regular polyps; (ii) the combination of ResUNet++ with CRF achieves the best performance in terms of DSC, mIoU, recall and precision when trained and tested on the same dataset (see <ref type="table" target="#tab_0">Table III, Table  VII, and Table VIII)</ref> whereas it remains competitive when tested on other datasets; (iii) the combination of ResUNet++ and TTA and the combination of ResUNet++, CRF and TTA performs similar for the mixed datasets; (iv) the combination of ResUNet++ and TTA outperforms others on still images; (v) the combination of ResUNet++, CRF and TTA shows improvement on all the video datasets compared to ResUNet++; (vi) all the models perform better when the images have higher contrast; (vii) ResUNet++ is particularly good at segmenting smaller and flat or sessile polyps, which is a prerequisite for developing an ideal CADx polyp detection system <ref type="bibr" target="#b0">[1]</ref>; (viii) ResUNet++ fails especially on the images that contains overexposed regions termed as saturation or contrast (see <ref type="figure" target="#fig_3">Figure 6</ref>); (ix) ResUNet and ResUNet-mod particularly showed oversegmented or under-segmented results, (see <ref type="figure">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. General Performance</head><p>The tables and figures suggest that applying CRF and TTA improved the performance of ResUNet++ on the same datasets, mixed datasets and cross-datasets. Specifically, the The total number of trainable parameters increases by increasing the number of blocks in the networks (see <ref type="table" target="#tab_0">Table XII</ref>). However, in ResUNet++, there is significant performance gain that compensates for the training time, and our model requires fewer parameters if we compare with the models that use pretrained encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross Dataset Performance</head><p>The cross-data test is an excellent technique to determine the generalizing capability of a model. The presented work is an initiative towards improving the generalizability of segmentation methods. Our contribution towards generalizability is to train on one dataset and test on several other public datasets that may come from different centers and use different scope manufacturers. Thus, we believe that to tackle this issue, out-of-sample multicenter data must be used to test the built methods. The work is a step forward in raising an issue regarding method interpretability and we also raise questions about generalizability and domain adaptation of supervised methods in general.</p><p>From the results analyses, we can see that different proposed algorithms perform well with different types of datasets. For instance, CRF outperformed others on tables III, VII, and VIII. TTA showed improvement on tables IV, IX, X and XI. CRF performs better than TTA while trained and tested on video datasets (see tables VII and VIII). CRF also outperformed TTA on most of the images dataset. However, TTA still remains competitive. On the mixed dataset and the cross-dataset test, TTA performs better than CRF on all the datasets. On the mixed datasets and on the cross-dataset test on videos, the combination of ResUNet++, CRF, and TTA remains the best choice (see tables IX, X, and XI). There is a performance improvement over ResUNet++ while combining CRF, TTA, and the combination of CRF and TTA.</p><p>However, there is no significant performance improvement of any methods on the others. From the results, we can see that the results are typically data-dependent. However, as the proposed methods perform well on video frames, it may work better in the clinic, as the output from a colonoscope is a video stream. Thus, it becomes critical to show the results with all three approaches on each dataset. Therefore, we provide extensive experiments showing both success <ref type="figure">(Figure 4</ref>, <ref type="figure" target="#fig_2">Figure 5</ref>) and failure cases ( <ref type="figure" target="#fig_3">Figure 6</ref>) and present the overall analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Challenges</head><p>There are several challenges associated with segmenting polyps, such as bowel-quality preparation during colonoscopy, angle of the cameras, superfluous information, and varying morphology, which can affect the overall performance of a DL model. For some of the images, there even exists variation in the decision between endoscopists. While ResUNet++ with CRF and TTA also struggle with producing satisfactory segmentation maps for these images, it performs considerably better than our previous model and also outperforms another SOTA algorithm.</p><p>The quality of a colonoscopy examination is largely determined by the experience and skill of the endoscopist <ref type="bibr" target="#b22">[23]</ref>. Our proposed model can help in two ways: (i) it can be used to segment a detected polyp, providing an extra pair of eyes to the endoscopist; and (ii) it performs well on both flat and small polyps, which are often missed during endoscopic examinations. The qualitative analysis (see <ref type="figure">Figure 4</ref>) and the quantitative analyses from the above tables and figures support this argument. This is a major strength of our work and makes it a candidate for clinical testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Possible Limitations</head><p>Possible limitations of this work are that it is a retrospective study. Prospective clinical evaluation is essential because data analyzed with the retrospective study is the different prospective study (for example, the case of missing data that should be considered on the basis of best-case and worse case scenarios) <ref type="bibr" target="#b59">[60]</ref>. Also, all data in these experiments are curated, while a prospective clinical trial would mean testing on full colonscopy videos. During model training, we have resized all the images to 256 ? 256 to reduce the complexity, which costs in loss of information, and can affect the overall performance. We have worked on optimizing the code, but further optimization may exist, that can potentially improve the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have presented the ResUNet++ architecture for semantic polyp segmentation. We took inspiration from the residual block, ASPP, and attention block to design the novel ResUNet++ architecture. Furthermore, we applied CRF and TTA to improve the results even more. We have trained and validated the combination of ResUNet++ with CRF and TTA using six publicly available datasets, and analyzed and compared the results with the SOTA algorithm on specific datasets. Moreover, we analyzed the cross-data generalizability of the proposed model towards developing generalizable semantic segmentation models for automatic polyp segmentation. A comprehensive evaluation of the proposed model trained and tested on six different datasets showed good performance of the (ResUNet++ and CRF) on image datasets and (ResUNet++ and TTA), (ResUNet++, CRF, and TTA) model for the mixed datasets and cross-datasets. Further, a detailed study on crossdataset generalizability of the models trained on Kvasir-SEG and CVC-ClinicDB and tested on five independent datasets, confirmed the robustness of the proposed ResUNet++ + TTA method for cross-dataset evaluation.</p><p>The strength of our method is that we successfully detected smaller and flat polyps, which are usually missed during colonoscopy examination <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b60">[61]</ref>. Our model can also detect the polyps that would be difficult for the endoscopists to identify without careful investigations. Therefore, we believe that the ResUNet++ architecture, along with the additional CRF and TTA steps, could be one of the potential areas to investigate, especially for the overlooked polyps. We also point out that the lack of generalization issues of the models, which is evidenced by the unsatisfactory result for crossdataset evaluation in most of the cases. In the future, our CADx system should also be investigated on other bowel conditions. Moreover, a prospective trial should also be conducted with image and video datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example images showing the variations in shape, size, color, and appearance of polyps from the Kvasir-SEG<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Example polyp and corresponding ground truth from the Kvasir-SEG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Result of model trained on CVC-ClinicDB and tested on Kvasir-SEG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Example images where the proposed models fails on Kvasir-SEG and TTA has the maximum Area Under Curve -Receiver Operating Characteristic (AUC-ROC) of 0.9814, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>ROC curve of proposed models on the Kvasir-SEG ROC curve for all the models trained and tested on CVC-ClinicDB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ACKNOWLEDGEMENT</head><label></label><figDesc>This work is funded in part by Research Council of Norway project number 263248. Experiments are performed on the Experimental Infrastructure for Exploration of Exascale Computing (eX3), supported by the Research Council of Norway under contract 270053.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>BIOMEDICAL SEGMENTATION DATASETS USED IN OUR EXPERIMENTS</figDesc><table><row><cell>Dataset</cell><cell>Images</cell><cell>Input size</cell><cell>Availability</cell></row><row><cell>Kvasir-SEG [4]</cell><cell>1000</cell><cell>Variable</cell><cell>Public</cell></row><row><cell>CVC-ClinicDB [11]</cell><cell>612</cell><cell>384 ? 288</cell><cell>Public</cell></row><row><cell>CVC-ColonDB [12]</cell><cell>380</cell><cell>574 ? 500</cell><cell>Public</cell></row><row><cell>ETIS Larib Polyp DB [13]</cell><cell>196</cell><cell>1225 ? 966</cell><cell>Public</cell></row><row><cell>CVC-VideoClinicDB [15], [16]  ?</cell><cell>11,954</cell><cell>384 ? 288</cell><cell>Public</cell></row><row><cell>ASU-Mayo Clinic dataset [14]  ?</cell><cell>18,781</cell><cell>688 ? 550</cell><cell>Copyrighted</cell></row><row><cell>Kvasir-Sessile ?</cell><cell>196</cell><cell>Variable</cell><cell>Public</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Ground truth for test data not available Ground truth oval or circle shaped? Part of Kvasir-SEG [4], only sessile polyps</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">COMPARISON ON KVASIR-SEG</cell><cell></cell></row><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>UNet [35]</cell><cell>0.7147</cell><cell>0.4334</cell><cell>0.6306</cell><cell>0.9222</cell></row><row><cell>ResUNet [34]</cell><cell>0.5144</cell><cell>0.4364</cell><cell>0.5041</cell><cell>0.7292</cell></row><row><cell>ResUNet-mod [34]</cell><cell>0.7909</cell><cell>0.4287</cell><cell>0.6909</cell><cell>0.8713</cell></row><row><cell>ResUNet++ [1]</cell><cell>0.8119</cell><cell>0.8068</cell><cell>0.8578</cell><cell>0.7742</cell></row><row><cell>ResUNet++ + CRF</cell><cell>0.8129</cell><cell>0.8080</cell><cell>0.8574</cell><cell>0.7775</cell></row><row><cell>ResUNet++ TTA</cell><cell>0.8496</cell><cell>0.8318</cell><cell>0.8760</cell><cell>0.8203</cell></row><row><cell>ResUNet++ +TTA + CRF</cell><cell>0.8508</cell><cell>0.8329</cell><cell>0.8756</cell><cell>0.8228</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="4">RESULTS COMPARISON ON CVC-CLINICDB</cell><cell></cell></row><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>MultiResUNet [31]</cell><cell>-</cell><cell>0.8497</cell><cell>-</cell><cell>-</cell></row><row><cell>cGAN  ? [24]</cell><cell>0.8848</cell><cell>0.8127</cell><cell>-</cell><cell>-</cell></row><row><cell>SegNet [20]</cell><cell>-</cell><cell>-</cell><cell>0.8824</cell><cell>-</cell></row><row><cell>FCN ? [54]</cell><cell>-</cell><cell>-</cell><cell>0.7732</cell><cell>0.8999</cell></row><row><cell>CNN [55]</cell><cell>(0.62-0.87)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MSPB ? CNN [56]</cell><cell>0.8130</cell><cell>-</cell><cell>0.7860</cell><cell>0.8090</cell></row><row><cell>UNet [35]</cell><cell>0.6419</cell><cell>0.4711</cell><cell>0.6756</cell><cell>0.6868</cell></row><row><cell>ResUNet [34]</cell><cell>0.4510</cell><cell>0.4570</cell><cell>0.5775</cell><cell>0.5614</cell></row><row><cell>PraNet [57]</cell><cell>0.8980</cell><cell>0.8400</cell><cell>-</cell><cell>-</cell></row><row><cell>ResUNet-mod [34]</cell><cell>0.7788</cell><cell>0.4545</cell><cell>0.6683</cell><cell>0.8877</cell></row><row><cell>ResUNet++ [1]</cell><cell>0.9199</cell><cell>0.8892</cell><cell>0.9391</cell><cell>0.8445</cell></row><row><cell>ResUNet++ + CRF</cell><cell>0.9203</cell><cell>0.8898</cell><cell>0.9393</cell><cell>0.8459</cell></row><row><cell>ResUNet++ + TTA</cell><cell>0.9020</cell><cell>0.8826</cell><cell>0.9065</cell><cell>0.8539</cell></row><row><cell>ResUNet++ + TTA + CRF</cell><cell>0.9017</cell><cell>0.8828</cell><cell>0.9060</cell><cell>0.8549</cell></row><row><cell cols="2">? Conditional generative adversarial network</cell><cell cols="3">Data augmentation</cell></row></table><note>? Fully convolutional network ? multi-scale patch-based</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV RESULTS</head><label>IV</label><figDesc></figDesc><table><row><cell cols="4">COMPARISON ON CVC-COLONDB</cell><cell></cell></row><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>FCN-8S + Otsu [19]</cell><cell>0.8100</cell><cell>-</cell><cell>0.7480</cell><cell>-</cell></row><row><cell>FCN-8s + Texton [58]</cell><cell>0.7014</cell><cell>-</cell><cell>0.7566</cell><cell>-</cell></row><row><cell>SA-DOVA Descriptor [12]</cell><cell>0.5533</cell><cell>-</cell><cell>0.6191</cell><cell>-</cell></row><row><cell>PraNet [57]</cell><cell>0.7090</cell><cell>0.6400</cell><cell>-</cell><cell>-</cell></row><row><cell>ResUNet++ [1]</cell><cell>0.8469</cell><cell>0.8456</cell><cell>0.8511</cell><cell>0.8003</cell></row><row><cell>ResUNet++ + CRF</cell><cell>0.8458</cell><cell>0.8456</cell><cell>0.8497</cell><cell>0.7767</cell></row><row><cell>ResUNet++ + TTA</cell><cell>0.8474</cell><cell>0.8466</cell><cell>0.8434</cell><cell>0.8118</cell></row><row><cell>ResUNet++ + TTA + CRF</cell><cell>0.8452</cell><cell>0.8459</cell><cell>0.8411</cell><cell>0.8125</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V RESULTS</head><label>V</label><figDesc>ON ETIS-LARIB POLYP DB</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>PraNet [57]</cell><cell>0.6280</cell><cell>0.5670</cell><cell>-</cell><cell>-</cell></row><row><cell>ResUNet++ [1]</cell><cell>0.6364</cell><cell>0.7534</cell><cell>0.6346</cell><cell>0.6467</cell></row><row><cell>ResUNet++ + CRF</cell><cell>0.6228</cell><cell>0.7520</cell><cell>0.6242</cell><cell>0.5648</cell></row><row><cell>ResUNet++ + TTA</cell><cell>0.6136</cell><cell>0.7458</cell><cell>0.5996</cell><cell>0.6565</cell></row><row><cell>ResUNet++ + TTA + CRF</cell><cell>0.6018</cell><cell>0.7426</cell><cell>0.5914</cell><cell>0.5755</cell></row><row><cell cols="5">1.22% in precision. There are negligible differences in recall,</cell></row><row><cell cols="5">with ResUNet++ slightly outperforming the others.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table Vshows the results of the proposed models on the ETIS-Larib Polyp DB. In this case, we do not compare the results with UNet and ResUNet, but compare the models</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI RESULTS</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">ON KVASIR-SESSILE</cell><cell></cell></row><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>ResUNet++ [1]</cell><cell>0.4600</cell><cell>0.64086</cell><cell>0.4382</cell><cell>0.5838</cell></row><row><cell>ResUNet++ + CRF</cell><cell>0.4522</cell><cell>0.6394</cell><cell>0.4326</cell><cell>0.5708</cell></row><row><cell>ResUNet++ + TTA</cell><cell>0.5042</cell><cell>0.6606</cell><cell>0.4851</cell><cell>0.6796</cell></row><row><cell>ResUNet++ + TTA + CRF</cell><cell>0.4901</cell><cell>0.6565</cell><cell>0.4766</cell><cell>0.6277</cell></row><row><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="5">RESULTS COMPARISON ON CVC-VIDEOCLINICDB</cell></row><row><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>ResUNet++ [1]</cell><cell>0.8798</cell><cell>0.8730</cell><cell>0.7749</cell><cell>0.6702</cell></row><row><cell>ResUNet++ + CRF</cell><cell>0.8811</cell><cell>0.8739</cell><cell>0.7743</cell><cell>0.6706</cell></row><row><cell>ResUNet++ + TTA</cell><cell>0.8125</cell><cell>0.8467</cell><cell>0.6896</cell><cell>0.6421</cell></row><row><cell>ResUNet++ + TTA + CRF</cell><cell>0.8130</cell><cell>0.8477</cell><cell>0.6875</cell><cell>0.6276</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII RESULTS</head><label>VIII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">COMPARISON ON ASUMAYO CLINIC</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell>DSC</cell><cell></cell><cell cols="2">mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResUNet++ [1]</cell><cell cols="2">0.8743</cell><cell cols="2">0.8569</cell><cell>0.6534</cell><cell>0.4896</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResUNet++ + CRF</cell><cell cols="2">0.8850</cell><cell cols="2">0.8635</cell><cell>0.6504</cell><cell>0.4858</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResUNet++ + TTA</cell><cell cols="2">0.8553</cell><cell cols="2">0.8535</cell><cell>0.6162</cell><cell>0.4912</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResUNet++ + TTA + CRF</cell><cell cols="2">0.8550</cell><cell cols="2">0.8551</cell><cell>0.6107</cell><cell>0.4743</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE IX</cell></row><row><cell cols="9">RESULTS COMPARISON USING (KVASIR-SEG + CVC-CLINICDB) AS THE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TRAINING SET</cell></row><row><cell cols="4">Test set</cell><cell>Method</cell><cell></cell><cell></cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell cols="2">CVC-</cell><cell cols="2">ColonDB</cell><cell cols="2">ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell cols="2">0.4974 0.4920 0.5084 0.5061</cell><cell>0.6800 0.6788 0.6859 0.6852</cell><cell>0.4787 0.4744 0.4795 0.4775</cell><cell>0.6019 0.5636 0.5973 0.5770</cell></row><row><cell>CVC-</cell><cell cols="2">Video-</cell><cell>ClinicDB</cell><cell cols="2">ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell cols="2">0.3460 0.3552 0.3573 0.3603</cell><cell>0.6348 0.6412 0.6440 0.6468</cell><cell>0.2272 0.2228 0.2104 0.2068</cell><cell>0.3383 0.3065 0.3338 0.3038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X CROSS</head><label>X</label><figDesc>-DATASET RESULTS USING KVASIR-SEG AS THE TRAINING SET</figDesc><table><row><cell cols="4">Test set</cell><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell cols="2">CVC-</cell><cell cols="2">ClinicDB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.6468 0.6458 0.6737 0.6712</cell><cell>0.7311 0.7321 0.7507 0.7506</cell><cell>0.6984 0.6955 0.7108 0.7078</cell><cell>0.6510 0.6425 0.6833 0.6680</cell></row><row><cell>ETIS-</cell><cell cols="2">Larib</cell><cell>Polyp DB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.4017 0.4012 0.4014 0.3997</cell><cell>0.6415 0.6427 0.6468 0.6466</cell><cell>0.4412 0.4379 0.4294 0.4267</cell><cell>0.3925 0.3755 0.4014 0.3710</cell></row><row><cell cols="2">CVC-</cell><cell cols="2">ColonDB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.5135 0.5122 0.5593 0.5563</cell><cell>0.6742 0.6748 0.7030 0.7024</cell><cell>0.5398 0.5367 0.5626 0.5595</cell><cell>0.5461 0.5285 0.5944 0.5811</cell></row><row><cell>CVC-</cell><cell cols="2">Video-</cell><cell>ClinicDB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.3175 0.3334 0.3505 0.3601</cell><cell>0.6082 0.6185 0.6337 0.6402</cell><cell>0.2915 0.2862 0.2601 0.2555</cell><cell>0.3299 0.3141 0.3488 0.3252</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResUNet++ [1]</cell><cell>0.3482</cell><cell>0.6346</cell><cell>0.2196</cell><cell>0.2021</cell></row><row><cell cols="2">ASU-</cell><cell cols="2">Mayo</cell><cell>ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.3747 0.3823 0.3950</cell><cell>0.6516 0.6583 0.6681</cell><cell>0.2136 0.1962 0.1890</cell><cell>0.1797 0.2165 0.1781</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE XI CROSS</head><label>XI</label><figDesc>-DATASET RESULTS ON CVC-CLINICDB AS THE TRAINING SET</figDesc><table><row><cell cols="4">Test set</cell><cell>Method</cell><cell>DSC</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResUNet++ [1]</cell><cell>0.6876</cell><cell>0.7374</cell><cell>0.7027</cell><cell>0.7354</cell></row><row><cell cols="2">Kvasir-</cell><cell cols="2">SEG</cell><cell>ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.6877 0.7218 0.7208</cell><cell>0.7389 0.7616 0.7621</cell><cell>0.7004 0.7225 0.7204</cell><cell>0.7371 0.7855 0.7831</cell></row><row><cell cols="2">CVC-</cell><cell cols="2">ColonDB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + + TTA + CRF</cell><cell>0.5489 0.5470 0.5686 0.5667</cell><cell>0.6942 0.6949 0.7080 0.7081</cell><cell>0.5577 0.5546 0.5702 0.5687</cell><cell>0.5816 0.5727 0.5935 0.5773</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FCN-VGG [59]</cell><cell>0.7023</cell><cell>0.5420</cell><cell>-</cell><cell>-</cell></row><row><cell>ETIS-</cell><cell cols="2">Larib</cell><cell>Polyp DB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.4012 0.3990 0.4027 0.3973</cell><cell>0.6398 0.6403 0.6522 0.6514</cell><cell>0.4232 0.4191 0.3969 0.3906</cell><cell>0.4013 0.3974 0.4235 0.4078</cell></row><row><cell>CVC-</cell><cell cols="2">Video-</cell><cell>ClinicDB</cell><cell>ResUNet++ [1] ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.3666 0.3788 0.3941 0.3988</cell><cell>0.6422 0.6500 0.6582 0.6616</cell><cell>0.2568 0.2530 0.2516 0.2481</cell><cell>0.3632 0.3399 0.3829 0.3542</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResUNet++ [1]</cell><cell>0.2797</cell><cell>0.6113</cell><cell>0.1627</cell><cell>0.1443</cell></row><row><cell cols="2">ASU-</cell><cell cols="2">Mayo</cell><cell>ResUNet++ + CRF ResUNet++ + TTA ResUNet++ + TTA + CRF</cell><cell>0.3167 0.3085 0.3233</cell><cell>0.6323 0.6331 0.6426</cell><cell>0.1591 0.1265 0.1225</cell><cell>0.1348 0.1571 0.1270</cell></row><row><cell cols="9">and TTA still remain competitive. Moreover, the values of</cell></row><row><cell cols="9">DSC and mIoU of the best model are similar for both the</cell></row><row><cell cols="9">CVC-VideoClinicDB and the ASU-Mayo Clinic dataset. We</cell></row><row><cell cols="9">have compared the results with the existing work that used</cell></row><row><cell cols="9">CVC-CliniDB for training and ETIS-Larib for testing. Our</cell></row><row><cell cols="7">model achieves highest mIoU of 0.6522.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XII TOTAL</head><label>XII</label><figDesc>CRF and TTA are more generalizable for all the datasets, where TTA with ResUNet++ performs best on the still images, and the combinations of ResUNet++, CRF, and TTA are outperforming others on video datasets. For all of the proposed models, the value of AUC is greater than 0.93. This indicates that our models are good at distinguishing between the polyp and non-polyps. It also suggests that the model produces sufficient sensitivity.</figDesc><table><row><cell cols="2">NUMBER OF TRAINABLE PARAMETERS</cell></row><row><cell>Model</cell><cell>Trainable parameters</cell></row><row><cell>U-Net</cell><cell>5,400,289</cell></row><row><cell>ResUNet</cell><cell>8,221,121</cell></row><row><cell>ResUNet-mod</cell><cell>2,058,465</cell></row><row><cell>ResUNet++</cell><cell>16,228,001</cell></row><row><cell cols="2">combination of ResUNet++ and TTA, and the combination</cell></row><row><cell>of ResUNet++,</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://giana.grand-challenge.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE ISM</title>
		<meeting>of IEEE ISM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2018: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Soerjomataram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: a cancer journal for clinicians</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="394" to="424" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advances in image enhancement in colonoscopy for detection of adenomas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sekiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Revi. Gastroenter. &amp; Hepato</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MMM</title>
		<meeting>of MMM</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The miss rate for colorectal adenoma determined by quality-adjusted, back-to-back colonoscopies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Eun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gut and liver</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="70" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Miss rate for colorectal neoplastic polyps: a prospective multicenter study of back-to-back video colonoscopies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Heresbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="284" to="290" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Right-sided location not associated with missed colorectal adenomas in an individual-level reanalysis of tandem colonoscopy studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann-Fraedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="660" to="671" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Longer withdrawal time is associated with a reduced incidence of interval cancer after screening colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaukat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="952" to="957" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of healthcare engineering</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robust medical instrument segmentation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ro?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10299v1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computeri. Med. Imag. and Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Jour. of Comput. Assis. Radiol. and Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards real-time polyp detection in colonoscopy videos: Adapting still frame-based methodologies for video sequences analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Angermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Assis. and Robot. Endos. and Clin. Image-Based Proced</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Polyp detection benchmark in colonoscopy videos using gtcreator: A novel fully configurable tool for easy and fast annotation of image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CARS conference</title>
		<meeting>CARS conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic analysis of endoscopic images for polyps detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Pozdeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Obukhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Motyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EIConRus</title>
		<meeting>of EIConRus</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1216" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polyp segmentation in colonoscopy images using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Akbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMBC</title>
		<meeting>of EMBC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Development and validation of a deep-learning algorithm for the detection of polyps during colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. biomed. engineer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="741" to="748" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on patt. analys. and mach. intellige</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Giana polyp segmentation with fully convolutional dilation neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matuszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VISIGRAPP</title>
		<meeting>of VISIGRAPP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="632" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Development of a real-time endoscopic image diagnosis support system using deep learning technology in colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scienti. repo</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Polyp segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poomeshwaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Santhosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sivaprakasam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMBC</title>
		<meeting>of EMBC</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7201" to="7204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ensemble of instance segmentation models for polyp segmentation in colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="26" to="440" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Endoscopy artifact detection (ead 2019) challenge dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03209</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust boundary segmentation in medical images using a consecutive deep encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="33" to="795" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training data enhancements for robust polyp segmentation in colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Almeida Thomaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Sierra-Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Raposo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CBMS</title>
		<meeting>of CBMS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="192" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Colorectal polyp segmentation by u-net with dilation convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CBMS</title>
		<meeting>of IEEE CBMS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards a computed-aided diagnosis system in colonoscopy: automatic polyp segmentation using convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jour. of Medi. Robot. Resear</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page">1840002</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. and Remo. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICCAI</title>
		<meeting>of MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nested dilation network (ndn) for multi-task medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="44" to="676" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on pattern anal. and mach. intelli</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep attentional features for prostate segmentation in ultrasound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICCAI</title>
		<meeting>of MICCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Asdnet: Attention based semisupervised deep networks for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICCAI</title>
		<meeting>of MICCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-scale guided attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Conditional random field and deep feature learning for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-C</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geosci. and Remo. Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1612" to="1628" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kvasir: A multi-class image dataset for computer aided gastrointestinal disease detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MMSYS</title>
		<meeting>of MMSYS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI</title>
		<meeting>of OSDI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Colorectal polyp segmentation using a fully convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CISP-BMEI</title>
		<meeting>of CISP-BMEI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Colorectal segmentation using multiple encoder-decoder network in colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IKE</title>
		<meeting>of IKE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="208" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A multi-scale patch-based deep learning system for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasipuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advan. Comput. and Syst. for Secur</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICCAI</title>
		<meeting>of MICCAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automated polyp segmentation in colonoscopy frames using fully convolutional neural network and textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dolwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ov MIUA</title>
		<meeting>ov MIUA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="707" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fully convolutional neural networks for polyp segmentation in colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="101" to="340" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detecting colorectal polyps via machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. biomed. engineer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="713" to="714" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Factors influencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leufkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
