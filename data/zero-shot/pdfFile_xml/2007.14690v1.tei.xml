<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition , Huim- ing Tang 2 . 2020. Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition. In Proceedings of MM &apos;20: the 28th ACM ? Average Pooling Context-encoding Network-1 ? 1 2 FC+Softmax output Dynamic GConv layer Context-encoding Network-2 Context-encoding Network-n GConv-2 GConv-n</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanfan</forename><surname>Ye</surname></persName>
							<email>yefanfan@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science &amp; Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<email>pushiliang.hri@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
							<email>zhongqiaoyong@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<email>lichao15@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
							<email>xiedi@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science &amp; Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanfan</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science &amp; Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition , Huim- ing Tang 2 . 2020. Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition. In Proceedings of MM &apos;20: the 28th ACM ? Average Pooling Context-encoding Network-1 ? 1 2 FC+Softmax output Dynamic GConv layer Context-encoding Network-2 Context-encoding Network-n GConv-2 GConv-n</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Seattle, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">October 12-16, 2020</date>
						</imprint>
					</monogr>
					<note>ACM Reference Format: * Corresponding author. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 Figure 1: An overview of the proposed Dynamic GCN frame-work. The backbone is comprised of n Dynamic GConv lay-ers, which accept two graph topologies as input, i.e. static topology G st at ic of pre-defined physical connections and dynamic topology {G 1 , G 2 , . . . , G n } predicted by the context-encoding networks. International Conference on Multimedia (MM &apos;20). ACM, New York, NY, USA, 9 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Skeleton Topology</term>
					<term>Context-encoding Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have attracted increasing interests for the task of skeleton-based action recognition. The key lies in the design of the graph structure, which encodes skeleton topology information. In this paper, we propose Dynamic GCN, in which a novel convolutional neural network named Contextencoding Network (CeN) is introduced to learn skeleton topology automatically. In particular, when learning the dependency between two joints, contextual features from the rest joints are incorporated in a global manner. CeN is extremely lightweight yet effective, and can be embedded into a graph convolutional layer. By stacking multiple CeN-enabled graph convolutional layers, we build Dynamic GCN. Notably, as a merit of CeN, dynamic graph topologies are constructed for different input samples as well as graph convolutional layers of various depths. Besides, three alternative context modeling architectures are well explored, which may serve as a guideline for future research on graph topology learning. CeN brings only~7% extra FLOPs for the baseline model, and Dynamic GCN achieves better performance with 2?~4? fewer FLOPs than existing methods. By further combining static physical body connections and motion modalities, we achieve state-of-the-art performance on three large-scale benchmarks, namely NTU-RGB+D, NTU-RGB+D 120 and Skeleton-Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Activity recognition and understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An overview of the proposed Dynamic GCN framework. The backbone is comprised of n Dynamic GConv layers, which accept two graph topologies as input, i.e. static topology G st at ic of pre-defined physical connections and dynamic topology {G 1 , G 2 , . . . , G n } predicted by the contextencoding networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human action recognition is a popular topic in the area of computer vision. Especially, skeleton-based action recognition has attracted more and more attention. Compared with RGB data, skeleton data are considered as a more robust representation for human action dynamics. Meanwhile, skeleton data are extremely compact in terms of data size. This makes it possible to design more lightweight models. Skeleton data can be easily captured by depth cameras (e.g. Kinetics) or estimated with human pose estimation algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>In the task of skeleton-based action recognition, we are given a time series of human joint coordinates and expected to predict the action being performed. Considering the sequence property, Recurrent Neural Networks (RNNs) are a natural choice and have been widely studied <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. On the other hand, some recent works attempted to cast a skeleton sequence as a pseudo-image. Then Convolutional Neural Networks (CNNs) are employed to classify the image directly <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, which also achieve great success. The advantage of deep neural networks (DNNs) lies in their powerful feature learning capability. Nevertheless, none of these methods explicitly exploits the skeleton topology information, which is very informative for discriminating different action categories. <ref type="bibr" target="#b32">[33]</ref> first introduced Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b10">[11]</ref> in the context of skeleton-based action recognition. They model skeleton data as a graph and extract the topology information by an adjacency matrix according to the physical connections of human body. However, the dependencies among skeleton joints of different samples vary, especially when they are performing different actions. Such topology information derived from fixed graph is relatively weak. Recently, there have been some attempts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36</ref>] to construct different graphs for different samples. They are basically inspired by the non-local operation <ref type="bibr" target="#b1">[2]</ref>, where a distance metric like inner product is utilized to measure the degree of dependency between two arbitrary skeleton joints. To some extent, the topology information is strengthened and the recognition performance gets improved. However, they also bring three issues for future improvement. 1) Non-local-based methods, measuring the dependency between two skeleton joints while ignoring the influence of all other contextual joints, is essentially a local method. We argue that besides the underlying two joints, the contextual information from the rest joints is critical for learning reliable and stable topology. 2) Using an arbitrary function like inner product to compute the dependency between two joints introduces strong prior knowledge, which may not be optimal. 3) In the skeleton dynamic system, non-local-based methods consider the dependency of every pair of joints undirected. Since the contextual information of each joint is different, the dependency should be directed. Moreover, for different pairs of queries, their similarities yielded by the non-local-based methods may be almost identical <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this work, we propose a hybrid GCN-CNN framework named Dynamic GCN as shown in <ref type="figure">Figure 1</ref>. We aim to attack the weaknesses of existing learning-based skeleton topology by leveraging the feature learning capability of CNNs. Specifically, a novel convolutional neural network named Context-encoding Network (CeN) is introduced to learn skeleton topology automatically. It can be embedded into a graph convolutional (GConv) layer and learned end-to-end. Compared with non-local-based methods, CeN has the following advantages. 1) CeN takes full account of the contextual information of each joint from a global perspective. 2) CeN is completely data-driven and does not require any prior knowledge, which is more flexible. 3) CeN regards the dependency of each pair of joints as directed and yields directed graphs (asymmetric adjacency matrices), which can represent the dynamics of the skeleton system more accurately. 4) Compared with other topology-learning algorithms, CeN is extremely lightweight yet effective, and can be integrated into the GCN-based methods easily.</p><p>Notably, CeN predicts a unique graph topology per-sample as well as per-GConv layer. This feature results in a dynamic graph topology rather than static topology, which enhances the capacity and expressiveness of the model.</p><p>For context modeling in CeN, various feature aggregation architectures are explored. As pointed out by <ref type="bibr" target="#b13">[14]</ref>, for a two-dimensional (2D) convolutional layer, features are aggregated globally along the channel dimension and locally along the spatial dimensions. A skeleton sequence can be represented as a tensor of C ? T ? N , where C, T and N denote the feature, temporal and joint dimensions respectively. In the context of topology learning, we argue that the contextual information from the surrounding joints are the most important. To this end, in the proposed CeN features are aggregated globally along the joint dimension by treating it as channel. Ablation studies show that it is superior over two other alternatives, where either the temporal or feature dimension is treated as channel. Further discussion is given, which may guide future research on graph topology learning.</p><p>In terms of performance, CeN alone surpasses existing non-localbased topology learning methods significantly. Combining dynamic</p><formula xml:id="formula_0">?/? A B C D A B C D A B C D A B C D A B C D A B C D (a) Dependency Measurement 1 A B C D A B C D A B C D A B C D C N T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton Sequence</head><p>Context-encoding Network (CeN) topology predicted by CeN with static topology leads to further performance gain, which indicates that CeN is complementary to static topology. In terms of efficiency, CeN only brings~7% extra FLOPs for the GCN-based baseline method. By using the jointlevel feature aggregation mechanism, Dynamic GCN is 2?~4? less expensive than other GCN-based methods on calculation. Moreover, by incorporating the spatial and motion modalities of skeleton sequences, our final model achieves state-of-the-art performance on three large-scale benchmarks, namely NTU-RGB+D, NTU-RGB+D 120 and Skeleton-Kinetics.</p><formula xml:id="formula_1">(b) (c) (d) Dependency Measurement 2</formula><p>Our main contributions can be summarized as follows.</p><p>? We propose the Dynamic GCN framework, which leverages the complementary benefits of GCN's topology learning and CNN's feature learning capabilities. ? We introduce a lightweight context-encoding network, which learns context-enriched dynamic skeleton topology in a global way. ? Three alternative context modeling architectures are well explored, which may serve as a guideline for future research on graph topology learning. ? Our final model achieves state-of-the-art performance on three large-scale benchmarks for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DNN-based Methods</head><p>RNN is a straightforward model for sequence data. <ref type="bibr" target="#b22">[23]</ref> divides the cell of LSTM into five part-cells which correspond to five body parts respectively. <ref type="bibr" target="#b33">[34]</ref> proposed a view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. While RNNs aggregate temporal information sequentially, CNNs are able to encode spatiotemporal information jointly.</p><p>[13] treats a skeleton sequence as a pseudo-image, in which the coordinate of each skeleton joint (x, y, z) is regarded as three channels of the image. Then a CNN is designed to directly classify the image into action categories. <ref type="bibr" target="#b13">[14]</ref> proposes a co-occurrence feature learning framework, which inspires the global contextual feature aggregation of the proposed CeN. The success of CNNs is attributed to their strong capability in feature representation. However, after converting the irregularly structured skeleton data into regularly structured images, the skeleton topology information is lost. Although CeN is also a CNN model, it is designed for learning of the graph topology rather than final action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GCN-based Methods</head><p>GCN is able to effectively deal with irregularly structured graphs like skeleton data. Given skeleton data with N joints, the graph topology can be well represented by an N ? N adjacency matrix A.</p><p>The key of GCN-based methods lies in the design of graph topology, i.e. A. The most straightforward way is to define a fixed graph according to the physical connections of human body <ref type="figure" target="#fig_0">(Figure 2(a)</ref>), which is adopted in ST-GCN <ref type="bibr" target="#b32">[33]</ref>. In order to put slight attention on edges, they also create a learnable mask which is multiplied or added with the physical adjacency matrix <ref type="figure" target="#fig_0">(Figure 2(b)</ref>). Later, <ref type="bibr" target="#b27">[28]</ref> adopted the conception of virtual connection as a supplement to physical adjacency matrix.</p><p>In the above methods, the adjacency matrices are either predefined or fixed after training finishes. To make the graph topology more flexible, <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref> and <ref type="bibr" target="#b11">[12]</ref> attempted to construct different graphs for different samples. Specifically, non-local-based operations are employed to infer the connectedness between two arbitrary joints. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c), when measuring the dependency between two joints, only features of the underlying two joints are taken into account, while the influence of the contextual joints is ignored. On the contrary, in our Dynamic GCN, features of all contextual joints are fully incorporated with the proposed CeN ( <ref type="figure" target="#fig_0">Figure 2(d)</ref>). The graph learned in this way can be more robust and expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we first briefly recap GCN in the context of skeletonbased action recognition. Then we illustrate the details of the proposed Dynamic GCN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GCN for Skeleton-based Action Recognition</head><p>In the context of skeleton-based action recognition, a GCN is typically composed of graph convolutional blocks (GC-blocks) and temporal convolutional blocks (TC-blocks). Given a skeleton sequence of C ? T ? N , GC-blocks and TC-blocks aggregate features along the joint (N ) and temporal (T ) dimensions respectively. Note X ?, , L2 Normalization ?, * , 1,1</p><formula xml:id="formula_2">Conv-T Conv-C ?, , , ?, 1, , ?, , 1, ?, 1,1, Conv-J ?, , 1,1</formula><p>Flatten &amp; Reshape <ref type="figure">Figure 3</ref>: Architecture of the proposed CeN. It only consists of three 1?1 convolutional layers. Conv-C and Conv-T are first adopted to squeeze the feature and temporal dimensions. And then the joint dimension is treated as channel to acquire the global topology. Feature map permutation (block in pink) is applied on demand. Batch Normalization and ReLU activation function are applied after each convolutional layer.</p><p>that the number of joints N is kept unchanged, while C and T normally differ in different GConv layers. Specifically, GC-blocks can be formulated as:</p><formula xml:id="formula_3">Y = K k =1 ? ? 1 2 k A k ? ? 1 2 k XW,<label>(1)</label></formula><p>where K denotes the number of spatial configurations according to ST-GCN <ref type="bibr" target="#b32">[33]</ref>. X and Y denote the input and output feature maps respectively. W denotes the learnable kernels. For each spatial configuration, A is the adjacency matrix and ? is the diagonal node degree matrix for normalization. Specifically, the degree of node i is computed by ? ii = j A i j + ?, where A i j denotes the element of the i-th row and j-th column in A, and ? is added to avoid the all-zero problem. TC-blocks are normal convolutional layers with a kernel size of t ? 1. To learn spatiotemporal features jointly, a GCN is typically built by stacking GC-blocks and TC-blocks alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic GCN</head><p>In this section, we elaborately introduce the proposed Dynamic GCN framework. We first introduce the architecture of Contextencoding Network (CeN) for topology learning. Then we describe how to integrate CeN into GConv layers and build the complete framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Context-encoding Network. The adjacency matrix A in GCN fully represents the graph topology, which corresponds to the dependencies among different skeleton joints. When A is pre-defined with prior knowledge, the topology information is static and limited. Existing learning-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref> predict the dependency between two joints A i j independently, and hand-crafted functions (e.g. inner product) to map input features into dependency are employed. On the contrary, we design an extremely lightweight convolutional neural network which takes the whole feature map as input and predicts the full adjacency matrix A directly. Notably, contextual information along the joint, temporal and feature dimensions are well explored, yielding a more flexible and expressive graph topology. The architecture of CeN is shown in <ref type="figure">Figure 3</ref>. Given a feature map of intermediate layer X ? R C?T ?N , firstly the feature and temporal dimensions of each joint are squeezed by two 1?1 convolutional layers named Conv-C and Conv-T. Then the joint dimension is treated as the channel of convolution, and a single 1?1 convolutional layer is utilized to map the N -dimensional vector into the N ? N adjacency matrix. This design fully considers the impact of all other joints when measuring the dependency between every pair of joints. After that, the topology is represented as matrices with the shape of (Batch, N , N ). In addition, the L2 normalization is applied to each row of the adjacency matrices, which eases the optimization.</p><p>It is worth noting that CeN learns a dynamic and unique adjacency matrix per-sample as well as per-GConv layer. The graph topology is not shared among different samples even if they belong to the same action class. Rather than hand-crafted functions, the parameters in CeN are learned in a data-driven way without any prior assumption. Moreover, by treating the joint dimension as channel, global contextual information of all joints can be encoded by the trainable kernels.</p><p>The adjacency matrix predicted by CeN is fed into a GConv layer as its graph topology representation. With the help of L2 normalization, the normalization of node degree in Eq. (1) is unnecessary. For simplicity, henceforth we use G to refer to either normalized static adjacency matrix or learned adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dynamic</head><p>GConv Layer. <ref type="figure" target="#fig_1">Figure 4</ref> shows the pipeline of the Dynamic GConv layer, which is the basic building block of the Dynamic GCN framework. Besides the dynamic graph predicted by CeN, static graphs are also incorporated. The static branch takes the skeleton features from the last layer and the physical graph G physical with learnable parameter-mask G mask as input. The dynamic branch takes the skeleton features and the context-enriched graph G ?lobal as input. Their outputs further get fused by elementwise summation.</p><p>In detail, according to Eq. (1), the static branch can be formulated as:</p><formula xml:id="formula_4">Y st at ic = K k =1 G physical + G mask XW,<label>(2)</label></formula><p>where G physical denotes the physical graph from the physical connections of human body. G mask denotes the parameterized mask, which is used as an attention onto the physical graph. Following 2s-AGCN <ref type="bibr" target="#b25">[26]</ref>, these two graphs are combined in an additive manner. Y st at ic is the output features of the static branch. The static branch extracts the static topology information of the skeleton data, which has been proven helpful for final prediction. More importantly, the dynamic branch can be formulated as:</p><formula xml:id="formula_5">Y dynamic = G ?lobal XW ? ,<label>(3)</label></formula><p>where G ?lobal is the dynamic graph predicted by CeN. Y ?lobal is the output of the dynamic branch, which extracts the global contextenriched topology of the skeleton data. Note that the learnable parameters W ? are not shared between the static and dynamic branches.</p><p>After extracting the static and context-enriched topology features with the static branch and dynamic branch, a weighted summation operation is applied for fusion. That is:</p><formula xml:id="formula_6">Y = Y dynamic + ?Y st at ic ,<label>(4)</label></formula><p>where ? is the weight used to balance Y dynamic and Y st at ic , as they may differ in magnitude.</p><p>After the topology features are aggregated, a TC-block is appended for temporal feature aggregation. A shortcut connection is added after the TC-block. As shown in <ref type="figure">Figure 1</ref>, the complete Dynamic GCN framework is built by stacking 10 Dynamic GConv layers. Global average pooling layer and a fully-connected layer along with softmax are appended after the GConv layers for final classification. The numbers of output channels in the GConv layers are kept the same as ST-GCN <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Joint-level Feature Aggregation and Ensemble of Spatial-motion</head><p>Modalities. In the previous GCN-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>, the number of nodes in the graph is kept unchanged. That is, given a skeleton sequence X ? R C?T ?N , all GConv layers share the same number of joints N . In contrast, we propose a very simple way to gradually aggregate features at the joint level. Specifically, we use a projection matrix P ? R N i ?N i +1 to shrink the size of the joint dimension, where N i+1 = ? N i and 0 &lt; ? ? 1. We insert P into some intermediate layers of the graph convolutional network, so that X = XP ? R C?T ?N i +1 . By using the joint-level feature aggregation, the FLOPs of the model can be greatly reduced, and the performance of the model is barely affected.</p><p>Moreover, to further boost the performance, we explore multiple modalities, namely joint, bone and their corresponding motion modalities as Shi et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> did. The temporal motion of skeleton joints has been shown informative to discriminate fine-grained actions, such as "put on jacket" and "take off jacket". For one person in frame t, each joint can be denoted as J t ? R 3?N . Then the joint motion M t is defined as the temporal movement of each joint</p><formula xml:id="formula_7">M t = J t +1 ? J t .</formula><p>Moreover, the bone is another spatial information, which has been proved important in previous works <ref type="bibr" target="#b25">[26]</ref>. Each bone is defined as the vector pointing from a source joint to a target joint. It can be formulated as B t = J t t arg et ? J t sour ce . Temporal motion can also be computed for the bone stream, in the same way as joint motion.</p><p>For ensemble of multiple modalities, we train one model permodality separately. Then the logits before softmax of the four models are fused by summation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate Dynamic GCN on three large-scale skeleton-based action recognition benchmarks, namely NTU-RGB+D, NTU-RGB+D 120 and Skeleton-Kinetics. Extensive ablation studies are conducted to verify the impact of different components of the framework. Lastly, our final model is evaluated and compared with current state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NTU-RGB+D <ref type="bibr" target="#b22">[23]</ref> is the most widely used action recognition dataset. It contains 56,880 skeleton clips of 60 action classes. These clips were captured in the lab environment from three camera views. The annotations provide the 3D location (x, y, z) of each joint in the camera coordinate system. There are 25 joints per-subject. Each clip is guaranteed to contain at most 2 subjects. We follow the standard evaluation protocols, namely cross-subject (C-Subject) and cross-view (C-View). In the C-subject setting, 40,320 clips from 20 subjects are used for training, and the rest for testing. In the C-View setting, 37,920 clips captured from camera 2 and 3 are used for training and those from camera 1 for testing. NTU-RGB+D 120 <ref type="bibr" target="#b15">[16]</ref> is an extension of NTU-RGB+D, where the number of classes is expanded to 120 and the number of samples is expanded to 114,480. There are also two recommended evaluation protocols, namely cross-subject (C-Subject) and cross-setup (C-Setup). In the C-Subject setting, 63,026 clips from 53 subjects are used for training, and the remaining subjects are reserved for testing. In the C-Setup setting, 54,471 clips with even collection setup IDs are used for training, and the rest clips with odd setup IDs are used for testing. Skeleton-Kinetics is derived from the Kinetics video action recognition dataset <ref type="bibr" target="#b7">[8]</ref>. The dataset contains 300,000 video clips of 400 classes. Each video clip lasts around 10 seconds. Human skeletons are estimated by <ref type="bibr" target="#b32">[33]</ref> from the RGB videos using the OpenPose toolbox <ref type="bibr" target="#b3">[4]</ref>. Each joint consists of 2D coordinates (x, y) in the pixel coordinate system and its confidence score s. Thus it is finally represented as a tuple of (x, y, s). There are 18 joints for each person. In each frame at most two subjects are considered. We follow the same train-validation split as <ref type="bibr" target="#b32">[33]</ref>. That is, the training and validation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Dynamic GCN is implemented in PyTorch <ref type="bibr" target="#b21">[22]</ref>. To verify the effectiveness of CeN on a high-performance baseline, we follow the same data processing strategy and the attention mechanism in MS-AAGCN <ref type="bibr" target="#b24">[25]</ref>. SGD <ref type="bibr" target="#b0">[1]</ref> with a 0.9 Nesterov momentum is used for optimization. The learning rate is set to 0.1 initially and reduced twice at the 35-th and 55-th epochs with a factor of 0.1. On all the three datasets, the model is trained for 65 epochs in total. And the ? is set to 1. The joint aggregation rate ? is set to 0.6, and the projection P is inserted twice after the 5-th and 8-th GConv layers. The input skeleton sequences are resized to a fixed length, i.e. 64 frames for both NTU-RGB+D and NTU-RGB+D 120, and 150 frames for Skeleton-Kinetics. For a fair comparison with methods based on ST-GCN, the number of spatial configurations is set to 3. To alleviate overfitting, weight decay is set to 0.0004. Batch size is set to 64, and cross-entropy loss is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Ablation studies are conducted on the NTU-RGB+D dataset with the C-Subject setting. We first validate the effectiveness of the proposed CeN by comparing it with existing non-local-based methods. Next two alternative context modeling architectures are compared. Finally, the contribution of ensemble of spatial-motion modalities is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Choosing the Optimal Position of Joint Aggregation.</head><p>For the joint-level feature aggregation with the projection matrix P, the aggregation rate ? is set to 0.6 empirically. To choose the appropriate GConv layers to apply the projection, we try a few configurations. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the position of applying P barely affects the performance. When applying it to the 5-th &amp; 8-th layers, it even slightly improves the accuracy from 88.9% to 89.2%. <ref type="figure" target="#fig_1">Figure 4</ref>, the Dynamic GConv layer is composed of static and dynamic branches. To measure the contribution of individual component, we train the model either using the static or dynamic branch only. To compare CeN with existing non-local-based method, we investigate the case when all CeNs are replaced with the non-local operation following <ref type="bibr" target="#b25">[26]</ref>. Meanwhile, to verify the importance of directed graphs, we compare CeN with a variant CeN* which predicts undirected graphs by forcing the adjacency matrices to be symmetric. As shown in <ref type="table" target="#tab_1">Table 2</ref>, whether or not the static branch is enabled, the directed graph structure predicted by CeN achieves better performance than the undirected graph by CeN*. It clearly demonstrates the importance of directed graph, which can represent the dynamic characteristics of the skeleton more effectively. This also  verifies our opinion that for different joints, different contextual information is preferred. That is why learning-based dynamic graph topology is necessary. In addition, using static graph alone achieves an accuracy of 88.2%, while non-local-based dynamic graph is inferior to static graph. CeN-predicted graph surpasses non-local-based graph by 1.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effectiveness of CeN. As shown in</head><p>When combining static graph with dynamic graph, non-localbased graph barely improves the accuracy (i.e. by 0.2%), while CeNpredicted graph significantly improves the accuracy from 88.2% to 89.2%, setting a new state-of-the-art for single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Alternative</head><p>Context-enriched Topology. When learning the skeleton topology, CeN enriches contextual information globally along the joint dimension. Two straightforward alternatives are to learn contextual information along the feature or temporal dimension rather than the joint dimension. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, they can be easily implemented by changing the order of the three 1 ? 1   <ref type="table" target="#tab_2">Table 3</ref> shows a comparison of CeN with its two variants. The performance of the model decreases by 0.7% and 0.8% for CeN-F and CeN-T respectively. The results verify our motivation that the surrounding joints are the most important for learning dynamic skeleton topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Contribution of Model Ensemble.</head><p>To fully exploiting the skeleton data, we evaluate the ensemble of four modalities, i.e. joints, bones, joint motion and bone motion. The modalities of joints and bones are combined as spatial information, and the modalities of joint motion and bone motion are combined as motion information. Using spatial and motion information alone as well as fusion of all modalities are evaluated.</p><p>As shown in <ref type="table" target="#tab_3">Table 4</ref>, with ensemble of spatial modalities, the accuracy gets significantly improved from 89.2% to 90.9%. After combining the motion information, we achieve a top-1 accuracy of 91.5% in the C-Subject setting of NTU-RGB+D, which is the new state-of-the-art for ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Comparison of FLOPs with</head><p>Other GCN-based Methods. In order to highlight the efficiency of the proposed Dynamic GCN, we compare it with existing GCN-based methods in terms of FLOPs and accuracy. For simplicity, for all methods, FLOPs and accuracy of single model are reported. As shown in <ref type="table" target="#tab_4">Table 5</ref>, CeN only brings extra~7% FLOPs for the baseline method, whose FLOPs increase from~1.86G to~1.99G. Compared with other methods, Dynamic GCN has 2?~4? advantage in terms of FLOPs, while achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Visualization of Learned Topology. To investigate how well</head><p>CeN learns the graph topology, we attempt to visualize the learned skeleton topology. Since CeN predicts dynamic graph for different samples, for clarification, we compute the average adjacency matrix of the 5-th GConv block for a specific action category. If the action involves two people, only the first person will be visualized. <ref type="figure" target="#fig_3">Figure 6</ref> visualizes the learned skeleton topologies of three actions. Response values larger than 0.4 are drawn as dark-orange  lines. The blue lines are the physical connections of human body, and the red dots are the joints. As we can see, the strong dependencies captured by CeN are mainly related to two hands for the action of Wipe Face <ref type="figure" target="#fig_3">(Figure 6(c)</ref>) which is in line with cognition. For the action of Jump <ref type="figure" target="#fig_3">(Figure 6(a)</ref>), CeN mainly attends to the dependencies related to the knee joint and the feet joints. The action of walking ( <ref type="figure" target="#fig_3">Figure 6(b)</ref>), CeN put attention to the hands and feet significantly. These results validate that reasonable potential topological patterns of different actions can be captured by CeN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the State-of-the-arts</head><p>We compare the proposed Dynamic GCN with other state-of-theart methods on the NTU-RGB+D and Skeleton-Kinetics datasets in <ref type="table" target="#tab_5">Table 6</ref> and <ref type="table" target="#tab_6">Table 7</ref> respectively. For NTU-RGB+D 120, besides two recent methods, the baseline models mentioned in the original paper are also listed ( <ref type="table">Table 8</ref>).</p><p>As shown in <ref type="table" target="#tab_5">Table 6</ref>, Dynamic GCN achieves state-of-the-art performance, i.e. 91.5% and 96.0% in the C-Subject and C-View settings of NTU-RGB+D respectively. Although our accuracy is on par with MS-G3D Net <ref type="bibr" target="#b20">[21]</ref>, our model is much more efficient as analyzed in <ref type="table" target="#tab_4">Table 5</ref>. <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b13">[14]</ref> are two representative methods for GCN-based and CNN-based methods. Dynamic GCN outperforms them by 10% and 5% respectively.   <ref type="bibr" target="#b24">[25]</ref> 37.8 61.0 2020 MS-G3D Net <ref type="bibr" target="#b20">[21]</ref> 38.0 60.9 2020</p><p>Dynamic GCN (ours) 37.9 61.3</p><p>As for the Kinetics-Skeleton dataset (shown in <ref type="table" target="#tab_6">Table 7</ref>), our model also achieves state-of-the-art performance (37.9% top-1 accuracy and 61.3% top-5 accuracy).</p><p>For the recently released NTU-RGB+D 120 dataset, so far few state-of-the-art methods have been evaluated on it. We compare the Dynamic GCN with the baseline methods mentioned in the original paper. As shown in <ref type="table">Table 8</ref>, our model achieves a top-1 accuracy of 87.3% and 88.6% in the C-Subject and C-Setup settings respectively, surpassing the baseline methods by a large margin. Compared with MS-G3D Net <ref type="bibr" target="#b20">[21]</ref>, we achieve slightly better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION 5.1 On Hybrid GCN-CNN</head><p>Compared with GCN alone, combining GCN and CNN into a hybrid GCN-CNN framework is indeed beneficial. In this work, the proposed Dynamic GCN is merely a simple form of the hybrid <ref type="table">Table 8</ref>: Performance comparison on the NTU-RGB+D 120 dataset in both cross subject and cross setup settings in terms of top-1 accuracy (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>C-Subject C-Setup Year Dynamic Skeleton <ref type="bibr" target="#b6">[7]</ref> 50.8 54.7 2015 PA-LSTM <ref type="bibr" target="#b22">[23]</ref> 25.5 26.3 2016 ST-LSTM+TS <ref type="bibr" target="#b17">[18]</ref> 55.7 57.9 2016 FSNet <ref type="bibr" target="#b16">[17]</ref> 59.9 62.4 2019 2s-ALSTM <ref type="bibr" target="#b18">[19]</ref> 61.2 63.3 2017 MT CNN <ref type="bibr" target="#b8">[9]</ref> 62.2 61.8 2018 BPEM <ref type="bibr" target="#b19">[20]</ref> 64.6 66.9 2018 2s-AGCN <ref type="bibr" target="#b25">[26]</ref> 82.9 84.9 2019 MS-G3D Net <ref type="bibr" target="#b20">[21]</ref> 86.9 88.4 2020</p><p>Dynamic GCN (ours) 87.3 88.6</p><p>GCN-CNN architectures, in which CNN is used for topology learning. The capacity of GCN is greatly expanded with a flexible and expressive graph topology.</p><p>Beyond the task of skeleton-based action recognition, the hybrid GCN-CNN can be applied in many fields, such as social network modeling. Although the Dynamic GCN we propose is generally proven effective, the form of hybrid GCN-CNN requires further exploration for different tasks. For example, normal convolution and graph convolution can be stacked sequentially or in parallel for better skeleton feature learning. We will leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">On Learned Graph Toplogy</head><p>For skeleton data, the physical graph enjoys the benefit of being definite and explainable. According to our experiment in <ref type="table" target="#tab_1">Table 2</ref>, its performance is reasonable and competitive. Nevertheless, learned graph by CeN is complementary to physical graph, which indicates that there exist potential connections that are informative but missed by physical connections. Our visualization of learned topology in <ref type="figure" target="#fig_3">Figure 6</ref> verifies this point. We believe that for other graph-structured data like social network, the proposed contextenriched topology learning is also applicable, considering the fact that the pre-defined topology is noisy, incomplete and unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>How to extract the topology of skeleton data effectively for graph convolutional networks is the major challenge for skeleton-based action recognition. In this paper, we propose the Dynamic GCN framework, which leverages the advantages of GCN and CNN. The context-encoding network is designed to learn global contextenriched topology. Extensive experiments on three large-scale datasets validate the superiority of the proposed method. The significant reduction of FLOPs compared with existing methods makes our model more competitive for deployment, in particular on edge devices where computing power is limited. The novelly introduced hybrid GCN-CNN architecture as well as the technique of contextenriched topology learning may provide insights for future research on skeleton data analysis and beyond.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of various ways to construct the adjacency matrix. (a) Pre-defined physical adjacency matrix. (b) Learnable mask multiplied or added with physical adjacency matrix. (c) Existing non-local-based adjacency matrix taking only the underlying two joints into account. (d) The proposed dynamic topology predicted by CeN, in which features of all contextual joints are incorporated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Pipeline of the Dynamic GConv layer. Topology features derived from static graph (static branch) and graph predicted by CeN (dynamic branch) are fused. Afterwards, a TC-block is appended.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Two variants of CeN. (a) CeN-F aggregates global context along the feature dimension. (b) CeN-T aggregates global context along the temporal dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of learned topologies of three actions. Dynamic topologies (dark-orange lines) inferred by CeN are superimposed to physical topology (blue lines) of human body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Impact of joint-level feature aggregation applied to different GConv layers.</figDesc><table><row><cell>Position</cell><cell cols="5">3&amp;6-th 4&amp;7-th 5&amp;8-th 6&amp;9-th w/o P</cell></row><row><cell>Accuracy (%)</cell><cell>88.5</cell><cell>88.7</cell><cell>89.2</cell><cell>88.9</cell><cell>88.9</cell></row><row><cell cols="6">sets contain 240,000 and 20,000 video clips respectively. Top-1 and</cell></row><row><cell cols="2">top-5 accuracies are reported.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of static, CeN-predicted and non-localbased graphs, as well as combinations of static and dynamic graphs. CeN predicts directed graphs, and CeN* predicts undirected graphs.</figDesc><table><row><cell cols="4">Static Non-local CeN* CeN Accuracy (%)</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>88.2</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>87.1</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>88.1</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>88.6</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>88.4</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>88.7</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>89.2</cell></row><row><cell></cell><cell cols="2">?, ,</cell></row><row><cell cols="2">Flatten &amp; Reshape</cell><cell cols="2">Flatten &amp; Reshape</cell></row><row><cell cols="2">?,  *  , 1,1</cell><cell></cell><cell>?,  *  , 1,1</cell></row><row><cell></cell><cell>Conv-C</cell><cell>Conv-T</cell></row><row><cell>?, , 1,1</cell><cell></cell><cell></cell><cell>?, , 1,1</cell></row><row><cell>?, 1, , 1</cell><cell></cell><cell></cell><cell>?, 1, , 1</cell></row><row><cell></cell><cell>Conv-J</cell><cell>Conv-C</cell></row><row><cell>?, , , 1</cell><cell></cell><cell></cell><cell>?, , , 1</cell></row><row><cell>?, 1, ,</cell><cell></cell><cell></cell><cell>?, 1, ,</cell></row><row><cell></cell><cell>Conv-T</cell><cell>Conv-J</cell></row><row><cell>?, , ,</cell><cell></cell><cell></cell><cell>?, , ,</cell></row><row><cell>?, , ,</cell><cell></cell><cell></cell><cell>?, , ,</cell></row><row><cell></cell><cell>X</cell><cell>X</cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of CeN and two alternative contextencoding architectures. In all experiments the static branch is enabled. ? means that global context modeling is applied.</figDesc><table><row><cell cols="3">Methods Feature Temporal Joint Accuracy (%)</cell></row><row><cell>CeN-F</cell><cell>?</cell><cell>88.5</cell></row><row><cell>CeN-T</cell><cell>?</cell><cell>88.4</cell></row><row><cell>CeN</cell><cell>?</cell><cell>89.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of spatial information, motion information and fusion of all modalities.</figDesc><table><row><cell cols="3">Spatial Fusion Motion Fusion Accuracy (%)</cell></row><row><cell>?</cell><cell></cell><cell>90.9</cell></row><row><cell></cell><cell>?</cell><cell>87.1</cell></row><row><cell>?</cell><cell>?</cell><cell>91.5</cell></row><row><cell cols="3">convolutional layers. The two variants of CeN are referred to as</cell></row><row><cell>CeN-F and CeN-T respectively.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of FLOPs and accuracy with other GCNbased methods.</figDesc><table><row><cell>Method</cell><cell>FLOPs</cell><cell>Accuracy (%)</cell></row><row><cell cols="2">ST-GCN [33]~3.56G</cell><cell>81.5</cell></row><row><cell cols="2">AS-GCN [15]~6.10G</cell><cell>86.8</cell></row><row><cell cols="2">MS-AAGCN [25]~3.98G</cell><cell>88.0</cell></row><row><cell cols="2">MS-G3D Net (1 pathway) [21]~5.21G</cell><cell>89.1</cell></row><row><cell cols="2">MS-G3D Net (2 pathways) [21]~8.32G</cell><cell>89.4</cell></row><row><cell cols="2">Dynamic GCN (w/o CeN)~1.86G</cell><cell>88.2</cell></row><row><cell cols="2">Dynamic GCN (ours)~1.99G (+~7%)</cell><cell>89.2</cell></row><row><cell>(a) Jump</cell><cell>(b) Walking</cell><cell>(c) Wipe Face</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on the NTU-RGB+D dataset in both cross subject and cross view settings in terms of top-1 accuracy (%).</figDesc><table><row><cell>Method</cell><cell cols="3">C-Subject C-View Year</cell></row><row><cell>Lie Group [29]</cell><cell>50.1</cell><cell>82.8</cell><cell>2014</cell></row><row><cell>Deep LSTM [23]</cell><cell>60.7</cell><cell>67.3</cell><cell>2016</cell></row><row><cell>ST-LSTM+TS [18]</cell><cell>69.2</cell><cell>77.7</cell><cell>2016</cell></row><row><cell>TCN [10]</cell><cell>74.3</cell><cell>83.1</cell><cell>2017</cell></row><row><cell>VA-LSTM [34]</cell><cell>79.4</cell><cell>87.6</cell><cell>2017</cell></row><row><cell>ST-GCN [33]</cell><cell>81.5</cell><cell>88.3</cell><cell>2018</cell></row><row><cell>DPRL [28]</cell><cell>83.5</cell><cell>89.8</cell><cell>2018</cell></row><row><cell>HCN [14]</cell><cell>86.5</cell><cell>91.1</cell><cell>2018</cell></row><row><cell>GR-GCN [6]</cell><cell>87.5</cell><cell>94.3</cell><cell>2019</cell></row><row><cell>AGC-LSTM [27]</cell><cell>89.2</cell><cell>95.0</cell><cell>2019</cell></row><row><cell>DGNN [24]</cell><cell>89.9</cell><cell>96.1</cell><cell>2019</cell></row><row><cell>STGR-GCN [12]</cell><cell>86.9</cell><cell>92.3</cell><cell>2019</cell></row><row><cell>AS-GCN [15]</cell><cell>86.8</cell><cell>94.2</cell><cell>2019</cell></row><row><cell>2s-AGCN [26]</cell><cell>88.5</cell><cell>95.1</cell><cell>2019</cell></row><row><cell>MS-AAGCN [25]</cell><cell>90.0</cell><cell>96.2</cell><cell>2020</cell></row><row><cell>MS-G3D Net [21]</cell><cell>91.5</cell><cell>96.2</cell><cell>2020</cell></row><row><cell>Dynamic GCN (ours)</cell><cell>91.5</cell><cell>96.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison on the Skeleton-Kinetics dataset. Both top-1 and top-5 accuracies (%) are reported.</figDesc><table><row><cell>Method</cell><cell cols="3">Top-1 Acc. Top-5 Acc. Year</cell></row><row><cell>Deep LSTM [23]</cell><cell>16.4</cell><cell>35.3</cell><cell>2016</cell></row><row><cell>TCN [10]</cell><cell>20.3</cell><cell>40.0</cell><cell>2017</cell></row><row><cell>ST-GCN [33]</cell><cell>30.7</cell><cell>52.8</cell><cell>2018</cell></row><row><cell>STGR-GCN [12]</cell><cell>33.6</cell><cell>56.1</cell><cell>2019</cell></row><row><cell>AS-GCN [15]</cell><cell>34.8</cell><cell>56.5</cell><cell>2019</cell></row><row><cell>2s-AGCN [26]</cell><cell>36.1</cell><cell>58.7</cell><cell>2019</cell></row><row><cell>MS-AAGCN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">RELATED WORKSSkeleton-based action recognition has been dominated by deep learning-based methods. They have been proven more effective than those methods based on hand-crafted features<ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. We will summarize recent works into two major categories, i.e. DNN-based methods and GCN-based methods.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11492</idno>
		<title level="m">GCNet: Nonlocal Networks Meet Squeeze-Excitation Networks and Beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<title level="m">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Senjian An, Ferdous Sohel, and Farid Boussaid</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatio-Temporal Graph Routing for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMEW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
		<title level="m">NTU RGB+ D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamila</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14111</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Directed Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Hanqing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Ddd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Chih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">View adaptive neural networks for high performance skeletonbased human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Jianru Xue, and Nanning Zheng</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01189</idno>
	</analytic>
	<monogr>
		<title level="m">Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
