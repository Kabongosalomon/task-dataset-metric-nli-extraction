<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FaceController: Controllable Attribute Editing for Face in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FaceController: Controllable Attribute Editing for Face in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face attribute editing aims to generate faces with one or multiple desired face attributes manipulated while other details are preserved. Unlike prior works such as GAN inversion, which has an expensive reverse mapping process, we propose a simple feed-forward network to generate high-fidelity manipulated faces. By simply employing some existing and easy-obtainable prior information, our method can control, transfer, and edit diverse attributes of faces in the wild. The proposed method can consequently be applied to various applications such as face swapping, face relighting, and makeup transfer. In our method, we decouple identity, expression, pose, and illumination using 3D priors; separate texture and colors by using region-wise style codes. All the information is embedded into adversarial learning by our identity-style normalization module. Disentanglement losses are proposed to enhance the generator to extract information independently from each attribute. Comprehensive quantitative and qualitative evaluations have been conducted. In a single framework, our method achieves the best or competitive scores on a variety of face applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Manipulating and editing real faces in the wild has countless applications in the areas of visual effects and e-commerce. Face editing tasks often benefit from the highly disentangled latent representation of face attributes such that one can precisely manipulate corresponding attributes. For example, face swapping, a special case of face editing, aims to preserve the identity of a source face while retaining the pose and expression of a target face. Therefore, achieving fortified disentanglement of face attribute latent codes is the core of many works <ref type="bibr" target="#b48">(Zhang et al. 2019;</ref><ref type="bibr" target="#b38">Shen et al. 2020;</ref><ref type="bibr" target="#b14">Ha et al. 2019;</ref><ref type="bibr" target="#b47">Zeng et al. 2020)</ref>.</p><p>However, deriving such disentangled latent representation from a real face is still challenging. Directly mapping a face into the latent space by an encoder <ref type="bibr" target="#b4">(Chen et al. 2018;</ref><ref type="bibr" target="#b23">Klys, Snell, and Zemel 2018)</ref> can be efficient. But it is not ensured to extract latent codes that express semantically-meaningful factors of variations <ref type="bibr" target="#b29">(Nie et al. 2020</ref>). Thus, another common practice is to obtain the reverse mapping in latent space for a real face, known as GAN inversion. For example, <ref type="bibr" target="#b0">(Abdal, Qin, and Wonka 2019;</ref><ref type="bibr" target="#b49">Zhu et al. 2020a</ref>) opt to embed a real face into the latent space of StyleGAN <ref type="bibr" target="#b20">(Karras, Laine, and Aila 2019)</ref> by optimizing a randomly initialized code through back-propagation. In practice, approaches of this kind are often time-consuming and cumbersome.</p><p>Inspired by recent face synthesis methods such as <ref type="bibr" target="#b12">(Gecer et al. 2018;</ref><ref type="bibr" target="#b8">Deng et al. 2020)</ref>, we propose a simple feedforward face generation network that is fed with some existing and easy-obtainable prior information. As such, this method can avoid the costly learning process of disentangled representation. In this paper, we name our method as FaceController, which enables the control of face attributes. We embed 3DMM coefficients into an adversarial learning framework. However, we observe there exists a domain gap between real faces and the counterpart ones rendered from 3DMM. For example, the makeup of a woman cannot be perfectly expressed by 3DMM. To combat this, we further employ the embedding of identity and region-wise style codes as side information to complement the information gap between 3DMM coefficients and real faces, making our method stand out from previous works.</p><p>In the proposed FaceController, a new identity-style normalization module is proposed to fuse all the prior information. The identity-style normalization module is an extension of SPADE <ref type="bibr" target="#b31">(Park et al. 2019)</ref> in which we use the embedding of identity and region-wise style codes to modulate the feature maps originated from 3DMM coefficients. Furthermore, to make the generator generalize to different editing, we devise unsupervised face generation losses on identity, landmarks, and textures to enforce the generator to control each attribute independently.</p><p>FaceController provides more comprehensive functions of real face editing by precisely controlling the latent codes of a wide range of face attributes. Unlikely previous methods, which mainly address the disentanglement of attributes such as identity and expression, identity and pose, our method is able to disentangle in total five different attributes, including identity, pose, expression, illumination, and local region styles. As a result, FaceController enables a variety of face applications. For example, it can transfer the lip color or eye shadow makeup by editing the region-wise codes; it can also be applied to adjust the illumination of a given face image. In this paper, we empirically validate that the proposed method achieves better quantitative scores and visual results in most of the above applications.</p><p>The main contributions of this paper are: (1) FaceController generates high-quality face with desired attributes using a simple feed-forward face generation network and improves efficiency by employing easily-obtainable prior information rather than the time-consuming reverse mapping process;</p><p>(2) FaceController also increases the freedom of disentangled and controllable dimensions of attributes, making it better to control, transfer, and edit face attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Disentangled representation learning. In face attribute editing, many attempts have been made to learn an independent representation of face attributes. Early attempts include unsupervised disentangled latent representation learning of real face images. For example, InfoGAN <ref type="bibr" target="#b5">(Chen et al. 2016</ref>) and its variants <ref type="bibr" target="#b25">(Lin et al. 2019)</ref> target to maximize the mutual information between latent representation. Holo-GAN (Nguyen-Phuoc et al. 2019) learns 3D representation from natural images to disentangle identity and pose. Style-GAN <ref type="bibr" target="#b20">(Karras, Laine, and Aila 2019)</ref> can synthesize highly photorealistic face images and allow scale-specific modifications to the styles. Variational Autoencoder (VAE) based methods <ref type="bibr" target="#b37">(Shen and Liu 2017;</ref><ref type="bibr" target="#b34">Razavi, van den Oord, and Vinyals 2019;</ref><ref type="bibr" target="#b4">Chen et al. 2018;</ref><ref type="bibr" target="#b40">Sun et al. 2018</ref>) often encourage independence of the latent representation by regularizing their total correlation. These methods achieved a promising performance. However, the disentangled latent codes are not provably identifiable <ref type="bibr" target="#b26">(Locatello et al. 2019)</ref>, and strong regularization on latent representation sometimes may be harmful to generate high-fidelity face images due to information loss <ref type="bibr" target="#b15">(He et al. 2019)</ref>.</p><p>Other attempts aim to extract disentangled latent representation using some explicit or implicit supervision information <ref type="bibr" target="#b2">(Bouchacourt, Tomioka, and Nowozin 2018)</ref>. For example, <ref type="bibr" target="#b41">Tewari et al. 2020)</ref> aim to manipulate the StyleGAN latent space by using 3DMM information as supervision during learning. <ref type="bibr" target="#b38">(Shen et al. 2020)</ref> tries to classify the latent space into multiple linear subspaces with semantic meaning. It is also possible to impose explicit supervision on latent codes <ref type="bibr" target="#b29">(Nie et al. 2020</ref>). Most of these works can manipulate fake faces with high precision. However, in order to edit real faces, an optimization-based embedding process named GAN inversion has to be conducted to infer the best latent code for this face. It is often timeconsuming.</p><p>Facial manipulation using 3D priors. 3DMMs <ref type="bibr" target="#b1">(Blanz, Vetter et al. 1999;</ref><ref type="bibr" target="#b3">Cao et al. 2013)</ref> represent the shape and appearance of faces by projecting them into lowdimensional spaces using techniques such as Principal Component Analysis (PCA). 3DMMs provide an explicit way to independently represent information such as identity, expression, and pose. Therefore, it can be naturally applied to face manipulation. For example, Face2Face <ref type="bibr" target="#b42">(Thies et al. 2016</ref>) models both the driving and source face via a 3DMM; then applies the expression components of driving face to source face. <ref type="bibr" target="#b45">Xu et al. 2020</ref>) try to manipulate expression and pose by modifying 3DMM coefficients. <ref type="bibr" target="#b12">(Gecer et al. 2018</ref>) proposes a semi-supervised adversarial framework for generating photorealistic faces directly from 3DMM. However, we observe that simply deploying 3D priors often results in a domain gap between real faces and generated faces due to the information loss of 3DMM. To minimize this domain gap, we propose an identity-style normalization module to fuse some side information. Consequently, we can obtain more photorealistic faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controllable Attribute Editing</head><p>In this paper, we aim to efficiently edit faces in the wild with desired attributes. We propose a simple feed-forward framework to generate the edited faces directly from some easily obtainable latent representation of attributes. Specifically, denote I as a face image and {a 1 , ? ? ? , a n } as its latent representation of n different attributes. With replacing a i , (i ? 1, ? ? ? , n) by desired latent code a i , we can generate desired face image I and achieve attribute editing.</p><p>To achieve this, FaceController requires at least three stages: 1) an encoding stage that aims to produce attributeindependent latent codes; 2) an attribute editing stage that modifies the produced latent codes to the desired ones through simple replacement; 3) a decoding stage that receives the edited latent codes as input and produces face images that correspond to the edited latent codes.</p><p>FaceController has specific and useful designs for stronger disentanglement of a wide range of face attributes and interpreting them to high-quality and plausible face images that conform to the edited latent codes. Taking face swapping as an example, the architecture of FaceController generator is shown in <ref type="figure">Fig.. 1 (a)</ref>. Each part of FaceController is built with clear and reasonable motivations and narrated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face Attribute Disentanglement</head><p>Among available solutions to perform face attribute disentanglement, 3DMM <ref type="bibr" target="#b11">(Egger et al. 2020</ref>) stands as a nearoptimal option because it disentangles a wide range of different attributes including identity, expression, pose, illumination, and texture. Many previous works  conduct attributes disentanglement based on 3DMM coefficients and show impressive results. However, we notice that the expressive power of its identity and texture coefficients are not sufficient to reconstruct a perceptually photo-realistic face. This drawback attributes to the limited volume of 3D face data where the original 3DMM <ref type="bibr" target="#b9">(Deng et al. 2019b</ref>) is trained, limiting its generalization ability to real-world faces. Considering this, we believe identity embedding and regionwise texture representation should be included to complement the insufficient 3DMM identity and texture representation. A recent work <ref type="bibr" target="#b50">(Zhu et al. 2020b</ref>) particularly centers on region-wise face control while it is not designed for other attributes control. To successfully modulate 3DMM attributes control and region-wise styles control in a unified architecture, we first extract 3DMM coefficients and then  <ref type="figure">Figure 1</ref>: (a) The architecture of FaceController. We take the task of face swapping as an example. The 3DMM coefficients (? s , ? s , ? s , ? s , ? s ) and (? t , ? t , ? t , ? t , ? t ) are extracted from source and target images I s and I t , respectively. First, we transfer the coefficients of identity from source to target and obtain (? s , ? t , ? t , ? t , ? t ). The transferred 3DMM coefficients with the style and identity codes are fed into the generator to synthesis a swapped face. Here, P denotes region-wise pooling; and B denotes the broadcasting process which is the inverted operation of pooling. (b) The architecture of IS block. IS block contains several Identity-Style Normalization modules that integrate all the information of above latent representation. (c) The architecture of IS normalization module. z id and z style represent identity and region-wise style respectively.</p><p>build two encoders that encode more representative identity and region-wise styles, respectively.</p><p>3DMM cofficients extraction. Specifically in a 3DMM, a face is often modeled with 3D shape S and texture T , which is obtained by:</p><formula xml:id="formula_0">S = S + ?I base + ?E base , T = T + ?T base .<label>(1)</label></formula><p>In the above equation, S and T are the average face shape and texture, respectively; I base , E base , and T base are the PCA bases of identity, expression, and texture, respectively; ? ? R 80 , ? ? R 64 , and ? ? R 80 are the corresponding 3DMM coefficients for rendering a 3D face. Besides shape and texture, the illumination ? ? R 27 of the face can be approximated with Spherical Harmonics (SH) and the pose is defined as three rotation angles. For simplicity, we use ? to denote the parameters of rotation and translation. Luckily, (?, ?, ?, ?, ?) can be obtained by an off-the-shelf 3D face reconstruction network <ref type="bibr" target="#b9">(Deng et al. 2019b</ref>) and we use this network as the 3DMM coefficients extractor.</p><p>Identity encoder. As discussed previously, 3DMM is not able to perfectly depict the identity of a wild face. To complement the identity information, we employ an identity encoder to extract identity information from real images. Here, a state-of-the-art pretrained face recognition model <ref type="bibr" target="#b7">(Deng et al. 2019a</ref>) is used as the identity encoder. We use the last feature maps before the final fully-connected (FC) layer to obtain a precise and high-level representation of identity. It is worthy to note that the face images fed into the 3DMM coefficients extractor and identity encoder are aligned in a different way. Then, in our identity encoder, we introduce the spatial transformation network <ref type="bibr" target="#b17">(Jaderberg et al. 2015)</ref> to provide a precisely aligned face for our identity encoder.</p><p>Region-wise style encoder. To support local face region editing, we aim to obtain region-wise style codes by taking advantage of the semantic segmentation of face images. A segmented region that is assigned with the same class label associates with a style code. We adopt the encoder of SEAN <ref type="bibr" target="#b50">(Zhu et al. 2020b</ref>) as our region-wise style encoder for its superior ability to extract region-wise styles. Specifically, a region pooling layer is applied at the end of the encoder for each semantic label to extract a 512 dimensional style code. This region-wise style encoder naturally provides a disentangled representation of style for local face regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face Attribute Remapping</head><p>At this point, we have acquired different sorts of latent codes-3DMM coefficients, identity embedding, and region-wise style codes-for detailed and precise attribute disentanglement. The next is how to leverage these codes and decode them back to a plausible generated face image. Our task is generally related to translate semantic labels to natural face images with specific region-wise styles. SPADE <ref type="bibr" target="#b31">(Park et al. 2019</ref>) is especially useful to this task and some recent works SEAN <ref type="bibr" target="#b50">(Zhu et al. 2020b</ref>) and SMIS <ref type="bibr" target="#b51">(Zhu et al. 2020c</ref>) have built their methods based on SPADE to support region-wise editing. Based on this observation, we also take the spirit of SPADE to design our decoder. However, our task requires the decoder to additionally consider region-irrelevant attributes such as identity. To support this, we design an Identity-Style Normalization module that integrates the identity and region-wise style information to the decoder, shown in <ref type="figure">Fig. 1 (c)</ref>.</p><p>Let F i ? R B?C i ?H i ?W i denote the feature maps of the i-th layer of the decoder; B, C i , H i , and W i represent the batch size, channel numbers, height, and width, respectively. We first perform the batch normalization on F i and get F i .</p><p>To integrate identity information, we map the identity embedding to a vector z id with the same dimension of channel numbers by using an FC layer. After that, two normalization parameters, ? id and ? id , are learned from this vector to modulate F i like a style code. The modulated output is,</p><formula xml:id="formula_1">I i = ? id ? F i + ? id ,</formula><p>(2) where ? id and ? id are with the same size of F i ; we denote ? as the element-wise production.</p><p>To further integrate region-wise style codes, we first extract a style matrix S ? R B?N ?D for region style codes by our region-wise style encoder. We also provide a corresponding segmentation map M i ? R B?N ?H i ?W i which is interpolated from the segmentation map of the input face.</p><p>Here, N denotes the number of semantic classes; D is the dimension of style code for each semantic class. Such segmentation map has N channels as every channel is responsible for a single class. To adaptively adjust a specific local region by its corresponding style code, we broadcast the style code into a feature map according to its segmentation mask. This broadcast process can be achieved by applying a batchversion matrix multiplication between the segmentation map M i and style matrix S, i.e.,</p><formula xml:id="formula_2">z style = S ? M i ,<label>(3)</label></formula><p>where S ? R B?D?N denotes the transposed matrix of S; z style ? R B?D?H i ?W i are the broadcast style codes. Similar to identity normalization, we learn two parameters ? s and ? s from z style to modulate I i , then we have the output of IS normalization module,</p><formula xml:id="formula_3">F o = ? s ? I i + ? s .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Losses</head><p>During training, we mainly consider two different training processes, i.e., face reconstruction and unsupervised face generation training. During face reconstruction training, the model receives attribute latent codes from the same face image and tries to reconstruct such a face. In this case, I s is equal to I t as depicted in <ref type="figure">Fig. 1</ref>. Such training ensures that the generated image is reasonable when I s = I t . However, the model is not ensured to work well when I s = I t . Consequently, we include unsupervised face generation training to guarantee even with unpair input (I s = I t ), the model is still able to translate the combined latent codes to a plausible face image. Note that GAN loss is applied to both training processes for better image fidelity.</p><p>Face Reconstruction Loss. Following FaceShifter <ref type="bibr" target="#b24">(Li et al. 2020)</ref>, we utilize 20% training data for face reconstruction. We implement face reconstruction loss using the perceptual loss <ref type="bibr" target="#b18">(Johnson, Alahi, and Fei-Fei 2016)</ref> to supervise the reconstruction. The intermediate feature maps of the generated face I g and the given face I t are extracted from a pre-trained VGG network (Simonyan and Zisserman 2014), and then we conduct pixel-level reconstruction as follows,</p><formula xml:id="formula_4">L per = 1 2 F per (I t ) ? F per (I g ) 2 ,<label>(5)</label></formula><p>where F per denotes the extractor of feature maps. Unsupervised Face Generation Loss. The main purpose of our work is to support free and dynamic face attribute control over face images. Basically, when we alter some attributes, we wish to see results with these attributes changed accordingly while other attributes are unchanged. We design identity loss, face landmark loss and histogram matching loss to assist the unsupervised training and further fortify attribute disentanglement.</p><p>If we aim to transfer the attributes such as expression and pose from I t to I s while keeping the identity, we can encourage the generated face to keep the same identity with I s by utilizing the identity encoder:</p><formula xml:id="formula_5">L id = 1 ? cos(F id (I g ), F id (I s )).<label>(6)</label></formula><p>Here, L id refers to the identity loss and F id is the extractor of identity embedding. We use cosine similarity to estimate the similarity between the identity embedding of the generated image and the source image. Then, if we want to change attributes like identity and textures from I s to I t but retain the expression and pose, we can leverage the landmark loss to ensure expression and pose consistency between I t and the generated face I g :</p><formula xml:id="formula_6">L lm = 1 2 F lm (I g ) ? F lm (I t ) 2 ,<label>(7)</label></formula><p>where F lm represents the face landmark extractor. In this paper, the landmark loss is also specifically designed for identity preservation. As we want to keep the generated image I g having the same expression and pose with target image I t , a common implementation is to constrain the I g and I t to have the same landmarks. However, face landmarks contain the shape of the eyes, mouth, eyebrows, and nose. These are also linked with identity information. As shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref> and <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, different persons have different face features and landmarks. Consequently, as demonstrated in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>, using landmarks of I t to constrain the generated image is not a good way as we want to preserve identity information of I s . We need to adjust the landmarks to have the same pose and expression with I t while keeping the same identity information with I s . To solve this problem, we utilize the swapped 3DMM coefficients as shown in <ref type="figure">Fig. 1</ref> to extract 3D landmarks. These landmarks often can precisely keep the same face features with I s , seen in <ref type="figure" target="#fig_1">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head><p>FaceShifter DeepFake Ours FaceSwap FSGAN <ref type="figure">Figure 3</ref>: Comparison with DeepFake, FaceSwap, FSGAN, and FaceShifter on FaceForensics++ dataset.</p><p>(d). Then, matching the 3D landmarks of I g and the aligned landmarks can better preserve the identity information.</p><p>Even though identity loss can maintain correct identity, consistency in local textures and colors is still hard to keep. To solve this problem, we further introduce a histogram matching loss <ref type="bibr" target="#b35">(Risser, Wilmot, and Barnes 2017)</ref> to encourage region-wise style consistency with a target face. Let I re = HM(I t , I g ) be a remapped image obtained by the histogram matching between I g and I t . Then, we have</p><formula xml:id="formula_7">L hm = 1 2 I g ? I re 2 .<label>(8)</label></formula><p>For this loss to be minimized, the color distribution of I g is learned to be similar to I t , making local styles of the generated face more similar to the target.</p><p>Full Loss During training, the full loss is defined as a weighted sum of above losses:</p><formula xml:id="formula_8">L = L adv + ? id L id + ? lm L lm + ? hm L hm + ? per L per ,<label>(9)</label></formula><p>where L adv denotes GAN loss. We set the ? id = 10, ? lm = 10000, ? hm = 100, and ? per = 100, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Implementation details. The training images for Face-Controller are collected from CelebA-HQ <ref type="bibr" target="#b19">(Karras et al. 2017)</ref>, FFHQ <ref type="bibr" target="#b20">(Karras, Laine, and Aila 2019)</ref>, and VG-GFace <ref type="bibr" target="#b32">(Parkhi, Vedaldi, and Zisserman 2015)</ref> datasets. The face alignment for 3DMM coefficients extraction is processed according to <ref type="bibr" target="#b9">(Deng et al. 2019b)</ref>. The size of an aligned face is 224 ? 224. For region-wise style codes, we employ an off-the-shelf BiSeNet model <ref type="bibr" target="#b46">(Yu et al. 2018)</ref> to obtain semantic labels, which provides 19 different region categories. The generator and discriminator are trained around 500K steps, respectively. More details can be found in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>Face swapping. We first apply our method to the task of face swapping. Our method is compared with recent stateof-the-art methods including DeepFake <ref type="bibr" target="#b33">(Petrov et al. 2020)</ref>, FaceSwap 1 , FSGAN <ref type="bibr" target="#b30">(Nirkin, Keller, and Hassner 2019)</ref>, and FaceShifter <ref type="bibr" target="#b24">(Li et al. 2020)</ref>.All methods are evaluated on the FaceForensics++ dataset <ref type="bibr" target="#b36">(Rossler et al. 2019)</ref>. As shown in <ref type="figure">Fig. 3</ref>, even though the rival methods are specifically designed for this problem, our method still obtains better or comparable results. Note that some methods, such as Deep-Fake and FSGAN, include a blending process to fuse the swapped faces with the backgrounds of target faces. Such a blending process is unstable and prone to failure, resulting in unpleasant illumination and artifacts. As contrast, our method and FaceShifter discard the blending process and achieve better results. Compared to FaceShifter, our method shows better preservation of identity because we have explicitly enhanced the disentanglement of attributes by carefully designing the representation and learning losses.</p><p>To demonstrate the power of our method on editing diverse face images, we also evaluate our method on face images downloaded from the Internet and faces generated by StyleGAN2 . The results are shown in <ref type="figure">Fig. 4</ref>. We observe that our method achieves very impressive results even when swapping faces with uncommon illumination or textures.</p><p>Region-wise style editing. The disentanglement of region-wise style codes enables our method to achieve high performance in makeup transfer. To validate this, we compare with recent methods such as LADN <ref type="bibr" target="#b13">(Gu et al. 2019</ref>) and SEAN <ref type="bibr" target="#b50">(Zhu et al. 2020b</ref>). All methods are evaluated on MT-dataset <ref type="bibr" target="#b6">(Dantcheva, Chen, and Ross 2012)</ref>. As illustrated in <ref type="figure">Fig. 5</ref>, our method appears to preserve more identity as compared to SEAN. Our method also ensures the fine-grained makeup transfer by utilizing region-wise style codes to adaptively adjust different face regions. This cannot be achieved by LADN, which uses a single entangled code to represent the whole face. To further demonstrate our capability of fine-grained makeup transfer, we show more results in <ref type="figure" target="#fig_3">Fig. 6</ref>. Here, we progressively transfer the styles of eyes, lips, and faces based on the results of the previous column. This can be simply implemented by editing corresponding region-wise style codes. These results also further verify the strong disentanglement of region-wise style codes.</p><p>Face manipulation and relighting. Our method can independently adjust the expression, pose, and illumination of faces. We demonstrate this ability by interpolating the above factors independently. The results are presented in <ref type="figure">Fig 7.</ref> Based on the visual inspection of these results, we observe our method enables precise manipulation of the illumination and expression. To obtain a photorealistic background, we directly embed the background information into the generator rather than enforcing the network to learn how to inpaint. Therefore, The generated image may appear black holes along the edge of the face when the pose varies hugely. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Analysis</head><p>Face swapping. We report the quantitative results on FaceForensics++ dataset and compare our method with DeepFake, FaceSwap, FSGAN, and FaceShifter. We follow the metrics in FaceShifter. We uniformly sample 10 frames from each of 1000 videos and get 10K faces in total. Then, we evaluate the accuracy of identity retrieval (abbrev. Retr.) to check whether the identity of source face is preserved after face swapping; the expression (abbrev. Exp.) and pose error to check whether the swapped face keeps the expression and pose of the target face. We also use FID scores <ref type="bibr" target="#b16">(Heusel et al. 2017)</ref> to evaluate the fidelity of swapped faces. For identity retrieval, we apply CosFace <ref type="bibr" target="#b43">(Wang et al. 2018a)</ref> to extract identity embedding and retrieve the closest face by using cosine similarity. For pose and expression evaluation, we use a pose estimator to estimate head pose and an expression recognition model to extract expression embedding. Then, we report the L2 distance of pose vectors and expression embedding. The pose estimator is the same with that in FaceShifter. Since the expression model used in FaceShifter is not open-source, we employ another model, i.e. <ref type="bibr" target="#b27">(Meng et al. 2019)</ref>, to enable expression error comparison. As shown in Tab. 1, our method achieves the best results in identity preservation and fidelity.</p><p>Region-wise style editing. We evaluate the performance of makeup transfer by users study. We mainly consider three dimensions: (1) the identity preservation;</p><p>(2) the makeup transferability;</p><p>(3) the fidelity of faces. All methods generate 1K images with the same source and reference faces. Five users with rich knowledge in image generation are asked to rate the best result in each case under each metric. Finally, the average scores are reported in Tab. 2. Since LADN is specially designed to disentangle the identity and makeup, it enables the excellent preservation of identity and makeup. But compared with LADN, our method can generate higherquality faces, as shown in <ref type="figure">Fig. 5</ref>. SEAN can perform very well on fine-grained makeup transfer. But it cannot precisely preserve the identity and often contains unnatural textures which affects its scores in all aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Face representation. To verify the necessity of the identity encoder and style encoder, we conduct ablation experiments: the proposed generator with <ref type="formula" target="#formula_0">(1)</ref>     <ref type="figure">Figure 9</ref>: The ablation study about loss functions. Here, Target lm refers we use target image's landmark as ground truth. w/o hm refers to drop out histogram matching loss.</p><p>ple, the results are demonstrated in <ref type="figure">Fig. 8</ref>. If we discard the identity embedding, the generated images have identity gaps with source images, as shown in the third column of <ref type="figure">Fig. 8</ref>. Further, when we discard the style encoder, the generated styles aren't consistent with target image in regions like lips, as shown in the fourth column of <ref type="figure">Fig. 8</ref>. This indicates that 3DMM coefficients don't provide detailed texture for generating high-fidelity faces. The quantitative results are also consistent with the above conclusion. When style information isn't given, the identity is nearly not affected due to the disentangled representation. When identity information isn't provided, the accuracy of identity retrieval severely drops. Note that identity encoder and style encoder are irrelevant to pose and expression change, the pose and expression errors of these two models are very close to our full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss functions.</head><p>To verify the benefit of our landmark and histogram matching loss, we make comparisons with (1) using the target image's landmark as ground truth.</p><p>(2) dropping out the histogram matching loss. The results can be seen in <ref type="figure">Fig. 9</ref>. If we use the target image's landmark as ground truth, the eyes and the eyebrows are not consistent with the source image. The generated image also has artifacts near the landmark's area. If we drop out the histogram matching loss, the generated image's color is not consistent with the target image. As a comparison, our method can generate better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Conclusion</head><p>Currently, our approach has some limitations that can be addressed in follow-up work. As we mentioned above, when changing pose hugely, the generated image may appear black holes. The gaze direction may also be not consistent with the target image. This can be optimized by adding landmarks of the eyeball to constrain the gaze direction.</p><p>In conclusion, we propose a feed-forward network to generate photorealistic faces with desired attributes. This network designs a disentangled face representation and devises unsupervised losses to ensure the control of various face attributes. A variety of face applications can be implemented by this method. <ref type="figure">Figure 10</ref>: The architecture of style encoder. Note "Conv" and "TranConv" mean convolutional layer, transposed convolutional layer respectively. The numbers after "-c" and "s" represent the channel number, stride the corresponding convolution. The kernel size of convolution is 3. "IN" represents instance normalization layer and "LReLU" means leaky ReLU layer.</p><p>Generator. The architecture of generator can be seen in <ref type="figure" target="#fig_5">Fig. 11</ref>. The generator contains eight IS blocks which are mentioned above. We embed the background information into the last two blocks to keep the background unchanged with the target image.</p><p>Discriminator. We utilize the same discriminator architecture with Pix2PixHD <ref type="bibr" target="#b44">(Wang et al. 2018b</ref>) and SPADE <ref type="bibr" target="#b31">(Park et al. 2019</ref>). There exist two multi-scale discriminator with instance normalization (IN) and Leaky ReLU (LReLU). The architecture of our discriminator is shown in <ref type="figure" target="#fig_1">Fig. 12.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.Training details.</head><p>We train the generator and the discriminator about 500K iterations. For the first 10K iteration, we drop out the landmark loss as it is unsteady when the generated face images are unnatural. Subsequently, when the generator can generate   <ref type="figure" target="#fig_1">Figure 12</ref>: The architecture of our discriminator. Note that "Conv" denotes convolutional layer. The numbers after "-c" and "-s" represent the channel number and the stride of the corresponding convolutional layer. The kernel size of convolution is 4. "IN" represents instance normalization layer; "LReLU" detnotes leaky ReLU layer. a face image, the landmark loss will be optimized steadily. The learning rate decay to 0 in the last 100K iterations. We apply spectral norm for the generator and discriminator following SPADE. All experiments are conducted at one P40 GPU and the batch size is 16.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Demonstration of our landmark loss. Our aligned landmarks can better preserve the identity of the source image. (a) Source face with its landmarks. (b) Target face with its landmarks. (c) The swapped face with the landmarks of target face. As we can see, the landmarks and the face image are not well aligned (Zoom view better). (d) The swapped face with our aligned landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Face swapping on different sources of face images. Here, the first three images are generated by StyleGAN2 and others are collected from the Internet. Comparison with other makeup transfer methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Progressive makeup transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>The interpolated results with respect to illumination, expression, and poses from top to bottom. The face swapping results in ablation study. Here, w/o id. refers to our generator without identity encoder; w/o style. refers to our generator without style encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>The architecture of our generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Users study results w.r.t. identity preservation, makeup transfer, and fidelity.</figDesc><table><row><cell>Method</cell><cell>Retr. Pose Exp. FID</cell></row><row><cell>w/o id.</cell><cell>25.97 2.57 0.36 4.12</cell></row><row><cell cols="2">w/o style. 98.21 2.83 0.41 4.84</cell></row><row><cell>Ours</cell><cell>98.27 2.65 0.39 3.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The accuracies of identity retrieval, pose and expression errors, and FID scores in ablation experiments.</figDesc><table><row><cell>Source</cell><cell>Target</cell><cell>Target lm</cell><cell>w/o hm</cell><cell>Ours</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/MarekKowalski/FaceSwap</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Program for Support of Top-notch Young Professionals and the Program for HUST Academic Frontier Youth Team 2017QYTD08, which are given to Dr. Xiang Bai.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix 1.Network architectures.</p><p>Style Encoder. We utilize the same architecture from SEAN <ref type="bibr" target="#b50">(Zhu et al. 2020b</ref>) as our style encoder. The details can be seen in <ref type="figure">Fig. 10</ref>. To obtain the style matrix S, we employ region-wise average pooling at last layer output. Finally, we get S ? R B?N ?D to represent the style codes of each class.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image2StyleGAN: How to embed images into the StyleGAN latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilevel variational autoencoder: Learning disentangled representations from grouped observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face-WareHouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can facial cosmetics affect the matching accuracy of face recognition systems?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BTAS</title>
		<meeting>BTAS</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5154" to="5163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate 3D face reconstruction with weakly-supervised learning: From single image to image set</title>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">3D Morphable Face Models-Past, Present, and Future. TOG</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised adversarial learning to generate photorealistic face images of new identities from 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="217" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LADN: Local adversarial disentangling network for facial makeup and de-makeup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10481" to="10490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mari-oNETte: Few-shot face reenactment preserving identity of unseen targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AttGAN: Facial attribute editing by only changing what you want</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5464" to="5478" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of Style-GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning latent subspaces in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6444" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FaceShifter: Towards high fidelity and occlusion aware face swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06034</idno>
		<title level="m">InfoGAN-CR: Disentangling Generative Adversarial Networks with Contrastive Regularizers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frame attention networks for facial expression recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3866" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HoloGAN: Unsupervised learning of 3D representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7588" to="7597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Debhath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<title level="m">Semi-Supervised StyleGAN for Disentanglement Learning. arXiv arXiv</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FSGAN: Subject Agnostic Face Swapping and Reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7184" to="7193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic Image Synthesis With Spatially-Adaptive Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chervoniy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marangonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Um?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05535</idno>
		<title level="m">DeepFaceLab: A simple, flexible and extensible face swapping framework</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Risser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wilmot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08893</idno>
		<title level="m">Stable and controllable neural texture synthesis and style transfer using histogram losses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FaceForensics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning residual images for face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4030" to="4038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of GANs for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9243" to="9252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Maskaware photorealistic face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08882</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">StyleRig: Rigging StyleGAN for 3D Control over Portrait Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6142" to="6151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep 3D Portrait from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7710" to="7720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiattribute transfer via disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9195" to="9202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">In-domain GAN inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SEAN: Image Synthesis with Semantic Region-Adaptive Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5104" to="5113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantically Multimodal Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5467" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<title level="m">Conv-c64-s2, IN, LReLU Conv-c32-s1, IN, LReLU Conv-c32-s2</title>
		<meeting><address><addrLine>Tanh</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<title level="m">Conv-c128-s2, IN, LReLU TranConv-c32-s2, IN, LReLU Region-wise average pooling</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
