<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Video-centralised Transformer for Video Face Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhi</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jie</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yiming</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
						</author>
						<title level="a" type="main">Self-supervised Video-centralised Transformer for Video Face Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformer in Vision</term>
					<term>Video Face Clustering</term>
					<term>Self-supervised Learning</term>
					<term>Contrastive Learning</term>
					<term>Video Centralised Learning</term>
					<term>Egocentric Video Analysis !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel method for face clustering in videos using a video-centralised transformer. Previous works often employed contrastive learning to learn frame-level representation and used average pooling to aggregate the features along the temporal dimension. This approach may not fully capture the complicated video dynamics. In addition, despite the recent progress in video-based contrastive learning, few have attempted to learn a self-supervised clustering-friendly face representation that benefits the video face clustering task. To overcome these limitations, our method employs a transformer to directly learn video-level representations that can better reflect the temporally-varying property of faces in videos, while we also propose a video-centralised self-supervised framework to train the transformer model. We also investigate face clustering in egocentric videos, a fast-emerging field that has not been studied yet in works related to face clustering. To this end, we present and release the first large-scale egocentric video face clustering dataset named EasyCom-Clustering. We evaluate our proposed method on both the widely used Big Bang Theory (BBT) dataset and the new EasyCom-Clustering dataset. Results show the performance of our video-centralised transformer has surpassed all previous state-of-the-art methods on both benchmarks, exhibiting a self-attentive understanding of face videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE idea of video face clustering can be traced back to the pursuit of automatic labelling of character names in TV shows <ref type="bibr" target="#b0">[1]</ref>. Aiming to classify facial images detected in videos into identity-based clusters without any supervision, video face clustering has becoming increasingly influential and valuable today. The prevalence of easy-to-access video streaming platforms has led to an rapid growth in the amount of public video data that are most relevant to human activities and behaviours. The character-level analysis is essential to understanding those videos, while a successful video face clustering framework can answer the key question of character understanding, Who are those people? without the involvement of any laborious human annotation. This feature can also be extremely useful for other applications such as video captioning <ref type="bibr" target="#b3">[2]</ref>, video summarisation <ref type="bibr" target="#b4">[3]</ref>, content-based video retrieval <ref type="bibr" target="#b5">[4]</ref>, etc.</p><p>Despite the broad applications, video face clustering is still a challenging task. Facial images in real-world videos are typically accomplished with a high level of intra-class variations, e.g. the varying facial appearance due to head pose changes, varying illuminations, different backgrounds, facial expressions, occlusions, and so on. Those facial images that are more difficult to be classified are therefore named as visual distractors <ref type="bibr" target="#b6">[5]</ref>, and how to appropriately cluster those visual distractors stands at the heart of video face clustering. Further challenges can arise from the continuation of videos, i.e. the new detected faces may or may not be seen previously, and certain characters may only be observed once.</p><p>Most relevant works <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref> address those challenges through the utilisation of must-link and cannot-link constraints reasoned automatically from face detection. Unlike face recognition in still images, video face clustering can benefit from the temporal property of a video sequence, based on a simple yet rather effective heuristics. That is, the temporally consecutive facial images with overlapped detection boxes are deemed as from the same person, <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b12">[11]</ref> and they can be grouped together to formulate a face track <ref type="bibr" target="#b13">[12]</ref>. All facial images within a face track can be regarded to have the same identity, which is the socalled must-link. If two face tracks have co-occurred, it is safe to conclude that they are from different persons, given that the two subjects have appeared in the same frame simultaneously. This is the cannot-link constraint. It should be noticed that both must-links and cannot-links can be automatically gathered from face detection in videos, and video face clustering can be considered as a self-supervised problem with such prior knowledge embedded. <ref type="bibr" target="#b14">[13]</ref> Most prior works of video face clustering, therefore, can be roughly divided into two categories based on how they utilise the constraints. The first category of works focuses on the improvement of the clustering stage <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>. Facial embedding for each target facial image is extracted using a certain feature extractor, and the main interest is to better cluster those descriptors with must-links and cannot-links. Some representative solutions include the application of constrained multi-view clustering <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b15">[14]</ref>, constrained 1NN clustering <ref type="bibr" target="#b8">[7]</ref>, Erdos-Renyi clustering <ref type="bibr" target="#b17">[16]</ref>, Hidden Markov Random Fields <ref type="bibr" target="#b12">[11]</ref>, etc. Those works usually do not consider how to learn a better facial representation using the constraints.</p><p>Another family of video face clustering concentrates on the generation of more discriminative and robust facial representations. Despite a few earlier attempts <ref type="bibr" target="#b14">[13]</ref>, this route started to receive attention from the community since deep learning has been popular. A basic video face clustering framework is proposed in <ref type="bibr" target="#b18">[17]</ref>. A pre-trained CNN model is first involved to extract frame-level face descriptors, and then a simple temporal average operator is applied to summarise the descriptors into track-level ones. The clustering is then performed on those track-level representations using Hierarchical Agglomerative Clustering (HAC). In TSiam <ref type="bibr" target="#b7">[6]</ref>, this framework is further extended through the application of contrastive learning <ref type="bibr" target="#b19">[18]</ref> to train a Siamese Multi-Layer Perceptron (MLP) that can generate more discriminative facial representations. The general idea is that all pairs of descriptors selected from the same face track (must-link) are positive samples, while the pairs drawn from two cooccurring face tracks (cannot-link) can be seen as negative ones. A Siamese MLP can therefore be trained with contrastive learning to project facial descriptors into a new latent space, followed by a temporal aggregation and a clustering algorithm like HAC. The pipeline of TSiam is inspiring and can be found in other video face clustering methods like SSiam <ref type="bibr" target="#b7">[6]</ref> and CCL <ref type="bibr" target="#b9">[8]</ref>. The main difference lies in the way of generating contrastive supervisions, i.e. instead of using must-links and cannot-links, SSiam <ref type="bibr" target="#b7">[6]</ref> and CCL <ref type="bibr" target="#b9">[8]</ref> explore how to determine positive/negative samples via mining challenging data and via the usage of FINCH <ref type="bibr" target="#b20">[19]</ref> clustering algorithm, respectively. In those works, Siamese MLP is a popular choice for obtaining more discriminative representations, and its effectiveness has been validated by the superior clustering performance achieved.</p><p>However, the temporal nature of video face clustering still leaves room to improve the Siamese MLP-based framework. As noticed in <ref type="bibr" target="#b21">[20]</ref>, videos consist of both temporallyinvariant and temporally-varying properties. In the domain of video face clustering, those properties can be intuitively understood as the absences and presences of visual distractors, respectively. When a face track only consists of facial images that can be distinguished with ease, for instance, a track of constantly frontal faces without significant changes, it can be illustrated as a temporally-invariant video. However, face clustering in real-world videos will inevitably introduce visual distractors that can be confusing to classifiers, and it is common to find face tracks with multiple visual distractors. As such, the temporally-varying aspects of face track should be more explicitly considered to increase the robustness against visual distractors. The Siamese MLPbased methods, however, follow a temporally-invariant way of constructing track-level representations, i.e. a simple temporal pooling like average, and thus they fail to reflect the temporally-varying attributes in face tracks, which may lead to degraded video clustering performance.</p><p>In this paper, unlike previous works that learn a framelevel representation and then perform temporally-invariant aggregation, we propose to directly obtain a video-level representation for each face track via a transformer model <ref type="bibr" target="#b22">[21]</ref>. Characterised by the self-attention mechanisms, transformers can implicitly capture the global relationships within the data and dynamically adjust the weights of each input signal, and therefore it can be a promising architecture to learn the temporally-variant and temporally-varying properties in videos. Through the usage of the transformer, we are aiming to achieve a video-level understanding of different persons in face videos.</p><p>To learn such a transformer, however, is not a straightforward task, as video face clustering is a self-supervised task that requires clustering-friendly representations. The transformer is first proposed for Natural Language Processing (NLP) tasks <ref type="bibr" target="#b22">[21]</ref>. The recent progress of Vision Transformer <ref type="bibr" target="#b23">[22]</ref> (ViT) has indicated that it can also be applied to visual recognition tasks and can achieve competitive performance against the commonly-used Convolutional Networks (ConvNets). Inspired by ViT, the application of transformers in image-based computer vision tasks has received extensive research interest <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>. There are also some works on video-based transformers <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b32">[31]</ref>. However, self-supervised video understanding with transformers is still limited <ref type="bibr" target="#b33">[32]</ref>. To the best of our knowledge, there are still no studies on how to train a self-supervised video-based transformer that can better benefit the video clustering tasks.</p><p>Inspired by the idea of deep clustering <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b35">[34]</ref>, we propose the video-centralised learning to train a self-supervised transformer for video face clustering, based on the intuitive motivation that each face track should maintain a distinct video centre in the latent space. The general learning paradigms between the previous works and ours are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (Left), contrastive learning <ref type="bibr" target="#b19">[18]</ref> is utilised in <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b9">[8]</ref> to train a Siamese MLP on frame-level facial descriptors. The objective is to minimise the distance between the two positive descriptors in the new latent space, and to maximise the distance between negative ones. A naive extension of this learning paradigm to transformer models is displayed in <ref type="figure" target="#fig_0">Fig. 1</ref> (Middle). The major modification is that the new representation is obtained via a transformer with temporally-augmented clips as input, while the contrastive learning is still conducted in a pairwise way as in <ref type="figure" target="#fig_0">Fig. 1</ref> (Left). Note this paradigm of training a transformer has not yet been adopted by any other video face clustering works.</p><p>Our proposed video-centralised learning is depicted in <ref type="figure" target="#fig_0">Fig. 1 (Right)</ref>. For each face track, we maintain a distinct video centre in the new latent space. When sampling a clip from a face track, the latent representation predicted by a transformer should be attracted to the video centre of that track, and also should be repelled from the centres of tracks that are already known to be negative, e.g. cannot links. With the introduction of video centres, we are enforcing the video representation of the same face track to be more compact and to be more centralised, which can lead to a more clustering-friendly structure in the latent space. This kind of representation is generally more desirable for a clustering problem like video face clustering. Note that there is no Our proposed video-centralised transformer. Each face tack is assigned with a distinct video centre, and the objective is to attract the video representation of each sampled clip to its own centre, while repelling its distances with centres of those cannot-link face tracks. (Best seen in colour) need to pair-wisely sample the data in our video-centralised learning, given the video centres can be learned jointly with the transformer. How to define and learn the video centres is a crucial problem here, and we have explored several potential options to determine the best-working one in this work.</p><p>As an additional contribution, we also investigate video face clustering in egocentric videos, given that most previous works are evaluated on datasets of TV shows or movies. Compared with those videos taken by third-person cameras, egocentric videos <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr" target="#b38">[37]</ref> are usually recorded by first-view wearable devices. Character analysis in egocentric videos is becoming increasingly crucial today with the rapid growth of live video streaming platforms and VR/AR markets. To facilitate the research on egocentric video face clustering, we extract the facial embedding for all the 94 047 face tracks of the recently released EasyCom dataset <ref type="bibr" target="#b39">[38]</ref> using ArcFace <ref type="bibr" target="#b40">[39]</ref>, and annotate each track with humanexamined identities. The extracted face tracks and annotations constitute a complementary dataset of EasyCom <ref type="bibr" target="#b39">[38]</ref> in the field of video face clustering, and it is named as EasyCom-Clustering. EasyCom-Clustering is a large-scale dataset and significantly exceeds the commonly used TV show datasets like Big Bang Theory (BBT) <ref type="bibr" target="#b41">[40]</ref>, Buffy the Vampire Slayer <ref type="bibr" target="#b0">[1]</ref> in terms of annotated face tracks and total duration.</p><p>We mainly compare the performance of the proposed video-centralised transformer with that of three state-ofthe-art video face clustering methods, TSiam <ref type="bibr" target="#b7">[6]</ref>, SSiam <ref type="bibr" target="#b7">[6]</ref>, CCL <ref type="bibr" target="#b9">[8]</ref>, plus a Siamese contrastive transformer ( <ref type="figure" target="#fig_0">Fig.  1</ref> Middle). All approaches are evaluated on two datasets of different video domains, i.e. 1). BBT dataset <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref> of 6 episodes representing TV shows, and 2). our EasyCom-Clustering dataset consisting of 22 egocentric videos. The proposed method has outperformed all baselines on those two datasets under two different clustering scenarios, and it also has exhibited a certain degree of video understanding which is not achievable using baseline methods.</p><p>Overall, we have made the following contributions in this paper:</p><p>? We propose to tackle video face clustering from a video-level perspective to better model the temporally-varying property in face videos. A selfsupervised transformer <ref type="bibr" target="#b22">[21]</ref> is employed to address this issue, which is the first time to the best of our knowledge.</p><p>? A self-supervised video-centralised learning that can generate more clustering-friendly representations is proposed to train the transformer model. The performance achieved on two datasets have demonstrated the effectiveness of the proposed video-centralised transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We also release a large-scale dataset, EasyCom-Clustering, to facilitate the research on egocentric video face clustering. A benchmark evaluation of different methods on this dataset is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Face Clustering</head><p>Face clustering is a computer vision problem with a long view of history <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr">[43]</ref>. Research on image-based face clustering <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b46">[45]</ref>, <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref> aims to reveal the underlying relationships between facial image representations through the usage of Approximate Nearest Neighbours <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b49">[48]</ref>, the graph convolution networks (GCN) <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b51">[50]</ref>, unsupervised clustering methods <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b52">[51]</ref>, etc. Face clustering in images is commonly considered as an unsupervised problem, as it typically employs a pre-trained CNN to extract the facial descriptors to be clustered without any supervision.</p><p>Face clustering in videos is different from image-based face clustering due to the availability of must-link and cannot-link constraints. Given a video with face detection, the two constraints can be reliably exploited using the simple and effective heuristic of consecutive face detection overlapping <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>. Therefore, the major concern in video face clustering is how to incorporate those constraints, and therefore video face clustering can be regarded as a selfsupervised problem. Based on how must-links and cannotlinks are used, the works can be roughly categorised into two groups, constrained clustering methods <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b15">[14]</ref> and contrastive learning approaches <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b42">[41]</ref>. The first group focuses on improving the clustering algorithm in latent space with constraints. Treating face tracks as multiview data, some works adopt Constrained Multi-View Spectral Clustering <ref type="bibr" target="#b15">[14]</ref> or Multi-view Correlation Objective <ref type="bibr" target="#b6">[5]</ref> as solutions. An Erdos-Renyi clustering method is proposed by Jin et al. <ref type="bibr" target="#b17">[16]</ref> based on the idea of ranking counts. Hidden Markov Random Fields is also a popular choice to model such constraints in the clustering space <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b12">[11]</ref>. Recently, Kalogeiton and Zisserman <ref type="bibr" target="#b8">[7]</ref> improve the FINCH clustering <ref type="bibr" target="#b20">[19]</ref> with must-link and cannot link constraints, leading to a Constrained 1NN clustering method. These works usually use a pre-trained feature extractor to obtain facial embedding, without further investigation of the facial representations.</p><p>Another group of video face clustering focuses on how to obtain more discriminative and robust facial representations. The Weighted Block-Sparse Low-Rank Representation is proposed in <ref type="bibr" target="#b14">[13]</ref> to learn a low-rank facial representation with prior knowledge from constraints. A simple video face clustering framework is present by Sharma et al. <ref type="bibr" target="#b18">[17]</ref>, using a pre-trained CNN to extract facial descriptors and performing clustering at temporally aggregated track representations. This framework is further advanced by TSiam and SSiam <ref type="bibr" target="#b7">[6]</ref> based on the idea of contrastive learning <ref type="bibr" target="#b19">[18]</ref>. A Siamese MLP is adopted to learn a more discriminate facial representation from either track-level supervision (TSiam) or the mining of challenging samples (SSiam). Similar ideas can be found in CCL <ref type="bibr" target="#b9">[8]</ref> that utilise weak supervisions generated by FINCH algorithm <ref type="bibr" target="#b20">[19]</ref> to train the Siamese MLP. Video face clustering with unknown cluster number is studied in <ref type="bibr" target="#b42">[41]</ref>, while the authors of <ref type="bibr" target="#b53">[52]</ref> developed an improved triplet loss based on deep metric learning. The Siamese MLP-based methods like TSiam <ref type="bibr" target="#b7">[6]</ref> and CCL <ref type="bibr" target="#b9">[8]</ref> have achieved impressing clustering performance in video face clustering datasets such as Big Bang Theory <ref type="bibr" target="#b41">[40]</ref> or Buffy the Vampire Slayer <ref type="bibr" target="#b0">[1]</ref>. However, the lack of video-level understanding may impair their potentials. In this work, we examine how to obtain video-level facial representation via transformer <ref type="bibr" target="#b22">[21]</ref>.</p><p>Besides, the datasets used by previous works are mostly in a specific video domain, i.e. the TV shows taken with third-person cameras, while the analysis of egocentric videos <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b56">[54]</ref>, <ref type="bibr" target="#b57">[55]</ref> is increasingly receiving attention with the popularity of first-person live video streaming platform, VR/AR devices, etc. However, to the best of our knowledge, there is still no egocentric-based video face clustering dataset so far. In this paper, we present the first largescale egocentric video face clustering dataset, EasyCom-Clustering, to facilitate relevant research, and we also evaluate multiple methods including ours on it as benchmarks. EasyCom-Clustering is a complementary dataset on clustering of the recently released EasyCom <ref type="bibr" target="#b39">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Video Understanding with Transformers</head><p>Transformers <ref type="bibr" target="#b22">[21]</ref> are initially developed for Natural Language Processing (NLP) tasks. The introduction of multihead attention mechanism enables transformers to capture the global dependency in a self-attentive manner. But the performance of transformers on visual tasks was not very satisfying until the pioneering work of Vision Transformers (ViT) <ref type="bibr" target="#b23">[22]</ref>. ViT is a variant of the transformer model that is modified to address general image recognition tasks, and it has achieved very competitive performance against the commonly-used ConvNets. Since ViT, image recognition with transformers has attracted enormous attention, and considerable efforts have been made to improve ViT architectures, including Swin transformer with local attentions <ref type="bibr" target="#b24">[23]</ref>, distillation transformer <ref type="bibr" target="#b25">[24]</ref>, Deeper ViT <ref type="bibr" target="#b26">[25]</ref>, Tokento-Token ViT <ref type="bibr" target="#b28">[27]</ref>, just to name a few of them. We refer the readers to <ref type="bibr" target="#b58">[56]</ref> for more details.</p><p>The application of transformers in the video domain has also been widely investigated, mainly via extending the 2Dbased self-attention mechanism into spatial-temporal ones <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b59">[57]</ref>. TimeSformer <ref type="bibr" target="#b30">[29]</ref> aims to increase the computational efficiency of transformers on spatial-temporal dimension via factorisation. VidTr <ref type="bibr" target="#b32">[31]</ref> shares a similar target of reducing computational complexity and memory usage of video-based transformers through a standarddeviation-based topK pooling. Swin transformer <ref type="bibr" target="#b24">[23]</ref> for image recognition tasks is extended into a Space-Time Swin transformer in <ref type="bibr" target="#b33">[32]</ref> by leveraging 3D shifting windows. Fan et al. <ref type="bibr" target="#b60">[58]</ref> propose a multi-scale ViT that can efficiently operate on videos. However, since video face clustering is a self-supervised problem, our work is more related to the field of self-supervised video understanding.</p><p>Recent works on self-supervised video understanding <ref type="bibr" target="#b33">[32]</ref>, <ref type="bibr" target="#b61">[59]</ref>, <ref type="bibr" target="#b62">[60]</ref>, <ref type="bibr" target="#b63">[61]</ref>, <ref type="bibr" target="#b64">[62]</ref>, <ref type="bibr" target="#b65">[63]</ref> usually follow the popular self-supervised contrastive learning framework like MoCo <ref type="bibr" target="#b66">[64]</ref>, SimCLR <ref type="bibr" target="#b67">[65]</ref>, etc. The general idea is that the video clips sampled (usually with several temporal/spatial augmentations) from the same video are deemed as positive samples, and an InfoNCE loss <ref type="bibr" target="#b61">[59]</ref> which is essentially a cross-entropy loss is adopted to maximise the probability distributions of those positive samples. In this field, however, the application of video-based transformer is comparatively rare, while the Long-Short Temporal Contrastive Learning (LSTCL) <ref type="bibr" target="#b33">[32]</ref> is the most related one, to the best of our knowledge. In LSTCL <ref type="bibr" target="#b33">[32]</ref>, a long clip and a short clip are sampled simultaneously to train a video transformer on several popular self-supervised contrastive learning frameworks. None of those works has ever explored the generation of self-supervised clustering-friendly video representations through transformers. In this work, we take inspirations from deep clustering <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b35">[34]</ref> to train such a transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Clustering</head><p>Research on deep clustering <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b68">[66]</ref>, <ref type="bibr" target="#b69">[67]</ref> typically involves a clustering-guided method to optimise the feature representations, and the assignment of cluster centroids is a crucial idea. There are also some relevant works leveraging the predictions from clustering algorithms as pseudo labels to perform unsupervised learning such as <ref type="bibr" target="#b70">[68]</ref>, but we focus on the idea of centroid-based representations. Deep Embedded Clustering <ref type="bibr" target="#b34">[33]</ref> employs Student's T-distribution as a soft-assignment to measure the similarity between a latent representation and cluster centroids, which allows the gradients to be back-propagated to a deep network. Similar ideas can be found in <ref type="bibr" target="#b35">[34]</ref>. It proposes to optimise cluster centroids and latent representations iteratively to obtain a k-means-friendly representation. The determination of the total cluster number is usually a pre-requisite in those methods, while our video-centralised framework assigns a distinct video centre for each face track, with no need to estimate the cluster number. Our work is also distinguished from others in that we design a clustering-friendly selfsupervised framework for video understanding with transformers for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO-CENTRALISED TRANSFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Backgrounds</head><p>With prior knowledge such as must-links (a face track itself) and cannot-links (two temporally co-occurring face tracks) embedded, video face clustering is essentially a self-supervised problem. For a given video V, let T m = {x m 1 , x m 2 , ..., x m n } be the m-th face track of length n where x m i denotes the i-th facial image, and assuming there are a total of M face tracks. We can denote the video as the collections of those tracks for simplicity, i.e. V = {T 1 , T 2 , ..., T M }, and the target is to classify those face tracks into K disjoint clusters of different identities (IDs).</p><p>Two constraints can be leveraged to address this issue, i.e. the must-link and cannot-link, respectively. The mustlink indicates that two arbitrary facial images drawn from the same track can be deemed to have the common ID. This constraint can be written as a set</p><formula xml:id="formula_0">M = {(x m i , x m j ) | ?x m i , x m j ? T m , i = j, ?T m ? V}. Let 1(x i , x j ) be the indicator function for identity, i.e. 1(x i , x j ) = 1 if x i and x j have the same ID, and 1(x i , x j ) = 0 otherwise. We have 1(x i , x j ) = 1, ?(x i , x j ) ? M.</formula><p>Cannot-links, on the other hand, are derived from the co-occurrences of face tracks. To depict such a constraint, we can define a binary matrix N ? R M ?M where N ab = 1 if T a and T b co-occurs and therefore cannot share an identical ID, and N ab = 0 for the rest. Consequently, we can describe the cannot-link as</p><formula xml:id="formula_1">a set N = {(x a i , x b j ) | ?x a i ? T a , ?x b j ? T b , N ab = 1}, and similarly we know that 1(x i , x j ) = 0, ?(x i , x j ) ? N .</formula><p>To start with, most video face clustering works <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b42">[41]</ref> employ a certain feature extractor to obtain the facial embedding of each detected facial image, typically through a pre-trained backbone ConvNet. Denote the backbone network as CNN, and denote the embedding of a facial image x i as e i ? R e where e is the embedding dimensionality, we have e i = CNN(x i ). Note that this backbone network is usually used as it is and is left untouched. A Siamese MLP, denoted as MLP, is proposed in <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b42">[41]</ref> to project e i into a lower-dimensional latent space, i.e. z i = MLP(e i ) where z i ? R z and z &lt; e, and contrastive learning <ref type="bibr" target="#b19">[18]</ref> is used to train the MLP. There are various ways to constitute the positive/negative samples like SSiam <ref type="bibr" target="#b7">[6]</ref> or CCL <ref type="bibr" target="#b9">[8]</ref>, and we adopt TSiam <ref type="bibr" target="#b7">[6]</ref> that involves must-links and cannotlinks as the example here. An image pair (x i , x j ) is sampled either from the must-link set M or from the cannot-link set N , with their embedding extracted as (e i , e j ), and the contrastive learning loss L CT can be formulated as</p><formula xml:id="formula_2">L CT = y 2 z i ?z j 2 2 + (1 ? y) 2 {max(g ? z i ?z j 2 , 0)} 2 (1) where z i = MLP(e i ), z j = MLP(e j ), y = 1 if (x i , x j ) ? M, y = 0 if (x i , x j ) ? N ,</formula><p>and g &gt; 0 is the margin. During evaluation, the learned MLP is frame-wisely applied to project the extracted embedding of facial images, and a simple temporal aggregation like average is utilised to summarise the projected embeddings of each face track into a single one, followed by a Hierarchical Agglomerative Clustering (HAC) to gain final clustering predictions. Note that the contrastive learning in Eq. 1 is performed at the frame level, and a simple temporal aggregation may ignore the temporally-varying property of the video. To overcome those limitations, we propose to obtain a video-level representation through a video-centralised transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Framework</head><p>The overall framework of the proposed video-centralised transformer is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the major improvement comes from the employment of the transformer and the video-centralised learning. Following the common practice in this field, we first extract the embedding for each facial image in a track T a using a pre-trained ConvNet CNN. This can written as E a = {e a 1 , e a 2 , ..., e a n } where E a represent the track T a at embedding level and e a i = CNN(x a i ). During the training stage ( <ref type="figure" target="#fig_1">Fig. 2</ref> Top), a certain temporal augmentation technique, denoted as ? , is applied on E a to sample a clip E ? a out of it, i.e. E ? a = ? (E a ). The sampled clip is then fed into a transformer encoder to learn the video representation in a self-attentive fashion. We generally follow the transformer encoder architecture in ViT <ref type="bibr" target="#b23">[22]</ref> which stacks several Multiheaded Self-Attention (MSA) layers together with a MLP head on top, but we also have made some modifications to suit our needs. Note that in <ref type="figure" target="#fig_1">Fig. 2</ref> we ignore the LayerNorm (LN) <ref type="bibr" target="#b71">[69]</ref>, MLP layers and residual connections <ref type="bibr" target="#b72">[70]</ref> accompanied with each MSA layer for simplicity. The output of the MLP head is in the new latent space with reduced dimensionality, while the transformer is trained with the proposed video-centralised learning in this space.</p><p>The idea of video-centralised learning is based on the simple intuition that for each track T a , a distinct video centre c a is maintained in the latent space. The transformer-based representation of a sampled clip E ? a should be attracted to c a , which is an implicit way of utilising the must-link constraints. As for the cannot-link constraints, if two face tracks T a and T b co-occur and are known to have exclusive IDs, i.e. N ab = 1, we can push the representation of clip E ? a to be far away from T b 's video centre c b , and similarly for E ? b 's representation and c a . The purpose of video-centralised learning is to enforce the video-level representation from the transformer to be more discriminative, more compact, and most importantly, more centralised.</p><p>During the evaluation stage, we discard the sampling step and make use of the whole track for predictions. As shown in <ref type="figure" target="#fig_1">Fig. 2 (Bottom)</ref>, the facial images are first embedded via the pre-trained ConvNet, which are subsequently input into the transformer without any sampling. The output embedding of the transformer encoder without the MLP head is fetched as the video-level representation, and then a HAC is applied to get the final clustering results, following the practices in <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b9">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Video-level representation with transformer</head><p>The transformer used in our work generally follows the architecture of ViT <ref type="bibr" target="#b23">[22]</ref>, yet several modifications have been made to suit our task. In particular, there is no linear projection layer operating on image patches in our transformer, since we have already used a ConvNet to extract embedding. We also discard the learnable positional embedding in ViT, and the reasons are two-fold. Unlike ViT that split an image into a fixed number of patches and therefore can learn a position embedding with a fixed size, we are working on face tracks with variable lengths that cannot provide a pre-defined positional size. Eliminating the learnable positional embedding allows the acceptance of variable-length sequences and also simplifies the training process. Besides, the information of the temporal orders does not seem to be very essential in this task, considering that temporal augmentations have already been applied. We have actually achieved better performance than state-of-the-art methods without any positional embedding.</p><p>Formally, given a face track T a , and let E a be the set of its facial embedding extracted by a pre-trained ConvNet CNN, i.e. E a = {e a 1 , e a 2 , ..., e a n } where e a i = CNN(x a i ). We first apply a certain temporal augmentation ? to get a clip E ? a . Since the augmentation method is not the focus of this work, we use a simple sampling method ? , i.e. to sample a clip with consecutive frames at arbitrary length, which can be written as</p><formula xml:id="formula_3">E ? a = ? (E a ) = {e a i , e a i+1 , ..., e a i+j }<label>(2)</label></formula><p>where i ? Z &gt;0 and j ? Z &gt;0 are uniformly sampled from [1, n) and [1, n ? i], respectively. Similarly to the [class] token used by BERT <ref type="bibr" target="#b73">[71]</ref> and ViT <ref type="bibr" target="#b23">[22]</ref>, we pre-append a learnable embedding token e 0 to E ? a before feeding it into the transformer encoder, i.e. the input turns into [e 0 , E ? a ], and the state of e 0 at the encoder output side is used as the representation of the track.</p><p>In transformer, each Multiheaded Self-Attention (MSA) layer is accomplished with certain LayerNorm (LN) layers, MLP layer and Residual Connections. Please refer to <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref> for a more detailed illustration. Denote the MSA layer as MSA, the LayerNorm as LN, the MLP layer as MLP, the MLP head at the top as MLH, and assume there are a total of L MSA layers, the video-level representation of the sampled clip E ? a , denoted as z ? a , can be explained as z 0 a = [e 0 , E ? a ] = [e 0 , e a i , e a i+1 , ..., e a i+j ], </p><p>where z L a [0] denotes the state of the encoder's output that is related with the learnable embedding e 0 . With video representation z ? a obtained, we can therefore train the transformer model with the proposed video-centralised learning. Note that we discard the MLP head MLH during the evaluation stage and z L a [0] is fetched as the representation instead. For the simplicity of notations, let f ? be the transformer model described in Eq. 3 -6 where ? denotes its learnable parameters, this process can be simplified as z ? a = f ? (E ? a ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Video-centralised learning</head><p>In this work, we present the video-centralised learning based on the hinge-loss-like contrastive learning <ref type="bibr" target="#b19">[18]</ref> (described in Eq. 1) which is widely adopted by prior works <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b9">[8]</ref>. However, it is worth noticing that the idea of introducing video centres can also be potentially applied to InfoNCE <ref type="bibr" target="#b61">[59]</ref>, a cross-entropy-based loss, with appropriate modifications.</p><p>Let c a be the video centre that is assigned with face track T a in the target latent space, we aims to attract the representations of any sampled clip E ? a to c a . This can be written as L att = z ? a ? c a 2 <ref type="bibr" target="#b8">(7)</ref> where L att denotes the loss from this attraction process. This is essentially the utilisation of the must-links at the video level. Note that there is no need to perform the pairwise sampling as in Eq. 1, as long as c a can be established. As for the cannot-links, let T b be a co-occurring face track with T a , i.e. N ab = 1, and let c b be the video centre of T b . We require the representation of T a to be repelled from c b (and similarly for T b and c a ), which can be described as</p><formula xml:id="formula_6">L rpl = max(g ? z ? a ? c b 2 , 0)<label>(8)</label></formula><p>where L rpl is the loss function for this repelling process, and g &gt; 0 is the margin. The definition of L rpl also eliminates the pre-requisite of pairwise sampling in Eq. 1. Putting Eq. 7 and Eq. 8 together, the video-centralised loss L can be written as</p><formula xml:id="formula_7">L = y 2 L att + (1 ? y) 2 L rpl , = y 2 z ? a ? c a 2 + (1 ? y) 2 max(g ? z ? a ? c b 2 , 0) (9)</formula><p>where y = 1 if z a is to be attracted to its own centre, and y = 0 if z ? a is to be repelled from a negative centre. In theory, L can also have a triplet form, i.e. z ? a being attracted and repelled at the time, but we stick to Eq. 9 in this work for the ease of explanations.</p><p>How to represent the video centre c a is a core issue here. An intuitive way is that we can perform a forward propagation on the whole embedded track E a and use the output as the centre, i.e.</p><p>c a = f ? (E a ).</p><p>This centre representation covers the whole face track and therefore can be more unbiased than using representations of sampled clips. In this work, Eq. 10 is mainly used to initialise or to re-establish the video centres for each face track during the training stage. However, it is computationally infeasible to apply Eq. 10 on a per-training-step basis, since each call of Eq. 10 will require a forward pass of transformers on all face tracks, which will lead to extremely high time complexity considering the number of potential steps. Therefore, we employ a more computationally-efficient way to represent video centres and to optimise the videocentralised loss in Eq. 9. A two-step optimisation process is involved to update the transformer parameters ? and the video centre representations in an iterative manner following <ref type="bibr" target="#b35">[34]</ref>, after those video centres being initialised with Eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimisation</head><p>Following a full forward propagation, the first optimisation step is to minimise L with respect to (w.r.t.) E ? a through updating transformer's parameters ?, while the video centres c a and c b are seen as constant values. This is achieved Algorithm 1 Optimisation of video-centralised learning Require: Training steps T 1 , an interval T 2 &lt; T 1 1: Compute video centres with Eq. 10 2: for t = 1 : T 1 do <ref type="bibr">3:</ref> Update network parameters ? using Eq. 11 <ref type="bibr">4:</ref> Update video centres using Eq. 12 and 13 <ref type="bibr">5:</ref> if t mod T 2 = 0 then <ref type="bibr">6:</ref> Repeat step 1 <ref type="bibr">7:</ref> end if 8: end for through a SGD-based back-propagation and can be depicted as</p><formula xml:id="formula_9">? ? ? ? ?? ? L<label>(11)</label></formula><p>where ? is the learning rate, and ? ? L = ?L ?? = ?L ?z ? a ?z ? a ?? . With c a and c b frozen, we can easily compute the partial derivative component on the left-hand side as</p><formula xml:id="formula_10">?L ?z ? a = y 2 (z ? a ? c a ) z ? a ? c a 2 ? (1 ? y) 2 1(L rpl &gt; 0)(z ? a ? c b ) z ? a ? c b 2 where 1(L rpl &gt; 0) = 1 if L rpl &gt; 0 and 1(L rpl &gt; 0) = 0 otherwise. Another component ?z ? a</formula><p>?? can be obtained with chain rules and is omitted here.</p><p>The second step is to update the video centres with ? frozen. To save computations, we use the pre-computed values of z ? a from the latest forward propagation to perform this update. Particularly, we aim to minimise L w.r.t. c a , c b through a SGD-like algorithm, considering z ? a from the last forward propagation as constant values. The optimisation of c a can be written as</p><formula xml:id="formula_11">c a ? c a ? ?? ca L,<label>(12)</label></formula><formula xml:id="formula_12">? ca L = ?L ?c a = y 2 (c a ? z ? a ) z ? a ? c a 2</formula><p>where ? is the learning rate. Similarly, the update of c b can be described as</p><formula xml:id="formula_13">c b ? c b ? ?? c b L,<label>(13)</label></formula><formula xml:id="formula_14">? c b L = ?L ?c b = (1 ? y) 2 1(L rpl &gt; 0) (z ? a ? c b ) z ? a ? c b 2 .</formula><p>The updating of transformer parameters (Eq. 11) and video centres (Eq. 12 and 13) are performed iteratively during each training step. Since we use the pre-computed z ? a to update video centres in Eq. 12 and 13, the computations are significantly accelerated when compared with Eq. 10. In practice, in addition to the SGD-like updates in Eq. 12 and 13, we also invoke Eq. 10 to regularly re-compute video centres after certain steps to obtain a more unbiased centre representation. The algorithm to optimise the proposed video-centralised learning is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Our experiments are conducted on two video clustering datasets, the Big Bang Theory (BBT) dataset <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref> consisting of six episodes from the first season of the TV show "Big Bang Theory", and our EasyCom-Clustering with 22 egocentric videos recorded in a simulated restaurant setting.</p><p>BBT dataset is initially released by <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b74">[72]</ref> with annotations for six characters from the first-season TV shows Big Bang Theory. This dataset is further enhanced in <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b42">[41]</ref> by annotating more characters. In this work, we use the most recent version of BBT <ref type="bibr" target="#b42">[41]</ref>. There are a total of 6 episodes in it, indexed from "s01e01" to "s01e06" where "s" represents the season and "e" denotes the episode. Each BBT episode lasts for around 20 minutes, with a total of 3 908 annotated face tracks of 232 176 facial images detected from 103 characters. In BBT datasets, episodes are not fully subject-independent with each other, and some characters shown in one episode may still be found in another episode. The average duration of each track is 59 frames, which is approximately 2 seconds for videos at 30 FPS. We evaluate the performance of various methods on each of the six episodes for completeness.</p><p>We also construct the EasyCom-Clustering dataset as another dataset for evaluation. This dataset contains 22 sessions of egocentric recordings collected in a simulated restaurant setting, and each session lasts for around 30 minutes. 15 out of the 22 sessions are already used in the recently released EasyCom <ref type="bibr" target="#b39">[38]</ref> dataset on multi-modal audio-visual analysis under noisy environments. In this work, we make use of all the 22 sessions, and we have detected, embedded and annotated a total of 94 047 face tracks with 1 623 633 facial images from 53 participants, using RetinaFace <ref type="bibr" target="#b75">[73]</ref> as the face detector and Arcface <ref type="bibr" target="#b40">[39]</ref> for facial embedding. Unlike the noisy multi-modal analysis problem in EasyCom, this dataset is exclusively designed for identity-related problems. Besides, EasyCom-Clustering is mostly subject-independent between sessions, i.e. except for one subject that appears in every session, all other characters can only be found in one of the 22 sessions.</p><p>EasyCom-Clustering has significantly different track length distributions when compared with BBT dataset, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The majority duration of BBT face tracks falls into the range between 10 and 150, while most face tracks in EasyCom-Clustering last for less than 20 frames. In fact, the average duration of the face tracks in EasyCom-Clustering is 17, which is significantly less than the 59 of BBT. This is not surprising, considering that the gaze points from the egocentric view can change much frequently than a third-person camera, resulting in shorter face tracks. Similar to BBT, we also evaluate our method and different methods on each of the 22 sessions of EasyCom-Clustering, and those sessions are indexed from "V01" to "V22" in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation Metrics</head><p>In this work, we examine the quality of the video representation under two different clustering settings, i.e. the total cluster number is either 1). known, or 2). unknown. This is achieved by applying different stopping criteria to HAC. For known cluster numbers, the agglomerative merging will be stopped when this cluster number is achieved, while for the case of unknown cluster numbers, a pre-defined distance threshold is required as the stopping criterion.</p><p>We, therefore, adopt slightly different metrics for those two cases. Currently, there are two commonly used metrics in previous works, the Weighted Clustering Purity (WCP) <ref type="bibr" target="#b76">[74]</ref>, <ref type="bibr" target="#b77">[75]</ref> and the Normalised Mutual Information (NMI) <ref type="bibr" target="#b78">[76]</ref>. WCP evaluates the purity of a predicted cluster by assigning it to be the class of its most-frequent samples. As discussed in previous works <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b8">[7]</ref>, WCP is a fair evaluation metric if the cluster number is pre-known and all methods predict the same number of clusters, therefore we also adopt this metric for the known-cluster-number experiments. In addition to WCP, we also employ NMI in this setting. Given a cluster prediction P and the ground-truth Y, the NMI can be computed as NMI = 2 I(Y,P) H(P)+H(Y) where I refers to the mutual information and H is the entropy.</p><p>For unknown cluster experiments, situations are somehow different. The cluster number is no long pre-known, and therefore different algorithms can predict a different number of clusters. The one closest to the true cluster number should be considered as the most desirable. Therefore, we consider the absolute differences between the predicted and true cluster number as an important metric, which will be abbreviated as "#C DIF". As shown in related works <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b42">[41]</ref>, WCP is a biased metric for unknown clustering tasks, e.g. the WCP value will be 1.0 if each sample is assigned with a separate cluster number, and therefore it is not a suitable metric for this experimental setting. NMI, on the other hand, is still a fair metric that can better reveal the trade-offs between cluster number and clustering accuracy. Therefore we incorporate NMI in addition to the absolute differences of cluster number ("#C DIF") to evaluate the clustering performance with unknown cluster numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Baselines</head><p>We mainly compare the performance of our videocentralised transformer (VC TRSF.) with the following five baselines:</p><p>1) The simple video face clustering framework in <ref type="bibr" target="#b18">[17]</ref>, abbreviated as "Temp. AVG", in which the extracted facial embedding of a face track is directly averaged along the temporal dimension without any learning models like MLP. 2) SSiam <ref type="bibr" target="#b7">[6]</ref> that trains a Siamese MLP as in Eq. 1, while the positive/negative samples are generated through the mining of the challenging data.</p><p>3) CCL <ref type="bibr" target="#b9">[8]</ref> which also trains a Siamese MLP with Eq. 1, while the clustering results from FINCH <ref type="bibr" target="#b20">[19]</ref> are used as the pseudo labels. 4) TSiam <ref type="bibr" target="#b7">[6]</ref> that learns the Siamese MLP with Eq. 1 via exploiting the must-links M and cannot links N . 5) A transformer that is trained using the naive extension framework of contrastive learning as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (Middle)</ref>. The training setting is mostly identical to our method, except that it is learned in a pairwise manner similar to Eq. 1 without using video centres. This baseline is abbreviated as "CT TRSF." for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Training Settings</head><p>Following the optimisation process in Algorithm 1, there are essentially two sets of optimisation settings in our system, one for optimising the transformer parameters ? and another for updating video centres. For the former, we utilise SGD with momentum as the optimiser with a weight decay of 1e?5, and leverage the OneCycle learning rate policy <ref type="bibr" target="#b79">[77]</ref> to implement ?. As for the latter, we directly use Eq. 12 and 13 to update the video centres, and its learning rate is defined as ? = p? where p &gt; 0 is a constant number. The intuition is that we should keep the step size of updating video centres to be proportional to that of updating network parameters. There are other ways of designing the video learning rate ?, but we find this way can work the best in practice.</p><p>We train a total of 900 epochs in each training session, with the first 400 epochs as the warm-up stage. The maximum learning rate in the OneCycle scheduler is set to 5.1e?4. A batch size of 512 is used, and a maximum length of 90 is posed on the sampled clip to ensure reasonable memory usages. For each training epoch, we sampled 10 clips out of each face track to be attracted to its own centre, while 16 clips are sampled per track to be repelled to one of its negative video centres. The margin value g in Eq. 9 is empirically set to be 1.0. Besides, we use Eq. 10 to initialise the video centres at the beginning of each training session, while for every 50 epochs we re-compute video centres with Eq. 10.</p><p>We have employed a transformer architecture consisting of 4 MSA layers, and the number of heads in each MSA layer is 16. The input embedding dimension varies between datasets. A 256-dimensional embedding is used on BBT, while the input dimensionality on EasyCom-Clustering is 512. The MLP head consists of a LayerNorm and a Fully-Connected Layer to reduce the dimensionality to 2. All transformer models are trained from scratch without using any pre-trained checkpoints. Since we are working on a self-supervised problem, we use the same video for both training and evaluation. For validation on the BBT dataset, we leverage S-Dbw <ref type="bibr" target="#b80">[78]</ref> computed on the same video as training/evaluation. S-Dbw is a kind of internal clustering index <ref type="bibr" target="#b81">[79]</ref> that can be computed without true clustering labels, and we choose the checkpoint with the best S-Dbw value. As for the EasyCom-Clustering dataset, we simply choose the final checkpoint of each training session for evaluation. For unknown cluster experiments, the thresholds The ablation study on video "V04" of EasyCom-Clustering with known cluster number. The baseline model in the first row is a transformer trained with the paradigm in <ref type="figure" target="#fig_0">Fig. 1 (Middle)</ref>, i.e. a CT TRSF. The option "a" in each component is considered as the naive solution and are gradually replaced by others to illustrate the improvement. to stop the agglomerative merging for different methods are selected on a per-method and per-dataset basis and are determined by fine-tuning on a certain video of each dataset, i.e. video "s01e02" on BBT dataset, and video "V09" on EasyCom-Clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Implementations</head><p>We implement our method in the PyTorch framework <ref type="bibr" target="#b82">[80]</ref>, and all experiments are run on an Amazon AWS machine with multiple A100 GPUs. We re-implement the baseline methods SSiam <ref type="bibr" target="#b7">[6]</ref>, CCL <ref type="bibr" target="#b9">[8]</ref> and TSiam <ref type="bibr" target="#b7">[6]</ref> in PyTorch for a fair comparison, based on the papers and the released code in Matlab. It takes around 7 hours to run a full training session for our video-centralised transformer on a BBT video, while the total training time increases to approximately 23 hours on an EasyCom-Clustering video. We evaluate all methods on each of the 6 BBT videos (or episodes), and similarly on each of the 22 EasyCom-Clustering videos (or sessions), while the overall performance is obtained by averaging the video-level performance on each of the two datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation studies on different components</head><p>We first justify the design of our system framework through an ablation study on a single Easycom-Clustering video, namely "V04", under the setting of known cluster numbers. When changing the design of one component, we freeze others for a fair comparison. A transformer trained with the contrastive learning illustrated in <ref type="figure" target="#fig_0">Fig. 1 (Middle)</ref>, i.e. CT TRSF., is used as the baseline model. We mainly examine four key components of our framework, 1). the introduction of video centres, 2). the temporal augmentation techniques ? , 3). different methods of representing video centres, and 4). the learning rate ? for updating video centres.</p><p>The first component that we study is the introduction of video centres, mainly through the application of videocentralised loss in Eq. 9.</p><p>The next ablation is placed on the temporal augmentation techniques, and we explore two different ways of implementing ? , i.e. a). a uniform sampler that samples uniformly from a temporal interval, and b). a consecutive sampler as in Eq. 2.</p><p>We also show the effectiveness of three different ways to represent video centres c a , which are: a). use the value of z ? a from the last forward propagation as c a , b). the incomplete version of Algorithm 1 which skips step 5-6, i.e. without recomputing video centres with Eq. 10, and c). the complete Algorithm 1.</p><p>For the video centre's learning rate ?, we explore two strategies, which are: a). the simple incremental learning rate as in <ref type="bibr" target="#b35">[34]</ref>, and b). the learning rate that is proportional to the learning rate of ?, i.e. ? = p? where p &gt; 0.</p><p>We consider the option "a" in each key component as the naive solution, and they are gradually replaced by other potential solutions to examine the introduced improvement. Results of the ablation study results are shown in <ref type="table" target="#tab_1">Table  1</ref>. We can see that the initial performance of our videocentralised transformer (VC TRSF.) is not working well with all naive solutions embedded, when compared with the baseline CT TRSF. However, the switching from the uniform sampler to the consecutive one as in Eq. 2 has slightly enhanced the accuracy, while even higher improvement can be introduced via using the incomplete version of Algorithm 1 to represent and to optimise video centres. The complete Algorithm 1 can better cluster the videos, while keeping ? in proportional to ? can significantly improve the performance than a simple incremental ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Clustering Performance</head><p>The performance of different methods on each BBT and EasyCom-Clustering videos with known cluster numbers are shown in <ref type="table" target="#tab_3">Table 2</ref> and 3, respectively. The simple video face clustering framework (Temp. AVG) <ref type="bibr" target="#b18">[17]</ref> typically gives the lowest clustering performance, which is in line with the expectation since no learning process is involved. When compared with the Siamese-based-MLP methods (SSiam, CCL, TSiam), a transformer trained with the naive extension of contrastive loss (CT TRSF.) can already achieve competitive accuracy. With our video-centralised learning, the transformer (VC TRSF.) has outperformed all baselines on 5 out of 6 BBT videos and on 20 out of 22 EasyCom-Clustering videos in terms of WCP, which verifies the advantages of our video-centralised transformer-based representation.</p><p>The evaluation results on videos with unknown cluster number are demonstrated in <ref type="table" target="#tab_5">Table 4</ref> (BBT) and 5 (EasyCom-Clustering), respectively. Different from the evaluation with known cluster numbers, we focus on the accuracy of the predicted cluster number and the achieved NMI. As can be seen, although TSiam is serving a competitive baseline, the best NMIs are still obtained by our VC TRSF. on 4 out of all 6 BBT videos, and on 12 out of all 22 EasyCom-Clustering videos.   The overall performance on two datasets under known/unknown clustering settings is exhibited in <ref type="table">Table 6</ref>. The overall accuracy achieved by Temp. AVG <ref type="bibr" target="#b18">[17]</ref> is lower than other learning-based models. Among all Siamese-MLPbased methods, TSiam is the most competitive one that surpasses the performance of others on both datasets and both clustering scenarios. CT TRSF., on the other hand, has shown comparable clustering results, which is already impressive considering that no previous works have ever examined the potentials of a self-supervised transformer to obtain video-level representation for clustering. Our VC TRSF., however, have exceeded the performance of all other approaches on those two datasets. It can also be observed that the improvement achieved on the EasyCom-Clustering dataset is generally more significant than on BBT videos. This is not a surprising result, considering that each BBT video only consists of a small amount of data (around 650 tracks with 38 thousand facial images). For now, most transformers <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref> are trained on large-scale datasets, such as ImageNet <ref type="bibr" target="#b83">[81]</ref> with more than 1.3 million training images, to achieve performance comparable to state-of-thearts. Our results on BBT videos demonstrate that the transformer can also be used to perform video understanding with small-scale data in a self-supervised fashion. The more significant improvement achieved on EasyCom-Clustering videos (around 4 270 tracks with 73 thousand facial images per video) illustrates that transformers can still benefit from larger-scale data to better fulfil the potentials. Whatever the <ref type="bibr">TABLE 5</ref> Video clustering performance of different methods on EasyCom-Clustering dataset with unknown cluster numbers. The true cluster number of each video is post-appended to the video name. "#C" refers to the predicted cluster number. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>The overall performance of different methods on BBT and EasyCom-Clustering with cluster number known/unknown. "#C DIF" refers to the sum of the absolute differences between the predicted and true cluster number of all videos of the dataset, which is the smaller the better. situation, the proposed video-centralised learning is playing a critical role to guide the transformer towards a more discriminate and more centralised video-level representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Advantages of transformers</head><p>We also investigate why the video-level representation obtained by our VC TRSF. can outperform others through an intuitive visualisation. In <ref type="figure">Fig. 4</ref>, our VC TRSF.'s attention score <ref type="bibr" target="#b84">[82]</ref> on the temporal dimension of several EasyCom-Clustering face tracks are portrayed. The heatmap under each face track stands for the L2-Normalised attention scores, and the deeper it is, the more attentive is on that position. As can be discovered, those frames with potential visual distractors, e.g. face with a certain amount of occlusions, those profile faces that are more difficult to be recognised, images taken under extremely dark illuminations, faces that are partially outside of the frame (those with partially invisible contents), etc. are mostly identified with lower attentive scores on those frames. On the other hand, those images with better visual qualities were generally granted a higher level of attention, and therefore they can better contribute to the generation of video-level representations. This kind of temporal awareness is a unique advantage of the transformer model and cannot be achieved in previous works such as TSiam <ref type="bibr" target="#b7">[6]</ref> or CCL <ref type="bibr" target="#b9">[8]</ref>, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we demonstrate a self-supervised transformer along with the video-centralised learning to address the video face clustering problem. The transformer is employed to obtain the video-level representation for each face track, which is a valuable perspective untouched by previous works. To improve the self-supervised learning for videos, we propose the video-centralised learning to train the transformer, motivated by the idea that each face track should be assigned with a distinct video centre in the latent space. We also release a new dataset, dubbed EasyCom-Clustering, which is the first large-scale video face clustering dataset on egocentric videos. We evaluate the performance of the proposed video-centralised transformer with several state-ofthe-art methods, and the superior performance achieved on two datasets has verified its effectiveness. The experimental results also reveal that the transformer can be applied to a self-supervised video understanding task with smallscale data and can achieve state-of-the-art performance. The proposed video-centralised learning can be inspiring to future research on self-supervised learning and transformer in video-based vision problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustrations of the learning paradigms of different video face clustering methods. "Track A" and "Track B" are the two face tracks of descriptors that are known to be from a different person. Left: Previous works on video face clustering mainly utilised contrastive learning to train a framelevel Siamese MLP. Middle: A naive contrastive learning framework of using transformer model to obtain video-level representation. Temporal augmentation is involved to sample clips from the face track, while the contrastive learning is still performed in a pairwise manner. Right:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of the general training and evaluation framework of the proposed video-centralised transformer. Top: The training framework. For each face track, a ConvNet (CNN) is first employed to extract embedding, followed by a temporal augmentation ? . The sampled clip is fed into a transformer encoder with an MLP head to a new latent space with reduced dimensionality. The video-centralised learned is performed in this new space. Bottom: The evaluation pipeline. The embedding of each face track is also extracted first, while the whole track is input into the transformer encoder without augmentation ? . The output from the encoder is used as the video representation, while the MLP head is also discarded. The final clustering results are obtained by a HAC clustering algorithm. Note that the LayerNorm and MLP layers accompanied with each Multiheaded Self-Attention (MSA) layer of transformer are not shown for compactness. (Best seen in colour)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>z a ) = MSA(LN(z ?1 a )) + z ?1 a , = 1 . . . L (4) z a = MLP(LN((z a ) )) + (z a ) , = 1 . . . L (5) z ? a = MLH(LN(z L a [0]))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The histogram plot of face track lengths in BBT (Left) and EasyCom-Clustering (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yujiang Wang is with Department of Computing, Imperial College London, UK (e-mail: yujiang.wang14@imperial.ac.uk). Mingzhi Dong is with School of Computer Science, Fudan University, China (e-mail: mingzhidong@gmail.com).</figDesc><table /><note>??? Jie Shen is with Department of Computing, Imperial College London, UK (e-mail: jie.shen07@imperial.ac.uk).? Yiming Luo is with Department of Industrial Engineering, Tsinghua University, China (e-mail: luoym18@mails.tsinghua.edu.cn).? Yiming Lin, Pingchuan Ma, Stavros Petridis, and Maja Pantic are with Department of Computing, Imperial College London, UK (e-mail: yiming.lin15@imperial.ac.uk; pingchuan.ma16@imperial.ac.uk; stavros.petridis04@imperial.ac.uk; maja.pantic@gmail.com.? Jie Shen is the corresponding author (e-mail: jie.shen07@imperial.ac.uk).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Video clustering performance of different methods on BBT dataset with known cluster numbers. The true cluster number of each video is post-appended to the video name. AVG [17] 95.74 97.87 87.80 95.45 89.46 96.82 83.74 95.60 91.57 97.52 83.10 91.79 SSiam [6] 97.49 99.39 93.44 98.54 95.92 97.12 88.01 96.25 94.35 97.90 84.87 92.86 CCL [8] 98.06 98.63 96.27 99.51 95.30 97.27 91.03 97.06 96.78 97.71 86.45 92.50 TSiam [6] 97.06 98.48 95.46 99.51 96.91 97.12 91.55 96.08 97.40 98.28 86.30 92.74 CT TRSF. 97.51 99.09 97.32 99.19 93.01 96.52 86.70 95.92 92.63 98.09 84.02 91.79 VC TRSF. (Ours) 98.52 99.39 99.32 99.84 96.60 97.58 91.13 96.41 94.70 98.47 84.91 93.33</figDesc><table><row><cell>BBT (%), Known Cluster Number</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 98.30 92.99 98.61 90.20 95.87 92.96 98.55 97.22 99.46 95.37 99.11 93.15 98.63 98.41 99.72 93.80 98.80 84.35 94.04 86.97 96.38</head><label>3</label><figDesc>Video clustering performance of different methods on EasyCom-Clustering dataset with known cluster numbers. The true cluster number of each video is post-appended to the video name. Methods NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP Temp. AVG [17] 85.96 91.44 45.99 58.37 80.36 94.00 75.31 92.34 53.63 53.63 37.56 54.10 81.04 94.80 61.89 70.88 69.88 91.63 61.36 87.27 55.83 79.48 SSiam [6] 90.93 96.96 79.06 94.18 91.19 98.26 79.76 94.49 80.94 92.84 73.66 90.01 91.07 98.26 74.79 88.99 79.31 92.52 82.87 94.73 81.89 95.07 CCL [8] 94.39 98.45 80.11 94.47 92.79 98.52 83.93 94.36 89.49 97.41 76.73 92.13 92.07 98.39 83.99 93.98 89.38 97.07 75.67 93.20 79.47 94.18 TSiam [6] 95.25 98.79 79.69 93.53 87.65 96.60 84.44 95.03 90.66 97.01 78.29 91.96 90.79 98.09 82.17 92.61 74.80 91.69 83.17 94.05 84.62 95.35 CT TRSF. 90.16 95.33 74.41 91.87 94.40 98.88 85.20 95.19 80.09 91.10 76.46 92.21 91.54 98.05 82.57 94.05 85.99 94.79 81.29 91.89 80.37 93.82 NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP NMI WCP Temp. AVG [17] 66.59 88.63 39.30 71.07 82.68 85.43 38.24 58.15 91.32 98.11 62.60 61.86 82.40 93.91 59.95 64.59 77.97 92.92 60.23 66.50 60.90 76.61 SSiam [6] 83.87 95.30 88.85 97.30 84.11 84.65 79.52 94.31 92.41 98.16 62.06 61.78 92.03 98.45 94.06 98.75 88.29 96.75 82.04 91.67 82.59 94.09 CCL [8] 86.63 96.84 88.71 97.03 89.25 96.25 86.85 96.94 95.45 99.10 61.12 61.86 89.38 97.45 93.86 98.66 89.46 97.80 85.36 93.49 84.44 94.01 TSiam [6] 86.85 96.56 85.36 95.67 89.34 95.59 89.49 97.32 94.84 98.90 91.00 97.72 90.87 97.81 97.62 99.58 89.61 97.70 85.88 93.07 83.82 93.77 CT TRSF. 81.08 93.62 87.76 96.76 89.89 96.25 83.99 95.23 83.60 95.03 73.13 85.65 90.68 97.90 93.41 98.43 90.97 98.07 80.42 91.65 81.20 93.07</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">EasyCom-Clustering (%), Known Cluster Number</cell><cell></cell><cell></cell></row><row><cell></cell><cell>V01 (4)</cell><cell>V02 (3)</cell><cell>V03 (3)</cell><cell>V04 (3)</cell><cell>V05 (4)</cell><cell>V06 (3)</cell><cell>V07 (3)</cell><cell>V08 (4)</cell><cell>V09 (3)</cell><cell>V10 (3)</cell><cell>V11 (3)</cell></row><row><cell></cell><cell>V12 (3)</cell><cell>V13 (3)</cell><cell>V14 (5)</cell><cell>V15 (3)</cell><cell>V16 (3)</cell><cell>V17 (3)</cell><cell>V18 (3)</cell><cell>V19 (3)</cell><cell>V20 (3)</cell><cell>V21 (5)</cell><cell>V22 (4)</cell></row><row><cell>VC TRSF. (Ours)</cell><cell>90.89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>VC TRSF. (Ours) 96.26 99.02 82.25 95.35 94.86 99.05 90.69 97.83 87.39 96.01 79.81 92.43 93.29 98.59 86.70 95.10 98.17 99.75 89.47 97.52 87.12 95.70</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Video clustering performance of different methods on BBT dataset with unknown cluster numbers. The true cluster number of each video is post-appended to the video name. "#C" refers to the predicted cluster number.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">BBT (%), Unknown Cluster Number</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">s01e01 (8) #C NMI #C NMI #C NMI s01e02 (6) s01e03 (26)</cell><cell cols="2">s01e04 (28) #C NMI</cell><cell cols="2">s01e05 (25) #C NMI</cell><cell cols="2">s01e06 (37) #C NMI</cell></row><row><cell>Temp. AVG [17]</cell><cell>7</cell><cell>95.43 8</cell><cell>89.80 11</cell><cell>91.01</cell><cell>13</cell><cell>87.80</cell><cell>11</cell><cell>94.45</cell><cell>22</cell><cell>83.07</cell></row><row><cell>SSiam [6]</cell><cell>7</cell><cell>87.05 6</cell><cell>89.44 16</cell><cell>90.20</cell><cell>10</cell><cell>85.58</cell><cell>8</cell><cell>89.12</cell><cell>9</cell><cell>71.37</cell></row><row><cell>CCL [8]</cell><cell>6</cell><cell>96.45 5</cell><cell>98.55 13</cell><cell>93.54</cell><cell>12</cell><cell>89.60</cell><cell>7</cell><cell>89.15</cell><cell>12</cell><cell>79.53</cell></row><row><cell>TSiam [6]</cell><cell>6</cell><cell>95.59 5</cell><cell>98.55 18</cell><cell>95.24</cell><cell>14</cell><cell>90.83</cell><cell>14</cell><cell>93.66</cell><cell>12</cell><cell>82.62</cell></row><row><cell>CT TRSF.</cell><cell>5</cell><cell>94.80 6</cell><cell>97.32 11</cell><cell>90.81</cell><cell>11</cell><cell>88.84</cell><cell>9</cell><cell>89.25</cell><cell>12</cell><cell>79.20</cell></row><row><cell>VC TRSF. (Ours)</cell><cell>6</cell><cell>97.06 5</cell><cell>98.81 12</cell><cell>94.11</cell><cell>11</cell><cell>91.03</cell><cell>11</cell><cell>92.50</cell><cell>29</cell><cell>84.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>#C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI #C NMI</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">EasyCom-Clustering (%), Unknown Cluster Number</cell></row><row><cell>Methods</cell><cell cols="2">V01 (4)</cell><cell cols="2">V02 (3)</cell><cell cols="2">V03 (3)</cell><cell cols="2">V04 (3)</cell><cell cols="2">V05 (4)</cell><cell cols="2">V06 (3)</cell><cell cols="2">V07 (3)</cell><cell>V08 (4)</cell><cell>V09 (3)</cell><cell>V10 (3)</cell><cell>V11 (3)</cell></row><row><cell>Temp. AVG [17]</cell><cell>2</cell><cell cols="2">61.36 2</cell><cell cols="2">19.46 2</cell><cell cols="2">55.09 2</cell><cell cols="2">69.27 3</cell><cell cols="2">51.58 3</cell><cell cols="2">37.56 3</cell><cell>81.04 3</cell><cell>59.44 2</cell><cell>84.08 2</cell><cell>18.82 1</cell><cell>0.00</cell></row><row><cell>SSiam [6]</cell><cell>5</cell><cell cols="2">92.15 4</cell><cell cols="2">76.76 4</cell><cell cols="2">85.80 5</cell><cell cols="2">82.86 5</cell><cell cols="2">82.71 4</cell><cell cols="2">77.71 4</cell><cell>90.05 6</cell><cell>84.54 4</cell><cell>89.89 5</cell><cell>81.56 4</cell><cell>78.57</cell></row><row><cell>CCL [8]</cell><cell>7</cell><cell cols="2">90.05 9</cell><cell cols="2">71.92 7</cell><cell cols="2">86.21 4</cell><cell cols="4">83.16 10 84.37 7</cell><cell cols="2">74.60 5</cell><cell>88.21 9</cell><cell>84.53 5</cell><cell>91.92 6</cell><cell>78.33 7</cell><cell>78.73</cell></row><row><cell>TSiam [6]</cell><cell>7</cell><cell cols="2">92.31 7</cell><cell cols="2">80.12 7</cell><cell cols="2">85.09 7</cell><cell cols="2">82.16 8</cell><cell cols="2">86.34 7</cell><cell cols="2">84.04 7</cell><cell>88.17 7</cell><cell>83.86 5</cell><cell>86.06 6</cell><cell>83.11 6</cell><cell>81.09</cell></row><row><cell>CT TRSF.</cell><cell>4</cell><cell cols="2">90.16 3</cell><cell cols="2">74.41 3</cell><cell cols="2">94.40 3</cell><cell>85.2</cell><cell>5</cell><cell cols="2">82.37 2</cell><cell cols="2">33.74 3</cell><cell>91.54 6</cell><cell>79.45 3</cell><cell>85.99 4</cell><cell>70.40 3</cell><cell>80.37</cell></row><row><cell>VC TRSF. (Ours)</cell><cell>3</cell><cell cols="2">90.23 3</cell><cell cols="2">82.25 3</cell><cell cols="2">94.86 3</cell><cell cols="2">90.69 3</cell><cell cols="2">81.82 3</cell><cell cols="2">79.81 3</cell><cell>93.29 6</cell><cell>76.73 3</cell><cell>98.17 3</cell><cell>89.47 3</cell><cell>87.12</cell></row><row><cell></cell><cell cols="2">V12 (3)</cell><cell cols="2">V13 (3)</cell><cell cols="2">V14 (5)</cell><cell cols="2">V15 (3)</cell><cell cols="2">V16 (3)</cell><cell cols="2">V17 (3)</cell><cell cols="2">V18 (3)</cell><cell>V19 (3)</cell><cell>V20 (3)</cell><cell>V21 (5)</cell><cell>V22 (4)</cell></row><row><cell>Temp. AVG [17]</cell><cell>2</cell><cell cols="2">48.56 3</cell><cell cols="2">39.30 2</cell><cell cols="2">52.23 1</cell><cell>0.00</cell><cell>2</cell><cell cols="2">46.91 2</cell><cell cols="2">49.97 2</cell><cell>46.79 2</cell><cell>65.42 3</cell><cell>77.97 2</cell><cell>34.74 3</cell><cell>59.21</cell></row><row><cell>SSiam [6]</cell><cell>4</cell><cell cols="2">82.46 6</cell><cell cols="2">86.50 7</cell><cell cols="2">83.37 3</cell><cell cols="2">79.52 3</cell><cell cols="2">92.41 3</cell><cell cols="2">62.06 5</cell><cell>90.18 3</cell><cell>94.06 4</cell><cell>88.08 8</cell><cell>81.82 5</cell><cell>82.60</cell></row><row><cell>CCL [8]</cell><cell>6</cell><cell cols="2">86.59 9</cell><cell cols="4">86.59 11 88.12 5</cell><cell cols="2">83.43 6</cell><cell cols="2">89.31 5</cell><cell cols="2">85.38 7</cell><cell>84.09 5</cell><cell>88.61 6</cell><cell>89.92 9</cell><cell>84.14 11 83.15</cell></row><row><cell>TSiam [6]</cell><cell>5</cell><cell cols="6">82.01 10 77.91 11 91.48 7</cell><cell cols="2">84.81 5</cell><cell cols="2">94.69 3</cell><cell cols="2">91.00 6</cell><cell>84.57 4</cell><cell>92.26 6</cell><cell>85.38 7</cell><cell>84.20 12 84.82</cell></row><row><cell>CT TRSF.</cell><cell>3</cell><cell cols="2">81.08 6</cell><cell cols="2">68.00 8</cell><cell cols="2">81.03 3</cell><cell cols="2">83.99 3</cell><cell cols="2">83.60 3</cell><cell cols="2">73.13 4</cell><cell>81.11 3</cell><cell>93.41 3</cell><cell>90.97 5</cell><cell>80.42 7</cell><cell>70.37</cell></row><row><cell>VC TRSF. (Ours)</cell><cell>3</cell><cell cols="2">90.89 3</cell><cell cols="2">92.99 8</cell><cell cols="2">84.50 2</cell><cell cols="2">72.89 3</cell><cell cols="2">97.22 4</cell><cell cols="2">86.23 3</cell><cell>93.15 3</cell><cell>98.41 4</cell><cell>89.05 7</cell><cell>79.57 7</cell><cell>74.27</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Hello! my name is</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">automatic naming of characters in tv video</title>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The visualisation of temporal attention score predicted by our video-centralised transformer on EasyCom-Clustering face tracks. The deeper the heatmap, the more attention is paid on that frame</title>
		<imprint/>
	</monogr>
	<note>Best seen in colour</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stat: Spatial-temporal attention mechanism for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="241" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video summarization with attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1709" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A systematic review on content-based video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Spola?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S R</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ensina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S R</forename><surname>Coy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">103557</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-face: Selfsupervised multiview adaptation for robust face clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Somandepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hebbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11289</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of face representations for video face clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constrained video face clustering using 1nn relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustering based contrastive learning for improving face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous clustering and tracklet linking for multi-face tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constrained clustering and its application to face clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person spotting: video shot retrieval for face sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on image and video retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weighted block-sparse low rank representation for face clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constrained multi-view video face clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical face clustering using sift image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Antonopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Symposium on Computational Intelligence in Image and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="325" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end face detection and cast grouping in movies using erdos-renyi clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple and effective technique for face clustering in tv series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Computer Vision and Pattern Recognition (CVPR) Workshop: Brave New Motion</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient parameterfree clustering using first neighbor relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tclr: Temporal contrastive learning for video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07974</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deepvit: Towards deeper vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multiscale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Long-short temporal contrastive learning of video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09212</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards kmeans-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3216" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2714" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Easycom: An augmented reality dataset to support algorithms for easy communication in noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tourbabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broyles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04174</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tvd: a reproducible and multiply aligned tv series dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barras</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in LREC 2014, 2014. 3, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video face clustering with unknown number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Clustering appearances of objects under varying illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Names and faces in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<ptr target="II-II.4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Merge or not? learning to group faces via imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A rank-order distance based clustering algorithm for face tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Clustering millions of faces by identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="289" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep density clustering of unconstrained faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8128" to="8137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face clustering: representation and pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1626" to="1640" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1117" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to cluster faces on an affinity graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2298" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A proximity-aware hierarchical clustering of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep metric learning with improved triplet loss for face clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">4d human body capture from egocentric video via 3d scene grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="930" to="939" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Predicting gaze in egocentric video by learning task-dependent attention transition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">You2me: Inferring body pose in egocentric video via first and second person interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9890" to="9900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169,2021.4</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal selfsupervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep clustering with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on neural information processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">knock! knock! who is it?&quot; probabilistic person identification in tv-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>B?uml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2658" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Total cluster: A person agnostic clustering method for broadcast videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sommerlade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing</title>
		<meeting>the 2014 Indian Conference on Computer Vision Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Joint face representation adaptation and clustering in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press Cambridge</publisher>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page" from="1100612" to="1100621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Clustering validity assessment: Finding the optimal partitioning of a data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halkidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2001 IEEE international conference on data mining</title>
		<meeting>2001 IEEE international conference on data mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Understanding of internal clustering validation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="911" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05714</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
