<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Segmentation-assisted Scene Completion for LiDAR Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanlong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Semantic Segmentation-assisted Scene Completion for LiDAR Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Outdoor scene completion is a challenging issue in 3D scene understanding, which plays an important role in intelligent robotics and autonomous driving. Due to the sparsity of LiDAR acquisition, it is far more complex for 3D scene completion and semantic segmentation. Since semantic features can provide constraints and semantic priors for completion tasks, the relationship between them is worth exploring. Therefore, we propose an end-to-end semantic segmentation-assisted scene completion network, including a 2D completion branch and a 3D semantic segmentation branch. Specifically, the network takes a raw point cloud as input, and merges the features from the segmentation branch into the completion branch hierarchically to provide semantic information. By adopting BEV representation and 3D sparse convolution, we can benefit from the lower operand while maintaining effective expression. Besides, the decoder of the segmentation branch is used as an auxiliary, which can be discarded in the inference stage to save computational consumption. Extensive experiments demonstrate that our method achieves competitive performance on SemanticKITTI dataset with low latency. Code and models will be released at https://github.com/jokester-zzz/SSA-SC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene completion plays an important role in autonomous driving, which is a fundamental block of 3D scene understanding. In autonomous driving scenarios, LiDAR is the most commonly used 3D sensor. However, the point cloud collected by a LiDAR is inherently sparse due to its acquisition method, and can only collect data on object surface, which makes it more difficult for machines to infer and understand the scene than humans. Therefore, semantic scene completion task has been proposed to complete the entire scene from limited information, and segment the semantics. <ref type="figure">Fig. 1</ref> is an illustration of outdoor point cloud scene completion.</p><p>Compared with scene completion of depth image, the point cloud scene completion contains much more input data, which will bring greater challenges. Furthermore, since the memory will grows cubically with the increasing with the input voxel resolution, the 3D convolutional neural network also requires noticeable overhead costs <ref type="bibr" target="#b0">[1]</ref>. It was not until the emergence of sparse convolution <ref type="bibr" target="#b1">[2]</ref> that this situation was alleviated. However, submanifold sparse convolution always maintains the sparsity of voxels, it is hard to be directly applied to the task of scene completion. So Zhang et al. <ref type="bibr" target="#b2">[3]</ref> insert a "dense" deconvolution layer in the Sparse <ref type="figure">Fig. 1</ref>: Semantic scene completion on SemanticKITTI dataset. The input is the sparse raw point cloud and the output is the complete semantic scene predicted by our approach.</p><p>Convolutional Network (SCN) to generate new voxels. LM-SCNet <ref type="bibr" target="#b3">[4]</ref> tackles this problem by using a lightweight 2D convolutions followed by a 3D segmentation head block.</p><p>Inspired by <ref type="bibr" target="#b4">[5]</ref> which adopts a completion network after semantic segmentation, we explore the relationship between semantic segmentation and scene completion tasks. For scene completion, semantic features are critical in recovering the complete shape of the specific objects and the entire scene. It's easy to infer the whole shape from an incomplete object by identifying it's category, which means the semantics essentially provide the constraints and priori for completion. Based on this insight, we believe that semantic segmentation tasks and scene completion tasks are inseparable, and semantic segmentation can provide potential semantic information for the scene completion task.</p><p>In this paper, we try to explore the combination of semantic segmentation and scene completion, and benefit from both the 2D and 3D convolutional neural network. The whole network consists of two parts including a 2D completion branch and an assistant 3D segmentation branch. Since 3D dense convolution consumes too much resources, and 3D sparse convolution is difficult to generate new voxels, we use a 2D network on Bird's Eye View (BEV) to complete the scene. It is efficient and easy to diffuse features with 2D convolution. In addition, we introduce the features of the semantic segmentation branch as an auxiliary. We assume features from the semantic segmentation branch as source that can continuously deliver semantic features to the completion branch with combining the advantages of 2D and 3D networks from multi-view fusion. The main contributions of this paper are three-fold:</p><p>? We propose a novel semantic segmentation-assisted scene completion network that leverages the complementary information between BEV map and 3D voxels from multi-views. ? The proposed network can produce reasonable completion results with low latency and memory cost for outdoor 3D scenes by the aid of auxiliary semantic segmentation branch. ? Experiments on SemanticKITTI dataset show that our approach achieves the state-of-the-art performance. We rank the 3 rd in public semantic scene completion benchmark 1 and outperform all the published work in terms of completion metric (IoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we introduce the development of point cloud segmentation and completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D semantic segmentation</head><p>Semantic segmentation on point clouds can be classified into three types: projection-based, voxel-based and pointbased.</p><p>Projection-based works usually project raw point clouds onto various 2D planes. SqueezeSegs <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> and RangeNet++ <ref type="bibr" target="#b8">[9]</ref> project the point cloud onto the spherical image. SalsaNet <ref type="bibr" target="#b9">[10]</ref> organizes point clouds into BEV feature maps. PolarNet <ref type="bibr" target="#b10">[11]</ref> proposes a polar BEV representation, achieving a balanced grid distribution. They all use 2D semantic segmentation networks as the backbone. Voxel-based methods split the raw point clouds into tightly arranged voxels and usually apply 3D convolutions to abstract features. Due to the sparsity, 3D sparse convolution <ref type="bibr" target="#b1">[2]</ref> is widely used and 3D UNet-like <ref type="bibr" target="#b11">[12]</ref> is often adopted as the backbone network for semantic segmentation. Cylinder3D <ref type="bibr" target="#b12">[13]</ref> and Cylinder3D++ <ref type="bibr" target="#b13">[14]</ref> use a cylindrical partition on the point cloud and design an Asymmetric Residual Block and a Dimension-decomposition based Context Modeling based on 3D sparse convolution, which can significantly reduce the computational cost. Zhang et al. <ref type="bibr" target="#b14">[15]</ref> propose a deep fusion network architecture with a unique voxel-based "mini-PointNet" point cloud representation, which fully integrates the features of points and voxels. Some researchers continue the PointNets <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> approach, processing on the original point cloud without intermediate representation. They aim to segment 1 https://competitions.codalab.org/competitions/ 22037#results the point cloud by directly extracting features from 3D points hierarchically to capture the local or global context <ref type="bibr" target="#b17">[18]</ref>, or design a new convolution method <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Among all these methods, the voxel-based method can be best combined with the scene completion task, as it is convenient to indicate whether the space is occupied or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D scene completion</head><p>Given a single frame point cloud or depth map as input, the scene completion work should predict C + 1 labels for each voxel in the 3D space, indicating whether it is occupied and what the semantic category is. The earliest scene completion work is based on depth image proposed by Song et al. <ref type="bibr" target="#b0">[1]</ref>. They formulate an end-to-end 3D ConvNet model (SSCNet) for the volumetric scene completion and semantic labeling with a dilation-based 3D context module. In order to complete the scene, the entire network is composed of dense 3D convolution, which consumes a lot of resources. Most of the subsequent works are based on 3D convolution networks. ScanComplete <ref type="bibr" target="#b20">[21]</ref> introduces a coarse-to-fine inference strategy to produce high-resolution output. SGC <ref type="bibr" target="#b2">[3]</ref> benefits from 3D sparse convolution and adopts Spatial Group Convolutions for efficient processing at the cost of small performance degradation. SATNet <ref type="bibr" target="#b21">[22]</ref> decomposes semantic scene completion tasks into 2D semantic segmentation and 3D scene completion, which are connected by a 2D-3D reprojection layer.</p><p>Until the emergence of the SemanticKITTI dataset <ref type="bibr" target="#b22">[23]</ref>, researchers turn their attention to the semantic scene completion of outdoor point clouds. LMSCNet <ref type="bibr" target="#b3">[4]</ref> designs a lightweight network with a mix of 2D and 3D convolutions, using a 2D UNet backbone followed by a 3D segmentation head. In the case of using only occupied voxels as input, they achieve acceptable results. Rist et al. <ref type="bibr" target="#b23">[24]</ref> produce a continuous scene representation instead of voxelization. This method can predict any position in the scene without the need for spatial discretization. S3CNet <ref type="bibr" target="#b24">[25]</ref> proposes a multiview fusion method that performs semantic completion of the scene in 2D and 3D respectively, and finally uses a dynamic voxel fusion to merge the results. Besides, they leverage LiDAR-based flipped truncated symbol distance function (fTSDF <ref type="bibr" target="#b0">[1]</ref>) calculated from the spherical range image and point-wise normal vectors as spatial encoding. JS3C-Net <ref type="bibr" target="#b4">[5]</ref> adds a SSCNet after semantic segmentation network for completion, and proposes a Point-Voxel Interaction (PVI) module for refinement. However, it cannot perform in realtime due to the cascade architecture.</p><p>In this paper, we propose an end-to-end network that completely uses the network to extract features without manually designed features. We design a 2D scene completion network assisted by a semantic segmentation branch, whose decoder part is discarded in the inference stage to achieve a fast speed while reserving a helpful encoder with 3D sparse convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>The network structure of the proposed method. The upper part of the figure is an auxiliary 3D semantic segmentation branch, and the lower part is a 2D completion branch. Both branches follow the UNet structure and carry out four downsamplings. We merge the first three downsampling results from the semantic segmentation branch into the completion branch at the same level to supplement semantic information. The same color rectangles represent the same downsampling stage, and the numbers in the rectangles represent the feature resolution. In the inference stage, the part in the red dashed line can be discarded to further save memory. (Best viewed in color.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The problem we solve is to infer the entire scene from a single frame point cloud. In this work, we combine 2D and 3D convolutional networks, as illustrated in <ref type="figure">Fig. 2</ref>. We use a 2D network for scene completion since it is far less complex and a 3D segmentation network as an auxiliary branch for the completion branch. The input of the network is a point cloud P with points features f p , which represents the coordinates of points and their corresponding point-wise information. The output is a label of C + 1 categories for each voxel, where C is the number of semantic categories, indicating whether a voxel is free or occupied by a semantic category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scene Completion Branch</head><p>Since the 3D completion network often needs "dense" convolution for dilation, it consumes more resources. In contrast, 2D networks are more lightweight and convenient in diffusing features. Therefore, we adopt a 2D encoderdecoder architecture as the backbone. In order to adapt to the scene completion task, we carry out the Cartesian voxelization instead of the commonly used spherical projection in segmentation. Given the point cloud P ? R N ?3 in the range of [R x , R y , R z ], we voxelize the irregular points into voxels with a resolution of L ? W ? H.</p><p>Here, we adopt a top-down view to generate a BEV feature map with the size of L ? W . Since the BEV map is very similar to image, 2D CNN can be directly applied on it. As with some detection tasks <ref type="bibr" target="#b25">[26]</ref>, we fuse the features of each column along the z-axis. Specifically, we use a simple MLP to learn point features, followed by a max-pooling layer in each column to get the preliminary column feature. After a feature dimension reduction layer, we obtain the column feature.</p><formula xml:id="formula_0">f x,y = A 1 MAX p?Vx,y (M(f p )) .<label>(1)</label></formula><p>A 1 is a dimension reduction layer including a simple Linear layer followed by a ReLU function. M represents an MLP that only contains fully connected layers, batch normalization and ReLU. V x,y denotes the (x, y) th column. f p represents the feature of point p and can be defined flexibly. In our implementation, the point feature f p is defined as:</p><p>f p = (?x, ?y, ?z, x, y, z, r),</p><p>(?x, ?y, ?z) is the coordinate difference between the point p and the center of the voxel it locates while r denotes the reflection intensity. So far, we have obtained a BEV feature of size C f ?L?W that can represent the entire scene, where C f is the feature dimension. Then, we input the obtained BEV features into the 2D completion branch. Unlike LMSCNet <ref type="bibr" target="#b3">[4]</ref> using a 3D segmentation head, we directly output a tensor of size L ? W , which encodes the prediction of each voxel along the height of that location. By reshaping the output tensor, the prediction of each voxel in the scene can be acquired. In order to fully diffuse the features, we use a series of 2D convolution layers to expand the receptive field. The entire network is downsampled four times, and each time the resolution size is reduced by 2 shown in <ref type="figure">Fig.2</ref>. Skip connections with concatenation are used at the same time. However, aggregating features with a 2D <ref type="figure">Fig. 3</ref>: The structure of the assistant semantic segmentation network, where AD stands for asymmetrical downsample block and AU stands for asymmetrical upsample block. network inevitably lacks some information along the z-axis, so we supplement additional features with the aid of the semantic segmentation branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation Branch</head><p>To supplement the height information lost in the 2D convolution, we introduce a 3D sparse convolutional branch. The sparse convolution can aggregate the voxel features effectively and continuously provide semantic features for the completion branch hierarchically. These voxel features from the segmentation branch help the completion task in extending voxels and predicting their semantic categories.</p><p>Similar to the operations in Section III-A, the input of the branch is aggregated voxel features. Based on the Cartesian voxelization, the point-wise features obtained from the same MLP in Eq.1 are reassigned to obtain voxel features. With a max pooling performed in each voxel and another dimension reduction layer A 2 (a Linear layer followed by a ReLU), we obtain the input of the 3D branch.</p><formula xml:id="formula_2">f x,y,z = A 2 MAX p?Vx,y,z (M(f p )) ,<label>(3)</label></formula><p>where V x,y,z denotes the (x, y, z) th voxel. Reusing the same MLP but only adopting different dimension reduction layers can save some resource consumption. Now, we obtain a feature of size C f ? L ? W ? H that represents the entire scene, where C f is the feature dimension and is consistent with Section III-A. Then, we input the obtained features into the 3D segmentation branch.</p><p>To better supplement features for the completion branch with each corresponding layer, we choose the same encoderdecoder structure in this semantic segmentation branch modified from the 3D UNet in Cylinder3D <ref type="bibr" target="#b12">[13]</ref>, which is implemented with 3D sparse convolution. As shown in <ref type="figure">Fig. 3</ref>, both the asymmetrical downsample and upsample blocks in the network contain Asymmetrical Residual Block. The usage of the convolution with a 3 ? 1 ? 3 kernel followed by a 1 ? 3 ? 3 kernel is equivalent to sliding a twolayer network with a conventional 3 ? 3 ? 3 kernel, but can save 33% of memory consumption. The asymmetrical downsample block also halves the resolution size each time, so that the tensors output from the same level of the 3D and 2D network have the same size in terms of length and width. Dimension-decomposition based Context Modeling Block divides high-level context information into low-level features in three dimensions (length, width, height), and aggregates all three low-level activations to obtain features representing a complete context. We use the most frequently appeared semantic label in the raw occupied voxel as supervision.</p><p>We draw semantic features from the encoder part and input them into the completion branch. Specifically, we stack the 3D features along the z-axis and reduce the stacked feature dimension so that it can have the same dimension (C s ? L ? W ) as the 2D feature of the same resolution. Then, we concatenate it with the completion feature in the C s dimension, which can provide features with information along the z-axis for the completion task. It is worth noting that we only use features from the encoder part and the decoder part can be discarded to further reduce the burden of GPU memory and calculation in the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss</head><p>In the experiment, both lovasz loss <ref type="bibr" target="#b26">[27]</ref> and cross-entropy loss are used for the two branches. Lovasz loss is a method for directly optimizing the mean intersection-over-union (mIoU) metric, which is defined as:</p><formula xml:id="formula_3">Loss lovasz = 1 |C| c?C J(e(c)),<label>(4)</label></formula><p>where J is the lovasz extension of IoU and denotes a piecewise linear function with a global minimum, and e(c) is the vector of errors for class c. Cross-entropy loss is widely used and optimizes the accuracy:</p><formula xml:id="formula_4">Loss CE = ? i y i log? i .<label>(5)</label></formula><p>y i and y i are the corresponding predicted and ground truth probability. The total loss is:</p><formula xml:id="formula_5">Loss all = ? 1 Loss seg + ? 2 Loss com .<label>(6)</label></formula><p>In our work, we set ? 1 = ? 2 = 0.5, and the loss for two branches are: Loss seg = Loss lovasz + Loss CE ,</p><formula xml:id="formula_6">Loss com = Loss lovasz + Loss CE .<label>(7)</label></formula><p>In the ablation study, we verify the effectiveness of lovasz loss in the completion branch.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Metrics</head><p>Dataset. We test our method on the public semantic scene completion benchmark SemanticKITTI <ref type="bibr" target="#b22">[23]</ref>. SemanticKITTI is a large-scale LiDAR point cloud dataset with a pointwise annotation collected by a single Velodyne HDL-64E laser scanner. The ground truth semantic labels of the scene completion task are composed of multiple consecutive point cloud frames with annotations. Following the official protocol, we select a single frame of the raw point cloud in the range of [0 ? 51.2m, ?25.6 ? 25.6m, ?2 ? 4.4m] as input, and divide it by 0.2m to obtain voxels with a resolution of 256?256?32. The output is the completed scene in the same area. The dataset contains 22 sequences with 19 semantic categories for training and testing. We use Sequences 0-7, 9-10 (3834 scans) for training, Sequence 8 (815 scans) for validation, and Sequences 11-21 (3901 scans) for testing.</p><p>Metrics. We follow the regulations set by Song et al. <ref type="bibr" target="#b0">[1]</ref> to calculate IoU for scene completion, representing the completion of the scene (not involving semantics), and mIoU for semantic scene completion to measure the semantic segmentation performance over 19 classes of a completed scene. The metrics for semantic segmentation measurement mIoU is defined as:</p><formula xml:id="formula_7">mIoU = 1 C C c=1 T P c T P c + F P c + F N c<label>(8)</label></formula><p>where T P c , F P c , F N c denote the number of true positive, false positive, and false negative predictions for class c respectively, and C denotes the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We augment the point cloud during training by randomly x-y flipping. We adopt the Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with a learning rate of 0.001 (? 1 = 0.9, ? 2 = 0.999) for training, and each epoch is reduced by 2%. All experiments are conducted on a single Nvidia GTX 1080 Ti with 11GB memory with batch size 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Results and Analysis</head><p>In this experiment, we submit the results of our work to the official evaluation server. The results of our method and the state-of-the-art methods are shown in <ref type="table">Table.</ref> I. It is worth noting that our method surpasses all previously published works in terms of completion metrics (IoU). By the time of submission, our method ranks 2 nd in completion metrics (IoU) and 3 rd in semantic completion metrics (mIoU) on the SemanticKITTI benchmark. In addition, we report the single scan inference latency on the entire validation split. Our network can reach a speed of 20.04 FPS with 2629 MB GPU memory when batch size is 1, which is much faster than other methods with comparable performance. In <ref type="table">Table.</ref> I, the numbers in brackets are measured on our device with batch size 1 using the official code.</p><p>Compared with the recently published voxel-based method LMSCNet <ref type="bibr" target="#b3">[4]</ref>, we have better results due to the integration of point cloud information. We have advantages on both IoU and mIoU, especially for mIoU, we have increased by 38.2%. Compared with other integrated point cloud methods, we still have some advantages such as inference time. JS3C-Net <ref type="bibr" target="#b4">[5]</ref> performs semantic segmentation on the point cloud before scene completion. Although it has obtained good mIoU results, it cannot achieve real-time results in inference time. S3CNet <ref type="bibr" target="#b24">[25]</ref> has achieved outstanding results in semantic scene completion, thanks to the geometric-aware loss which makes it have a very good performance on small objects such as bicycle and motorcycle. Nevertheless, it is not satisfied in terms of completion performance and inference time. Our method has better performance on "plane" categories, such as road and sidewalk. It is guessed that the 2D completion network has a better feature extraction effect on these categories and can better expand these features to the surroundings voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>We use the pretrained model and the official code of LMSCNet 2 and JS3C-Net 3 , and visualize the results in comparison with our results on SemanticKITTI validation set in <ref type="figure" target="#fig_1">Fig. 4</ref>. It can be seen that we do have obvious advantages in predicting the "plane" categories, which also verifies the metrics in Tab. I. For some difficult samples, the results obtained by other methods are somewhat distorted, but our results can still get recognizable scenes. In addition, in terms of the completion performance, we can also make up a relatively complete scene better than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>In this part, we perform ablation studies on each part of the network to verify the effectiveness of the proposed method. The experiment results are shown in Tab. II. All experiments are trained on the training set, and the results are evaluated on the validation set.</p><p>The input for all experiments is point clouds. It can be seen that the baseline of our 2D UNet has already achieved good performance, especially for the "plane" categories which indicate that the 2D network is indeed good at feature diffusion. The addition of lovasz loss on this basis has further improved segmentation performance, because it can directly optimize mIoU, though it is not helpful for the completion metrics. Adding an assistant 3D branch on the baseline (no matter supervised by both lovasz loss and cross entropy loss or not) the network retains the ability to segment "plane" categories and also has a better effect on the segmentation of small objects. At the same time, the 3D structure helps to improve the performance of the completion. Finally, the semantic segmentation branch and lovasz loss are added on the baseline together, and the best results are obtained. Compared with the method without segmentation loss supervision, the proposed method obtains better performance because the semantic segmentation branch can provide semantic features instead of only a 3D structure. Furthermore, lovasz loss is also used to optimize the segmentation performance, they can promote each other with the semantic feature extraction and propagation. The lovasz loss allows the network to further improve the effect of small objects and accelerate the convergence, but at the same time, it seems to cause a decline on the completion metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a novel network for scene completion benefits from both 2D and 3D networks. The efficient 2D branch is used for completion. The 3D segmentation branch based on sparse convolution is to provide semantic features for the completion branch, bringing improvements in both completion and segmentation. The decoder of the segmentation branch can be discarded during inference, which will not increase too much computational burden. We also use lovasz loss to improve the network effect and accelerate convergence. Finally, our experiments show that the proposed model has real-time inference speed, and set a state-of-the-art performance on the SemanticKITTI dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of qualitative results with other recent works. Experiments conducted on SemanticKITTI validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of published methods on the official SemanticKITTI<ref type="bibr" target="#b22">[23]</ref> benchmark. Our network surpasses all the published methods in terms of completion metrics (IoU), and ranks 3 rd on the semantic segmentation metrics (mIoU). (* originate from<ref type="bibr" target="#b3">[4]</ref>. The last column of data comes from their paper, the data in brackets are reproduced on our device.)</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicles</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>Approach</cell><cell>IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mIoU</cell><cell>FPS</cell></row><row><cell>*SSCNet [1]</cell><cell>29.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Ablation experiment on SemanticKITTI dataset. The results are conducted on the validation set. / ? 57.44 82.15 65.64 20.90 72.79 43.36 24.98 2.26 39.92 41.77 17.91 0.00 0.00 4.98 41.15 15.02 49.68 0.00 0.00 0.00 14.01 25.89 3.37 ? / ? / ? 56.43 79.17 66.28 22.17 72.13 43.55 31.09 2.10 39.87 45.22 21.35 6.50 4.55 13.90 40.36 17.55 48.27 3.76 0.00 0.00 13.21 14.09 3.72 / ? / 58.30 82.78 66.34 22.75 72.94 43.93 23.94 2.71 40.70 44.48 19.28 4.07 2.83 9.66 42.46 21.24 47.34 2.04 0.72 0.00 17.39 28.02 8.48 / ? ? / ? 58.61 79.64 68.94 22.88 73.31 43.50 23.33 2.67 40.05 44.91 25.57 3.18 3.93 9.62 41.78 18.15 49.22 1.99 0.33 0.00 15.06 27.95 10.10 / ? / ? 57.90 80.35 67.45 23.96 73.11 43.70 24.27 2.85 41.09 46.74 29.54 7.93 8.09 19.84 41.62 21.86 49.77 5.85 1.34 0.00 14.14 18.24 5.26 / / 58.25 78.49 69.31 24.54 72.81 44.31 21.09 4.10 41.48 46.97 39.65 9.18 7.41 19.10 41.86 21.98 49.45 6.32 3.17 0.00 15.20 17.78 4.40</figDesc><table><row><cell>2D 3D</cell><cell>Com CE / lvz</cell><cell>Seg CE / lvz</cell><cell>IoU</cell><cell>Precision</cell><cell>Recall</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicles</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>?</cell><cell cols="2">/ ? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/cv-rits/LMSCNet 3 https://github.com/yanx27/JS3C-Net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lmscnet: Lightweight multiscale 3d semantic completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rold?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV 2020-International Virtual Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03762</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RangeNet++: Fast and Accurate LiDAR Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="926" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cylinder3d: An effective 3d framework for driving-scene lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01550</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10033</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="11" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4578" to="4587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic scene completion using local deep implicit functions on lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emmerichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09141</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">S3cnet: A sparse semantic scene completion network for lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Agia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bingbing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09242</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">705</biblScope>
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The lov?szsoftmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
