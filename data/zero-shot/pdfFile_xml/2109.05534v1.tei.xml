<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021. 2021. Oct. 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aichun</forename><surname>Zhu</surname></persName>
							<email>aichun.zhu@njtech.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Wang</surname></persName>
							<email>zijiewang9928@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xili</forename><surname>Wan</surname></persName>
							<email>xiliwan@njtech.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
							<email>wangtian@buaa.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangqiang</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<email>ghua@cumt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aichun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xili</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangqiang</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><forename type="middle">Hua</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing Tech University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing Tech University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing Tech University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing Tech University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Nanjing Tech University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Nanjing Tech University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">University of Mining and Technology XuZhou</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM Int&apos;l Conference on Multimedia (MM &apos;21)</title>
						<meeting>the 29th ACM Int&apos;l Conference on Multimedia (MM &apos;21) <address><addrLine>China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021. 2021. Oct. 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475369</idno>
					<note>on text-based person retrieval, which will be publicly available at https://github.com/NjtechCVLab/RSTPReid-Dataset. ACM Reference Format: ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems ? Image search;</term>
					<term>Computing method- ologies ? Object identification KEYWORDS person retrieval, text-based person re-identification, cross-modal retrieval, surroundings-person separation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many previous methods on text-based person retrieval tasks are devoted to learning a latent common space mapping, with the purpose of extracting modality-invariant features from both visual and textual modality. Nevertheless, due to the complexity of highdimensional data, the unconstrained mapping paradigms are not able to properly catch discriminative clues about the corresponding person while drop the misaligned information. Intuitively, the information contained in visual data can be divided into person information (PI) and surroundings information (SI), which are mutually exclusive from each other. To this end, we propose a novel Deep Surroundings-person Separation Learning (DSSL) model in this paper to effectively extract and match person information, and hence achieve a superior retrieval accuracy. A surroundings-person separation and fusion mechanism plays the key role to realize an accurate and effective surroundings-person separation under a mutually exclusion constraint. In order to adequately utilize multi-modal and multi-granular information for a higher retrieval accuracy, five diverse alignment paradigms are adopted. Extensive experiments are carried out to evaluate the proposed DSSL on CUHK-PEDES, which is currently the only accessible dataset for text-base person retrieval task. DSSL achieves the state-of-the-art performance on CUHK-PEDES. To properly evaluate our proposed DSSL in the real scenarios, a Real Scenarios Text-based Person Reidentification (RSTPReid) dataset is constructed to benefit future research * Corresponding author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Person retrieval is a basic task in the field of video surveillance, which aims to identify the corresponding pedestrian in a largescale person image database with a given query. Current researches of person retrieval chiefly focus on image-based person retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> (aka. person re-identification), which may sometimes suffer from lacking query images of the target pedestrian in practical application. Considering that in most of the real-world scenes, textual description queries are much more accessible, text-based person retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> has drawn remarkable attention for its effectiveness and applicability.</p><p>As text-based person retrieval involves processing multi-modal data, it can be deemed as a specific subtask of cross-modal retrieval <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. Nevertheless, instead of containing various categories of objects in an image, each image cared by text-based person retrieval contains just one certain pedestrian. The textual description queries, meanwhile, offer much more details about the corresponding person rather than roughly mention the objects in an image. Owing to the particularity of text-based person retrieval, <ref type="figure">Figure 1</ref>: Due to the complexity of high-dimensional data, roughly extracting the person information without proper constraints may miss key clues while fail to drop redundant inferences, which further leads to a mismatched case. Within our proposed Deep Surroundings-person Separation Learning (DSSL) model, the person information is properly separated with the surroundings information, which hence gives a superior retrieval performance. many previous methods proposed on general cross-modal retrieval benchmarks (e.g. Flickr30K <ref type="bibr" target="#b18">[19]</ref> and MSCOCO <ref type="bibr" target="#b12">[13]</ref>) generalize on it poorly. In addition, CUHK-PEDES <ref type="bibr" target="#b11">[12]</ref> is currently the only accessible dataset for text-base person retrieval. It is large in scale and contains images collected from various re-identification datasets under different scenes, view points and camera specifications. Nevertheless, images of each specific person are mostly caught by a same camera under similar conditions of time and space, which is not consistent with the real application scenarios. Therefore, we construct a Real Scenarios Text-based Person Reidentification (RST-PReid) dataset based on MSMT17 <ref type="bibr" target="#b25">[26]</ref> to further train and evaluate the performance of our work, which also benefit future research. For each person, RSTPReid pools 5 images caught by 15 different cameras with complex both indoor and outdoor scene transformations and backgrounds in various periods of time, which makes RSTPReid much more challenging and more adaptable to real scenarios. Extensive experiments on RSTPReid and CUHK-PEDES can better validate the promising accuracy and efficiency of our work.</p><p>The major challenge of text-based person retrieval is to effectively extract and match features from both raw images and textual descriptions. Many previous methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> are devoted to learning a latent common space mapping, with the purpose of extracting modality-invariant features from both the visual and textual modalities. These proposed approaches are mainly based on the assumption that through a latent common space mapping, the intersection of information carried by the two modalities, namely, information of the targeting person can be retained into extracted modality-invariant common features. Nevertheless, due to the complexity of high-dimensional data, the unconstrained mapping paradigms are not able to properly catch discriminative clues about the corresponding person while drop the misaligned information (shown in <ref type="figure">Fig. 1</ref>).</p><p>Intuitively, information contained in visual data can be divided into person information (PI) and surroundings information (SI), which are mutually exclusive from each other. Meanwhile, the given textual description queries commonly describe the gender, appearance, clothing, carry-on items, possible movement, etc. of a certain pedestrian. In most of the real scenarios, the describer who offers a query nearly knows nothing about what kind of surroundings the target person is exactly in when captured by surveillance cameras, where the light conditions, viewpoints, etc. can be varied. Therefore, the given textual description basically contains only person information and there is no surroundings information included. On account of the structure of natural language sentences, noise signals (NS) like semantically irrelevant words and incorrect grammar are also inevitably included. Based on the above discussion, an efficient algorithm to accurately separate person and surroundings information in visual data and properly denoise features extracted from textual data is essential to enhance the retrieval performance.</p><p>To this end, we propose a novel Deep Surroundings-person Separation Learning (DSSL) model in this paper to effectively extract and match person information, and hence achieve a superior retrieval accuracy. DSSL takes raw images and textual descriptions as input and first extracts global and fine-grained local information from both modalities. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, DSSL aims to properly separate surroundings and person information. To achieve this goal, a novel Surroundings-Person Separation Module (SPSM) is proposed to split the visual information as person and surroundings features (denoted as and ) in a mutually exclusive manner. Then we adopt a Signal Denoising Module (SDM) to denoise and refine the extracted person feature (denoted as ) from the textual modality. As discussed above, ideally the person features and are purely about the target person without modality-specific interference. Hence the alignment between them ( ) can be regarded as matching the pedestrian cut out of the gallery image with the pedestrian in mind of the describer. In addition, through a proposed Surroundings-Person Fusion Module (SPFM), is fused with and reconstructed into the visual modality as . Then an alignment between and the visual feature before partitioned by SPSM ( ) is conducted, which can be viewed as placing the described person into the same surroundings as the gallery person and then matching it with the complete gallery image including the surroundings in the visual modality space. Besides, a Person Describing Module (PDM) is employed to reconstruct into textual modality as , which is then aligned with the nonrefined textual feature ( ). This proposed alignment can be regarded as describing the person in the gallery image with a text and then matching the text with the given query sentence in the textual modality space. Due to the mutually exclusion constraint in SPSM, and are orthogonal to each other, so the visual information is distributed between them without overlap. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, during the training process, and will form a constraint which forces to contain more complete information about the person. Based on the mutually exclusion precondition, person information in will accordingly be taken away into . Meanwhile, is conducted by putting the described person into the surroundings of the gallery person, which requires the surroundings information to be properly included in while peeled off in . As a result, these three alignments work in complementary to guide the correct information exchange between and under a mutually exclusion constraint, and finally lead to an accurate and effective surroundings-person separation. To adequately exploit fine-grained clues, a cross-modal attention (CA) mechanism and when training DSSL. During the training process, and will form a constraint which forces more complete information about the person to be contained in the person feature . Based on the mutually exclusion precondition, person information in the surroundings feature will accordingly be taken away into . Meanwhile, is conducted by putting the described person into the surroundings of the gallery person, which requires the surroundings information to be properly included in while peeled off in . As a result, these three alignments work in complementary to guide the correct information exchange under a mutually exclusion constraint, which finally leads to the change of extracted information from bad to good and an accurate and effective surroundings-person separation. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> is utilized to further align a local feature matrix extracted from one modality with the global feature in the other ( and shown in <ref type="figure" target="#fig_1">Fig. 3</ref>). Our contributions can be summarized as five folds: (1) A novel Deep Surroundings-person Separation Learning (DSSL) model is proposed to properly extract and match person information. A proposed surroundings-person separation and fusion mechanism plays the key role to realize an accurate and effective surroundings-person separation under a mutually exclusion constraint. (2) Five diverse alignment paradigms are adopted to adequately utilize multi-modal and multi-granular information and hence improve the retrieval accuracy. (3) A Signal Denoising Module (SDM) is employed to denoise and refine the extracted person feature from the textual modality. (4) Extensive experiments are carried out to evaluate the proposed DSSL on CUHK-PEDES <ref type="bibr" target="#b11">[12]</ref>. DSSL outperforms previous methods and achieves the state-of-the-art performance on CUHK-PEDES. (5) A Real Scenarios Text-based Person Reidentification (RSTPReid) dataset is constructed to benefit future research on text-based person retrieval, which will be publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Person Re-identification</head><p>Person re-identification has drawn increasing attention in both academical and industrial fields, and deep learning methods generally plays a major role in current state-of-the-art works. Yi et al. <ref type="bibr" target="#b28">[29]</ref> firstly proposed deep learning methods to match people with the same identification. Hou et al. <ref type="bibr" target="#b6">[7]</ref> proposed an Interaction-and-Aggregation (IA) Block, which consists of Spatial Interaction-and-Aggregation (SIA) and Channel Interaction-and-Aggregation (CIA) Modules to strengthen the representation capability of the deep neural network. Xia et al. <ref type="bibr" target="#b26">[27]</ref> proposed the Second-order Non-local Attention (SONA) Module to learn local/non-local information in a more end-to-end way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text-based Person Retrieval</head><p>Text-based person retrieval aims to search for the corresponding pedestrian image according to a given text query. This task is first put forward by Li et al. <ref type="bibr" target="#b11">[12]</ref> and they take an LSTM to handle the input image and text. An efficient patch-word matching model <ref type="bibr" target="#b2">[3]</ref> is proposed to capture the local similarity between image and text. Jing et al. <ref type="bibr" target="#b7">[8]</ref> utilize pose information as soft attention to localize the discriminative regions. Niu et al. <ref type="bibr" target="#b17">[18]</ref> propose a Multigranularity Image-text Alignments (MIA) model exploit the combination of multiple granularities. Nikolaos et al. <ref type="bibr" target="#b16">[17]</ref> propose a Text-Image Modality Adversarial Matching approach (TIMAM) to learn modality-invariant feature representation by means of adversarial and cross-modal matching objectives. Besides that, in order to better extract word embeddings, they employ the pre-trained publicly-available language model BERT. An IMG-Net model is proposed by Wang et al. <ref type="bibr" target="#b24">[25]</ref> to incorporate inner-modal self-attention and cross-modal hard-region attention with the fine-grained model for extracting the multi-granular semantic information. Liu et al. <ref type="bibr" target="#b13">[14]</ref> generate fine-grained structured representations from images and texts of pedestrians with an A-GANet model to exploit semantic scene graphs. A new approach CMAAM is introduced by Aggarwal et al. <ref type="bibr" target="#b0">[1]</ref> which learns an attribute-driven space along with a class-information driven space by introducing extra attribute annotation and prediction. Zheng et al. <ref type="bibr" target="#b29">[30]</ref> propose a Gumbel attention module to alleviate the matching redundancy problem and a hierarchical adaptive matching model is employed to learn subtle feature representations from three different granularities. Recently, the NAFS proposed by Gao et al. <ref type="bibr" target="#b4">[5]</ref> is designed to extract full-scale image and textual representations with a novel staircase CNN network and a local constrained BERT model. Besides, a multimodal re-ranking algorithm by comparing the visual neighbors of the query to the gallery (RVN) is utilized to further improve the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we describe the proposed Deep Surroundings-person Separation Learning (DSSL) model in detail (shown in <ref type="figure" target="#fig_1">Fig. 3</ref>), which consists of a Surroundings-Person Separation Module (SPSM), a Surroundings-Person Fusion Module (SPFM), a Signal Denoising Module (SDM), a Person Describing Module (PDM) and a Salient Attention Module (SAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction And Refinement</head><p>3.1.1 Feature Extraction. We utilize a ResNet-50 <ref type="bibr" target="#b5">[6]</ref> backbone pretrained on ImageNet to extract global/local visual features from a given image . To obtain the global feature ? R , the feature map before the last pooling layer of ResNet-50 is down-scaled to a vector ? R 1?1?2048 with an average pooling layer and then passed through a group normalization (GN) layer followed by a fully-connected (FC) layer. In the local branch, the same feature map is first horizontally -partitioned by pooling it to ? 1 ? 2048, and then the local strips are separately passed through a GN and two FCs with a ReLU layer between them to form -dim vectors, which are finally concatenated to obtained the local visual feature matrix ? R ? . For textual feature extraction, we take a whole sentence and the phrases extracted from it as textual materials, which are handled by a bi-directional GRU (bi-GRU). The last hidden states of the forward and backward GRUs are concatenated to give global/local -dim feature vectors. The -dim vector got from the whole sentence is passed through a GN followed by an FC to form the global textual feature ? R . With each certain input phrase, the corresponding output -dim vector is processed consecutively by a GN and two FCs with a ReLU layer between them and then concatenated with each other to form the local textual feature matrix ? R ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Textual Person Information Refinement.</head><p>To further remove the noise signals in the textual data, so as to refine the extracted person information, the global feature vector and local feature vectors in are separately handled by a Signal Denoising Module (SDM). With a fixed zeroing ratio , a fixed number of elements in an input vector is set to zero <ref type="bibr" target="#b21">[22]</ref>. And then the processed vector is reconstructed following an autoencoder manner to obtain the textual person feature vector and the local textual person feature matrix :</p><formula xml:id="formula_0">= ( ( ( , ))),<label>(1)</label></formula><p>where ( , ) denotes the zero setting operation with ratio , ? { } ? and ? { } ? . With the zeroing and reconstruction mechanism, the input vectors are required to fully retain effective information while discarding redundant noise signals. The reconstruction loss of SDM is defined as:</p><formula xml:id="formula_1">= ( , ) + ?? =1 (( ) , ( ) ),<label>(2)</label></formula><p>where ( ) and ( ) denote the -th vector in matrices and . Rather than being superficially look-alike, the denoised vector ought to be properly matched with the original vector because of the special nature of a retrieval task. Therefore, instead of utilizing the traditional Euclidean Distance to guide the reconstruction, a triplet ranking loss is adopted:</p><formula xml:id="formula_2">( 1 , 2 ) = ?? 2 { ? ( 1 , 2 ) + ( 1 , 2 ), 0} + ?? 1 { ? ( 1 , 2 ) + ( 1 , 2 ), 0},<label>(3)</label></formula><p>to more accurately constrain the matched pairs to be closer than the mismatched pairs with a margin , where ( 1 , 2 ) or ( 1 , 2 ) denotes a mismatched pair and (?, ?) is the cosine similarity between two vectors. Instead of using the furthest positive and closest negative sampled pairs, we adopt the sum of all pairs within each mini-batch when computing the loss following <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Surroundings-Person Separation Learning</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>, five alignment paradigms are adopted to adequately utilize multi-modal and multi-granular information for a robust Deep Surroundings-Person Separation Learning process, thereby improving the retrieval accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Align I.</head><p>To process the visual data, the person feature and surroundings feature are separated through a Surroundings-Person Separation Module (SPSM), which is implemented as two paralleled multi-layer perceptrons (MLP) (with the feature dimension conversion as ? 2 ? ) followed by a ? layer:</p><formula xml:id="formula_3">, = ( ).<label>(4)</label></formula><p>The person features extracted from both modalities are first aligned. The alignment loss for is = ( , ).</p><p>Besides, a Mutually Exclusion Constraint (MEC) is proposed to ensure that and are orthogonal to each other and the visual information is distributed between them without overlap. Let = { } =1 ? R ? and = { } =1 ? R ? denote matrices whose rows are person and surroundings features in a training batch, respectively, where is the batch size, and then the mutually exclusion loss is = ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Align II. With a proposed Surroundings-Person Fusion</head><p>Module (SPFM), is fused with and reconstructed into the visual modality as :</p><formula xml:id="formula_6">= ( , ),<label>(7)</label></formula><p>which is then aligned with and the alignment loss for is</p><formula xml:id="formula_7">= ( , ).<label>(8)</label></formula><p>first combine the two input vectors by addition or concatenation (compared in Section 4.2.1), and then the combined feature is processed by an MLP similar to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Align III. A Person Describing Module (PDM)</head><p>, which is implemented as an MLP with a ? activation function is employed to reconstruct into the textual modality as and then aligned with : = ( ).</p><p>The alignment loss for is = ( , ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Align IV. A Salient Attention Module (SAM)</head><p>is first employed to highlight person information in the local visual feature matrix :</p><formula xml:id="formula_10">( ) = ( 2 ( ( ( 1 ( ) + 1 ))) + 2 ) ? ( ) ,<label>(11)</label></formula><p>where denotes the group normalization layer while 1 , 2 and 1 , 2 denote the linear transformation. To adequately exploit finegrained clues, a cross-modal attention (CA) mechanism <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> is utilized to align with the textual person feature and form a -dim vector:</p><formula xml:id="formula_11">( , ) = ?? &gt; 1 ( ) , = ( (( ) , )) =1 ( (( ) , )) ,<label>(12)</label></formula><p>where represents the relation between the -th local visual part and textual person feature. And the alignment loss for is</p><formula xml:id="formula_12">= ( ( , ), ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Align V. Similar with , the alignment loss for</head><formula xml:id="formula_13">is = ( ( , ), ),<label>(14)</label></formula><formula xml:id="formula_14">( , ) = ?? &gt; 1 ( ) , = ( (( ) , )) =1 ( (( ) , )) .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function for Training</head><p>The complete training process includes 2 stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Stage-1.</head><p>We first fix the parameters of the ResNet-50 backbone and train the left feature extraction part of DSSL with the identification (ID) loss</p><formula xml:id="formula_15">( ) = ? ( ( ? ( ))<label>(16)</label></formula><p>to cluster person images into groups according to their identification, where ? R ? is a shared transformation matrix implemented as a FC layer without bias and is the number of different people in the training set. As global features can provide more complete information for clustering, only and are utilized here:</p><formula xml:id="formula_16">1 = ( ) + ( ).<label>(17)</label></formula><p>And the entire loss in Stage-1 is</p><formula xml:id="formula_17">1 = 1 .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Stage-2.</head><p>In this stage, all the parameters of DSSL are finetuned together. Here the ID loss is also employed to ensure that the person features and reconstructed features can be correctly related to the corresponding person:</p><formula xml:id="formula_18">2 = 1 + ( ) + ( ) + ( ) + ( ).<label>(19)</label></formula><p>The five alignment losses are utilized to improve retrieval accuracy: CUHK-PEDES. Previously, CUHK-PEDES <ref type="bibr" target="#b11">[12]</ref> is the only available dataset for text-based person retrieval task. Following the official data split approach, the training set contains 34054 images, 11003 persons and 68126 textual descriptions. The validation set contains 3078 images, 1000 persons and 6158 textual descriptions while the testing set has 3074 images, 1000 persons and 6156 descriptions. Every image generally has two descriptions, and each sentence is commonly no shorter than 23 words. After dropping words that appear less than twice, the word number is 4984.</p><formula xml:id="formula_19">= + + + + .<label>(20</label></formula><p>RSTPReid. To properly handle real scenarios, we construct a new dataset called Real Scenario Text-based Person Re-identification (RSTPReid) based on MSMT17 <ref type="bibr" target="#b25">[26]</ref>. RSTPReid contains 20505 images of 4,101 persons from 15 cameras. Each person has 5 corresponding images taken by different cameras and each image is annotated with 2 textual descriptions. For data division, 3701, 200 and 200 identities are utilized for training, validation and testing, respectively. Each sentence is no shorter than 23 words. After dropping words that appear less than twice, the word number is 2204. High-frequency words and examples of person images in RSTPReid are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>The performance is evaluated by the top-accuracy. Given a query description, all test images are ranked by their similarities with this sentence. If any image of the corresponding person is contained in the top-images, we call this a successful search. The top-1, top-5, and top-10 accuracy for all experiments are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation details.</head><p>The feature dimension is set to 1024 and the number of local strips is set to 6. The total number of phrases obtained from each sentence is kept flexible with an upper bound 26, which are obtained with the Natural Language ToolKit (NLTK) by syntactic analysis, word segmentation and partof-speech tagging. We adopt an Adam optimizer to train DSSL with a batch size of 32. The margin of ranking losses is set to 0.2. In training stage-1, DSSL is trained with a learning rate of 1 ? 10 ?3 for 10 epochs with the ResNet-50 backbone fixed. In stage-2, the learning rate is initialized as 2 ? 10 ?4 to optimize all parameters including the visual backbone for extra 30 epochs. The learning rate is down-scaled by <ref type="bibr" target="#b0">1</ref> 10 every 10 epochs. 1 and 2 in are both set to 0.5. In testing and real application, a cross-modal reranking scheme (RR) based on <ref type="bibr" target="#b23">[24]</ref> is employed to further improve the retrieval accuracy in testing and real application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Analysis</head><p>To further investigate the effectiveness and contribution of each proposed component in DSSL, we perform a series of ablation studies on the CUHK-PEDES dataset. The top-1, top-5 and top-10 accuracies (%) are reported and the best result in each table is presented in bold. As shown in <ref type="table" target="#tab_1">Table 1</ref>, comparing with a baseline which is proposed following IMG-Net <ref type="bibr" target="#b24">[25]</ref> without the Inner-Modal Self-Attention Module, DSSL achieves superior performance on both CUHK-PEDES <ref type="bibr" target="#b11">[12]</ref> and our proposed RSTPReid with the aid of proper surroundings-person separation. Additionally, images of each person in RSTPReid are caught by different ones out of 15 independent cameras in both indoor and outdoor scenarios in various periods of time and thereby differ in illumination condition, weather, view angle, body position, etc., which makes RSTPReid obviously a much more challenging benchmark, on which the retrieval performance stumbles, hence leaving much space for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>Surroundings-person separation and fusion mechanism. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the retrieval result in the first row is given by a model without the surroundings-person separation and fusion mechanism ( + ) along with the mutually exclusion constraint ( ). It directly mapping multi-modal data into a latent common space as many of the existing methods do. The top-1, top-5 and top-10 performances drop sharply by 4.46%, 2.79% and 2.37%, respectively, which demonstrates that our proposed method is more able to properly catch discriminative clues about the corresponding person while drop the misaligned information from complex high-dimensional multi-modal data than unconstrained mapping paradigms. By merely utilizing a + mechanism, without a mutually exclusion constraint, the top-1, top-5 and top-10 performances improve by 1.79%, 1.71% and 1.67%, respectively, which further proves the validity of + . However, the performance is still 2.67%, 1.08% and 0.70% respectively worse than the complete DSSL. This suggests that without the mutually exclusion constraint to ensure the orthogonality between person and surroundings features, information is not able to be well distributed between them. In <ref type="table" target="#tab_3">Table 3</ref>, the addition and concatenation methods for combing the two input vectors before handled by the MLP in SPFM are compared. It turns out that the two methods give similar results, with the addition method slightly better.</p><p>Some examples of the top-5 text-based person retrieval results by DSSL are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Images of the target pedestrian are marked by red rectangles. As can be seen in the figure, many of the pedestrians in mismatched person images also look quite similar to the target one, which is consistent with the distribution pattern of features discussed above. It seems necessary to find ways to dig deeper into the semantic information and draw similar clusters    closer without mixing with each other, which remains for our future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Alignment paradigms.</head><p>To adequately utilize multi-modal and multi-granular information for a higher retrieval accuracy, five different alignment paradigms are adopted. Extensive ablation experiments are conducted on both CUHK-PEDES and RSTPReid to prove the effectiveness of them and the results are reported in <ref type="table" target="#tab_1">Table 1</ref>. The results show that utilizing more than one single alignment brings performance gain, which indicates that the use of multi-modal and multi-granular features in DSSL can provide more comprehensive information, hence leading to a more accurate retrieval. By combining and with , SPSM can more completely separate person and surroundings information with the aid of SPFM and PDM. Besides, comparing the third row from the bottom with the last row in <ref type="table" target="#tab_1">Table 1</ref>, the top-1, top-5 and top-10 performance increase by 1.12%, 0.71%, 0.61% and 1.43, 1.25, 0.56 respectively on CUHK-PEDES and RSTPReid after the two fine-grained alignments and are added, which reals the effect of utilizing multi-granular clues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Signal denoising module (SDM).</head><p>Comprehensive experimental analysis is as well carried out to study the proposed signal denoising module (SDM). As shown in <ref type="table" target="#tab_4">Table 4</ref>, ablation experiments are conducted to search for the optimal zeroing rate . It can be observed that initially the performance of DSSL follows a increasing tendency with the growth of . After reaching a peak, the performance begins to turn worse as continues to go larger. It is conceivable that by randomly dropping a certain number of elements in the input vector at random and then reconstructing it following a autoencoder manner, which is required to be well matched with the original feature under a ranking loss, redundant noise signals are inclined to be removed. Note that when is set to 0, there is no zero setting process before the input vector is reconstructed. With the growth of , SDM gradually finds an optimal zeroing rate that the noise signal are just properly dropped while person information is well retained, which gives a summit in performance. After bypassing the summit, an excess of amount of information will be discarded, and hence the retrieval performance will undoubtedly go down. As can be seen in <ref type="table" target="#tab_4">Table 4</ref>, when reaches 0.9, the accuracies of top-1, top-5 and top-10 all fall sharply.</p><p>Besides, we compare the performance of utilizing Euclidean distance and ranking loss in SDM (shown in <ref type="table" target="#tab_5">Table 5</ref>). The top-1 accuracy of DSSL with ranking loss in SDM is 1.05% higher than the one with Euclidean distance, which indicates that rather than the commonly used Euclidean distance for reconstruction, ranking loss is better at dealing with the particularity of retrieval problems. We also train and evaluate DSSL without the whole SDM (shown in <ref type="table" target="#tab_2">Table 2</ref>). The top-1, top-5 and top-10 performance drop by 2.52%, 0.74% and 0.37% respectively, which reveals the effect of SDM as well. <ref type="table" target="#tab_2">Table 2</ref>, the top-1 accuracy drops by 2.03% without the salient attention module (SAM) which utilize the extracted person information to highlight and catch body part information in the visual local features. The results indicate the effectiveness of SAM. <ref type="table" target="#tab_6">Table 6</ref> shows the comparison of DSSL against 15 previous stateof-the-art methods including CNN-RNN <ref type="bibr" target="#b19">[20]</ref>, Neural Talk <ref type="bibr" target="#b22">[23]</ref>, GNA-RNN <ref type="bibr" target="#b11">[12]</ref>, IATV <ref type="bibr" target="#b10">[11]</ref>, PWM-ATH <ref type="bibr" target="#b2">[3]</ref>, Dual Path <ref type="bibr" target="#b30">[31]</ref>, GLA <ref type="bibr" target="#b1">[2]</ref>, MIA <ref type="bibr" target="#b17">[18]</ref>, A-GANet <ref type="bibr" target="#b13">[14]</ref>, GALM <ref type="bibr" target="#b7">[8]</ref>, TIMAM <ref type="bibr" target="#b16">[17]</ref>, IMG-Net <ref type="bibr" target="#b24">[25]</ref>, CMAAM <ref type="bibr" target="#b0">[1]</ref>, HGAN <ref type="bibr" target="#b29">[30]</ref> and NAFS <ref type="bibr" target="#b4">[5]</ref> in terms of top-1, top-5 and top-10 accuracies in the text-based person retrieval task. Our proposed DSSL achieves 59.98%, 80.41% and 87.56% of top-1, top-5 and top-10 accuracies, respectively. It can be observed that DSSL outperforms existing methods, which proves the effectiveness of our proposed method. Both with a cross-modal re-ranking method, DSSL outperforms NAFS as well. With person and surroundings information separated properly, DSSL surpasses methods which directly map data into a common space. Moreover, compared to methods building similarities based on attention mechanism, DSSL achieves a significant performance improvement, which indicates that our proposed surroundings-person separation mechanism is more able to properly capture detailed person information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Salient attention module (SAM). As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison With Other State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a novel Deep Surroundings-person Separation Learning (DSSL) model to effectively extract and match person information, and hence achieve a superior retrieval accuracy. A surroundings-person separation and fusion mechanism plays the key role to realize an accurate and effective surroundingsperson separation under a mutually exclusion constraint. In order to adequately utilize multi-modal and multi-granular information for a higher retrieval accuracy, Five diverse alignment paradigms are adopted. Extensive experiments are carried out to evaluate the proposed DSSL on CUHK-PEDES, which is currently the only accessible dataset for text-base person retrieval task. DSSL outperforms previous methods and achieves the state-of-the-art performance on CUHK-PEDES. To properly evaluate the proposed method in the real scenarios, a Real Scenarios Text-based Person Reidentification (RSTPReid) dataset is further constructed to benefit future research on text-based person retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the complementary relationship among ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The overall framework of the proposed Deep Surroundings-person Separation Learning (DSSL) model. A surroundings-person separation and fusion mechanism plays the key role to realize an accurate and effective surroundingsperson separation under a mutually exclusion constraint. (a) Illustration of the proposed feature extraction procedure in DSSL. (b) Illustration of the five diverse alignment paradigms adopted to adequately utilize multi-modal and multi-granular information and hence improve the retrieval accuracy. / , / , / and / denote the extracted visual/textual global, person, reconstructed and local features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>High-frequency words and person images in our constructed RSTPReid dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Examples of top-5 text-based person retrieval results by DSSL. Images of the target pedestrian are marked by red rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dataset and metrics. Our approach is evaluated on two challenging datasets: CUHK-PEDES<ref type="bibr" target="#b11">[12]</ref> and our proposed Real Scenario Text-based Person Re-identification (RSTPReid) dataset.</figDesc><table><row><cell>4 EXPERIMENTS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Experimental setup</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell></row><row><cell cols="5">Along with the mutually exclusion loss, the entire loss in Stage-2 is</cell></row><row><cell>2 =</cell><cell>2 +</cell><cell>+</cell><cell>.</cell><cell>(21)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation analysis of the five alignment paradigms in DSSL on CUHK-PEDES and RSTPReid.</figDesc><table><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>CUHK-PEDES</cell><cell></cell><cell></cell><cell>RSTPReid</cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-1</cell><cell>Top-5</cell><cell cols="2">Top-10 Top-1</cell><cell>Top-5</cell><cell>Top-10</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>52.42</cell><cell>76.06</cell><cell>84.94</cell><cell>26.31</cell><cell>46.90</cell><cell>58.33</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>56.18</cell><cell>79.56</cell><cell>86.45</cell><cell>28.74</cell><cell>50.88</cell><cell>61.69</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>55.01</cell><cell>77.68</cell><cell>85.25</cell><cell>26.83</cell><cell>49.55</cell><cell>59.91</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>54.56</cell><cell>78.49</cell><cell>85.64</cell><cell>27.01</cell><cell>50.02</cell><cell>60.67</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>54.65</cell><cell>78.30</cell><cell>85.51</cell><cell>26.73</cell><cell>50.71</cell><cell>60.25</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>51.01</cell><cell>75.47</cell><cell>83.01</cell><cell>25.73</cell><cell>48.99</cell><cell>59.82</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>57.31</cell><cell>79.56</cell><cell>86.42</cell><cell>29.23</cell><cell>51.55</cell><cell>61.77</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>56.73</cell><cell>79.21</cell><cell>86.65</cell><cell>28.87</cell><cell>51.81</cell><cell>62.43</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>57.08</cell><cell>79.11</cell><cell>86.06</cell><cell>29.51</cell><cell>51.89</cell><cell>62.22</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>58.86</cell><cell>79.70</cell><cell>86.95</cell><cell>31.00</cell><cell>53.83</cell><cell>62.63</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>58.19</cell><cell>79.41</cell><cell>86.52</cell><cell>30.81</cell><cell>53.67</cell><cell>62.71</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>59.98</cell><cell>80.41</cell><cell cols="2">87.56 32.43</cell><cell>55.08</cell><cell>63.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Ablation analysis of the mutually exclusion con-</cell></row><row><cell cols="6">straint (MEC), surroundings-person separation and fusion</cell></row><row><cell cols="6">(SPSM + SPFM), salient attention module (SAM) and signal</cell></row><row><cell cols="5">denoising module (SDM) on CUHK-PEDES.</cell><cell></cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell><cell cols="2">Top-1 Top-5 Top-10</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>55.52 77.62</cell><cell>85.19</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>57.31 79.33</cell><cell>86.86</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">59.98 80.41 87.56</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>57.95 79.89</cell><cell>87.20</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>57.46 79.67</cell><cell>87.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of the feature combination method utilized in SPFM on CUHK-PEDES.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Top-5 Top-10</cell></row><row><cell>Addition</cell><cell>59.98 80.41</cell><cell>87.56</cell></row><row><cell cols="2">Concatenation 59.54 80.45</cell><cell>87.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of zeroing rate in SDM on CUHK-PEDES.</figDesc><table><row><cell></cell><cell cols="2">Top-1 Top-5 Top-10</cell></row><row><cell>0</cell><cell>57.84 79.97</cell><cell>87.47</cell></row><row><cell cols="2">0.25 58.61 81.05</cell><cell>87.28</cell></row><row><cell cols="2">0.5 59.98 80.41</cell><cell>87.56</cell></row><row><cell cols="2">0.75 58.48 80.18</cell><cell>87.29</cell></row><row><cell cols="2">0.9 54.35 77.71</cell><cell>86.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of Euclidean distance and ranking loss utilized in SDM on CUHK-PEDES.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Top-5 Top-10</cell></row><row><cell cols="2">Euclidean Distance 58.93 80.32</cell><cell>87.47</cell></row><row><cell>Ranking Loss</cell><cell cols="2">59.98 80.41 87.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with other state-of-the-art methods on CUHK-PEDES.</figDesc><table><row><cell>Method</cell><cell cols="3">Top-1 Top-5 Top-10</cell></row><row><cell>CNN-RNN [20]</cell><cell>8.07</cell><cell>-</cell><cell>32.47</cell></row><row><cell>Neural Talk [23]</cell><cell>13.66</cell><cell>-</cell><cell>41.72</cell></row><row><cell>GNA-RNN [12]</cell><cell>19.05</cell><cell>-</cell><cell>53.64</cell></row><row><cell>IATV [11]</cell><cell>25.94</cell><cell>-</cell><cell>60.48</cell></row><row><cell>PWM-ATH [3]</cell><cell cols="2">27.14 49.45</cell><cell>61.02</cell></row><row><cell>Dual Path [31]</cell><cell cols="2">44.40 66.26</cell><cell>75.07</cell></row><row><cell>GLA [2]</cell><cell cols="2">43.58 66.93</cell><cell>76.26</cell></row><row><cell>MIA [18]</cell><cell cols="2">53.10 75.00</cell><cell>82.90</cell></row><row><cell>A-GANet [14]</cell><cell cols="2">53.14 74.03</cell><cell>81.95</cell></row><row><cell>GALM [8]</cell><cell cols="2">54.12 75.45</cell><cell>82.97</cell></row><row><cell>TIMAM [17]</cell><cell cols="2">54.51 77.56</cell><cell>84.78</cell></row><row><cell>IMG-Net [25]</cell><cell cols="2">56.48 76.89</cell><cell>85.01</cell></row><row><cell>CMAAM [1]</cell><cell cols="2">56.68 77.18</cell><cell>84.86</cell></row><row><cell>HGAN [30]</cell><cell cols="2">59.00 79.49</cell><cell>86.6</cell></row><row><cell>NAFS [5]</cell><cell cols="2">59.94 79.86</cell><cell>86.70</cell></row><row><cell>DSSL (ours)</cell><cell cols="3">59.98 80.41 87.56</cell></row><row><cell>NAFS + RVN [5]</cell><cell cols="2">61.50 81.19</cell><cell>87.51</cell></row><row><cell cols="4">DSSL + RR (ours) 62.33 82.11 88.01</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-based person search via attribute-aided matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surbhi</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Venkatesh Babu Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2617" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local image-language association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving Text-Based Person Search by Spatial Matching and Adaptive Threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1879" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Contextual Non-Local Alignment over Full-Scale Representation for Text-Based Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03036</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pose-Guided Multi-Granularity Attention Network for Text-Based Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08440</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Gang Hua, Houdong Hu, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep adversarial graph attention convolution network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4107" to="4116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial Representation Learning for Text-to-Image Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kakadiaris Nikolaos Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5813" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving descriptionbased person re-identification by multi-granularity image-text alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5542" to="5556" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-tophrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervised hierarchical cross-modal hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching images and text with multi-modal tensor fusion and reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on multimedia</title>
		<meeting>the 27th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">IMG-Net: inner-cross-modal attentional multigranular network for descriptionbased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aichun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">43028</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Zhouxin Xue, and Gang Hua</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan Ning Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical Gumbel Attention Network for Text-based Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3441" to="3449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dual-Path Convolutional Image-Text Embeddings with Instance Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
