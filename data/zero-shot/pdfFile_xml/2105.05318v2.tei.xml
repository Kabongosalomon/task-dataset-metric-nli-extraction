<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GANs for Medical Image Synthesis: An Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Skandarani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bourgogne Franche-Comte</orgName>
								<address>
									<settlement>Dijon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CASIS inc</orgName>
								<address>
									<settlement>Quetigny</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sherbrooke</orgName>
								<address>
									<settlement>Sherbrooke</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Lalande</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bourgogne Franche-Comte</orgName>
								<address>
									<settlement>Dijon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University Hospital of Dijon</orgName>
								<address>
									<settlement>Dijon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GANs for Medical Image Synthesis: An Empirical Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>GAN</term>
					<term>MRI</term>
					<term>CT</term>
					<term>Heart</term>
					<term>Retina</term>
					<term>Liver</term>
					<term>Adversarial</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they were trained to replicate. One recurrent theme in medical imaging is whether GANs can also be effective at generating workable medical data as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study to gauge the benefits of GANs in medical imaging. We tested various GAN architectures from basic DCGAN to more sophisticated style-based GANs on three medical imaging modalities and organs namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on well-known and widely utilized datasets from which their FID score were computed to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images and the original data. Results reveal that GANs are far from being equal as some are ill-suited for medical imaging applications while others are much better off. The top-performing GANs are capable of generating realistic-looking medical images by FID standards that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggests that no GAN is capable of reproducing the full richness of medical datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last decade, machine learning have been widely adopted mainly due to the advent of deep neural networks and their state-of-the-art results on a variety of medical imaging tasks. Meanwhile, the introduction of Generative Adversarial Networks (GANs) by <ref type="bibr" target="#b13">Goodfellow et al. (2014)</ref> drove generative modeling and data synthesis to levels of quality never achieved before. The research on GANs grew at an ever increasing pace, with each iteration pushing back the limits of image quality. Perhaps, one notable breakthrough in image quality came from <ref type="bibr" target="#b7">Brock et al. (2019)</ref> and their Big GAN. Not so long after, another dramatic jump in the quality and diversity of generated images came with Style GAN <ref type="bibr" target="#b18">(Karras et al., 2019)</ref> which exhibited highly realistic high-resolution human faces. Motivated by the impressive results achieved by GANs on natural images, the goal of this work is to evaluate how well these machines perform on medical data, an area well-known for its smaller datasets and strict anatomical requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Medical image analysis</head><p>Medical image analysis aims to extract uninvasively information about a patient's medical condition. Medical images are images acquired from one of multiple modalities, be it Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), or Ultrasound (US) to name a few. The acquired images are generally processed using image analysis and/or computer vision techniques to extract certain useful information about the data at hand, for example, to classify whether the case is normal or pathological. One of the most routine tasks in clinical practice is image contouring, or segmentation. Image segmentation is the operation of outlining parts of the images that belong to certain classes of interest. For example, in the case of cardiac MRI, one may delineate the left ventricular cavity and myocardium with the objective of measuring blood volumes and contraction rates.</p><p>In recent years, machine learning and deep learning garnered a large interest from the medical imaging community due to their unprecedented achievements in a large swath of computer vision tasks. However, machine learning software have not yet been widely adopted in clinical practice largely due to the fact that neural nets are still error-prone under certain conditions (domain adaptation, different acquisition protocols, missing data, etc). One reason for this derives from the fact that fully-annotated medical imaging datasets are much smaller than those from other imaging areas. For example, the gold standard computer vision ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009)</ref> dataset contains more than 14 million annotated images, while a typical medical image dataset is three to four orders of magnitude smaller. This is because the creation of medical imaging datasets is costly and difficult due to the sensitive nature of the data and the highly specific domain knowledge required to reliably annotate it. The paucity of training data in medical imaging made the search for other venues of acquiring training sets an active area of research <ref type="bibr" target="#b11">(Frangi et al., 2018)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Synthetic Data and Medical Imaging</head><p>Recently, GANs got a growing attention by the medical research community which used it to synthesis realistic-looking medical images. For example, <ref type="bibr" target="#b3">Bermudez et al. (2018)</ref> trained a GAN to synthesis new T1-weighted brain MRI with comparable quality as real images and <ref type="bibr" target="#b2">Baur et al. (2018)</ref> succeeded in generating high resolution skin lesion images which experts could not reliably tell apart from real images. <ref type="bibr" target="#b8">Calimeri et al. (2017)</ref> took advantage of GANs to generate brain MRI that achieves high scores both in qualitative and quantitative evaluation. In <ref type="bibr" target="#b9">Chuquicusma et al. (2018)</ref>, authors have shown that GAN-generated images of lung cancer nodules are nearly indistinguishable from real images, even by trained radiologists.</p><p>GANs were also used as a mean for generating more training data. In <ref type="bibr" target="#b33">Shin et al. (2018)</ref> the authors trained a GAN to generate synthetic brain tumor MRI and evaluated the performance of subsequent segmentation networks trained with the generated data. Looking at the reported results, the segmentation networks trained solely with synthetic data do not come close to those trained with real data performance wise. Likewise, <ref type="bibr" target="#b35">Skandarani et al. (2020)</ref> proposed a combination of a variational autoencoder and a GAN as a data augmentation framework for an image segmentation problem. Here again, the use of GANs to train downstream neural networks produced mixed (and yet more or less convincing) results.</p><p>In addition and as reported in <ref type="bibr" target="#b20">Kazeminia et al. (2020)</ref> survey paper, application of GANs in medical imaging extend beyond image synthesis to other tasks, such as domain adaptation, classification, and reconstruction to name a few. For these applications, the capability of GANs to generate realistic looking images lead to a partial disregard of the usefulness of the generated medical images or whether they hold any value compared to real data in routine clinical tasks.</p><p>In the light of these publications, one might wonder how useful GANs truly are in medical imaging. In this paper, we set out to evaluate the richness and the benefit of using GAN-generated data in the context of medical imaging. We assess their performances on three datasets of different organs and different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Generative Adversarial Networks</head><p>Adversarial networks in general, and GANs more specifically, are trained to play a minimax game between a generator network which tries to maximize a certain objective function in tandem with a discriminator network which tries to minimize that same objective function hence the 'Adversarial' denomination. In their most basic formulation, GANs are trained to optimize the following value function <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref>:</p><formula xml:id="formula_0">min G max D V (D, G) = E x?p data (x) [log D(x)] +E z?pz(z) [log(1 ? D(G(z)))].</formula><p>(1)</p><p>Here, G(z) is the generator network with parameters ? G . It is fed with a random variable z ? p z sampled from a given prior distribution that G tries to map to x ? p data . To achieve this, another network D (aka the discriminator) with parameters ? D is trained to differentiate between real samples x ? p data from a given dataset and fake samplesx ? p ? G (x|z) produced by the generator. In doing so, the generator is pushed to gradually produce more and more realistic samples with the goal of making the discriminator misclassify them as real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">GAN Selection</head><p>The number of papers published on GANs have been growing steadily in the last years. This has been underlined by <ref type="bibr" target="#b12">Gonog and Zhou (2019)</ref> survey paper which reports no less than 460 references. Given this large palette of models, we based our choice on those that are the most widely adopted and/or ushered an improvement to the quality of generated images. We also selected GANs based on their ability to fit on a single 12Gb GPU.</p><p>Training GANs can be tricky. Since learning involves two opposing networks, GANs are known for suffering from several training problems, the following three being among the most widely documented.</p><p>Convergence. GANs (and adversarial training in general) often suffer from a lack of defined convergence state. This is because the training process involves two networks pushing in opposite direction without one out matching the other. This frequently proves to be a difficult task. For example, the generator could become too powerful and learn to fool the discriminator with faulty output. It could also happen that the discriminator reaches a 50% accuracy effectively outputting random guesses which does not help the generator learn any meaningful information about the true data distribution.</p><p>Vanishing Gradients. As GANs train a generator with the output of a discriminator, whenever the discriminator significantly outperforms the generator, its loss goes to zero pushing the retropropagated gradient to smaller and smaller value, hence the vanishing gradient name. Because of that, the generator does not get enough gradient updates and sees its learning stall to some sub-optimal solutions .</p><p>Mode Collapse. From all the challenges that obstruct the training of powerful GANs, mode collapse might probably be the most difficult one to deal with. Mode collapse occurs when the generator gets stuck outputting only one (or a few) modes of the input data distribution. An example could be a generator producing images of healthy subjects while ignoring the diseased ones. This pitfall leads to a loss of diversity in the generated datasets that can greatly hurt the performance of subsequent networks trained with these generated data.</p><p>In regard of the aforementioned criteria and the different challenges, we selected the following GANs for our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">DCGAN</head><p>Deep Convolutional GANs  are the first GANs to use convolutional layers compared to the inital GAN which used only fully connected layers. With its simplicity, DCGAN is often the de facto baseline GAN one implements. DCGANs showed a considerable jump in image quality and training stability while providing some useful insights on the network design (use of strided convolutions instead of pooling layers, extensive use of batchNorm, etc.). To our knowledge, DCGAN is among the most widely implemented GAN as of today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">LSGAN</head><p>Least Squares GANs <ref type="bibr" target="#b23">(Mao et al., 2017</ref>) us a different loss for the discriminator than the original GANs which helps alleviate certain challenges and improve the generated sample quality. LSGANs replace the cross entropy loss of the original GAN with the mean squared error which mitigates the vanishing gradient problem leading to a more stable learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">WGAN and WGAN-GP</head><p>Wasserstein GANs  are considered as a major breakthrough to overcome GAN training challenges. In particular, it is known to reduce the effect of mode collapse and stabilize the learning procedure. The idea is to use a Wasserstein earth-mover distance as GAN loss function together with some other optimization tricks like weight clipping and gradient penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">HingeGAN (Geometric GAN)</head><p>Introduced by <ref type="bibr" target="#b21">Lim and Ye (2017)</ref>, HingeGANs substitute the original GAN loss for a margin maximization loss which theoretically converges to a Nash equilibrium between the generator and discriminator. As for WGAN and LSGAN, HingeGAN has the sole benefit of easing the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5.">SPADE GAN</head><p>Spatially Adaptative DEnormalization (SPADE) GANs <ref type="bibr" target="#b26">(Park et al., 2019</ref>) is a member of the so-called image-to-image translation GAN family. SPADE GANs produce state-of-the-art results on a wide range of datasets producing high quality images perfectly aligned to a semantic input mask. SPADE GANs come as an improvement of the previously-published pix2pix <ref type="bibr" target="#b16">(Isola et al., 2017)</ref> model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.6.">Style based GANs</head><p>StyleGAN <ref type="bibr" target="#b19">(Karras et al., 2020)</ref>, often considered as the state-of-the-art generative neural network, introduces multiple tricks to GAN borrowed from previous works such as progressive GANs <ref type="bibr" target="#b17">(Karras et al., 2018</ref>) that gradually trains the GAN with different resolutions which leads to better quality and a more stable training process. StyleGAN also comes with a greatly modified generator which includes adaptive instance normalization blocks (AdaIN), the injection of noise at every level of the network and use a 8-layer MLP mapping function on the input latent vector z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation Metrics</head><p>Broadly speaking, the metrics used to quantify the effectiveness of GANs are the same as those used to evaluate traditional image synthesis tasks. This boils down to computing a similarity distance between a set of images. In their early stages, GANs were evaluated using the traditional metrics such as Peak Signal to Noise Ratio (PSNR) <ref type="bibr" target="#b30">(Regmi and Borji, 2018)</ref> or Structural Similarity Index Measure (SSIM) <ref type="bibr" target="#b25">(Odena et al., 2017)</ref>. As the field advanced, more image quality metrics emerged and became the de facto evaluation criteria, such as Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b38">(Zhang et al., 2018)</ref>, Inception Score (IS) <ref type="bibr" target="#b32">(Salimans et al., 2016)</ref> and the Frechet Inception Distance (FID) <ref type="bibr" target="#b14">(Heusel et al., 2017)</ref>.</p><p>First introduced by Heusel et al. <ref type="formula">(2017)</ref>, the Frechet Inception Distance (FID), makes use of a pretrained Inception network on the ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009</ref>) dataset to assess the quality of GAN generated images. The FID is a distance between the distribution of the GAN sampled images and the real dataset used to train the GAN. Generated samples and real images are fed to the pretrained Inception network and the mean and covariance of the activations in the final block, assumed to be of a Gaussian distribution, are collected for both sets then the Frechet distance is computed between both. The FID is computed on a learned feature space and was shown to correlate well to human visual perception <ref type="bibr" target="#b38">(Zhang et al., 2018)</ref>. Although, it still suffers from a number of drawbacks <ref type="bibr" target="#b6">(Borji, 2019)</ref>, most prominently, it suffers from a high bias <ref type="bibr" target="#b5">(Bi?kowski et al., 2018)</ref>. Also, FID can not detect a GAN that memorizes the training set <ref type="bibr" target="#b22">(Lucic et al., 2018)</ref>.</p><p>In this work, we use the FID metric as it evolves in tandem with human perception. In addition, it makes use of the original dataset to compute a distance in a learned feature space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACDC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SLiver07 IDRID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Material and Methods</head><p>To make informed decisions about the usefulness of GANs in medical imaging as a source of synthetic data, we had to take into account different GANs and cover a diverse set of image modalities. In parallel, a wide range of hyperparameters had to be covered to assess their effect on the GANs at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hyperparameters search</head><p>GANs are known for their sensitivity to hyperparameters tweaking <ref type="bibr" target="#b22">(Lucic et al., 2018)</ref>. In order to achieve a fair comparison between the selected GANs, we covered a wide spectrum of hyperparameters (some affecting the GAN architecture), through a vast hyperparameter search, totaling roughly 500 GPU-days. We retained the best performing runs with regards to the reference metric FID for its correlation with subjective evaluation.</p><p>Moreover, since the number of runs needed to sweep a large hyperparameter space grows exponentially with the number of hyperparameters we set to optimize over, we choose a number of sensible initial configurations for each dataset/GAN pair mostly based on their default configuration. <ref type="table" target="#tab_1">Table 1</ref> lists the hyperparameters we searched over. Iterating over these hyperparameters enabled us to find the set that works best for each GAN/dataset pair. In addition, this hyperparameters search also gave us a peek at how the training stability is affected by the selected hyperparameters. Note that some combinations were only tested for specific GANs, such as "weight clipping" for WGAN or "Gradient Penalty" for WGAN-GP.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GANs setup</head><p>The training of the DCGAN, LSGAN, WGAN and HingeGAN followed the same protocol. A traditional fully convolutional network architecture with a standard generator and discriminator composed of upconvolutions and strided convolutions respectively was implemented, as a basis of our DCGAN. Then the loss function was swapped to convert it to either a LS-GAN, a WGAN or a hingeGAN. For StyleGAN and SPADE GAN, we relied on the publicly available implementations without any change to the networks architecture. <ref type="figure" target="#fig_0">Fig. 1</ref> schematically summarize the architecture of each GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">GAN Training Tricks</head><p>In order to make the GAN training process more stable, we rely on a few tricks that have been shown useful in this regard.</p><p>Label smoothing. First applied to GANs by <ref type="bibr" target="#b32">Salimans et al. (2016)</ref>, it consists in replacing the true classification labels given to the discriminator to a smooth value ?.</p><p>Feature matching. Also introduced by Salimans et al. <ref type="formula">(2016)</ref>, feature matching adds another objective to the generator of the GAN which consists in minimizing a distance between the activations of the discriminator for real and generated data.</p><p>Differentiable augmentation. Presented by <ref type="bibr" target="#b39">Zhao et al. (2020)</ref>, differentiable augmentation imposes various types of augmentation on the fake and real samples fed into the discriminator yielding a more stable training and a better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">GAN evaluation in medical imaging</head><p>While image fidelity is fundamentally important for a practitioner to deliver a good diagnostic, the visual acuity of generated images cannot be the sole marker to assess the true performances of GANs. In this paper, we want to assess how rich and diverse a synthetically generated dataset really is in the context of medical imaging.</p><p>Thus, to verify the medical viability of GAN-generated images, we trained independently a second network as a downstream task on the GANs-generated datasets and compared its results to those obtained on the original (yet real) datasets. In this work, the downstream task is segmentation.</p><p>This assessment sets a common evaluation protocol for every GAN. This evaluation is also insightful considering that the objective for using GANs is often to artificially increase the size of a dataset and thus provide more training data to a subsequent task <ref type="bibr" target="#b35">(Skandarani et al., 2020;</ref><ref type="bibr" target="#b33">Shin et al., 2018)</ref>. This approach has been explored before with GANs trained on natural images and evaluated through a classification task <ref type="bibr" target="#b29">(Ravuri and Vinyals, 2019;</ref><ref type="bibr" target="#b34">Shmelkov et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Datasets</head><p>To cover a good spectrum of image and medical applications, we picked up three different datasets based on their imaging modalities, their organ of interest and size, namely cardiac cine-MR images, liver CT and retina imaging. These datasets offer a varied selection of data. Different dataset sizes are present from large (Sliver07) to moderate (ACDC) to small (IDRID). Coupled with that, different image modalities and organ shapes are considered. <ref type="figure" target="#fig_1">Fig. 2</ref> shows an example of images for each of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">ACDC</head><p>The Automated Cardiac Diagnosis Challenge (ACDC) dataset <ref type="bibr" target="#b4">(Bernard et al., 2018)</ref> consists of 150 exams (100 training and 50 testing) of short-axis cardiac cine-MRI acquired at the University Hospital of Dijon (all from different patients). The exams are divided into 5 evenly distributed subgroups (4 pathological plus 1 healthy subject groups) and further split into 100 exams (1902 2D slices) for training and 50 exams (1078 2D slices) held out set for testing. The pixel spacing varied from 0.7mm to 1.9mm with a slice spacing between 5mm to 10mm. The exams come with multi structure segmentation masks for the right ventricular cavity, the left ventricular cavity and the left ventricular myocardium at end-diastole and end-systole times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">SLiver07</head><p>The Segmentation of the Liver Competition 2007 (SLIVER07) <ref type="bibr" target="#b36">(Styner et al., 2008)</ref> dataset contains 40 CT volumes of the liver enhanced with contrast agent. Most livers are pathological and include at least one tumor. The pixel spacing ranges from 0.55mm to 0.8mm and the inter-slice gap between 1mm to 3mm. The 40 CT datasets are randomly split in three groups: a group of 20 volumes for training, another group of 10 volumes for validation and remaining 10 volumes for testing. For our study, we only use on the 20 training volumes provided with manual segmentations for the liver which totals 4159 2D slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3.">IDRID</head><p>The Indian Diabetic Retinopathy Image Dataset (IDRiD) <ref type="bibr" target="#b27">(Porwal et al., 2018)</ref> contains a total of 516 retinal fundus images of normal and pathological cases. Images are provided with disease grading ground truth for the full dataset and segmentation masks for 81 images. We used part of the 81 images for our study, specifically the 54 training images with the optical disc segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Dataset generation</head><p>For our study, our selected GANs were trained on the aforementioned datasets with the goal of synthesizing new medical data. The overarching objective of this study is to assess whether or not GANs offer a reliable framework for synthesizing realistic and diverse medical images. To examine how well GANs manage to learn the original data distribution, a large number of images was sampled from each of our trained GANs which we later used to train a segmentation network.</p><p>To be able to train a downstream segmentation network, the different GANs were trained on the joint distribution of the image and the mask by concatenating the channel axis. We did so for every GAN except for SPADE which is by nature conditioned on a segmentation mask. Once properly trained with the right set of hyperparameters, each GAN was used to generate a dataset of 10,000 images by randomly sampling the input latent space. No further processing was done on the generated datasets as the objective was  to gauge the quality of the raw images output by the GANs. <ref type="figure">Fig. 5</ref> shows some examples of images generated by each GAN on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>This section goes through the experiments and results obtained by each GAN on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hyperparameter search and overall results</head><p>The hyperparameter search performed on DCGAN, LSGAN, WGAN and HingeGAN revealed interesting insights. The first one is that some GANs are very sensitive to their hyperparameters. To underline this, the FID score obtained for every set of hyperparameters of each GAN and each dataset are shown on the <ref type="figure" target="#fig_3">Fig. 3</ref>. As one can see, the HingeGAN has the lowest variance and, overall, the best FID score. On the other hand, DCGAN and LSGAN are overall much more sensitive to hyperparameter tweaking. This is inline with our qualitative experience as the training of DCGAN and LSGAN often ended up producing degenerated images. As for SPADE and Style GAN, they were not included in the graph due to the shear amount of training time they require (it took respectively 10 days and 30 days to train them) but also due to their remarkable stability. Empirical evidence obtained with different hyper-parameters on a few epochs suggest that their FID variance is much lower than that of HingeGAN hence why they ended up with top results with almost no hyperparameter tweaking.  Another insight comes from the impact a dataset has on the performances of GANs. As can be seen from <ref type="figure" target="#fig_3">Fig. 3</ref>, the larger the reference dataset is, the better the resulting FID will be. It goes from IDRID, the smaller one with FID values well above 150. Then ACDC with FIDs values roughly between 100 and 150 and finally Sliver07, the largest dataset in number of images, with most FID values below 100. A similar trend can be seen in <ref type="figure" target="#fig_5">Fig. 4</ref> where the overall FID values for every GANs is shown against the number of convolutional filters in the discriminator network. This shows how volatile GANs can be when trained on smaller datasets such as IDRID. Similar plots with other hyperparameters can be found in the supplementary materials.</p><p>The best FID score obtained for each GAN and each dataset is shown in the third column of <ref type="table" target="#tab_2">Table 2</ref>. Example of generated images can also be seen in <ref type="figure">Fig. 5</ref> (and in high resolution in the supplementary material). By far, the two best models are StyleGAN and SPADE GAN. The most extreme case is for IDRID where a SPADE GAN got a surprising FID of 1.09 and remarkably vivid images in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation evaluation</head><p>As mentioned before, the true value of the generated images was validated with a downstream segmentation network trained on the synthetic  <ref type="figure">Figure 5</ref>: Examples of generated images for each GAN on the ACDC, SLiver07 and IDRID datasets. The first column is an example of image from the real dataset. High resolution versions of these images are available in the supplementary material.</p><p>data instead of the original (yet real) data. To do so, 10, 000 new images were generated for each dataset and a U-Net <ref type="bibr" target="#b31">(Ronneberger et al., 2015)</ref> was trained to predict the segmentation mask. The last column of <ref type="table" target="#tab_2">Table 2</ref> contains the Dice score obtained on the real test set of each dataset. Unsurprisingly, as suggested by the FID scores, Style-GAN and SPADE GAN achieve the highest Dice scores on all the datasets with StyleGAN reaching 87% Dice on the ACDC dataset, 2% less than when training with the original data.</p><p>These results reveal three important things about GANs in medical imaging. First, simpler models such as DCGAN, LSGAN, WGAN, and Hinge-GAN perform systematically poorly on every dataset despite intensive hyperparameter search. This suggest that these models might be ill-suited for medical imaging applications.</p><p>Second, despite their visual similarity, GAN-generated datasets do not have the same richness as real datasets. This is illustrated by the fact that despite being trained on far more images, none of the GAN Dice score equals or outperforms the ones obtained on the original datasets. Moreover, the generated datasets, when used as augmentation data, achieve similar per- Red points: Real images, Blue points: Generated images formance to traditional augmentation techniques (rotations, shifts, flips) illustrated by the Dice score of training with a mix of the original data and generated data and the augmented original data only.</p><p>Third, while the FID score is a good proxy to distinguish the best methods from the least effective ones, it does not correlate well with an application score such as Dice. For example, the FID score of 29.06 of StyleGAN on Sliver07 suggest that the produced images are much more accurate than those of SPADE GAN (FID=47.62). However, the resulting Dice scores show that SPADE GAN is significantly better than any other model. A similar comment can be made for IDRID and ACDC as StyleGAN and SPADE GAN got similar Dice score but very different FIDs. As for the FID score of 1.09 obtained by SPADE GAN, the associated 82% Dice score suggest that the network has most likely memorized the training set. This might be attributed to the small size of the IDRID dataset as well as the simple shape of the input segmentation mask.</p><p>To further analyze whether the FID score is a reliable medical imaging metric, we plotted the InceptionNet latent space of the generated images obtained with the most and the least effective GANs, i.e. DCGAN and StyleGAN (c.f. top row of <ref type="figure" target="#fig_6">Fig. 6</ref>, plots were obtained with <ref type="bibr">UMap (McInnes et al., 2018)</ref>). In parallel, we plotted the U-Net latent space for the same images and the same GANs (cf. bottom row of <ref type="figure" target="#fig_6">Fig. 6</ref>). While the red and the blue InceptionNet scatter plot distributions are very similar for DCGAN and StyleGAN the U-Net ones reveal much more distinctive patterns. Indeed, the U-Net distributions of StyleGAN follow very similar distributions (hence suggesting that the syntehtic images of StyleGANs are visually very close to those of the original dataset) while the ones from DCGAN show a clear case of mode collapse. This underlines a fundamental limit of the FID metric: since the InceptionNet was trained on ImageNet (a non-medical dataset) its use in medical imaging must be made with great care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual Turing Test</head><p>Considering how realistic looking some of GAN-generated images are, we asked four medical experts, each with more than 15 years of experience in cardiology, to classify fake and real cine MRI images generated by StyleGAN and the ACDC dataset. Each expert was shown 100 images consisting of a 50/50 mixture of real and synthetic images and were asked to classify it based only on their visual appreciation. The experts managed to achieves an average accuracy of only 60% thus showing how visually accurate the generated images are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we go through the aspects that play a major role in the process of training GANs with medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Volatility</head><p>Throughout this work, the training instability of GANs was a recurrent theme underlying how slight hyperparameter adjustments can considerably affect the training process. In contrast, GANs were not equally sensitive to the selected hyperparameters. While it is true that DCGAN and LSGAN showed the highest variability, it came to be easier to train WGAN and HingeGAN which were less sensitive to hyperparameters selection.</p><p>Moreover, even though the state-of-the-art GANs, such as SPADE or StyleGAN, seem to be the only viable pick to produce images of high quality, they still suffer from long training times and can sometimes lead to overfitting and "Memory GAN", i.e. a GAN that outputs the training set.</p><p>Likewise, in the case of the smaller GANs, finding the right set of hyperparameters was not always simple. To illustrate this point, we went through a total of 1,500 training runs with different hyperparameter combinations. Most of the runs lead to models that could not generate meaningful images, while the remaining runs did not always fair well when evaluated with the FID or through the image segmentation task. Concurrently, although a considerable amount of hyperparameters were explored, we have not had enough GPUs to go through a GAN architecture search which could have provided better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">FID and Image Quality</head><p>We relied on the FID score to monitor the training of the GANs. We also compared FID to a domain specific evaluation (segmentation Dice score). This process, enabled us to better understand to what extent a FID metric optimized for natural images can be used in medical imaging. Our results reveal that the FID score continuously improves as the training of any GAN moves forward. In contrast, the FID score could not be consistently relied on as a measure of the image quality when used as training input for subsequent tasks. <ref type="table" target="#tab_2">Table 2</ref> clearly shows that lower FID does not always yield better performance on a subsequent task of image segmentation. These results make it interesting to ask whether metrics grounded in domain specific knowledge could help make GANs easier to evaluate and compare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Data Scale</head><p>When comparing the results on the three datasets, an important trend related to the performance of the GANs and the data is visible. Indeed, when the size of the input dataset is exceedingly small, as is the case for the IDRID dataset in our study, the expected benefit of training a GAN to increase the dataset size quickly dissipates as they often overfit which can have the adverse effect on the subsequent task. In parallel, when the input dataset is highly unbalanced, portrayed by the SLiver07 dataset in our study with only 5% foreground pixels, the trained GANs can further exacerbates this imbalance as they will ultimately learn the underlying biases of the training data. Moreover, most of the prevalent GAN architectures deal with 2D images, which is not necessarily the best format to train with when dealing with medical imaging data as it might have been acquired in 3D. This might explain further the bad performance shown on the liver CT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Compute Scale</head><p>One shall keep in mind that training a GAN is often computationally intensive (typically because it involves two or more networks) and require a large amount of memory. Also, training GANs requires a lot of hyperparameter tuning which may or may not lead to better results when considering the downstream tasks the generated data is intended for. This also affects more sophisticated GANs which, despite their good performances which can fool medical experts, require large computing resources to train. For example, the StyleGAN took roughly 30 days to train on the ACDC dataset with a NVidia Titan V GPU with 12Gb of memory. And yet, StyleGAN did not always offer a guarantee to the usefulness of the generated samples (Dice score of 0.36 for StyleGAN on the Sliver07 dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Medical worth</head><p>As there is no automated objective way to assess whether a medical image conveys the information for the diagnosis it is intended for, we based our analysis on a proxy task that aims to mimic the process for which a dataset is created, and compared its performance to that of the original data. The results show that, although most of the images generated by the tested GANs fail in reaching the baseline performance, some of the more advanced ones manage to close the gap. However, when subjectively assessing the images generated by the larger GANs, we can still see that they exhibit a remarkable degree of complexity and quality. This might be related to the smaller scale of the datasets in medical imaging and the difference in their nature with the original datasets for which most of the GANs were tailored. Likewise, a considerable amount of the medical data is acquired in a 3D fashion and voxel wise, e.g. CT. Typical GANs might not capture the full extent of the medical information when trained solely on 2D views. Indeed, this makes exploring GANs specially made for medical data an interesting research venue and could lead to an improvement in quality and ultimately clinical usability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Currently the use of deep learning approaches in medical image analysis stay hindered by the limited access to huge annotated dataset. To address this limitation, we have probed both the limitations and promising aspects of Generative Adversarial Networks as medical image synthesis tools through an experimental approach on three different datasets. As a result, GANs effectiveness as a source of medical imaging data was found to be not always reliable, even if the produced images are nearly indistinguishable from real data. Tangentially, results point that traditional metrics used to evaluate GANs are less robust than task based evaluations.</p><p>All in all, this study should drive more research in GANs that take into account the different subtleties of medical data and hopefully lead to better generative models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architectures of the various GANs we used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of images and the segmented structures for ACDC, SLiver07 and IDRID datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>FID Score for different GAN types on IDRID, ACDC and SLiver07 Datasets across different hyperparameters settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>FID Score for different number of filters for the discriminator of the DCGAN, LSGAN, WGAN and HingeGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of UMap projection of activations of images generated by a DCGAN and others generated by a StyleGAN, with an InceptionNet trained on ImageNet (top row) and a U-Net trained on the original dataset (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>List of the different hyperparameters we optimized over.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID and U-Net Dice score for different GANs on ACDC, IDRID and Sliver07 datasets.</figDesc><table><row><cell>Dataset</cell><cell>GAN</cell><cell cols="2">FID score U-Net Dice score</cell></row><row><cell></cell><cell>Original Data</cell><cell>-</cell><cell>0.89</cell></row><row><cell></cell><cell>Augmented Original Data</cell><cell>-</cell><cell>0.90</cell></row><row><cell></cell><cell>DCGAN</cell><cell>60.12</cell><cell>0.30</cell></row><row><cell></cell><cell>LSGAN</cell><cell>59.65</cell><cell>0.39</cell></row><row><cell>ACDC</cell><cell>WGAN</cell><cell>74.30</cell><cell>0.70</cell></row><row><cell></cell><cell>Hinge GAN</cell><cell>61.00</cell><cell>0.63</cell></row><row><cell></cell><cell>SPADE GAN</cell><cell>41.54</cell><cell>0.86</cell></row><row><cell></cell><cell>StyleGAN</cell><cell>24.74</cell><cell>0.87</cell></row><row><cell></cell><cell>Orig. Data + SPADE GAN</cell><cell>-</cell><cell>0.90</cell></row><row><cell></cell><cell>Orig. Data + StyleGAN</cell><cell>-</cell><cell>0.90</cell></row><row><cell></cell><cell>Original Data</cell><cell>-</cell><cell>0.83</cell></row><row><cell></cell><cell>Augmented Original Data</cell><cell>-</cell><cell>0.84</cell></row><row><cell></cell><cell>DCGAN</cell><cell>91.34</cell><cell>0.29</cell></row><row><cell></cell><cell>LSGAN</cell><cell>78.61</cell><cell>0.20</cell></row><row><cell>IDRID</cell><cell>WGAN</cell><cell>62.12</cell><cell>0.72</cell></row><row><cell></cell><cell>Hinge GAN</cell><cell>78.61</cell><cell>0.69</cell></row><row><cell></cell><cell>SPADE GAN</cell><cell>1.09</cell><cell>0.82</cell></row><row><cell></cell><cell>StyleGAN</cell><cell>23.72</cell><cell>0.80</cell></row><row><cell></cell><cell>Orig. Data + SPADE GAN</cell><cell>-</cell><cell>0.84</cell></row><row><cell></cell><cell>Orig. Data + StyleGAN</cell><cell>-</cell><cell>0.84</cell></row><row><cell></cell><cell>Original Data</cell><cell>-</cell><cell>0.72</cell></row><row><cell></cell><cell>Augmented Original Data</cell><cell>-</cell><cell>0.70</cell></row><row><cell></cell><cell>DCGAN</cell><cell>56.41</cell><cell>0.14</cell></row><row><cell></cell><cell>LSGAN</cell><cell>56.82</cell><cell>0.15</cell></row><row><cell>Sliver07</cell><cell>WGAN</cell><cell>73.11</cell><cell>0.16</cell></row><row><cell></cell><cell>Hinge GAN</cell><cell>67.69</cell><cell>0.15</cell></row><row><cell></cell><cell>SPADE GAN</cell><cell>47.62</cell><cell>0.61</cell></row><row><cell></cell><cell>StyleGAN</cell><cell>29.06</cell><cell>0.36</cell></row><row><cell></cell><cell>Orig. Data + SPADE GAN</cell><cell>-</cell><cell>0.71</cell></row><row><cell></cell><cell>Orig. Data + StyleGAN</cell><cell>-</cell><cell>0.71</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to acknowledge Nathan Painchaud and Carl Lemaire for their help with implementations and computing resources. We would like to thank Kibrom Berihu Girum and Raabid Hussain for useful discussions and remarks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest statement</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D., Teh, Y.W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, International Convention Centre</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating highly realistic images of skin lesions with gans, in: OR 2.0 Context-Aware Operating Theaters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical Image-Based Procedures, and Skin Image Analysis</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
		<respStmt>
			<orgName>Computer Assisted Robotic Endoscopy</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning implicit brain mri manifolds with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plassard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2018: Image Processing, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">105741</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: Is the problem solved?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cervenansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lekadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A G</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmd</forename><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Biomedical data augmentation using generative adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Calimeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marzullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stamile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Terracina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to fool radiologists with generative adversarial networks? a visual turing test for lung cancer diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J M</forename><surname>Chuquicusma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bagci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="240" to="244" />
		</imprint>
	</monogr>
	<note>ISBI 2018</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simulation and synthesis in medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2800298</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="673" to="679" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A review: Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gonog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE Conference on Industrial Electronics and Applications (ICIEA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="505" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F., Blei, D.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gans for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazeminia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2020.101938</idno>
		<ptr target="https://doi.org/10.1016/j.artmed.2020.101938" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">101938</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno>abs/1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cesa-Bianchi, N., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D., Teh, Y.W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, International Convention Centre</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatiallyadaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Indian diabetic retinopathy image dataset (idrid): A database for diabetic retinopathy screening research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Porwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pachade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kokare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sahasrabuddhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>M?riaudeau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-view image synthesis using conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Medical image synthesis for data augmentation and anonymization using generative adversarial networks, in: Simulation and Synthesis in Medical Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Tenenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Senjem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Andriole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How good is my gan?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the effectiveness of gan generated cardiac mris for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Skandarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Painchaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Commowick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Markovic-Plese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jewells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>3d segmentation in the clinic: A grand challenge ii: Ms lesion segmentation</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for dataefficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
