<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Monocular Motion Capture</term>
					<term>Human Body</term>
					<term>Real-time</term>
					<term>IMU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D motion capture (mocap) is beneficial to many applications. The use of a single camera, however, often fails to handle occlusions of different body parts and hence it is limited to capture relatively simple movements. We present a light-weight, hybrid mocap technique called HybridCap that augments the camera with only 4 Inertial Measurement Units (IMUs) in a learning-and-optimization framework. We first employ a weakly-supervised and hierarchical motion inference module based on cooperative Gated Recurrent Unit (GRU) blocks that serve as limb, body and root trackers as well as an inverse kinematics solver. Our network effectively narrows the search space of plausible motions via coarse-to-fine pose estimation and manages to tackle challenging movements with high efficiency. We further develop a hybrid optimization scheme that combines inertial feedback and visual cues to improve tracking accuracy. Extensive experiments on various datasets demonstrate HybridCap can robustly handle challenging movements ranging from fitness actions to Latin dance. It also achieves real-time performance up to 60 fps with state-of-the-art accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past ten years have witnessed a rapid development of human motion capture <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b58">57]</ref>, which benefits broad applications like VR/AR, gaming, sports and movies. However, capturing challenging human motions in a light-weight and convenient manner remains unsolved.</p><p>The high-end vison-based solutions require attaching dense optical markers <ref type="bibr" target="#b54">[53]</ref> or expensive multi-camera setup <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b22">21]</ref> to capture professional motions, which are undesirable for consumer-level usage. Recent learning-based methods enable robust human capture from monocular RGB video <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b70">68,</ref><ref type="bibr" target="#b59">58]</ref>. They require specific human templates for space-time coherent capture <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b14">13]</ref>, or utilize parametric human model <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b36">35]</ref>. However, these monocular methods are fragile to capture specific challenging motions such as fitness actions or Latin dance, which suffer from complex motion patterns and severe self-occlusion. Recent advances compensate occlusions and challenging motions inertial measurements 2D detection 2D overlay 3D motion + + <ref type="figure">Fig. 1</ref>. Our HybridCap approach achieves robust 3D capture of challenging human motions from a single RGB camera and only 4 IMUs.</p><p>using probabilistic or attention-based partial occlusion modeling <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b28">27]</ref>, or using the generative or weakly-supervised prior <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b16">15]</ref>. But the above methods still suffer from the inherent monocular ambiguity due to the lack of reliable observation for self-occluded regions.</p><p>In stark contrast, combining occlusion-unaware body-worn sensors for robust motion capture has been widely explored <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b25">24]</ref>. Utilizing Inertial Measurement Units (IMUs) for motion inertia recording is a very popular trend. However, most previous work <ref type="bibr" target="#b56">[55,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b68">66]</ref> utilize multi-view video or a relatively large amount of IMUs (from 8 to 13), which is undesirable for daily usage. A few early methods <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b17">16]</ref> adopt only a monocular video and very sparse IMUs, maintaining the convenience of monocular marker-less capture. However, these methods only adopt the traditional optimization framework, which significantly leaves behind current data-driven neural era for high-quality capture of challenging motions. Recently, learning-based approaches <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b65">64]</ref> enable a realtime motion capture with only 6 IMUs. But the lack of visual cue leads to severe global localization and overlay artifacts. Surprisingly, researchers pay less attention to explore the neural solution for monocular challenging motion capture under the hybrid and extremely light-weight setting.</p><p>In this paper, we tackle the above challenges and present HybridCap -a high-quality inertia-aid monocular approach for capturing challenging human motions, as shown in <ref type="figure">Fig. 1</ref>. We revisit the hybrid setting using only a single RGB camera and sparse IMUs, and reframe such light-weight hybrid motion capture into the neural data-driven realm. It enables practical and robust human motion capture under challenging scenarios at 60 fps with state-of-the-art accuracy, which outperforms existing approaches significantly.</p><p>To this end, we introduce a learning-and-optimization framework, consisting of a hybrid motion inference module and a robust hybrid optimization scheme. The former is formulated in a weakly supervised and hierarchical multistage framework. Specifically, we adopt a fully differentiable architecture in an analysis-by-synthesis fashion without explicit 3D ground truth annotation. During training, we utilize both the AIST++ dataset <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b53">52]</ref> and a new hybrid dataset with 14 calibrated cameras and 17 IMUs covering 80 kinds of challenging motions with 4000k corresponding images. Our network takes the sequential 2D pose estimation and motion inertia from our light-weight capture setting as input, and learns to infer 3D human motion by comparing such light-weight feed-forward predictions against the weak sparse-view hybrid observations. To handle the challenging motions, we further formulate our network into a series of cooperative subtasks, including a limb tracker, a body tracker, a root tracker and a hybrid IK (inverse kinematics) solver. We also propose an efficient tracking block design for all these four subtasks, which consists of light-weight sequential Gated Recurrent Units (GRUs) with skip connection for temporal motion state modeling. Such hierarchical multi-stage design makes full use of our hybrid weak supervision by progressively narrowing the search space of plausible motions in a coarse-to-fine manner, so as to enable efficient and effective capture of challenging motions. Finally, besides the data-driven motion characteristics from previous module, current multi-modal input also encodes reliable visual and inertial hints, especially for the non-occluded regions. Thus, we further propose a robust hybrid optimization scheme to improve motion tracking and overlay performance. It not only utilizes the 2D and silhouette information from the input image, but also forces the inertial estimation to be physically plausible for high-quality motion capture. To summarize, our main contributions include:</p><p>-We propose a real-time and accurate motion capture approach in a learningand-optimization manner for the light-weight inertia-aid monocular setting, achieving significant superiority to state-of-the-arts. -We introduce a hybrid motion inference module in a weakly supervised and hierarchical multi-stage framework with a novel tracking block design, while considering bone length, resulting in efficient and effective motion inference. -We adopt an robust hybrid motion optimization scheme to faithfully utilize the visual cues and enable plausible motion estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision-based Motion Capture. Marker-based methods <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b55">54]</ref> have achieved success in capturing professional human motions which are widely utilized in industry, but they are inapplicable for daily usage due to expensive and tedious setup. The exploration of markerless mocap <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b52">51]</ref> has made great progress in order to get rid of body-worn markers and pursue efficiency. Benefiting from researches on parametric human models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b42">41]</ref> in the last decade, various data-driven approaches are proposed to estimate 3D human pose and shape by optimizing <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b30">29]</ref> or directly regressing <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b67">65]</ref> human model parameters. Taking specific template mesh as prior, multi-view <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b62">61]</ref> and monocular <ref type="bibr" target="#b64">[63,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b14">13]</ref> template-based approaches combine free-form and parametric methods, which produce high quality skeletal and surface motions. Besides, to alleviate the inherent estimation ambiguity from 2D input to 3D motion, recent approaches <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b28">27]</ref> handle complex patterns using probabilistic or attention-based semantic modeling, <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b16">15]</ref> learn to model generative or weaklysupervised prior to solve unseen and non-periodic motions. However, these methods still suffer from challenging motions, especially for rare pose patterns and extreme self-occlusion.</p><p>Sensor-aid Motion Capture. To overcome the limitations of vision cues only, another category of works propose to use IMUs. Previously, purely inertial methods using large amounts of sensors like Xsens MVN <ref type="bibr" target="#b61">[60]</ref> has been commercially used. However, intrusive capture system prompts researchers forward to sparsesensor setup. SIP <ref type="bibr" target="#b57">[56]</ref>, which uses only 6 IMUs, presents a pioneering exploration. However, limitations of its traditional optimization framework make real-time application impractical. Recent data-driven works <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b65">64]</ref> achieve great improvements on accuracy and efficiency with sparse-sensors, but substantial drift is still unsolved for challenging motions. Preceding sensor-aid solutions propose to combine IMUs with videos <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref>, RGB-D cameras <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b69">67]</ref>, or optical markers <ref type="bibr" target="#b1">[2]</ref>. Although these approaches partially solve scene-occlusion problem and correct drift effectively, they are restricted from either undesirable system complexity or physically implausible estimation for challenging motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary and Overview</head><p>The goal of our work is to capture challenging 3D human motions using a single camera with few inertial sensors aiding, which suffers from complex motion patterns and extreme self-occlusion. <ref type="figure">Fig. 2</ref> provides an overview of HybridCap, which relies on a template mesh of the actor and makes full usage of multimodal inputs in a learning-and-optimization framework. In the inference stage, our hierarchical design extracts different characteristics from multi-modal observations and learns to estimate plausible motions in a weakly supervised manner (Sec. 4.1). Then, a robust optimization stage is introduced to refine the skeletal motions to increase the tracking accuracy and overlay performance (Sec. 4.2).</p><p>Template and Motion Representation. We first scan the actor with a 3D body scanner to generate the textured template mesh of the actor. Then, we rig it automatically by fitting the Skinned Multi-Person Linear Model (SMPL) <ref type="bibr" target="#b36">[35]</ref> to the template mesh and transferring the SMPL skinning weights to our scanned mesh. The kinematic skeleton is parameterized as S = [?, R, t], including the Euler angles ? ? R N J ?3 of the N J joints, the global rotation R ? R 3 and translation t ? R 3 . Furthermore, let ? denotes the 6D representation <ref type="bibr" target="#b71">[69,</ref><ref type="bibr" target="#b60">59]</ref> of the global rotation and joint rotations. Then, we can formulate S = M(?, t) where M denotes the motion transformation between various representations.</p><p>Sensor Calibration. Before capturing, we first calibrate the IMUs and camera by solving rotation transformations R I2C and R S2B,n , where R I2C is the transformation between inertial frame F I and camera frame F C , and R S2B,n is the transformation between the n-th IMU sensor and its corresponding bone b n . With only one camera, it is difficult to calibrate by continuous per-frame optimization <ref type="bibr">[</ref> </p><formula xml:id="formula_0">... + + + t ? t S ? nertial measurements (calibrated) ... 3D E 2D E acc E ori E + S E ... ... + Fig. 2.</formula><p>The pipeline of HybridCap with multi-modal input. our approach combines a hybrid motion inference stage (Sec.4.1) with a robust hybrid motion optimization stage (Sec.4.2) to capture 3D challenging motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMPL estimation) as R</head><formula xml:id="formula_1">(A) n , R (T ) n andR (A) bn ,R (T )</formula><p>bn respectively. Thus both observation pairs of each IMU sensor can be unified by R I2C and R S2B,n under F C . Therefore, R I2C and R S2B,n can be determined by solving linear systems:</p><formula xml:id="formula_2">R I2C R (A) n =R (A) bn R S2B,n (1) R I2C R (T ) n =R (T ) bn R S2B,n<label>(2)</label></formula><p>Input Preprocessing. Our system takes the RGB video, inertial measurements and a well pre-scanned template of the actor as the overall input. Given an image frame, we extract N M 2D keypoints p ? R N M ?2 and corresponding confidence ? ? R N M by using OpenPose <ref type="bibr" target="#b7">[6]</ref>. To generalize to various camera settings during inference time, we refer <ref type="bibr" target="#b49">[48]</ref> to use canonicalized 2D keypoints p c by pre-reprojecting them onto Z = 1 plane. Then we transform inertial measurements into F C and obtain IMU accelerations A n ? R 3 and orientations R bn ? R 3?3 of N i corresponding bones b n with prepared R I2C and R S2B,n :</p><formula xml:id="formula_3">A n = R I2C A I,n<label>(3)</label></formula><formula xml:id="formula_4">R bn = R I2C R n R T S2B,n<label>(4)</label></formula><p>Besides, to provide prior knowledge on anthropometry, we heuristically calculate N b = 7 key bone lengths L k (uparm, lowarm, upleg, lowleg, foot, clavicle, and spine) from the rigging skeleton S and concatenate them into the inputs. Thus the overall input of the network in a single frame is  </p><formula xml:id="formula_5">[p c , ?, R b , A, L k ] ? R 2N M +N M +9Ni+3Ni+N b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hybrid Motion Inference</head><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, we adopt a hierarchical multi-stage motion inference scheme. It takes the sequential 2D pose estimation and motion inertia as input and generate 3D human motions in a weakly supervised manner without explicit 3D ground truth annotation. To tackle challenging motions, we divide the motion inference task into a series of cooperative subtasks inspired by <ref type="bibr" target="#b65">[64]</ref>. Specifically, we introduce a limb tracker, a body tracker, a root tracker and a hybrid IK (inverse kinematics) solver, so as to model the hierarchical knowledge of articulated human body structure. For temporal motion state modeling in these subtasks, we propose a novel and efficient block design using light-weight Gated Recurrent Units (GRUs) with skip connection. Our key observation is that the layer close to the output is responsible for adding low-level details, while the skip connection enables easier identity function learning to retain more information from the input layer. Thus the low-level 2D and inertial features could be selected and passed to the output layer directly, which helps more accurate and detailed motion inference. Besides, our design with pure multiple GRUs allows the inference module to efficiently retain more hidden states corresponding to sufficient history information.</p><p>Hierarchical Bone Trackers. In our hierarchical design, our trackers track bones rather than joints. For a bone b n , we track root-relative positions of its two endpoint? with distance constraint using the known bone lengths L bn . Specifically, the limb tracker focuses on accurately tracking four limbs bound by IMUs. Here, the loss of limb tracker is formulated as:</p><formula xml:id="formula_6">L limb = L (limb) joint + L (limb) bone ,<label>(5)</label></formula><p>where L (limb) joint is the 3D joint position loss and L (limb)</p><p>bone is the limb bone length loss. Next, the body tracker estimates the rest rigid body parts concatenating the initial input and well-estimated limb positions as the input. Bone length constraints are also utilized to reduce depth ambiguity. Furthermore, the bone orientations help to narrow the search space to the target joint positions. Similar to the limb tracker loss, the body tracker loss is formulated as:</p><formula xml:id="formula_7">L body = L (body) joint + L (body) bone .<label>(6)</label></formula><p>Note that L joint is formulated as the reprojection error to all camera views, which guides the corresponding tracker to predict joint positions in a weakly supervised manner:</p><formula xml:id="formula_8">L joint = T t=1 N C c=1 N J j=1 ? (t) c,j ||? c (? (t) j + t) ? p (t) c,j || 2 2 ,<label>(7)</label></formula><p>where ? (t) c,j denotes the confidence of 2D joint p (t) c,j ; ? c denotes the projection function of camera c; t denotes global translation from hybrid optimization of full observations; N J denotes the number of bone endpoints (8 for limb tracker and 7 for body tracker). Then, the bone length loss is formulated as the L 2 loss between the predicted bone length and ground-truth bone length L bn :</p><formula xml:id="formula_9">L bone = T t=1 N B n=1 (||? (t) bn,0 ?? (t) bn,1 || 2 ? L bn ) 2 ,<label>(8)</label></formula><p>where the predicted bone length can be calculated by the distance of two output endpoint positions? (t) bn,i (i = 0, 1) of bone b n . Note that N B is the number of target bones which is 4 for the limb tracker and 8 for the body tracker.</p><p>Hybrid Inverse Kinematics Solver. Based on the accurate tracking of bones, we introduce our hybrid IK solver and root tracker to solve rotations and translation respectively. The initial input and well-estimated root-relative 3D joints are concatenated and fed into our hybrid IK solver, which outputs global rotation and local joint rotations? in the 6D representation. Then we perform forward kinematics using predicted rotations? and bone lengths to obtain refined rootrelative 3D joints. Next we send them with the initial input into the root tracker which predicts root positiont (i.e. global translation) in the camera frame.</p><p>To utilize our weak multi-modal supervision, we further calculate N M 3D marker positionsP m (?,t) attached to the skeleton (corresponding to body joints and face landmarks of OpenPose <ref type="bibr" target="#b7">[6]</ref>), N I bone orientationsR bn (?), and N I simulated IMU sensor positionsP n (?,t) respectively. The IK loss is formulated as:</p><formula xml:id="formula_10">L IK = ? 2D L 2D + ? acc L acc + ? ori L ori + ? prior L prior + ? trans L trans .<label>(9)</label></formula><p>The 2D reprojection loss L 2D ensures each estimated 3D markerP m projects onto the corresponding 2D keypoint p c,m in all camera views, formulated as:</p><formula xml:id="formula_11">L 2D = T t=1 N C c=1 N M m=1 ? (t) c,m ||? c (P (t) m (?,t)) ? p (t) c,m || 2 2 ,<label>(10)</label></formula><p>where ? (t) c,m denotes the confidence of 2D keypoint p c,m and ? c is the projection function of camera c. Then, we introduce the acceleration loss L acc to encourage the network to learn the implicit physical constraints and generate plausible motions:</p><formula xml:id="formula_12">L acc = T ?1 t=2 N I n=1 ||? (t) n (?,t) ? A (t) n || 2 2 ,<label>(11)</label></formula><p>where? (t) n is the estimated acceleration calculated from predicted IMU position P n , formulated as below where st is the sampling time:</p><formula xml:id="formula_13">A (t) n = (P (t+1) n ? 2P (t) n +P (t?1) n )/st 2 .<label>(12)</label></formula><p>We further propose the orientation loss L ori , which ensures the predicted orien-tationR bn of each bone bound with IMU sensor fits the observation R bn of the corresponding IMU measurement.</p><formula xml:id="formula_14">L ori = T t=1 N I n=1 ||R (t) bn (?) ? R (t) bn || 2 2 .<label>(13)</label></formula><p>Furthermore, the prior loss L prior is the L 2 loss between predicted rotations? and reference ?, while L trans the one related to the predicted root positiont and reference position t. Both ? and t are obtained from hybrid optimization.</p><p>Training Details. We use mixed multi-modal datasets to train our motion inference module, which consist of sufficient records of RGB cameras and IMUs. For multi-modal supervision, we build a dataset Hybrid Challenging Motions (HCM) containing 16 subjects and over 100 one-minute sequences with a wide range of challenging motions. Each sequence consists of a video recorded by 14 synchronized RGB cameras and inertial measurements recorded by multiple calibrated IMUs (NOITOM Perception Neuron Studio <ref type="bibr" target="#b41">[40]</ref>), where the number of IMUs is N I = 17 for training set and N i = 4 for test set. Then, we obtain SMPL parameters with hybrid optimization combining sparse-view 2D detection and inertial measurements. We further utilize AIST++ <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b53">52]</ref> which consists of various challenging dance sequences. Note that when using AIST++ we simulate N i IMU measurements only as the input rather than supervision, and our HCM dataset will be made publicly available to stimulate future research. We train our hybrid motion inference module with Adam optimizer <ref type="bibr" target="#b26">[25]</ref> using multi-phase strategy. In the first phase, we train limb tracker using L limb for 20 epochs, and then we add L body to train body tracker for 20 epochs. In the second phase, we pretrain the hybrid IK solver and the root tracker for 10 epochs using L prior and L trans . In the last phase, we add full L IK to train the whole network for 100 epochs. In our experiments, L limb , L body and L IK are weighted equally. In L IK we set ? 2D = 1.0, ? acc = 10.0 and ? ori = 30.0, while ? prior = 0.01 and ? trans = 0.01. During training, two NVdia RTX3090 GPUs are utilized. We set the dropout ratio as 0.5 for all the GRU layers. The batch size is 16 and the sequence length is 360, while the learning rate is 1 ? 10 ?4 , the decay rate is 0.1 (final 50 epochs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hybrid Motion Optimization</head><p>Despite the data-driven motion inference stage learns the mapping from multimodal observations to 3D motions, the generalization error is non-negligible due to noisy 2D detection and inertial measurements.</p><p>Inspired by recent approaches <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b16">15]</ref>, we introduce a hybrid motion optimization stage to refine the skeletal motions to increase the tracking accuracy and overlay performance. It jointly utilizes the learned 3D prior from the network of multi-modal weak supervision, the 2D keypoints in the visible regions, the reliable silhouette information from the input images as well as inertial measurements.</p><p>In this stage, we first initialize the skeletal motion sequence S using network output by representation transformation M(?,t) and then perform the optimization procedure. We adopt the Euler angle representation so that the joint angles ? of S locate in the pre-defined range [? min , ? max ] of physically plausible joint angles to prevent unnatural poses. Our energy function is formulated as: Here, E 3D enforces the final motion sequence close to the predicted one; E 2D ensures that each final 3D marker reprojects onto the corresponding 2D keypoint. Besides, we adopt the acceleration energy E acc to enforce the final motion to be temporally consistent with the network estimating accelerations? n supervised by N I IMUs and the measured ground-truth accelerations A n from N i input IMUs. Specifically, the acceleration term E acc is formulated as:</p><formula xml:id="formula_15">E(S) = ? 3D E 3D + ? 2D E 2D + ? acc E acc + ? ori E ori .<label>(14)</label></formula><formula xml:id="formula_16">E acc = T ?1 t=2 Ni n=1 ||A (t) n (S) ? A (t) n || 2 2 + T ?1 t=2 N I n=Ni+1 ||A (t) n (S) ?? (t) n || 2 2 ,<label>(15)</label></formula><p>where the first term means that we directly use the acceleration observation from the input IMUs and the second term implies that we trust the network for n (S) is obtained from skeletal motion sequence S the same as Eqn. 12. Then, we adopt the orientation energy E ori to enforce the final bone orientations to be consistent with the observations of N i input IMUs, which is formulated as:</p><formula xml:id="formula_17">E ori = T t=1 Ni n=1 ||R (t) bn (S t ) ? R (t) bn || 2 2 ,<label>(16)</label></formula><p>where R (t) bn (S t ) denotes the final orientation of bone b n , which is obtained from skeleton S t .</p><p>Optimization. The constrained optimization problem to minimize the Eqn. 14 is solved using the Levenberg-Marquardt (LM) algorithm of ceres <ref type="bibr" target="#b0">[1]</ref> with 4 iterations. In our experiments, we empirically set the parameters ? 3D = 10.0, ? 2D = 1.0, ? acc = 10.0 and ? ori = 30.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>For thorough evaluation, we use 3DPW <ref type="bibr" target="#b40">[39]</ref>, AIST++ <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b53">52]</ref>, and our HCM test set which consists of over 20 indoor one-minute challenging motion sequences recorded by 14 synchronized cameras and 4 IMUs and 5 in-the-wild sequences recorded by a single camera and 4 IMUs. We follow the standard protocols to split training and test set for all the datasets. Note that the weakly supervision from sparse-view supervision in our training mechanism is critical for performance improvement as pointed out in <ref type="bibr" target="#b14">[13]</ref>. Thus, we use the 3DPW which only provides monocular data for testing rather than training. <ref type="figure" target="#fig_2">Fig. 4</ref> demonstrates our method achieves convincing 2D overlay performance and plausibility in 3D space on various datasets. Below we further qualitatively ( <ref type="figure" target="#fig_3">Fig. 5</ref>) and quantitatively (Tab. 1) illustrate that our method outperforms previous state-of-the-arts, and provide extensive ablation studies (Tab. 2, Tab. 3 and <ref type="figure" target="#fig_4">Fig. 6, Fig. 7</ref>) to evaluate our technical design and input setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison</head><p>We compare HybridCap with various representative methods using a single RGB camera or sparse IMUs. Specifically, we apply ChallenCap <ref type="bibr" target="#b16">[15]</ref> and HuMoR <ref type="bibr" target="#b47">[46]</ref> Ours <ref type="formula">(</ref> (a) (b) <ref type="figure">Fig. 6</ref>. Qualitative evaluation on our (a) inference module and (b) robust hybrid optimization. The figure demonstrates that our inference module significantly improves performance compared to pure optimization. Our pure-GRU design (middle of subfigure a) has better overlay than the network structure in TransPose <ref type="bibr" target="#b65">[64]</ref>. Our skip connection operation effectively preserves more information in the input layer, combined with the bone length utilization to make the skeletal orientation more plausible. Subfigure (b) shows that our robust optimization improves the overlay performance effectively.</p><p>which are also based on a learning-and-optimization framework, VIBE <ref type="bibr" target="#b27">[26]</ref> using adversarial learning from pure RGB input, as well as TransPose <ref type="bibr" target="#b65">[64]</ref> using pure IMUs. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, our method gets better overlays of the captured body and obtains more accurate limb orientation results.</p><p>Tab. 1 provides the corresponding quantitative comparison results under various metrics and data sets. To evaluate capture accuracy, we report the Mean Per Joint Position Error (MPJPE), and the Percentage of Correct Keypoints (PCK) with 0.2 or 0.3 torso size as the threshold. For HCM and 3DPW with groundtruth acceleration measured by IMUs, we report acceleration error (m/s 2 ), calculated as the difference between the ground-truth and estimated acceleration. For AIST++, we report mean per-joint accelerations (Accel) <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b47">46]</ref> to evaluate motion plausibility. Note that our approach consistently outperforms other approaches on various metrics, which denotes the effectiveness of our method to handle multi-modal input. Besides, our approach focuses on capture challenging motions with self-occlusion of a single actor. However, the 3DPW dataset lacks such self-occlusion scenarios and includes those sequences with incomplete 2D observation and external occlusions due to scene interactions. Thus, such domain gap in terms of occlusion leads to the performance drop off of our approach when evaluating on 3DPW. Nevertheless, we still achieves slightly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>Evaluation on inference module. We first evaluate our inference module by comparing to the variants of our approach. The quantitative analysis is provide in Rows 1-4 in Tab. 2, corresponding to the variants without our entire inference module, using the LSTM network design from TransPose <ref type="bibr" target="#b65">[64]</ref>, without using the skip connections in our GRU-based block design, without using bone length   in our hierarchical trackers, respectively. <ref type="figure">Fig. 6</ref> (a) provides the corresponding qualitative evaluations. These results illustrates the effectiveness of our motion inference and highlight the contribution of our algorithmic component designs.</p><p>Evaluation on optimization module. We further evaluate our optimization scheme with different configurations. Specifically, we compare to the two variants without the entire optimization module and without using the inertial terms in Eqn. 15 and Eqn. 16, respectively. The quantitative and qualitative results are provided in Rows 5-6 of Tab. 2 and <ref type="figure">Fig. 6 (b)</ref>. Note that these variants suffer from tracking loss especially for the limbs with fast and challenging motions. In contrast, our full pipeline achieves more robust capture, which illustrates the effectiveness of our optimization module design.</p><p>Evaluation on multi-modal input. Here, we evaluate to verify the necessity and rationality of our input setting. As shown in Tab. 3, even using 2 IMUs aiding, our multi-modal input outperforms pure RGB or IMU inputs under single modality. Besides, our approach with 4 IMUs significantly outperforms the one with only two IMUs (one for left leg and one for right arm), which serves as a good compromise of acceptable performance and light-weight convenient capture setting. The corresponding qualitative results are provided in <ref type="figure" target="#fig_4">Fig. 7</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 7 (a)</ref>, our hybrid input enables more accurate and plausible capture results in 3D, compared to the one using only RGB input. As shown in <ref type="figure" target="#fig_4">Fig. 7 (b)</ref>, our inertial observation enables to reduce the depth ambiguity from 2D observation by correcting the orientations of limbs and other extremely occluded body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Limitation. As the first trial to reframe the extremely light-weight and hybrid capture setting into the data-driven realm, the proposed HybridCap still owns limitations. First, our motion inference module relies on the performance of 2D detector since we only use 2D keypoints as visual cue, which probably can be improved by introducing other features or applying an end-to-end learning-based framework. In the monocular condition, our approach cannot capture the face and hands also due to keypoint input limitation. Besides, our method relies on a pre-scanned template and cannot handle non-rigid deformation like clothes removal. It is interesting to model finer human-object scenarios in a physically plausible way to capture more challenging and complicated motions.</p><p>Conclusion. We present a practical inertia-aid monocular approach to capture challenging 3D human motions augmenting the single camera with only 4 IMUs. It brings such extremely light-weight and hybrid capture setting into the datadriven neural era through a novel learning-and-optimization framework. Our hybrid inference module hierarchically narrows the search space of plausible motions for efficient and effective inference in a weakly supervised manner, while our robust hybrid motion optimization further improves the tracking accuracy by combining the inertial feedback and visual cues for physically plausible modeling. Our experimental results demonstrate the robustness of HybridCap in capturing challenging human motions in various scenarios. We believe that it is a significant step for convenient and robust capture of human motions, with many potential applications in VR/AR and motion evaluations for gymnastics and dancing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of our hybrid motion inference module, which is based on cooperative Gated Recurrent Unit (GRU) blocks that serve as limb, body and root trackers as well as a hybrid inverse kinematics (IK) solver. The limb tracker focuses on accurately tracking for 4 limbs while the body tracker estimates the rest bone positions from accurate limb positions. Then, the hybrid IK solver and root tracker are employed to combine initial input with well-estimated bone positions to rotations? and global translationt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Our qualitative results. Each picture shows the input image, 2D overlay and motion in 3D space from left to right respectively. The results verify that our weakly supervised training with multi-modal observations produces good 2D overlay and plausible 3D motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative comparison of capturing results on challenging human motions. Our method outperforms the state-of-the-art on the performance of both overlay and limb orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative evaluation on our (a) multi-modal input setup and (b) IMU number configurations.The adoption of inertial sensors in (a) significantly improves the tracking result. The misalignment of occluded part in (b) is corrected in our 4-IMU setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>67] on a complicated motion sequence. Instead, we adopt a twoframe calibration strategy, one frame for A-pose and the other one for T-pose. Denote orientation observations of IMUs and bones (recovered from monocular</figDesc><table><row><cell cols="2">inertial measurements</cell><cell></cell><cell>Limb</cell><cell>Root</cell></row><row><cell></cell><cell>(raw)</cell><cell></cell><cell>Tracker</cell><cell>Tracker</cell></row><row><cell cols="2">textured mesh with skeleton rig</cell><cell>2D Detection ... +</cell><cell>Body Tracker</cell><cell>HybridIK</cell><cell>2D overlay .. .</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hybrid Motion Inference (Sec 4.1)</cell></row><row><cell>. . .</cell><cell>image</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">sequence</cell><cell></cell><cell cols="2">Robust Hybrid Optimization (Sec 4.2)</cell><cell>3D view</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of several methods in terms of tracking accuracy and plausibility. MPJPE is in term of mm in both comparison and evaluation.</figDesc><table><row><cell>Method</cell><cell></cell><cell>HCM</cell><cell></cell><cell></cell><cell cols="2">3DPW</cell><cell></cell><cell>AIST++</cell><cell></cell></row><row><cell></cell><cell cols="9">MPJPE? PCK@0.2? Accel error? MPJPE ? PCK@0.2? Accel error? MPJPE ? PCK@0.2? Accel</cell></row><row><cell>VIBE [26]</cell><cell>103.4</cell><cell>61.5</cell><cell>96.8</cell><cell>82.9</cell><cell>70.3</cell><cell>21.1</cell><cell>73.5</cell><cell>69.1</cell><cell>94.6</cell></row><row><cell>HuMoR [46]</cell><cell>81.9</cell><cell>72.7</cell><cell>31.8</cell><cell>77.0</cell><cell>75.8</cell><cell>13.2</cell><cell>59.4</cell><cell>81.3</cell><cell>33.1</cell></row><row><cell>TransPose [64]</cell><cell>73.1</cell><cell>77.4</cell><cell>65.6</cell><cell>76.5</cell><cell>76.9</cell><cell>19.3</cell><cell>61.6</cell><cell>78.2</cell><cell>57.0</cell></row><row><cell cols="2">ChallenCap [15] 69.5</cell><cell>79.5</cell><cell>98.1</cell><cell>78.2</cell><cell>74.2</cell><cell>48.4</cell><cell>53.2</cell><cell>86.8</cell><cell>91.4</cell></row><row><cell>Ours</cell><cell>43.3</cell><cell>90.1</cell><cell>17.9</cell><cell>72.1</cell><cell>80.5</cell><cell>5.4</cell><cell>33.3</cell><cell>95.4</cell><cell>14.4</cell></row><row><cell cols="6">those unobserved parts. The final acceleration A</cell><cell>(t)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation on different network structures and optimization configurations. Our pure-GRU design effectively improves performance compared to the network structure in TransPose<ref type="bibr" target="#b65">[64]</ref>. Our skip connection operation and bone length utilization both prove their effectiveness. Our robust hybrid optimization improves both tracking accuracy and plausibility effectively.</figDesc><table><row><cell>Module</cell><cell>Ablation setting</cell><cell cols="4">MPJPE? PCK@0.2? PCK@0.3? Accel error ?</cell></row><row><cell></cell><cell>1) Ours w/o inference</cell><cell>136.5</cell><cell>51.1</cell><cell>65.2</cell><cell>82.1</cell></row><row><cell>Stage 1</cell><cell cols="2">2) TransPose net structure 3) Ours w/o skip connection 54.5 61.0</cell><cell>82.7 86.9</cell><cell>93.0 95.1</cell><cell>19.2 18.7</cell></row><row><cell></cell><cell>4) Ours w/o bone length</cell><cell>65.8</cell><cell>81.3</cell><cell>92.2</cell><cell>19.6</cell></row><row><cell>Stage 2</cell><cell>5) Ours w/o optimization 6) Ours w/o inertial terms</cell><cell>57.5 51.2</cell><cell>85.3 87.4</cell><cell>93.7 95.4</cell><cell>18.6 62.2</cell></row><row><cell></cell><cell>7) Ours (complete)</cell><cell>43.3</cell><cell>90.1</cell><cell>96.8</cell><cell>17.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation on input configurations, our 4-IMU input setting achieves better performance than both single-modality input and 2 IMUs, while performing close to tedious 12 IMUs.</figDesc><table><row><cell>Modality</cell><cell>Input setting</cell><cell cols="4">MPJPE? PCK@0.2? PCK@0.3? Accel error?</cell></row><row><cell>Single</cell><cell>1) 4 IMUs Only 2) RGB Only</cell><cell>104.7 77.6</cell><cell>57.4 74.2</cell><cell>74.9 88.4</cell><cell>18.5 28.1</cell></row><row><cell></cell><cell>3) 2-IMU-aid</cell><cell>65.0</cell><cell>81.0</cell><cell>92.3</cell><cell>23.4</cell></row><row><cell>Double</cell><cell cols="2">4) 4-IMU-aid (Ours) 43.3</cell><cell>90.1</cell><cell>96.8</cell><cell>17.9</cell></row><row><cell></cell><cell>5) 12-IMU-aid</cell><cell>36.1</cell><cell>93.2</cell><cell>98.3</cell><cell>15.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Others: Ceres solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mierle</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Real-time physics-based motion capture with sparse sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Huerta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2998559.2998564</idno>
		<ptr target="https://doi.org/10.1145/2998559.2998564" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scape: Shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186822.1073207</idno>
		<ptr target="https://doi.org/10.1145/1186822.1073207" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
	<note>SIGGRAPH &apos;05</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracking people with twists and exponential maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.1998.698581</idno>
		<ptr target="https://doi.org/10.1109/CVPR.1998.698581" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-quality streamable free-viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Evseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Markerless motion capture of complex fullbody movement for character animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deutscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Workshop on Computer Animation and Simulation</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Performance capture from sparse multi-view video pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="75" to="92" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fusing visual and inertial sensors with semantics for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="397" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Livecap: Real-time human performance capture from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>TOG)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepcap: Monocular human performance capture using weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Markerless motion capture with unsynchronized moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormahlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Challencap: Monocular 3d capture of challenging human performances using multi-modal references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11400" to="11411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time body tracking with one depth camera and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate long-term multiple people tracking using video and body-worn imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8476" to="8489" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2017.00055</idno>
		<ptr target="https://doi.org/10.1109/3DV.2017.00055" />
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Panoptic Studio: A Massively Multiview System for Social Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Em-pose: 3d human pose estimation from sparse electromagnetic trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Twigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11510" to="11520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pare: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11127" to="11137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic modeling for human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11605" to="11614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ai choreographer: Music conditioned 3d dance generation with aist++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Realtime human motion control with a small number of inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on interactive 3D graphics and games</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Markerless motion capture of multiple characters using multiview image segmentation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2720" to="2735" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time multi-person motion capture from multi-view video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time full-body motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Inertial Motion Capture Systems</title>
		<ptr target="https://noitom.com/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Star: Sparse trained articulated human body regressor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV). vol. LNCS 12355</title>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="598" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<ptr target="http://smpl-x.is.tue.mpg.de" />
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Outdoor human motion capture using inverse kinematics and von misesfisher sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1243" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multisensor-fusion for 3d full-body human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Humor: 3d human motion model for robust pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11488" to="11499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Model-based outdoor performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/OutdoorPerfcap/" />
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast articulated motion tracking using a sums of Gaussians body model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Performance capture from multi-view video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Geometry Processing for 3-D Cinematography</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="127" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference<address><addrLine>Delft, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<ptr target="https://www.vicon.com/" />
		<title level="m">Vicon Motion Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Practical motion capture in everyday surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adelsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vannucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barnwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human pose estimation from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1533" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sparse inertial poser: Automatic 3d human pose estimation from sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="2017" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Outdoor markerless motion capture with sparse handheld video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TVCG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06234</idno>
		<title level="m">Revisiting the continuity of rotation representations in neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Xsens Technologies B</title>
		<ptr target="https://www.xsens.com/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Flycap: Markerless motion capture using multiple autonomous flying cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2284" to="2297" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Eventcap: Monocular 3d capture of high-speed human motions using an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Monoperfcap: Human performance capture from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Transpose: Real-time 3d human translation and pose estimation with six inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3450626.3459786</idno>
		<ptr target="https://doi.org/10.1145/3450626.3459786" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06910</idno>
		<title level="m">Neural descent for visual 3d human pose and shape</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fusing wearable imus with multi-view images for human pose estimation: A geometric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hybridfusion: Realtime performance capture using a single depth sensor and sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
