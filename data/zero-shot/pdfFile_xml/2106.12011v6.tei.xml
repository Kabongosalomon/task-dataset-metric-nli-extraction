<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 P2T: Pyramid Pooling Transformer for Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 P2T: Pyramid Pooling Transformer for Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformer</term>
					<term>backbone network</term>
					<term>efficient self-attention</term>
					<term>pyramid pooling</term>
					<term>scene understanding !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the vision transformer has achieved great success by pushing the state-of-the-art of various vision tasks. One of the most challenging problems in the vision transformer is that the large sequence length of image tokens leads to high computational cost (quadratic complexity). A popular solution to this problem is to use a single pooling operation to reduce the sequence length. This paper considers how to improve existing vision transformers, where the pooled feature extracted by a single pooling operation seems less powerful. To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction. However, pyramid pooling has not been explored in backbone network design. To bridge this gap, we propose to adapt pyramid pooling to Multi-Head Self-Attention (MHSA) in the vision transformer, simultaneously reducing the sequence length and capturing powerful contextual features. Plugged with our pooling-based MHSA, we build a universal vision transformer backbone, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as the backbone network, it shows substantial superiority in various vision tasks such as image classification, semantic segmentation, object detection, and instance segmentation, compared to previous CNN-and transformer-based networks. The code will be released at https://github.com/yuhuan-wu/P2T.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N the past decade, convolutional neural networks (CNNs) have dominated computer vision and achieved many great stories <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref>. The state-of-the-art of various vision tasks on many large-scale datasets has been significantly pushed forward <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b13">[14]</ref>. In an orthogonal field, i.e., natural language processing (NLP), the dominating technique is transformer <ref type="bibr" target="#b14">[15]</ref>. Transformer entirely relies on selfattention to capture the long-range global relationships and has achieved brilliant successes. Considering that global information is also essential for vision tasks, a proper adaption of the transformer <ref type="bibr" target="#b14">[15]</ref> should be useful to overcome the limitation of CNNs, i.e., CNNs usually enlarge the receptive field by stacking more layers.</p><p>Lots of efforts are dedicated to exploring such a proper adaption of the transformer <ref type="bibr" target="#b14">[15]</ref>. Some early attempts use CNNs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref> to extract deep features that are fed into transformers for further processing and regressing the targets <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Dosovitskiy et al. <ref type="bibr" target="#b18">[19]</ref> made a thorough success by applying a pure transformer network for image classification. They split an image into patches and took each patch as a word/token in an NLP application so that transformer can then be directly adopted. This simple method attains competitive performance on ImageNet <ref type="bibr" target="#b9">[10]</ref>. Therefore, a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy VS Parameters</head><p>ResNet ResNeXt PVT Twins Swin PVTv2 P2T (Ours) <ref type="figure">Fig. 1</ref>. Experimental results for semantic segmentation on the ADE20K dataset <ref type="bibr" target="#b13">[14]</ref>. Following PVT <ref type="bibr" target="#b20">[21]</ref>, Semantic FPN <ref type="bibr" target="#b25">[26]</ref> is chosen as the basic method, equipped with different backbone networks, including ResNet <ref type="bibr" target="#b3">[4]</ref>, ResNeXt <ref type="bibr" target="#b26">[27]</ref>, PVT <ref type="bibr" target="#b20">[21]</ref>, Twins <ref type="bibr" target="#b27">[28]</ref>, Swin Transformer <ref type="bibr" target="#b21">[22]</ref>, PVTv2 <ref type="bibr" target="#b28">[29]</ref>, and our P2T.</p><p>new concept of the vision transformer appears. In a very short period, a large amount of literature has emerged to improve the vision transformer <ref type="bibr" target="#b18">[19]</ref>, and much better performance than CNNs has been achieved <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref>.</p><p>Nevertheless, there is still one challenging problem in vision transformers, i.e., the length of the data sequence. When viewing image patches as tokens, the sequence length is much longer than in NLP applications. For example, in arXiv:2106.12011v6 [cs.CV] 31 Aug 2022 NLP, the well-known WMT2014 English-German dataset <ref type="bibr" target="#b29">[30]</ref> has 50M English words with 2M sentences, with an average sequence length of <ref type="bibr">25.</ref> In contrast, in computer vision, we usually use the image resolution of 224 ? 224 for image classification on the ImageNet dataset <ref type="bibr" target="#b9">[10]</ref>, resulting in the sequence length of 3136 if we use the common patch size of 4 ? 4. Since the computational and space complexity of Multi-Head Self-Attention (MHSA) in the transformer is quadratic (rather than linear in CNNs) to the image size, directly applying the transformer to vision tasks has a high requirement for computational resources. To make a pure transformer network possible for image classification, ViT <ref type="bibr" target="#b18">[19]</ref> uses large patch sizes, e.g., <ref type="bibr" target="#b15">16</ref> and 32, to reduce the sequence length, and achieves great success for image classification. Later, many transformer works significantly improve the performance of ViT <ref type="bibr" target="#b18">[19]</ref> by introducing the pyramid structure <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, where the input layer first uses the small patch size of 4?4 and the sequence length is gradually reduced by merging adjacent image patches.</p><p>To further reduce the computational cost of MHSA, PVT <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b22">[23]</ref> use a single pooling operation to downsample the feature map in the computation of MHSA. With the pooled features, they model token-to-region relationship rather than the expected token-to-token relationship. Swin Transformer <ref type="bibr" target="#b21">[22]</ref> proposes to compute MHSA within small windows rather than across the whole input, modeling local relationships. It uses a window shift strategy to gradually enlarge the receptive field, like CNNs, enlarging the receptive field through stacking more layers <ref type="bibr" target="#b32">[33]</ref>. However, an essential characteristic of the vision transformer is its direct global relationship modeling, which is also why we transfer from CNNs to transformers.</p><p>Here, we consider how to improve PVT <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b22">[23]</ref> where the pooled feature extracted by a single pooling operation seems less powerful. If we can squeeze the input feature into a powerful representation with a short sequence length, we may achieve better performance. To this end, we note that pyramid pooling <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref> is a long-history technique for computer vision, which extracts contextual information and utilizes multiple pooling operations with different receptive fields and strides onto the input feature map. This simple technique has been demonstrated to be effective in various downstream vision tasks such as semantic segmentation <ref type="bibr" target="#b36">[37]</ref> and object detection <ref type="bibr" target="#b35">[36]</ref>. Nevertheless, recent pyramid pooling approaches highly rely on a pretrained CNN backbone, and thus they are limited to a specific task. In another word, the pyramid pooling technique has not been explored in the backbone network design that has broad applications. Motivated by this, we bridge this gap by adapting pyramid pooling to the vision transformer block for simultaneously reducing the sequence length and learning powerful contextual representations. Pyramid pooling is also very efficient and thus will only induce negligible computational cost for the vision transformer.</p><p>We achieve this goal by proposing a new transformer backbone network, i.e., Pyramid Pooling Transformer (P2T). We adapt the idea of pyramid pooling to the computation of multi-head self-attention (MHSA) in the vision transformer, reducing the computational cost of MHSA and capturing rich contextual information simultaneously. By applying the new pooling-based MHSA, P2T exhibits stronger ability in feature representation learning and visual recognition than PVT <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b22">[23]</ref> which are based on a single pooling operation. We evaluate P2T for various typical vision tasks, like image classification, semantic segmentation, object detection, and instance segmentation. Extensive experiments demonstrate that P2T performs better than all previous CNN-and transformer-based backbone networks for these fundamental vision tasks (see <ref type="figure">Fig. 1</ref> for comparisons on semantic segmentation).</p><p>In summary, our main contributions include:</p><p>? We encapsulate pyramid pooling to MHSA, simultaneously reducing the sequence length of image tokens and extracting powerful contextual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We plug our pooling-based MHSA into the vision transformer to build a new backbone network, i.e., P2T, making it flexible and powerful for visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We conduct extensive experiments to demonstrate that, when applied as a backbone network for various scene understanding tasks, P2T achieves substantially better performance than previous CNNand transformer-based networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Neural Networks</head><p>Since AlexNet <ref type="bibr" target="#b0">[1]</ref> won the champion in the ILSVRC-2012 competition <ref type="bibr" target="#b9">[10]</ref>, numerous advanced techniques have been invented for improving CNNs, achieving many successful stories in computer vision. VGG <ref type="bibr" target="#b1">[2]</ref> and GoogleNet <ref type="bibr" target="#b2">[3]</ref> first try to deepen CNNs for better image recognition. Then, ResNets <ref type="bibr" target="#b3">[4]</ref> succeed in building very deep CNNs with the help of residual connections. ResNeXts <ref type="bibr" target="#b26">[27]</ref> and Res2Nets <ref type="bibr" target="#b8">[9]</ref> improve ResNets <ref type="bibr" target="#b3">[4]</ref> by exploring its cardinal operation. DenseNets <ref type="bibr" target="#b4">[5]</ref> introduce dense connections that connect each layer to all its subsequent layers for easing optimization. MobileNets <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> decompose a vanilla convolution into a 1 ? 1 convolution and a depthwise separable convolution to build lightweight CNNs for mobile and embedded vision applications. ShuffleNets <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> further reduce the latency of MobileNets <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> by replacing the 1 ? 1 convolution with the grouped 1 ? 1 convolution and the channel shuffle operation. EfficientNet <ref type="bibr" target="#b6">[7]</ref> and MnasNet <ref type="bibr" target="#b41">[42]</ref> adopt neural architecture search (NAS) to search for optimal network architectures. Since our work focuses on the transformer <ref type="bibr" target="#b14">[15]</ref>, a comprehensive survey of CNNs is beyond the scope of this paper. Please refer to <ref type="bibr" target="#b42">[43]</ref> and <ref type="bibr" target="#b43">[44]</ref> for a more extensive survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer</head><p>Transformer is initially proposed for machine translation in NLP <ref type="bibr" target="#b14">[15]</ref>. Through MHSA, transformer entirely relies on self-attention to model global token-to-token dependencies.</p><p>Considering that global relationship is also highly required by computer vision tasks, it is a natural idea to adopt transformer for improving vision tasks. However, transformer is designed to process sequence data and thus cannot process images directly. Hence, some researchers use CNNs to extract 2D representation that is then flattened and fed into the transformer <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>. DETR <ref type="bibr" target="#b15">[16]</ref> is a milestone in this direction. Instead of relying on the CNN backbone for feature extraction, Dosovitskiy et al. <ref type="bibr" target="#b18">[19]</ref> proposed the first vision transformer (ViT). They split an image into small patches, and each patch is viewed as a word/token in an NLP application. Thus, a pure transformer network can be directly adopted with the class token used for image classification. Their method achieved competitive performance on Ima-geNet <ref type="bibr" target="#b9">[10]</ref>. Then, DeiT <ref type="bibr" target="#b47">[48]</ref> alleviates the required resources for training ViT <ref type="bibr" target="#b18">[19]</ref> via knowledge distillation. T2T-ViT <ref type="bibr" target="#b48">[49]</ref> proposes to split an image with overlapping to better preserve local structures. CvT <ref type="bibr" target="#b30">[31]</ref> introduces depthwise convolution for generating the query, key, and value in the computation of MHSA. CPVT <ref type="bibr" target="#b49">[50]</ref> proposes to replace the absolute positional encoding with the conditional positional encoding via a depthwise convolution. Some efforts are contributed to building the pyramid structure for the vision transformer using pooling operations <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Among them, PVT <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b22">[23]</ref> first adopt the single pooling operation to reduce the number of tokens when computing MHSA. In this way, they actually conduct tokento-region relationship modeling, not the expected tokento-token modeling. Since this paper also resolves the long sequence length problem through pooling, we adopt PVT <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b22">[23]</ref> as strong baselines in this paper. Swin Transformer <ref type="bibr" target="#b21">[22]</ref> reduces the computational load of MHSA by computing it within small windows. However, Swin Transformer gradually achieves global relationship modeling by window shift, somewhat like CNNs that enlarge the receptive field by stacking more layers <ref type="bibr" target="#b32">[33]</ref>. Hence, we think that Swin Transformer <ref type="bibr" target="#b21">[22]</ref> sacrifices an essential characteristic of the vision transformer, i.e., direct global relationship modeling.</p><p>Different from PVT <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b22">[23]</ref> where the pooled feature extracted by a single pooling operation seems less powerful, we adapt the idea of pyramid pooling to the vision transformer, simultaneously reducing the sequence length and learning powerful contextual representations. With more powerful representations, it is intuitive that pyramid pooling may work better than single pooling for computing self-attention in MHSA. Pyramid pooling is very efficient and thus will only induce negligible computational cost. Experiments show that the proposed P2T performs substantially better performance than previous CNN-and transformer-based networks. Besides, our design is also compatible with other transformer techniques such as patch embedding <ref type="bibr" target="#b50">[51]</ref>, positional encoding <ref type="bibr" target="#b49">[50]</ref>, and feed-forward network <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pyramid Pooling</head><p>In computer vision, pyramid pooling is a long-history and widely-acknowledged technique for extracting feature presentations. Before the renaissance of deep CNNs <ref type="bibr" target="#b0">[1]</ref>, there emerged several well-known works that applied pyramid pooling for recognizing natural scenes <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Inspired by <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, He et al. <ref type="bibr" target="#b35">[36]</ref> introduced pyramid pooling to deep CNNs for image classification and object detection. They adopted several pooling operations to pool the final convolutional feature map of a CNN backbone into several fixed-size maps. These resulting maps are then flattened and concatenated into a fixed-length representation for robust visual recognition. Then, Zhao et al. <ref type="bibr" target="#b36">[37]</ref> applied pyramid pooling for semantic segmentation. Instead of flattening in <ref type="bibr" target="#b35">[36]</ref>, they upsampled the pooled fixed-size maps into the original size and concatenated the upsampled maps for prediction. Their success suggests the effectiveness of pyramid pooling in dense prediction. After that, pyramid pooling has been widely applied to various vision tasks such as semantic segmentation <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b56">[57]</ref> and object detection <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref>.</p><p>Unlike existing literature that explores pyramid pooling in CNNs for specific tasks, we propose to adapt the concept of pyramid pooling to the vision transformer backbone network. With this idea, we first embed the pyramid pooling into the basic pooling-based attention block of our P2T backbone, which can simultaneously reduce the sequence length and learn powerful contextual feature representations. P2T can be easily used by various vision tasks for feature representation learning, while previous works about pyramid pooling are limited to a specific vision task. Extensive experiments on image classification, semantic segmentation, object detection, and instance segmentation demonstrate the superiority of P2T compared with existing CNN-and transformer-based networks. Therefore, this work is distinctive and would benefit the research on various vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first provide an overview of our P2T networks in ?3.1. Then, we present the architecture of P2T with pooling-based MHSA in ?3.2. Finally, we introduce some implementation details of our networks in ?3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overall architecture of P2T is illustrated in <ref type="figure">Fig. 2</ref>.</p><p>With a natural color image as input, P2T first splits it into H 4 ? W 4 patches, each flattened to 48 (4 ? 4 ? 3) elements. Following <ref type="bibr" target="#b20">[21]</ref>, we feed these flattened patches to a patch embedding module, which consists of a linear projection layer followed by the addition with a learnable positional encoding. The patch embedding module will expand the feature dimension from 48 to C 1 . Then, we stack the proposed pyramid pooling transformer blocks that will be introduced in ?3.2. The whole network can be divided into four stages with feature dimensions of C i (i = {1, 2, 3, 4}), respectively. Between every two stages, each 2 ? 2 patch group is concatenated and linearly projected from </p><formula xml:id="formula_0">4 ? C i to C i+1 dimension (i = {1, 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pyramid Pooling Transformer</head><p>Pyramid pooling has been widely used in many scene understanding tasks collaborating with CNNs <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, [61]- <ref type="bibr" target="#b67">[68]</ref>. However, existing literature usually applies pyramid pooling on top of CNN backbones for extracting global and contextual information for a specific task. In contrast, this paper is the first to explore pyramid pooling in transformers and backbone networks, targeting for improving various scene understanding tasks generically. To this end, we adapt the idea of pyramid pooling to the transformer, simultaneously reducing the computational load of MHSA and capturing rich contextual information.</p><p>Let us continue by introducing the proposed P2T, the structure of which is illustrated in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>. The input first passes through the pooling-based MHSA, whose output is added with the residual identity, followed by LayerNorm <ref type="bibr" target="#b68">[69]</ref>. Like the traditional transformer block <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b47">[48]</ref>, a feed-forward network (FFN) follows for feature projection. A residual connection and LayerNorm <ref type="bibr" target="#b68">[69]</ref> are applied again. The above process can be formulated as</p><formula xml:id="formula_1">X att = LayerNorm(X + P-MHSA(X)), X out = LayerNorm(X att + FFN(X att )),<label>(1)</label></formula><p>where X, X att , and X out are the input, the output of MHSA, and the output of the transformer block, respectively. P-MHSA is the abbreviation of pooling-based MHSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pooling-based MHSA</head><p>Here, we present the design of our pooling-based MHSA. Its structure is shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>. First, the input X is reshaped into the 2D space. Then, we apply multiple average pooling layers with various ratios onto the reshaped X to generate pyramid feature maps, like</p><formula xml:id="formula_2">P 1 = AvgPool 1 (X), P 2 = AvgPool 2 (X), ? ? ? , P n = AvgPool n (X),<label>(2)</label></formula><p>where {P 1 , P 2 , ..., P n } denote the generated pyramid feature maps and n is the number of pooling layers. Next, we feed pyramid feature maps to the depthwise convolution for relative positional encoding:</p><formula xml:id="formula_3">P enc i = DWConv(P i ) + P i , i = 1, 2, ? ? ? , n,<label>(3)</label></formula><p>where DWConv(?) indicates the depthwise convolution with the kernel size 3 ? 3, and P enc i is P i with the relative positional encoding. Since P i is the pooled feature, the operation in Equ. 3 only has a little computational cost. After that, we flatten and concatenate these pyramid feature maps:</p><formula xml:id="formula_4">P = LayerNorm(Concat(P enc 1 , P enc 2 , ..., P enc n )),<label>(4)</label></formula><p>where the flattening operation is omitted for simplicity. In this way, P can be a shorter sequence than the input X if pooling ratios are large enough. Besides, P contains the contextual abstraction of the input X and can thus serve as a strong substitute for the input X when computing MHSA. Suppose the query, key, and value tensors in MHSA <ref type="bibr" target="#b18">[19]</ref> are Q, K, and V, respectively. Instead of using traditional</p><formula xml:id="formula_5">(Q, K, V) = (XW q , XW k , XW v ),<label>(5)</label></formula><p>we propose to use</p><formula xml:id="formula_6">(Q, K, V) = (XW q , PW k , PW v ),<label>(6)</label></formula><p>TABLE 1 Detailed settings of the proposed P2T. The parameters of building blocks are shown in brackets, with the numbers of blocks stacked. For the first stage, we apply a 7 ? 7 convolution with C output channels and a stride of S for patch embedding. Each IRB uses an expansion ratio of E. For simplicity, we omit the patch embedding operation, i.e., a 3 ? 3 convolution with a stride of S = 2, after the t-th stage (t = {2, 3, 4}). "#Params" refers to the number of parameters. "#Flops" denotes the computational cost with the input size of 224 ? 224. </p><formula xml:id="formula_7">Stage Input Size Operator P2T-Tiny P2T-Small P2T-Base P2T-Large 1 224 ? 224 7 ? 7 conv. C = 48, S = 4 C = 64, S = 4 2 56 ? 56 P-MHSA IRB C = 48 E = 8 ? 2 C = 64 E = 8 ? 2 C = 64 E = 8 ? 3 C = 64 E = 8 ? 3 3 28 ? 28 P-MHSA IRB C = 96 E = 8 ? 2 C = 128 E = 8 ? 2 C = 128 E = 8 ? 4 C = 128 E = 8 ? 8 4 14 ? 14 P-MHSA IRB C = 240 E = 4 ? 6 C = 320 E = 4 ? 9 C = 320 E = 4 ? 18 C = 320 E = 4 ? 27 5 7 ? 7 P-MHSA IRB C = 384 E = 4 ? 3 C = 512 E = 4 ? 3 C = 512 E = 4 ? 3 C = 640 E = 4 ? 3 1 ? 1 - Global</formula><formula xml:id="formula_8">A = Softmax( Q ? K T ? d K ) ? V,<label>(7)</label></formula><p>where d K is the channel dimension of K, and ? d K can serve as an approximate normalization. The Softmax function is applied along the rows of the matrix. Equ. 7 omits the concept of multiple heads <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref> for simplicity.</p><p>Since K and V have a smaller length than X, the proposed P-MHSA is more efficient than traditional MHSA <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Besides, since K and V contains highly-abstracted multi-scale information, the proposed P-MHSA has a stronger capability in global contextual dependency modeling, which is helpful for scene understanding <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b64">[65]</ref>- <ref type="bibr" target="#b67">[68]</ref>. From a different perspective, pyramid pooling is usually used as an effective technique connected upon backbone networks; in contrast, this paper first exploits pyramid pooling within backbone networks through transformers, thus providing powerful feature representation learning for scene understanding. With the above analyses, P-MHSA is expected to be more efficient and more effective than traditional MHSA <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>Analysis of computational complexity. As described in Equ. 2, the proposed pooling-based attention leverages several pooling operations to generate pyramid feature maps. The pyramid pooling operation only has negligible O(N C) computational complexity, where N and C represent the sequence length and the feature dimension, respectively. Hence, the computational complexity for computing selfattention can be formulated as</p><formula xml:id="formula_9">O(P-MHSA) = (N + 2M )C 2 + 2N M C,<label>(8)</label></formula><p>where M is the concatenated sequence length of all pooled features. For the default pooling ratios of {12, 16, 20, 24}, we have M ? N 66.3 ? N 8 2 , which is comparable with the computational cost of MHSA in PVT <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feed-Forward Network</head><p>Feed-Forward Network (FFN) is an essential component of transformers for feature enhancement <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b69">[70]</ref>. Previous transformers usually apply an MLP as the FFN <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> and entirely rely on attention to capture inter-pixel dependencies. Though effective, this architecture is not good at learning 2D locality, which plays a critical role in scene understanding. To this end, we follow <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> to insert the depthwise convolution into FFN so that the resulting transformer can inherit the merits of both transformer (i.e., longrange dependency modeling) and CNN (i.e., 2D locality). Specifically, we adopt the Inverted Bottleneck Block (IRB), proposed in MobileNetV2 <ref type="bibr" target="#b38">[39]</ref>, as the FFN.</p><p>To adapt IRB for the vision transformer, we first transform the input sequence X att to a 2D feature map X I att :</p><formula xml:id="formula_10">X I att = Seq2Image(X att ),<label>(9)</label></formula><p>where Seq2Image(?) is to reshape the 1D sequence to a 2D feature map. Given the input X I att , IRB can be directly applied, like</p><formula xml:id="formula_11">X 1 IRB = Act(X I att W 1 IRB ), X out IRB = Act(DWConv(X 1 IRB ))W 2 IRB ,<label>(10)</label></formula><p>where W 1 IRB , W 2 IRB indicate the weight matrices of 1?1 convolutions, "Act" indicates the nonlinear activation function, X out IRB is the output of IRB. Since X out IRB is a 2D feature map, we finally transform it to a 1D sequence:</p><formula xml:id="formula_12">X S IRB = Image2Seq(X out IRB ),<label>(11)</label></formula><p>where Image2Seq(?) is the operation that reshapes the 2D feature map to a 1D sequence. X S IRB is the output of FFN, with the same shape as X att .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>P2T with different depths. Following previous backbone architectures <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, we build P2T with different depths via stacking the different number of pyramid pooling transformers at each stage. In this manner, we propose four versions of P2T, i.e., P2T-Tiny, P2T-Small, P2T-Base, and P2T-Large with similar numbers of parameters to ResNet-18 <ref type="bibr" target="#b3">[4]</ref>, ResNet-50 <ref type="bibr" target="#b3">[4]</ref>, ResNet-101 <ref type="bibr" target="#b3">[4]</ref>, and PVT-Large <ref type="bibr" target="#b20">[21]</ref>, respectively. Each head of P-MHSA has 64 feature channels except that each head has 48 feature channels in P2T-Tiny. Other configurations for different versions of P2T are shown in <ref type="table">Table 1</ref>.</p><p>Pyramid pooling settings. We empirically set the number of parallel pooling operations in P-MHSA as 4. At different stages, the pooling ratios of pyramid pooling transformers are different. The pooling ratios for the first stage are empirically set as {12, 16, 20, 24}. Pooling ratios in each next stage are divided by 2 except that, in the last stage, they are set as {1, 2, 3, 4}. In each transformer block, all depthwise convolutions (Equ. 3) in P-MHSA share the same parameters.</p><p>Other settings. Although a larger kernel size (e.g., 5 ? 5) of depthwise convolution (in Equ. 3) can bring better performance, the kernel size of all depthwise convolutions is set to 3 ? 3 for efficiency. We choose Hardswish <ref type="bibr" target="#b70">[71]</ref> as the nonlinear activation function because it saves much memory compared with GELU <ref type="bibr" target="#b71">[72]</ref>. Hardswish <ref type="bibr" target="#b70">[71]</ref> also empirically works well. Same with PVTv2 <ref type="bibr" target="#b28">[29]</ref>, we apply overlapped patch embedding. That is, we use 3 ? 3 convolution with a stride of 2 for patch embedding from the second to the last stage, while we apply a 7 ? 7 convolution with a stride of 4 for patch embedding in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first introduce the experiments on image classification in ?4.1. Then we validate P2T's effectiveness on several scene understanding tasks, i.e., semantic segmentation, object detection, and instance segmentation in ?4.2, ?4.3, and ?4.4, respectively. At last, we conduct ablation studies for better understanding our method in ?4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Classification</head><p>Image classification is the most common task for evaluating the capability of backbone networks. It aims to assign a class label to each natural image input. Many other tasks build on top of image classification via applying classification networks as the backbones for feature extraction. Experimental setup. As described in ?3.1, only the output feature B 4 of the last stage is utilized here. Following regular CNN networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, we append a global average pooling layer and a fully-connected layer on top of B 4 to obtain the final classification scores. We train our network on the ImageNet-1K dataset <ref type="bibr" target="#b9">[10]</ref>, which has 1.28M training images and 50k validation images. For a fair comparison, we follow PVT <ref type="bibr" target="#b20">[21]</ref> to adopt the same training protocols as DeiT <ref type="bibr" target="#b47">[48]</ref> (without knowledge distillation), which is a standard choice for training vision transformers. Specifically, we use AdamW <ref type="bibr" target="#b75">[76]</ref> as the optimizer, with the initial learning rate of 1e-3, weight decay of 0.05, and a mini-batch of 1024 images. We train P2T for 300 epochs with the cosine learning rate decay strategy. Images are resized to the size of 224 ? 224 for training and testing. Models are warmed up Image classification results on the ImageNet-1K dataset <ref type="bibr" target="#b9">[10]</ref>. "Top-1" indicates the top-1 accuracy rate. "*" indicates the results with knowledge distillation <ref type="bibr" target="#b47">[48]</ref>. "#P (M)" denotes the number of parameters (M). Both the number of computational cost (GFlops) and running speed (frames per second, FPS) are reported with the default 224 ? 224 input size for each network except that the speed of ViT-B <ref type="bibr" target="#b18">[19]</ref> is tested with the input size of 384 ? 384. FPS is tested on a single RTX 2070 GPU. The results of the proposed P2T are marked in bold. for the first five epochs. The data augmentation is also the same as <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Experimental results. The quantitative comparisons are summarized in  better than Swin Transformer <ref type="bibr" target="#b21">[22]</ref> with fewer network parameters and less computational cost. Although PVTv2 <ref type="bibr" target="#b28">[29]</ref> has large improvement over PVT <ref type="bibr" target="#b20">[21]</ref>, our P2T-Tiny/Small/Base/Large still have 1.1%/0.4%/0.3%/0.3% improvement over PVTv2-B1/B2/B3/B4 <ref type="bibr" target="#b28">[29]</ref> with fewer parameters and less computational cost. P2T applies four parallel pooling operations when computing self-attention, still achieving competitive speed with PVTv2 <ref type="bibr" target="#b28">[29]</ref>. ViL <ref type="bibr" target="#b72">[73]</ref> achieves competitive performance with P2T. Nevertheless, the speed of ViL <ref type="bibr" target="#b72">[73]</ref> is much slower than our P2T, and the computational cost of ViL <ref type="bibr" target="#b72">[73]</ref> is also much larger than P2T. P2T also largely outperforms ViT <ref type="bibr" target="#b18">[19]</ref> and DeiT <ref type="bibr" target="#b47">[48]</ref> with much fewer parameters, implying that P2T achieves better performance without a large amount of training data and knowledge distillation. Therefore, P2T is very capable for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Segmentation</head><p>Given a natural image input, semantic segmentation aims at assigning a semantic label to each pixel. It is one of the most fundamental dense prediction tasks in computer vision.</p><p>Experimental setup. We evaluate P2T and its competitors on the ADE20K <ref type="bibr" target="#b13">[14]</ref> dataset. The ADE20K dataset is a challenging scene understanding dataset with 150 fine-grained semantic classes. This dataset has 20000, 2000, and 3302 images for training, validation, and testing, respectively. Following <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>, Semantic FPN <ref type="bibr" target="#b25">[26]</ref> is chosen as the basic method for a fair comparison. We replace the backbone of Semantic FPN <ref type="bibr" target="#b25">[26]</ref> with various network architectures. All backbones of semantic FPN have been pretrained on the ImageNet-1K <ref type="bibr" target="#b9">[10]</ref> dataset, and other layers are initialized using the Xavier method <ref type="bibr" target="#b76">[77]</ref>. All networks are trained for 80k iterations. We apply AdamW <ref type="bibr" target="#b75">[76]</ref> as the network optimizer, with the initial learning rate of 1e-4 and weight decay of 1e-4. The poly learning rate schedule with ? = 0.9 is adopted. Each mini-batch has 16 images. Images are resized and randomly cropped to 512 ? 512 for training. Synchronized batch normalization across GPUs is also enabled. During testing, images are resized to the shorter side of 512 pixels. Multi-scale testing and flipping are disabled. Following <ref type="bibr" target="#b20">[21]</ref>, we use the MMSegmentation toolbox <ref type="bibr" target="#b77">[78]</ref> to implement the above experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results.</head><p>Quantitative comparison results are shown in <ref type="table" target="#tab_6">Table 3</ref>. We compare our proposed P2T with ResNets <ref type="bibr" target="#b3">[4]</ref>, ResNeXts <ref type="bibr" target="#b26">[27]</ref>, PVT <ref type="bibr" target="#b20">[21]</ref>, Swin Transformers <ref type="bibr" target="#b21">[22]</ref>, Twins <ref type="bibr" target="#b27">[28]</ref>, and PVTv2 <ref type="bibr" target="#b28">[29]</ref>. The results of each network are from the official papers or re-implemented using the official training configurations. Benefiting from the pyramid pooling technique, the results of Semantic FPN <ref type="bibr" target="#b25">[26]</ref> with our P2T backbone are much better than other CNN and transformer competitors. Typically, P2T-Tiny/Small/Base/Large are 10.5%/10.0%/9.9%/9.2% better than ResNet-18/50/101 <ref type="bibr" target="#b3">[4]</ref> and ResNeXt-10-64x4d <ref type="bibr" target="#b26">[27]</ref> with fewer parameters and GFlops, respectively. Compared with Swin Transformer <ref type="bibr" target="#b21">[22]</ref> which introduces local selfattention with transformers, P2T-Small/Base/Large achieve 5.2%/3.5%/3.4% improvement over Swin-T/S/B <ref type="bibr" target="#b21">[22]</ref>, respectively, showing that global relationship modeling is significant for visual recognition. Twins <ref type="bibr" target="#b27">[28]</ref> combine the local self-attention from Swin Transformers <ref type="bibr" target="#b21">[22]</ref> and the global self-attention from PVT <ref type="bibr" target="#b20">[21]</ref>. As can be observed, Twins <ref type="bibr" target="#b27">[28]</ref> perform better than Swin Transformers <ref type="bibr" target="#b21">[22]</ref>, suggesting that global self-attention is significant again. Unlike Twins <ref type="bibr" target="#b27">[28]</ref>, we apply pure global self-attention via pyramid pooling, learning richer contexts. P2T-Small/Base/Large are 3.5%/3.4%/2.7% better than Twins-SVT-S/B/L <ref type="bibr" target="#b27">[28]</ref>, respectively. PVTv2 <ref type="bibr" target="#b28">[29]</ref> is the improved version of PVT <ref type="bibr" target="#b20">[21]</ref>, serving as the strongest competitor for our P2T. P2T-Tiny/Small/Base/Large are 1.9%/0.6%/1.4%/0.8% better than PVTv2-B1/B2/B3/B4 <ref type="bibr" target="#b28">[29]</ref>, respectively. Besides, P2T-Tiny/Small/Base/Large always have fewer parameters, less computational cost, and faster speed than the corresponding PVTv2-B1/B2/B3/B4 <ref type="bibr" target="#b28">[29]</ref>. At last, we found that P2T-Tiny is 3.2% better than ResNeXt-101-64x4d <ref type="bibr" target="#b26">[27]</ref> with twice the speed. Based on the above observations, we can conclude that P2T is very capable for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Detection</head><p>Object detection is also one of the most fundamental and challenging tasks for decades in computer vision. It aims to detect and recognize instances of semantic objects of certain classes in natural images. Here, we evaluate P2T and its competitors on the MS-COCO <ref type="bibr" target="#b10">[11]</ref> dataset.</p><p>Experimental setup. MS-COCO <ref type="bibr" target="#b10">[11]</ref> is a large-scale challenging dataset for object detection, instance segmentation, and keypoint detection. MS-COCO train2017 (118k images) and val2017 (5k images) sets are used for training and validation in our experiments, respectively. RetinaNet <ref type="bibr" target="#b73">[74]</ref> is applied as the basic framework because it has been widely acknowledged by this community <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Each  <ref type="figure">R-CNN [75]</ref> on the MS-COCO val2017 set <ref type="bibr" target="#b10">[11]</ref>. "R" and "X" represent the ResNet <ref type="bibr" target="#b3">[4]</ref> and ResNeXt <ref type="bibr" target="#b26">[27]</ref>, respectively. The number of Flops is computed with the input size of 800 ? 1280.</p><p>FPS is tested on a single RTX 2070 GPU. The results of P2T backbones are marked in bold. mini-batch has 16 images with an initial learning rate of 1e-4. Following the popular MMDetection toolbox <ref type="bibr" target="#b78">[79]</ref>, we train each network for 12 epochs, and the learning rate is divided by 10 after 8 and 11 epochs. The network optimizer is AdamW <ref type="bibr" target="#b75">[76]</ref>, a popular optimizer for training transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>The weight decay is set as 1e-4. During training and testing, the shorter side of input images is resized to 800 pixels. The longer side will keep the ratio of the images within 1333 pixels. In the training stage, only random horizontal flipping is used for data augmentation. Standard COCO API is utilized for evaluation, and we report results in terms of AP, AP 50 , AP 75 , AP S , AP M , and AP L metrics. AP S , AP M , and AP L mean AP scores for small, medium, and large objects defined in <ref type="bibr" target="#b10">[11]</ref>, respectively. AP is usually viewed as the primary metric. For each metric, larger scores indicate better performance. We also report the number of parameters and the computational cost for reference.</p><p>Experimental results. The evaluation results on the MS-COCO dataset are summarized in the left part of <ref type="table" target="#tab_7">Table 4</ref>.</p><p>The results of other networks are from the official papers or re-implemented using the official configurations. The below discussion refers to the metric of AP if not stated. We can observe that our P2T achieves the best performance under all tiny/small/large complexity settings. For example, P2T-Small achieves 2.9%, 1.4%, and 0.6% higher AP over Swin-T <ref type="bibr" target="#b21">[22]</ref>, Twins-SVT-S <ref type="bibr" target="#b27">[28]</ref>, and PVTv2-B2 <ref type="bibr" target="#b28">[29]</ref>, respectively. P2T-Tiny is 1.1% better than PVTv2 <ref type="bibr" target="#b28">[29]</ref>. Compared with ViL <ref type="bibr" target="#b72">[73]</ref>, P2T-Tiny/Small are 0.5% and 0.2% better than ViL-Tiny/Small <ref type="bibr" target="#b72">[73]</ref>, respectively. Note that ViL <ref type="bibr" target="#b72">[73]</ref> runs at a much slower speed than P2T, as shown in <ref type="table" target="#tab_7">Table 4</ref>. With the base complexity setting, P2T-Base outperforms Swin-S</p><p>[29] by 1.0% and is 0.2% better than the best competitor PVTv2-B3. With the large complexity setting, P2T-Large achieves 1.1% and 1.9% better AP than PVTv2-B4 <ref type="bibr" target="#b28">[29]</ref> and Twins-SVT-B <ref type="bibr" target="#b27">[28]</ref>, respectively. At all complexity levels, P2T always outperforms PVTv2 <ref type="bibr" target="#b28">[29]</ref> with fewer network parameters, less computational cost, and faster speed. P2T-Tiny/Small/Base/Large are 9.5%/8.1%/7.0%/6.2% better than ResNet-18/50/101 <ref type="bibr" target="#b3">[4]</ref> and ResNeXt-101-64x4d <ref type="bibr" target="#b26">[27]</ref>, respectively. Therefore, P2T is very capable for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Instance Segmentation</head><p>Instance segmentation is another fundamental vision task. It can be regarded as an advanced case of object detection by outputting fine-grained object masks instead of bounding boxes in object detection.</p><p>Experimental setup. We evaluate the performance of instance segmentation on the well-known MS-COCO dataset <ref type="bibr" target="#b10">[11]</ref>. MS-COCO train2017 and val2017 sets are used for training and validation in our experiments. Mask R-CNN <ref type="bibr" target="#b74">[75]</ref> is applied as the basic framework using different backbone networks. The training settings are the same as what we use for object detection in ?4.3. We report evaluation results for object detection and instance segmentation in terms of AP b , AP b 50 , AP b 75 , AP m , AP m 50 , and AP m 75 metrics, where "b" and "m" indicate bounding box and mask metrics, respectively. AP b and AP m are set as the primary evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results.</head><p>The comparisons between P2T and its competitors are displayed in the right part of <ref type="bibr">TABLE 5</ref> Ablation studies on multiple pyramid pooling ratios. The "D ratio" indicates the downsampling ratio between the initial sequence length and the downsampled sequence length. "Top-1" denotes the top-1 classification accuracy rate on the ImageNet-1K validation set <ref type="bibr" target="#b9">[10]</ref>, and "mIoU" indicates the results for semantic segmentation on the ADE20K dataset <ref type="bibr" target="#b13">[14]</ref>. Ablation studies for replacing single pooling operation with multiple pooling operations at different stages. Since both stage 2 and 3 of our network only have two basic blocks, we merge them into one choice. "Top-1" denotes the top-1 classification accuracy rate on the ImageNet-1K validation set <ref type="bibr" target="#b9">[10]</ref>, and "mIoU" is the results for semantic segmentation on the ADE20K dataset <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Stage # of Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 7</head><p>Ablation studies on the choices of pooling operations. We can see that other choices work worse than average pooling. "Top-1" indicates the top-1 accuracy rate on the ImageNet-1K dataset <ref type="bibr" target="#b9">[10]</ref> for image classification. "mIoU" is the mean IoU rate on the ADE20K dataset <ref type="bibr" target="#b13">[14]</ref> for semantic segmentation.  <ref type="bibr" target="#b28">[29]</ref> with fewer parameters, less computational cost, and faster speed, respectively. In terms of the mask metric AP m , we also observe similar improvements as observed using bounding box metrics. Compared with ResNet-based backbones, P2T significantly outperforms ResNets <ref type="bibr" target="#b3">[4]</ref> and ResNeXts <ref type="bibr" target="#b26">[27]</ref> at all complexity levels. It is also surprising that our lightest P2T-Tiny is 0.5% and 1.2% better than ResNeXt-101-64x4d <ref type="bibr" target="#b26">[27]</ref> in terms of bounding box and mask metrics, respectively. Therefore, P2T is very capable for instance segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>Experimental setup. In this section, we perform ablation studies to analyze the efficacy of each design choice in P2T.</p><p>We evaluate the performance of model variants on semantic segmentation and image classification. Due to the limited computational resources, we only train each model variant on the ImageNet dataset <ref type="bibr" target="#b9">[10]</ref> for 100 epochs, while other training settings keep the same as in ?4.1. Then, we fine-tune the ImageNet-pretrained model on the ADE20K dataset <ref type="bibr" target="#b13">[14]</ref> with the same training settings as in ?4.2.</p><p>Multiple pyramid pooling ratios. To validate the significance of using multiple pooling ratios, we conduct experiments to evaluate the performance of P2T with one/two/four parallel pooling operations. The baseline is P2T-Small without relative positional encoding, IRB, and overlapping patch embedding. The results are shown in <ref type="table">Table 5</ref>. As can be seen, the single pooling operation with a large pooling ratio (e.g., <ref type="bibr" target="#b15">16</ref>, 24) has a large squeezed ratio for the sequence length. Still, it results in very poor performance for both image classification and semantic segmentation. However, when the single pooling operation is with a pooling ratio ? 12, the performance will be saturated if we further decrease the pooling ratio. When we adopt two parallel pooling operations, even with a high squeezed ratio, the performance still becomes better for both image classification and semantic segmentation. When we have four parallel pooling operations, we can derive the best performance with the comparable squeezed ratio for the pooling ratio of 8 (the setting in PVT <ref type="bibr" target="#b20">[21]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance of pyramid pooling for different stages.</head><p>We perform ablation studies of the pyramid pooling design of P2T. for different stages. Since stage 1 only contains convolutions for downsampling, we do not perform such ablation study at stage 1. The baseline is same with the last ablation studies. The pooling ratio of single pooling operation is set to 8 for ensuring comparable downsampling ratios. Results are shown in <ref type="table" target="#tab_9">Table 6</ref>. We can observe pyramid pooling can improve the performance at all stages. The performance becomes higher when more stages are applied with multiple pooling operations. From the results, the improvement on applying multiple pooling operations on stage 4 (No. 3 of <ref type="table" target="#tab_9">Table 6</ref>) is larger than that on other stages (No. 2, 4 of <ref type="table" target="#tab_9">Table 6</ref>), because stage 4 has more basic blocks than the summation of stage <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and stage 5.</p><p>Pooling operation choices. We conduct experiments for different pooling operations, as shown in <ref type="table">Table 7</ref>. There are three typical choices, i.e., max pooling, depthwise convolution, and the default average pooling. The kernel size of depthwise convolution is the same as max/average pooling to keep the same downsampling rate. It is obvious that different pooling types do not affect the computational complexity and they only affect the number of the parameters of the downsampling kernel. Regarding the results of ImageNet classification accuracy <ref type="bibr" target="#b9">[10]</ref> and ADE20K segmentation mIoU <ref type="bibr" target="#b13">[14]</ref>, average pooling is much better than the other two choices. Thus, we apply average pooling as the default pooling choice.</p><p>Fixed pooled sizes. When using fixed pooling ratios, the size of the pooled feature map will vary with the input feature map. Here, we try to fix the pooled sizes as {1, 2, 3, 6} for all stages, using adaptive average pooling. The results are shown in <ref type="table" target="#tab_12">Table 8</ref>. Compared with our default setting, about 10% of memory usage and 12% of computational cost are saved. However, the top-1 classification accuracy drops by 0.3%, and the semantic segmentation performance is 2.4% lower. Hence, we choose to use fixed pooling ratios rather than fixed pooled sizes.</p><p>Selection of activation functions. We use the Hardswish function <ref type="bibr" target="#b70">[71]</ref> for nonlinear activation to reduce GPU memory usage in the training phase. Typically, when we train P2T-Small on ImageNet <ref type="bibr" target="#b9">[10]</ref> with a batch size of 64, the GPU memory usage of GELU [72] is 10.5GB, which is 3.6GB (+52%) more than that of Hardswish <ref type="bibr" target="#b70">[71]</ref>. We also find that there is no significant accuracy decrease if we employ Hardswish <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other design choices.</head><p>To validate the effectiveness of other design choices like relative positional encoding, IRB, and overlapping patch embedding, we add these components one by one to the baseline. Experimental results are shown in <ref type="table" target="#tab_13">Table 9</ref>. As can be seen, relative positional encoding has significant improvement for both image classification and semantic segmentation. With large pooling ratios, the pooled features would have small scales, so relative positional encoding only needs negligible computational cost (5M Flops for the input size of 224 ? 224). An extra depthwise convolution in the feed-forward network, i.e., IRB, also shows significant improvement, demonstrating the necessity of the 2D locality enhancement. We further follow <ref type="bibr" target="#b28">[29]</ref> to add overlapping patch embedding, and 0.2%/1.4% improvement is observed for image classification and semantic segmentation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper introduces pyramid pooling into MHSA for alleviating the high computational cost of MHSA in the vision transformer. Compared with the strategy of applying a single pooling operation in MHSA <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, our poolingbased MHSA not only reduces the sequence length but also learns powerful contextual representations simultaneously via pyramid pooling. Equipped with the poolingbased MHSA, we construct a new backbone network, called Pyramid Pooling Transformer (P2T). To demonstrate the effectiveness of P2T, we conduct extensive experiments on several fundamental vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation. Experimental results suggest that P2T significantly outperforms previous CNN-and transformer-based backbone networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Y.-H. Wu and M.-M. Cheng are with TMCC, College of Computer Science, Nankai University, Tianjin, China. (E-mail: wuyuhuan@mail.nankai.edu.cn, cmm@nankai.edu.cn) ? Y. Liu is with Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR), Singapore. (E-mail: vagrantlyun@gmail.com) ? X. Zhan is with Alibaba DAMO Academy, Hangzhou, China. ? The first two authors contributed equally to this work. ? Corresponding author: M.-M. Cheng. (E-mail: cmm@nankai.edu.cn) ? This work is done while Y.-H. Wu is a research intern at Alibaba DAMO Academy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of Pyramid Pooling Transformer. (a) The brief structure of Pyramid Pooling Transformer. (b) The detailed structure of the pooling-based MHSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Architecture of the proposed P2T network.</head><label></label><figDesc>We replace the traditional MHSA with our pooling-based MHSA. The feature maps {B 1 , B 2 , B 3 , B 4 } can be used for downstream scene understanding tasks.</figDesc><table><row><cell>3 ? ? I m a g e</cell><cell>Patch Embedding</cell><cell>Pyramid Pooling</cell><cell>Transformer</cell><cell>! ? 4 ? !</cell><cell>4</cell><cell>Patch Embedding</cell><cell>Pyramid Pooling</cell><cell>Transformer</cell><cell>" ? 8 ? "</cell><cell>8</cell><cell>Patch Embedding</cell><cell>Pyramid Pooling</cell><cell>Transformer</cell><cell>? 1 6 ? 1 6 # #</cell><cell>Patch Embedding</cell><cell>Pyramid Pooling</cell><cell>Transformer</cell><cell>? 3 2 ? 3 2 $ $</cell><cell>Classification Task</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Downstream Scene Understanding Tasks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3}). In this way, the scales of four stages become H W 32 , respectively. From four stages, we can derive four feature representations {B 1 , B 2 , B 3 , B 4 }, respectively. Only B 4 will be used for final prediction for image classification, while all pyramid features can be utilized for downstream scene understanding tasks.</figDesc><table><row><cell>H 32 ?</cell><cell>4 ? W 4 , H 8 ? W 8 , H 16 ? W 16 , and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>All models are trained and evaluated with the input size of 224 ? 224 except that we follow the official ViT<ref type="bibr" target="#b18">[19]</ref> to train and evaluate it with the input size of 384 ? 384.</figDesc><table><row><cell>P2T largely out-</cell></row></table><note>that of ResNet-18/50/101 [4] and ResNeXt-101-64x4d [27], the top-1 accuracy of P2T-Tiny/Small/Base/Large is 11.3%/3.9%/3.7%/2.4% better than that of ResNet- 18/50/101 [4] and ResNeXt-101-64x4d [27], respectively. As can be seen, our P2T also achieves superior results compared with recent state-of-the-art transformer models. For example, P2T-Small/Base/Large is 1.1%/0.5%/0.6%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 Experimental results on the validation set of the ADE20K dataset [14] for semantic segmentation.</head><label>3</label><figDesc>We replace the backbone of Semantic FPN<ref type="bibr" target="#b25">[26]</ref> with various network architectures. The number of GFlops is calculated with the input size of 512 ? 512. FPS is tested on a single RTX 2070 GPU. The results of P2T backbones are marked in bold.</figDesc><table><row><cell>Backbone</cell><cell cols="4">Semantic FPN [26] #Param (M) ? GFlops ? mIoU (%) ? FPS ?</cell></row><row><cell>ResNet-18 [4]</cell><cell>15.5</cell><cell>31.9</cell><cell>32.9</cell><cell>68</cell></row><row><cell>PVT-Tiny [21]</cell><cell>17.0</cell><cell>32.1</cell><cell>35.7</cell><cell>36</cell></row><row><cell>PVTv2-B1 [29]</cell><cell>17.8</cell><cell>33.1</cell><cell>41.5</cell><cell>30</cell></row><row><cell>P2T-Tiny (Ours)</cell><cell>15.4</cell><cell>31.6</cell><cell>43.4</cell><cell>31</cell></row><row><cell>ResNet-50 [4]</cell><cell>28.5</cell><cell>45.4</cell><cell>36.7</cell><cell>35</cell></row><row><cell>PVT-Small [21]</cell><cell>28.2</cell><cell>42.9</cell><cell>39.8</cell><cell>26</cell></row><row><cell>Swin-T [22]</cell><cell>31.9</cell><cell>46</cell><cell>41.5</cell><cell>26</cell></row><row><cell>Twins-SVT-S [28]</cell><cell>28.3</cell><cell>37</cell><cell>43.2</cell><cell>27</cell></row><row><cell>PVTv2-B2 [29]</cell><cell>29.1</cell><cell>44.1</cell><cell>46.1</cell><cell>21</cell></row><row><cell>P2T-Small (Ours)</cell><cell>27.8</cell><cell>42.7</cell><cell>46.7</cell><cell>24</cell></row><row><cell>ResNet-101 [4]</cell><cell>47.5</cell><cell>64.8</cell><cell>38.8</cell><cell>26</cell></row><row><cell>ResNeXt-101-32x4d [27]</cell><cell>47.1</cell><cell>64.6</cell><cell>39.7</cell><cell>20</cell></row><row><cell>PVT-Medium [21]</cell><cell>48.0</cell><cell>59.4</cell><cell>41.6</cell><cell>19</cell></row><row><cell>Swin-S [22]</cell><cell>53.2</cell><cell>70</cell><cell>45.2</cell><cell>18</cell></row><row><cell>Twins-SVT-B [28]</cell><cell>60.4</cell><cell>67</cell><cell>45.3</cell><cell>17</cell></row><row><cell>PVTv2-B3 [29]</cell><cell>49.0</cell><cell>60.7</cell><cell>47.3</cell><cell>15</cell></row><row><cell>P2T-Base (Ours)</cell><cell>39.8</cell><cell>58.5</cell><cell>48.7</cell><cell>16</cell></row><row><cell>ResNeXt-101-64x4d [27]</cell><cell>86.4</cell><cell>104.2</cell><cell>40.2</cell><cell>15</cell></row><row><cell>PVT-Large [21]</cell><cell>65.1</cell><cell>78.0</cell><cell>42.1</cell><cell>15</cell></row><row><cell>Swin-B [22]</cell><cell>91.2</cell><cell>107</cell><cell>46.0</cell><cell>13</cell></row><row><cell>Twins-SVT-L [28]</cell><cell>102</cell><cell>103.7</cell><cell>46.7</cell><cell>13</cell></row><row><cell>PVTv2-B4 [29]</cell><cell>66.3</cell><cell>79.6</cell><cell>48.6</cell><cell>11</cell></row><row><cell>PVTv2-B5 [29]</cell><cell>85.7</cell><cell>89.4</cell><cell>48.9</cell><cell>10</cell></row><row><cell>P2T-Large (Ours)</cell><cell>58.1</cell><cell>77.7</cell><cell>49.4</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 Object detection results with RetinaNet [74] and instance segmentation results with Mask</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>AP 50 AP 75 AP S ? AP M AP L AP b ? AP b 50 AP b 75 AP m ? AP m 50</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Object Detection</cell><cell></cell><cell></cell><cell></cell><cell>Instance Segmentation</cell></row><row><cell></cell><cell>#Param</cell><cell>#Flops</cell><cell>#FPS</cell><cell>RetinaNet [74]</cell><cell></cell><cell>#Param</cell><cell>#Flops</cell><cell>#FPS</cell><cell>Mask R-CNN [75]</cell></row><row><cell></cell><cell>(M) ?</cell><cell>(G)?</cell><cell>?</cell><cell cols="5">(M) ? AP ? AP m (G) ? ? 75</cell></row><row><cell>R-18 [4]</cell><cell>21.3</cell><cell cols="3">190 19.3 31.8 49.6 33.6 16.3</cell><cell>34.3 43.2</cell><cell>31.2</cell><cell cols="2">209 17.3 34.0</cell><cell>54.0 36.7 31.2</cell><cell>51.0 32.7</cell></row><row><cell>ViL-Tiny [73]</cell><cell>16.6</cell><cell cols="3">204 4.2 40.8 61.3 43.6 26.7</cell><cell>44.9 53.6</cell><cell>26.9</cell><cell cols="2">223 3.9 41.4</cell><cell>63.5 45.0 38.1</cell><cell>60.3 40.8</cell></row><row><cell>PVT-Tiny [21]</cell><cell>23.0</cell><cell cols="3">205 10.7 36.7 56.9 38.9 22.6</cell><cell>38.8 50.0</cell><cell>32.9</cell><cell cols="2">223 10.0 36.7</cell><cell>59.2 39.3 35.1</cell><cell>56.7 37.3</cell></row><row><cell>PVTv2-B1 [29]</cell><cell>23.8</cell><cell cols="3">209 8.5 40.2 60.7 42.4 22.8</cell><cell>43.3 54.0</cell><cell>33.7</cell><cell cols="2">227 8.0 41.8</cell><cell>64.3 45.9 38.8</cell><cell>61.2 41.6</cell></row><row><cell>P2T-Tiny (Ours)</cell><cell>21.1</cell><cell cols="3">206 9.3 41.3 62.0 44.1 24.6</cell><cell>44.8 56.0</cell><cell>31.3</cell><cell cols="2">225 8.8 43.3</cell><cell>65.7 47.3 39.6</cell><cell>62.5 42.3</cell></row><row><cell>R-50 [4]</cell><cell>37.7</cell><cell cols="3">239 13.0 36.3 55.3 38.6 19.3</cell><cell>40.0 48.8</cell><cell>44.2</cell><cell cols="2">260 11.5 38.0</cell><cell>58.6 41.4 34.4</cell><cell>55.1 36.7</cell></row><row><cell>PVT-Small [21]</cell><cell>34.2</cell><cell cols="3">261 7.7 40.4 61.3 43.0 25.0</cell><cell>42.9 55.7</cell><cell>44.1</cell><cell cols="2">280 7.0 40.4</cell><cell>62.9 43.8 37.8</cell><cell>60.1 40.3</cell></row><row><cell>Swin-T [22]</cell><cell>38.5</cell><cell cols="3">248 9.7 41.5 62.1 44.2 25.1</cell><cell>44.9 55.5</cell><cell>47.8</cell><cell cols="2">264 8.8 42.2</cell><cell>64.6 46.2 39.1</cell><cell>61.6 42.0</cell></row><row><cell>ViL-Small [73]</cell><cell>35.7</cell><cell cols="3">292 3.4 44.2 65.2 47.6 28.8</cell><cell>48.0 57.8</cell><cell>45.0</cell><cell cols="2">310 3.2 44.9</cell><cell>67.1 49.3 41.0</cell><cell>64,2 44.1</cell></row><row><cell>Twins-SVT-S [28]</cell><cell>34.3</cell><cell cols="3">236 8.5 43.0 64.2 46.3 28.0</cell><cell>46.4 57.5</cell><cell>44.0</cell><cell cols="2">254 7.7 43.4</cell><cell>66.0 47.3 40.3</cell><cell>63.2 43.4</cell></row><row><cell>PVTv2-B2 [29]</cell><cell>35.1</cell><cell cols="3">266 5.8 43.8 64.8 46.8 26.0</cell><cell>47.6 59.2</cell><cell>45.0</cell><cell cols="2">285 5.4 45.3</cell><cell>67.1 49.6 41.2</cell><cell>64.2 44.4</cell></row><row><cell cols="2">P2T-Small (Ours) 33.8</cell><cell cols="3">260 7.4 44.4 65.3 47.6 27.0</cell><cell>48.3 59.4</cell><cell>43.7</cell><cell cols="2">279 6.7 45.5</cell><cell>67.7 49.8 41.4</cell><cell>64.6 44.5</cell></row><row><cell>R-101 [4]</cell><cell>56.7</cell><cell cols="3">315 9.8 38.5 57.8 41.2 21.4</cell><cell>42.6 51.1</cell><cell>63.2</cell><cell cols="2">336 9.1 40.4</cell><cell>61.1 44.2 36.4</cell><cell>57.7 38.8</cell></row><row><cell>X-101-32x4d [27]</cell><cell>56.4</cell><cell cols="3">319 8.5 39.9 59.6 42.7 22.3</cell><cell>44.2 52.5</cell><cell>62.8</cell><cell cols="2">340 7.9 41.9</cell><cell>62.5 45.9 37.5</cell><cell>59.4 40.2</cell></row><row><cell cols="2">PVT-Medium [21] 53.9</cell><cell cols="3">349 5.7 41.9 63.1 44.3 25.0</cell><cell>44.9 57.6</cell><cell>63.9</cell><cell cols="2">367 5.3 42.0</cell><cell>64.4 45.6 39.0</cell><cell>61.6 42.1</cell></row><row><cell>Swin-S [22]</cell><cell>59.8</cell><cell cols="3">336 7.1 44.5 65.7 47.5 27.4</cell><cell>48.0 59.9</cell><cell>69.1</cell><cell cols="2">354 6.6 44.8</cell><cell>66.6 48.9 40.9</cell><cell>63.4 44.2</cell></row><row><cell>PVTv2-B3 [29]</cell><cell>55.0</cell><cell cols="3">354 4.5 45.9 66.8 49.3 28.6</cell><cell>49.8 61.4</cell><cell>64.9</cell><cell cols="2">372 4.2 47.0</cell><cell>68.1 51.7 42.5</cell><cell>65.7 45.7</cell></row><row><cell>P2T-Base (Ours)</cell><cell>45.8</cell><cell cols="3">344 5.0 46.1 67.5 49.6 30.2</cell><cell>50.6 60.9</cell><cell>55.7</cell><cell cols="2">363 4.7 47.2</cell><cell>69.3 51.6 42.7</cell><cell>66.1 45.9</cell></row><row><cell>X-101-64x4d [27]</cell><cell>95.5</cell><cell cols="3">473 6.2 41.0 60.9 44.0 23.9</cell><cell cols="2">45.2 54.0 101.9</cell><cell cols="2">493 5.7 42.8</cell><cell>63.8 47.3 38.4</cell><cell>60.6 41.3</cell></row><row><cell>PVT-Large [21]</cell><cell>71.1</cell><cell cols="3">450 4.4 42.6 63.7 45.4 25.8</cell><cell>46.0 58.4</cell><cell>81.0</cell><cell cols="2">469 4.1 42.9</cell><cell>65.0 46.6 39.5</cell><cell>61.9 42.5</cell></row><row><cell>Twins-SVT-B [28]</cell><cell>67.0</cell><cell cols="3">376 5.1 45.3 66.7 48.1 28.5</cell><cell>48.9 60.6</cell><cell>76.3</cell><cell cols="2">395 4.6 45.2</cell><cell>67.6 49.3 41.5</cell><cell>64.5 44.8</cell></row><row><cell>PVTv2-B4 [29]</cell><cell>72.3</cell><cell cols="3">457 3.4 46.1 66.9 49.2 28.4</cell><cell>50.0 62.2</cell><cell>82.2</cell><cell cols="2">475 3.2 47.5</cell><cell>68.7 52.0 42.7</cell><cell>66.1 46.1</cell></row><row><cell>PVTv2-B5 [29]</cell><cell>91.7</cell><cell cols="3">514 3.2 46.2 67.1 49.5 28.5</cell><cell cols="2">50.0 62.5 101.6</cell><cell cols="2">532 3.0 47.4</cell><cell>68.6 51.9 42.5</cell><cell>65.7 46.0</cell></row><row><cell>P2T-Large (Ours)</cell><cell>64.4</cell><cell cols="3">449 3.8 47.2 68.4 50.9 32.4</cell><cell>51.6 62.2</cell><cell>74.0</cell><cell cols="2">467 3.5 48.3</cell><cell>70.2 53.3 43.5</cell><cell>67.3 46.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="5">No. Pooling Ratio(s) D Ratio ? Top-1 (%) ? mIoU (%) ?</cell></row><row><cell>1</cell><cell>24</cell><cell>576</cell><cell>70.6</cell><cell>27.5</cell></row><row><cell>2</cell><cell>16</cell><cell>256</cell><cell>72.5</cell><cell>33.0</cell></row><row><cell>3</cell><cell>12</cell><cell>144</cell><cell>73.9</cell><cell>34.3</cell></row><row><cell>4</cell><cell>8</cell><cell>64</cell><cell>73.9</cell><cell>34.4</cell></row><row><cell>5</cell><cell>12, 24</cell><cell>115</cell><cell>74.4</cell><cell>34.8</cell></row><row><cell>6</cell><cell>12, 16, 20, 24</cell><cell>66</cell><cell>74.7</cell><cell>35.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 Ablation study on the fixed pooled size.</head><label>8</label><figDesc>GFlops is computed with an input size of 512 ? 512 for the semantic segmentation model, i.e., Semantic FPN<ref type="bibr" target="#b25">[26]</ref>. Memory (Mem) denotes the training GPU memory usage for Semantic FPN<ref type="bibr" target="#b25">[26]</ref> with a batch size of 2. "Top-1" and "mIoU" indicate the top-1 classification accuracy on ImageNet-1K<ref type="bibr" target="#b9">[10]</ref> and segmentation mIoU on ADE20K<ref type="bibr" target="#b13">[14]</ref>, respectively.Pooling Operation GFlops ? Mem (GB) ? Top-1 (%) ? mIoU (%) ?</figDesc><table><row><cell>Fixed Pooled ratios</cell><cell>41.6</cell><cell>3.3</cell><cell>74.7</cell><cell>35.7</cell></row><row><cell>Fixed Pooled Sizes</cell><cell>38.9</cell><cell>2.9</cell><cell>74.4</cell><cell>33.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9 Ablation study on the relative positional encoding (RPE), IRB, and overlapping patch embedding (OPE</head><label>9</label><figDesc>). "Top-1" and "mIoU" indicate the top-1 classification accuracy on ImageNet-1K<ref type="bibr" target="#b9">[10]</ref> and segmentation mIoU on ADE20K<ref type="bibr" target="#b13">[14]</ref>, respectively.</figDesc><table><row><cell>RPE</cell><cell>IRB</cell><cell>OPE</cell><cell>Top-1 (%) ?</cell><cell>mIoU (%) ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>74.7</cell><cell>35.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>76.4</cell><cell>37.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79.5</cell><cell>42.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79.7</cell><cell>44.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks with dense connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="6105" to="6114" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MobileSal: Extremely efficient RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Res2Net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="6000" to="6010" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transformer is all you need: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10772</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pyramid Vision Transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chhatkuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03180</idno>
		<title level="m">Vision transformers with hierarchical attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SwinIR: Image restoration using Swin Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">PVTv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CvT: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LeViT: a vision transformer in ConvNet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Demystifying local vision transformer: Sparse connectivity, weight sharing, and dynamic weight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04263</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>IEEE Conf. Comput. Vis. Pattern Recog.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ShuffleNet v2: Practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Zahoora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of deep neural network architectures and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. Appl. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3694" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">ISTR: End-to-end instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00637</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="10" to="347" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tokens-to-token ViT: Training vision transformers from scratch on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
		<title level="m">Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56M parameters on ImageNet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Lo-calViT: Bringing locality to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Slsdeep: Skin lesion segmentation based on dilated residual and pyramid pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M K</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Banu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">U</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulwahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image. Comput. Comput. Assist. Interv. Springer</title>
		<editor>Med</editor>
		<imprint>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ocnet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cascaded hierarchical atrous spatial pyramid pooling module for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107622</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-scale pyramid pooling for deep convolutional representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parallel feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dc-sppyolo: Dense connection and spatial pyramid pooling based yolo for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">522</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Poolnet+: Exploring the potential of pooling for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Regularized densely-connected pyramid network for salient instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3897" to="3907" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficient module based single image super resolution for multiple problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Young</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="882" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3943" to="3956" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03404</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (GELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multi-scale vision Longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2998" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">MMDetection: Open MMLab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
