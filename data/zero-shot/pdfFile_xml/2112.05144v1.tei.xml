<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge-aware Guidance Fusion Network for RGB-Thermal Scene Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wujie</forename><surname>Zhou</surname></persName>
							<email>wujiezhou@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science &amp; Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Dong</surname></persName>
							<email>shaohuadong2021@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science &amp; Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caie</forename><surname>Xu</surname></persName>
							<email>caiexu@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science &amp; Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguan</forename><surname>Qian</surname></persName>
							<email>qianyaguan@zust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science &amp; Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Edge-aware Guidance Fusion Network for RGB-Thermal Scene Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGB-thermal scene parsing has recently attracted increasing research interest in the field of computer vision. However, most existing methods fail to perform good boundary extraction for prediction maps and cannot fully use high-level features. In addition, these methods simply fuse the features from RGB and thermal modalities but are unable to obtain comprehensive fused features. To address these problems, we propose an edge-aware guidance fusion network (EGFNet) for RGB-thermal scene parsing. First, we introduce a prior edge map generated using the RGB and thermal images to capture detailed information in the prediction map and then embed the prior edge information in the feature maps. To effectively fuse the RGB and thermal information, we propose a multimodal fusion module that guarantees adequate cross-modal fusion. Considering the importance of high-level semantic information, we propose a global information module and a semantic information module to extract rich semantic information from the high-level features. For decoding, we use simple elementwise addition for cascaded feature fusion. Finally, to improve the parsing accuracy, we apply multitask deep supervision to the semantic and boundary maps. Extensive experiments were performed on benchmark datasets to demonstrate the effectiveness of the proposed EGFNet and its superior performance compared with state-of-the-art methods. The code and results can be found at https://github.com/ShaohuaDong2021/EGFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Scene parsing is a fundamental technique in computer vision that aims to assign category labels to each of the pixels in a natural image. Hence, scene parsing has enhanced many applications in computer vision, such as autonomous driving <ref type="bibr" target="#b0">(Wang et al. 2020;</ref> and robot sensing <ref type="bibr" target="#b2">(Zhang et al. 2018;</ref><ref type="bibr" target="#b3">Zhou et al. 2018;</ref>. In recent years, deep learning has become a promising solution to scene parsing. Existing methods based on fully convolutional networks have achieved noteworthy results <ref type="bibr" target="#b5">(Lee et al. 2016;</ref><ref type="bibr" target="#b9">Chen et al. 2017;</ref><ref type="bibr" target="#b10">Luo et al. 2017;</ref><ref type="bibr" target="#b8">Zhang et al. 2017;</ref><ref type="bibr">Hou et al. 2018;</ref>). However, accurate scene parsing *Corresponding author. remains a challenge under poor light conditions. Some recent studies have noted this problem and proposed more robust methods via RGB-thermal scene parsing <ref type="bibr" target="#b11">(Ha et al. 2017;</ref><ref type="bibr" target="#b12">Sun et al. 2019;</ref><ref type="bibr" target="#b14">Shivakumar et al. 2020;</ref><ref type="bibr" target="#b13">Sun et al. 2021;</ref><ref type="bibr" target="#b15">Zhang et al. 2021;</ref>. These methods use the complementary rich information and semantic information provided by the thermal images to RGB images under poor lighting conditions, thereby achieving high parsing performance.</p><p>Despite the abovementioned developments, some problems of RGB-thermal scene parsing remain to be solved. Owing to the lack of specific guidance on extracting boundaries, boundary preservation needs to be further improved. Existing methods based on fully convolutional networks reduce feature resolution, leading to loss of spatial details and distortion of object boundaries. In addition, existing methods use simple fusion strategies, such as elementwise addition or multiplication, thus failing to fully integrate multimodal information and undermining scene parsing performance. Moreover, most methods do not fully use high-level features with their rich semantic information. Therefore, a method to suitably extract and use high-level semantic information is desired.</p><p>To address these scene parsing problems, we propose an edge-aware guidance fusion network (EGFNet) for scene parsing based on an encoder-decoder architecture. The proposed EGFNet achieves remarkable performance for RGB-thermal scene parsing. We first introduce a method of embedding prior edge maps into the boundary features to enhance boundary information. To extract more information from the RGB and thermal features, we propose a multimodal fusion module (MFM) that integrates the multimodal features using efficient strategies. Unlike simple methods, such as fusion based on addition or concatenation, the MFM uses a complex fusion strategy to fully combine the information from the RGB and thermal modalities. In addition, a global information module (GIM) and a semantic information module (SIM) are proposed to extract high-level semantic information efficiently. Finally, we adopt multitask deep supervision to improve the segmentation performance. In general, the proposed EGFNet shows superior performance compared with state-of-the-art (SOTA) RGB-thermal scene parsing methods.</p><p>The main contributions of the proposed EGFNet can thus be summarized as follows:</p><p>? The EGFNet is one of the pilot methods to use prior edge information for enhancing boundary extraction for RGB-thermal scene parsing and generating high-quality edge-aware prediction maps.</p><p>? We introduce the MFM to explore the effectiveness and complementarity between the RGB and thermal features. The MFM establishes a simple yet effective method to capture the complementarity of cross-modal features. ? To extract high-level semantic information, we propose the GIM and SIM, which fully and efficiently use high-level features.</p><p>? We adopt multitask deep supervision to obtain detailed object boundaries and improve parsing performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>In recent years, an increasing number of deep-learning-based scene parsing methods have been proposed and have achieved good performances. One of the essential aspects of these methods is the extraction of representative features. To this end,  proposed a context path with fast downsampling to enlarge the receptive field. <ref type="bibr" target="#b18">Pohlen et al. (2017)</ref> combined multiscale context with pixel-level accuracies using two processing streams within a neural network. <ref type="bibr" target="#b12">Sun et al. (2019)</ref> proposed a network to maintain high-resolution representations throughout the parsing stages by connecting high-to-low-resolution convolutions in parallel.  proposed a smooth network with a channel attention block to select the discriminative features. <ref type="bibr" target="#b21">Romera et al. (2018)</ref> proposed a deep architecture using residual connections and factorized convolutions for efficient parsing with remarkable accuracy. <ref type="bibr" target="#b22">Huang et al. (2019)</ref> proposed a criss-cross attention module to obtain rich contextual information. <ref type="bibr" target="#b23">He et al. (2019)</ref> proposed a network that adaptively constructs multiscale contextual representations with multiple well-designed adaptive context modules.</p><p>Recently, single-modal methods, such as those mentioned above, have been improved by employing information from complementary modalities (e.g., depth maps and thermal images). <ref type="bibr" target="#b24">Hazirbas et al. (2016)</ref> proposed a network to integrate multilevel depth features with an RGB encoder through a bottom-up approach to improve scene parsing. <ref type="bibr" target="#b25">Wang et al. (2018)</ref> proposed depth-aware convolution and average pooling operations for RGB-depth scene parsing. <ref type="bibr" target="#b30">Zhou et al. (2020)</ref> proposed a gate-fusion module to regularize feature fusion for detecting salient objects in RGB-depth images.  proposed a complementary interaction network to select useful representations from RGB images and their corresponding depth maps to integrate cross-modal features. <ref type="bibr" target="#b32">Chen et al. (2020)</ref> proposed a disentangled cross-modal fusion module to extract structural and content representations from RGB images and depth maps.  proposed a channelwise fusion module for multinetwork and multilevel selective fusion of RGB-depth parsing.</p><p>Boundary details can improve scene parsing substantially. To correct blurred boundaries, some methods extract specific boundary features.  proposed a boundary-guided deep neural network for scene parsing to suppress irrelevant boundary information while suitably localizing and exploring the structures of objects. <ref type="bibr" target="#b27">Yang et al. (2021)</ref> devised edge feature enhancement to use edge-specific features efficiently.  proposed a contour self-compensated module to generate accurate saliency maps with complete contours; the salient contours were then used as third labels for the ground truth. <ref type="bibr" target="#b29">Kong et al. (2021)</ref> proposed an adversarial edge-aware image colorization approach combining multitask outputs with scene parsing. Unlike these methods, the proposed EGFNet uses a novel prior edge map to enhance the boundaries and improve scene parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed EGFNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>The architecture of the proposed EGFNet is shown in <ref type="figure" target="#fig_0">Fig.  1</ref>. We use ResNet-152 <ref type="bibr" target="#b34">(He et al. 2016)</ref> as the backbone of the encoders for both the RGB and thermal branches for extracting features. Owing to the high computational overhead, we use a 1 ? 1 convolution to reduce the number of channels to 64. From low to high levels, the extracted RGB and thermal features are denoted as Ri and Ti (i ? {1, 2, ..., 5}), respectively. In the encoder, we use the novel MFM to fuse complementary information from the RGB and thermal modalities. The MFM provides fusion features fi (i ? {1, 2, ..., 5}), boundary features bj (j ? {1, 2, 3}), and semantic features st (t ? {4, 5}). We propose an edge-aware method to embed prior edge information in the feature maps and obtain clearer boundaries, thereby improving the parsing performance of EGFNet. In addition, the proposed GIM and SIM extract high-level semantic information.</p><p>In the decoder, we adopt a simple fusion module (SFM) to fuse cascaded features. By fusing the high-level semantic information and skip-connection features, we extract discriminative and comprehensive semantic information. Finally, we introduce multitask deep supervision for the semantic and boundary maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge-aware guidance</head><p>Edge-aware guidance for scene parsing aims to determine object boundaries in semantic maps accurately. Most existing methods use complex deep convolutional neural networks to capture the boundary features. To improve efficiency, we adopt a traditional edge-detection algorithm that allows obtaining details from the RGB and thermal images directly.</p><p>We first use the Sobel operator <ref type="bibr" target="#b35">(Sobel et al. 1968</ref>) to extract the edge information from the RGB and thermal images. Then, we add the extracted edge information from the two modalities to fuse their distinct features and obtain a prior edge map. Finally, we embed the prior edge information in the boundary feature maps using elementwise multiplication to increase boundary accuracy in the prediction map. In addition, we improve the parsing performance by fusing the prior edge map and side-out semantic prediction map with the final semantic features, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal fusion module</head><p>Exploring accurate multimodal fusion features is essential for achieving high-performance multimodal scene parsing. Thus, instead of using a simple fusion strategy, we propose the MFM to extract fusion features, thus outperforming simple feature concatenation or summation. The MFM architecture is shown in <ref type="figure">Fig. 2</ref>.</p><p>We first adopt elementwise summation of the features from the RGB and thermal modalities. Then, we apply various operations, including elementwise multiplication, channelwise concatenation, and convolution, to obtain complementary information as follows:</p><formula xml:id="formula_0">? ? ? ? ? ? ? ?, , 1 1 i i i i i i m T T R R T R Cat Conv f ? ? ? ? ? ? ? ? 5 , 4 , 3 , 2 , 1 ? i ,<label>(1)</label></formula><p>where Cat denotes concatenation, ? denotes elementwise multiplication, Conv1?1 denotes 1 ? 1 convolution, and Ri and Ti represent the side-out features of the RGB and thermal branches, respectively ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. We use residual learning to obtain deeper semantic features as follows:</p><formula xml:id="formula_1">? ? ? ? ? ? ? ?, f CBR Conv BN f relu f ? ? ? ? ? 5 , 4 , 3 , 2 , 1 ? i . (2)</formula><p>First, the features pass through the convolution block CBR, and the 3 ? 3 convolutions in CBR are followed by the batch normalization and rectified linear unit (ReLU) layers, BN and relu, respectively.</p><p>To enlarge the receptive field and extract a representative global context, we use atrous spatial pyramid pooling . Specifically, we construct four parallel dilated convolutions with rates r = {1, 2, 3, 4} and combine the four sets of features with the input features using concatenation. Then, we use a 3 ? 3 convolution to extract the fusion features fi: </p><formula xml:id="formula_2">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? m</formula><formula xml:id="formula_3">? ? ? ?, ,<label>, , , 4 3</label></formula><formula xml:id="formula_4">2 1 3 3 m m m m m i f f f f f Cat Conv f ? ? ? ? 5 , 4 , 3 , 2 , 1 ? i . (4)</formula><p>Finally, except for the explicit usage of the semantic cues in EGFNet, the convolution block CBR is applied to obtain detailed information and semantic information as follows:</p><formula xml:id="formula_5">? ? ? ? 3 , 2 , 1 , ? ? i f CBR b i i ,<label>(5)</label></formula><formula xml:id="formula_6">? ? ? ? 5 , 4 , ? ? i f CBR s i i .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIM and SIM</head><p>Low-level features contain detailed information, and high-level features contain comprehensive semantic information <ref type="bibr" target="#b37">(Zeiler et al. 2014)</ref>. Accordingly, we first introduce the GIM <ref type="figure" target="#fig_1">(Fig. 3)</ref> and SIM <ref type="figure" target="#fig_2">(Fig. 4)</ref> to capture high-level semantic information and then fuse the cascaded multilevel cross-modal features using the SFM <ref type="figure" target="#fig_3">(Fig. 5)</ref>.</p><p>The GIM is similar to atrous spatial pyramid pooling and aims to obtain discriminative semantic information as follows:</p><formula xml:id="formula_7">? ? 5 1 1 0 f Conv f a ? ? ,<label>(7)</label></formula><p>Figure 2: Architecture of proposed MFM. <ref type="table" target="#tab_5">3  3  4   5  3  ,  3  3  3   5  2  ,  3  3</ref>  </p><formula xml:id="formula_8">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 5 4 ,</formula><formula xml:id="formula_9">? ? ? ? a high f f CBR up f ? ? ? 5 2 ,<label>(10)</label></formula><p>where up?2 denotes upsampling by a factor of two. Before applying the 1 ? 1 convolutional layer to achieve comprehensive feature fusion, we combine fhigh and f4 to learn complementary information. Moreover, we apply elementwise multiplication to eliminate redundant information. Finally, a residual connection is used to preserve the original information and generate the final features as follows:</p><formula xml:id="formula_10">? ? ? ? 4 1 1 1 , f f Cat Conv f high s ? ? ,<label>(11)</label></formula><formula xml:id="formula_11">4 1 1 2 f f f f f s high s s ? ? ? ? ,<label>(12)</label></formula><formula xml:id="formula_12">? ? ? ? ? ? ? ? ? ? 3 , 2 3 3 1 1 2 ? ? ? ? ? ? i f CBR Conv BN f Conv up f high c i s .<label>(13)</label></formula><p>We aggregate the multilevel features and high-level deep semantic information in a coarse-to-fine approach using the SFM to obtain the comprehensive features. We apply upsampling by a factor of N (up?N) to fhigh such that it has the same size as fi:</p><formula xml:id="formula_13">? ? high N high f up f ? ? .</formula><p>(14)</p><p>Finally, we fuse the features through elementwise summation:</p><formula xml:id="formula_14">? ? ? ? 3 , 2 , 1 , 2 1 ? ? ? ? ? ? i f f f up f i c i high c i .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multitask deep supervision</head><p>To obtain more accurate boundaries and distinct semantic features, we use multitask deep supervision to supervise the boundary and semantic maps. We first resize bi to the same size as the edge map. Then, the prior edge map is embedded in the boundary feature map, as mentioned above. Hence, the boundary prediction map can be highlighted with a complete structure and sharp boundaries. This process can be formulated as follows ( <ref type="figure" target="#fig_0">Fig. 1)</ref>:</p><formula xml:id="formula_15">? ? ? ? 3 , 2 , 1 , ? ? ? ? i edge b up B i N i .<label>(16)</label></formula><p>Moreover, we apply the prior edge map to the intermediate and final semantic prediction maps. To improve learning from side-out semantic information, we propose a semantic guidance module (SGM) that fuses the corresponding features efficiently.</p><p>The SGM architecture is detailed in <ref type="figure" target="#fig_4">Fig. 6</ref>. We first upsample semantic maps ?1 and ?2 such that they have the same size as the ground-truth map. Then, we fuse the features through concatenation followed by a 1 ? 1 convolution:</p><formula xml:id="formula_16">? ? ? ? ? ? ? ? 5 32 4 16 1 1 1 e , s up s up Cat Conv f m s ? ? ? ? ,<label>(17)</label></formula><p>where up?16 and up?32 denote upsampling by factors of 16 and 32 using bilinear interpolation, respectively. Then, the side-out semantic prediction is generated as follows: </p><p>We then embed the prior edge information in the side-out semantic prediction as follows:    (20)</p><p>Similarly, we enhance the final semantic prediction of EGFNet as follows:</p><formula xml:id="formula_18">c c f f edge S 0 0 2 ? ? ? .<label>(21)</label></formula><p>Boundary maps B1, B2, and B3 and semantic maps S1 and S2 are supervised by the ground truth using the weighted cross-entropy loss with weights set as in the study by <ref type="bibr" target="#b38">Paszke et al. (2016)</ref>: </p><formula xml:id="formula_19">? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? W x</formula><p>where W and H are the width and height of the image, respectively, and T and P denote the ground-truth and prediction maps, respectively. The variable Weight denotes the boundary weight while calculating the boundary loss and describes the semantic weight when calculating the semantic loss.</p><p>The total loss for multitask deep supervision is defined as</p><formula xml:id="formula_21">? ? ? ? ? ? 5 4 3 1 j j i i total L L L ,<label>(23)</label></formula><p>where Li is the boundary loss and Lj is the semantic loss <ref type="figure" target="#fig_0">(Fig. 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head><p>We evaluated the proposed EGFNet and compared it with SOTA scene parsing methods through extensive experiments on two public datasets. We also conducted ablation studies to demonstrate the effectiveness of the various components of EGFNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We trained the EGFNet on the MFNet <ref type="bibr" target="#b11">(Ha et al. 2017</ref>) and PST900 <ref type="bibr" target="#b14">(Shivakumar et al. 2020)</ref> datasets. The MFNet dataset contains 1569 pairs of RGB and thermal images, with 820 pairs corresponding to daytime scenes and 749 pairs corresponding to nighttime scenes. The dataset comprises nine classes, including the background. The resolution of the image pairs is 480 ? 640 pixels. We followed the training, testing, verification, and dataset splitting approaches used by <ref type="bibr" target="#b11">Ha et al. (2017)</ref>. The PST900 dataset contains 894 aligned pairs of RGB and thermal images with pixel-level human annotations comprising five semantic classes, including the background. We used the splitting approach proposed by <ref type="bibr" target="#b14">Shivakumar et al. (2020)</ref> and resized each input image to 640 ? 1280 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training details</head><p>We used the PyTorch 1.7.0, CUDA 10.0, and cuDNN 7.6 libraries to implement the proposed EGFNet. A computer equipped with an Intel 3.6 GHz i7 CPU and a single NVIDIA TITAN Xp graphics card was used for training and testing. As the graphics card memory was limited to 12 GB, we adjusted the batch sizes for different evaluated networks accordingly. For training, we used data augmentation operations such as random flipping and cropping. The parameters of the backbone were initialized based on the ResNet-152 model <ref type="bibr" target="#b34">(He et al. 2016)</ref>. We trained EGFNet for 400 epochs and used the Ranger optimizer with an initial learning rate and weight decay of 5e?5 and 5e?4, respectively. We also used the weighted cross-entropy for both the semantic and boundary loss functions as well as weighting detailed by  <ref type="bibr" target="#b38">Paszke et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>For the quantitative evaluations, we adopted some widely used evaluation metrics, including mean intersection over union (mIoU) and mean accuracy (mAcc), to evaluate the performances of different scene parsing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparative results</head><p>For the MFNet dataset <ref type="bibr" target="#b11">(Ha et al. 2017)</ref>, we compared the proposed EGFNet with FRRN <ref type="bibr" target="#b18">(Pohlen et al. 2017)</ref>, BiSeNet , DFN , SegHRNet <ref type="bibr" target="#b19">(Ke et al. 2019)</ref>, MFNet <ref type="bibr" target="#b11">(Ha et al. 2017)</ref>, FuseNet <ref type="bibr" target="#b24">(Hazirbas et al. 2016)</ref>, DepthAwareCNN , RTFNet <ref type="bibr" target="#b12">(Sun et al. 2019</ref><ref type="bibr">), FuseSeg-161 (Sun et al. 2021</ref>, APCNet <ref type="bibr" target="#b23">(He et al. 2019)</ref>, CCNet <ref type="bibr" target="#b22">(Huang et al. 2019)</ref>, and ABMDRNet <ref type="bibr" target="#b15">(Zhang et al. 2021)</ref>. The quantitative results are summarized in <ref type="table" target="#tab_4">Table 1</ref> and demonstrate that our method outperforms other SOTA methods on the MFNet dataset. To further evaluate the proposed network, we tested it with the daytime and nighttime RGB-T images; <ref type="table" target="#tab_5">Table 2</ref> summarizes the comparative results. The visual comparison results are collated in <ref type="figure" target="#fig_5">Fig. 7</ref>, and we observe that our network provides superior results under various challenging lighting conditions compared with other SOTA methods for the MFNet dataset.</p><p>We designed additional experiments to prove the effectiveness of the proposed network on the PST900 dataset <ref type="bibr" target="#b14">(Shivakumar et al. 2020)</ref>. We compared the results from our network with those of CCNet <ref type="bibr" target="#b22">(Huang et al. 2019</ref><ref type="bibr">), ACNet (Hu et al. 2019</ref><ref type="bibr">), EFFicient FCN (Liu et al. 2020</ref>, RTFNet <ref type="bibr" target="#b12">(Sun et al. 2019)</ref>, and PSTNet <ref type="bibr" target="#b14">(Shivakumar et al. 2020)</ref>. The results summarized in <ref type="table" target="#tab_6">Table 3</ref> indicate the excellent applicability of the proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>To demonstrate the effectiveness of the key components of the proposed EGFNet, we applied the same network parameters for retraining each ablation experiment on the MFNet dataset, and these results are listed in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of prior edge information:</head><p>To demonstrate the effectiveness of edge information, we designed a variant without implicitly using edge cues in EGFNet, denoting it as w/o edge. The corresponding results are listed in <ref type="table">Table 4</ref>. The variant w/o edge exhibits worse performance compared to the EGFNet with edge information, demonstrating the importance of edge information for scene parsing and validating its use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of MFM:</head><p>To demonstrate the effectiveness of the MFM, we replaced it with simple addition and denoted this variant as w/o MFM. As summarized in <ref type="table">Table 4</ref>, the proposed EGFNet performs better than the variant w/o MFM, demonstrating the reliability of the module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of GIM and SIM:</head><p>To demonstrate the effectiveness of GIM and SIM, we designed three ablation experiments by removing GIM (denoted as w/o GIM), removing SIM (denoted as w/o SIM), and removing both GIM and SIM (denoted as w/o GIM &amp; SIM). We applied simple addition when removing each module. The three evaluated variants provided declined performances than when using the SIM and GIM in EGFNet. These results indicate the importance of the GIM and SIM for obtaining high-level semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of multitask deep supervision:</head><p>To demonstrate the efficiency of multitask deep supervision, we removed all the supervision except for the final supervision stage while maintaining all the other network parameters (denoted as w/o SUP). <ref type="table">Table 4</ref> indicates that the EGFNet performance considerably decreases when only one supervision stage is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose the EGFNet for RGB-thermal scene parsing. We demonstrate that prior edge information contributes toward generating high-quality and comprehensive scene parsing maps. Moreover, the MFM enables exploitation of the complementarity between the RGB and thermal modalities, while the GIM and SIM allow extraction of high-level semantic information. Furthermore, the proposed multitask deep supervision promotes effective and robust scene parsing. Experiments were performed with two benchmark datasets to demonstrate the high performance of the EGFNet, and the results from ablation experiments verify the contributions of the most important network components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the National Natural Science Foundation of China (Grant No. 61502429).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of proposed EGFNet for scene parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of proposed GIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of proposed SIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of proposed SFM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Architecture of proposed SGM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Segmentation results of fusion modules in typical nighttime and daytime RGB-T images shown in the right four and left four columns, respectively. The proposed EGFNet provides better segmentation under varying lighting conditions than the comparison networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Results on the MFNet dataset. Each value in boldface indicates the best result for the corresponding column.</figDesc><table><row><cell>Methods</cell><cell>Acc</cell><cell>Car</cell><cell>Iou</cell><cell>Person Acc Iou</cell><cell cols="2">Bike Acc Iou Acc Iou Acc Iou Acc Iou Acc Iou Acc Iou Curve Car Stop Guardrail Color Cone Bump</cell><cell>mAcc mIou</cell></row><row><cell>FRRN</cell><cell>80.0</cell><cell cols="2">71.2</cell><cell cols="4">53.0 46.1 65.1 53.0 34.0 27.1 21.6 19.1 0.0 0.0 34.7 32.5 36.2 30.5 47.1 41.8</cell></row><row><cell>BISeNet</cell><cell>90.0</cell><cell cols="2">84.5</cell><cell cols="4">65.0 54.3 75.0 61.4 32.1 25.7 32.3 26.2 3.2 0.9 49.6 43.3 48.1 40.5 54.9 48.2</cell></row><row><cell>DFN</cell><cell>90.7</cell><cell cols="2">81.4</cell><cell cols="4">67.7 52.8 71.5 57.5 49.2 34.9 35.1 23.8 4.1 0.9 44.2 31.0 54.6 47.5 57.3 47.5</cell></row><row><cell>SegHRNet</cell><cell>92.2</cell><cell cols="2">86.6</cell><cell cols="4">73.1 59.8 74.9 61.3 47.0 33.2 38.3 28.7 7.3 1.4 54.6 47.2 61.5 46.2 60.9 51.3</cell></row><row><cell>CCNet</cell><cell>86.7</cell><cell cols="2">79.5</cell><cell cols="2">59.4 52.7 66.0 56.2 39.2 32.2 34.8 29.0 1.3 1.2 45.7 41.0 0.2</cell><cell>0.2</cell><cell>48.1 43.3</cell></row><row><cell>APCNet</cell><cell>89.8</cell><cell cols="2">83.0</cell><cell cols="4">61.3 51.6 73.4 58.7 37.1 27.0 35.6 30.3 36.1 11.8 41.4 35.6 50.7 45.6 58.3 49.0</cell></row><row><cell>MFNet</cell><cell>77.2</cell><cell cols="2">65.9</cell><cell cols="4">67.0 58.9 53.9 42.9 36.2 29.9 12.5 9.9 0.1 0.0 30.3 25.2 30.0 27.7 45.1 39.7</cell></row><row><cell>FuseNet</cell><cell>81.0</cell><cell cols="2">75.6</cell><cell cols="4">75.2 66.3 64.5 51.9 51.0 37.8 17.4 15.0 0.0 0.0 31.1 21.4 51.9 45.0 52.4 45.6</cell></row><row><cell>DepthAwareCNN</cell><cell>85.2</cell><cell cols="2">77.0</cell><cell cols="4">61.7 53.4 76.0 56.5 40.2 30.9 41.3 29.3 22.8 8.5 32.9 30.1 36.5 32.3 55.1 46.1</cell></row><row><cell>RTFNet</cell><cell>93.0</cell><cell cols="2">87.4</cell><cell cols="4">79.3 70.3 76.8 62.7 60.7 45.3 38.5 29.8 0.0 0.0 45.5 29.1 74.7 55.7 63.1 53.2</cell></row><row><cell>FuseSeg-161</cell><cell>93.1</cell><cell cols="2">87.9</cell><cell cols="4">81.4 71.7 78.5 64.6 68.4 44.8 29.1 22.7 63.7 6.4 55.8 46.9 66.4 47.9 70.6 54.5</cell></row><row><cell>ABMDRNet</cell><cell>94.3</cell><cell cols="2">84.8</cell><cell cols="4">90.0 69.6 75.7 60.3 64.0 45.1 44.1 33.1 31.0 5.1 61.7 47.4 66.2 50.0 69.5 54.8</cell></row><row><cell>Ours</cell><cell>95.8</cell><cell cols="2">87.6</cell><cell cols="4">89.0 69.8 80.6 58.8 71.5 42.8 48.7 33.8 33.6 7.0 65.3 48.3 71.1 47.1 72.7 54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Results from nighttime and daytime images.</figDesc><table><row><cell>Methods</cell><cell cols="2">Daytime mAcc mIoU</cell><cell cols="2">Nighttime mAcc mIoU</cell></row><row><cell>FRRN</cell><cell>45.1</cell><cell>40.0</cell><cell>41.6</cell><cell>37.3</cell></row><row><cell>BiSeNet</cell><cell>52.1</cell><cell>44.5</cell><cell>50.3</cell><cell>45.0</cell></row><row><cell>DFN</cell><cell>53.7</cell><cell>42.2</cell><cell>52.4</cell><cell>44.6</cell></row><row><cell>SegHRNet</cell><cell>59.7</cell><cell>47.2</cell><cell>55.7</cell><cell>49.1</cell></row><row><cell>CCNet</cell><cell>55.3</cell><cell>43.5</cell><cell>42.4</cell><cell>38.1</cell></row><row><cell>APCNet</cell><cell>55.4</cell><cell>42.4</cell><cell>54.7</cell><cell>46.4</cell></row><row><cell>MFNet</cell><cell>42.6</cell><cell>36.1</cell><cell>41.4</cell><cell>36.8</cell></row><row><cell>FuseNet</cell><cell>49.5</cell><cell>41.0</cell><cell>48.9</cell><cell>43.9</cell></row><row><cell cols="3">Table 4. Results of ablation experiments.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>mAcc</cell><cell></cell><cell>mIoU</cell></row><row><cell cols="2">Model (w/o edge)</cell><cell>68.9</cell><cell></cell><cell>54.1</cell></row><row><cell cols="2">Model (w/o MFM)</cell><cell>68.1</cell><cell></cell><cell>53.1</cell></row><row><cell cols="2">Model (w/o GIM)</cell><cell>71.8</cell><cell></cell><cell>53.5</cell></row><row><cell cols="2">Model (w/o SIM)</cell><cell>69.1</cell><cell></cell><cell>53.2</cell></row><row><cell cols="2">Model (w/o GIM &amp; SIM)</cell><cell>71.4</cell><cell></cell><cell>54.0</cell></row><row><cell cols="2">Model (w/o SUP)</cell><cell>71.7</cell><cell></cell><cell>53.3</cell></row><row><cell>Model (Ours)</cell><cell></cell><cell>72.7</cell><cell></cell><cell>54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Results on the PST900 dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">Background</cell><cell cols="2">Hand-Drill</cell><cell cols="2">Backpack</cell><cell cols="2">Fire-Extinguisher</cell><cell cols="2">Survivor</cell><cell>mAcc</cell><cell>mIoU</cell></row><row><cell></cell><cell>Acc</cell><cell>IoU</cell><cell>Acc</cell><cell>IoU</cell><cell>Acc</cell><cell>IoU</cell><cell>Acc</cell><cell>IoU</cell><cell>Acc</cell><cell>IoU</cell><cell></cell><cell></cell></row><row><cell>Efficient FCN</cell><cell>99.81</cell><cell>98.63</cell><cell>32.08</cell><cell>30.12</cell><cell>60.06</cell><cell cols="2">58.15 78.87</cell><cell>39.96</cell><cell>32.76</cell><cell>28.00</cell><cell>60.72</cell><cell>50.98</cell></row><row><cell>CCNet</cell><cell>99.86</cell><cell>99.05</cell><cell>51.77</cell><cell>32.27</cell><cell>68.30</cell><cell cols="2">66.42 67.79</cell><cell>51.84</cell><cell>60.84</cell><cell>57.50</cell><cell>69.71</cell><cell>61.42</cell></row><row><cell>ACNet</cell><cell>99.83</cell><cell>99.25</cell><cell>53.59</cell><cell>51.46</cell><cell>85.56</cell><cell cols="2">83.19 84.88</cell><cell>59.95</cell><cell>69.10</cell><cell>65.19</cell><cell>78.67</cell><cell>71.81</cell></row><row><cell>RTFNet</cell><cell>99.78</cell><cell>99.02</cell><cell>7.79</cell><cell>7.07</cell><cell>79.96</cell><cell cols="2">74.17 62.39</cell><cell>51.93</cell><cell>78.51</cell><cell>70.11</cell><cell>65.69</cell><cell>60.46</cell></row><row><cell>PSTNet</cell><cell>-</cell><cell>98.85</cell><cell>-</cell><cell>53.60</cell><cell>-</cell><cell>69.20</cell><cell>-</cell><cell>70.12</cell><cell>-</cell><cell>50.03</cell><cell>-</cell><cell>68.36</cell></row><row><cell>Ours</cell><cell>99.48</cell><cell>99.26</cell><cell>97.99</cell><cell>64.67</cell><cell>94.17</cell><cell cols="2">83.05 95.17</cell><cell>71.29</cell><cell>83.30</cell><cell>74.30</cell><cell>94.02</cell><cell>78.51</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 m m m</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep clustering for weakly-supervised semantic segmentation in autonomous driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GMNet: Graded-feature multilabel-Learning network for RGB-Thermal urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. -N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7790" to="7802" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road segmentation for all-day outdoor robot navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">314</biblScope>
			<biblScope unit="page" from="316" to="325" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local and global feature learning for blind quality evaluation of screen content and natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2086" to="2095" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global and local-contrast guides content-aware fusion for RGB-D saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3641" to="3649" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with Short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">IRFR-Net: Interactive recursive feature-reshaping network for detecting salient objects in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2021.3105484</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Look, perceive and segment: finding the salient objects in images via two-stream fixation-semantic CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1050" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6593" to="6601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MFNet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5108" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RTFNet: RGB-thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2576" to="2583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FuseSeg: Semantic segmentation of urban scenes based on RGB and thermal data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PST900: RGB-thermal calibration, dataset and segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<biblScope unit="page" from="9441" to="9447" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ABMDRNet: Adaptive weighted bi-directional modality difference reduction network for RGB-T semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2633" to="2642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MFFENet: Multiscale feature fusion and enhancement network for RGB-Thermal urban road scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2021.3086618</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3309" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Europeon Conference on Computer Vision (ECCV)</title>
		<meeting>the Europeon Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized convNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on IEEE Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE Conference on IEEE Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7511" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Asia Conference on Computer Vision (ACCV)</title>
		<meeting>the IEEE Conference on Asia Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Europeon Conference on Computer Vision (ECCV)</title>
		<meeting>the Europeon Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention and boundary guided salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107484</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BiconNet: An edge-preserved connectivity-based Approach for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltanian-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">108231</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A contour self-compensated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Visual Computer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1467" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial edge-aware image colorization with semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="28194" to="28203" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GFNet: Gate fusion network with Res2Net for detecting salient objects in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="800" to="804" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Select, supplement and focus for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3469" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via disentangled cross-modal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8407" to="8416" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data-level recombination and lightweight fusion scheme for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="458" to="471" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3?3 isotropic gradient operator for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">At the Stanford Artif. Intell. Project (SAIL)</title>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>U-Net: Convolutional networks for biomedical image segmentation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MAVNet: An effective semantic segmentation micro-network for MAV-based tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ozaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loianno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wozencraft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3908" to="3915" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Fast-scnn: fast semantic segmentation network</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
