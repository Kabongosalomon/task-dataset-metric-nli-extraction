<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Parser: Representing Part-whole Hierarchies with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Parser: Representing Part-whole Hierarchies with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human vision is able to capture the part-whole hierarchical information from the entire scene. This paper presents the Visual Parser (ViP) that explicitly constructs such a hierarchy with transformers. ViP divides visual representations into two levels, the part level and the whole level. Information of each part represents a combination of several independent vectors within the whole. To model the representations of the two levels, we first encode the information from the whole into part vectors through an attention mechanism, then decode the global information within the part vectors back into the whole representation. By iteratively parsing the two levels with the proposed encoder-decoder interaction, the model can gradually refine the features on both levels. Experimental results demonstrate that ViP can achieve very competitive performance on three major tasks e.g. classification, detection and instance segmentation. In particular, it can surpass the previous stateof-the-art CNN backbones by a large margin on object detection. The tiny model of the ViP family with 7.2? fewer parameters and 10.9? fewer FLOPS can perform comparably with the largest model ResNeXt-101-64?4d of ResNe(X)t family. Visualization results also demonstrate that the learnt parts are highly informative of the predicting class, making ViP more explainable than previous fundamental architectures. Code is available at https://github.com/kevin-ssy/ViP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Strong evidence has been found in psychology that human vision is able to parse a complex scene into part-whole hierarchies with many different levels from the low-level pixels to the high-level properties (e.g. parts, objects, scenes) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref>. Constructing such a part-whole hierarchy enables neural networks to capture compositional representations directly from images, which can promisingly help to detect properties of many different levels with only one network.</p><p>To the best of our knowledge, most current visual feature extractors do not model such hierarchy explicitly. Due to the lack of such part-whole hierarchy in representation modeling, existing feature extractors cannot find the compositional features directly from the network. Ideal modeling of the visual representation should be able to model the part-whole hierarchy as humans do so that we can leverage representations of all levels directly from one backbone model.</p><p>Building up a framework that includes different levels of representations in the part-whole hierarchy is difficult for conventional neural networks as it requires neurons to dynamically respond to the input, while neural networks with fixed weights cannot dynamically allocate a group of neurons to represent a node in a parse tree <ref type="bibr" target="#b28">[29]</ref>. With the rise of the Transformer <ref type="bibr" target="#b61">[62]</ref>, such a problem can be possibly resolved due to its dynamic nature.</p><p>In this paper, we show how to construct a simplest part-whole hierarchy for visual representation. The hierarchy of the proposed network has two levels. One represents the part, which only contains the most essential information describing the visual input, and the other is for the whole, which S-50 <ref type="bibr" target="#b9">10</ref>.9? fewer FLOPS 7.2? fewer params <ref type="figure">Figure 1</ref>: ViP can outperform all state-of-the-art CNN backbones by a large margin on object detection. All listed results are trained under the regular 1? regime in Cascade Mask-RCNN <ref type="bibr" target="#b5">[6]</ref>. describes the visual input in a regular spatial coordinate frame system. Normally a part vector can be dynamically associated with several vectors of the whole, forming a one-to-many mapping between the two levels. To obtain information for the part, we first apply an encoder between the two levels to fill each part with the features of the whole. Then the encoded part feature will be mapped back to the whole by a transformer-based decoder. Such cross-level interaction is iteratively applied throughout the network, constituting a bi-directional pathway between the two levels.</p><p>Our main contributions are as follows: <ref type="bibr" target="#b0">(1)</ref> We propose Visual Parser (ViP) that can explicitly construct a part-whole hierarchy within the network for basic visual representations. This is the first step towards learning multi-level part-whole representations. (2) With the help of such a part-whole hierarchy, the network can be more explainable compared with previous networks. (3) ViP can be directly applied as a backbone for versatile uses. Experimental results also demonstrate that ViP can achieve very competitive results compared to the current state-of-the-art backbones. As shown in <ref type="figure">Figure 1</ref>, it outperforms all state-of-the-art CNN counterparts by a large margin on object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The overall pipeline is shown in <ref type="figure">Figure 2</ref>. There are two inputs of ViP, including an input image and a set of learnable parameters. These parameters represent the prototype of the parts, and will be used as initial clues indicating the region that each part should associate with. The entire network consists of several basic blocks (iterations). For block i ? {2, 3, ..., B}, there are two kinds of representations describing the two hierarchical levels. One is the part representation p i ? R N ?C and the other is the whole feature maps x i ? R L?C . Here N is a pre-defined constant number indicating the number of parts within the input image, and L is the number of pixels of the feature map, which is identical to width?height. B, C are the numbers of blocks and channels respectively. The representation of parts for each block is dynamically encoded from the corresponding whole feature maps through an attention-based approach. Given the representation of the part p i?1 and the whole x i?1 from the previous block i ? 1, an attention-based encoder is applied to fill the information of the whole into the part p i of the current block. Since the attention mechanism assigns each pixel on the feature map with a weight indicating the affinity between the pixel and the corresponding part, only the spatial information in x i?1 that is semantically close to the input part p i?1 can be updated into p i .</p><p>Information within the encoded parts will be then transferred back into the feature maps, so each pixel on the feature map can interact with the information in a wider range. Since the information within the parts is highly condensed, the computational cost between the pixels and the parts is much lower than the original pixel-wise global attention <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b53">54]</ref>. This encoder-decoder process constitutes the basic building block of ViP. By stacking the building block iteratively, the network can learn to construct a two-level part-whole hierarchy explicitly.  <ref type="figure">Figure 2</ref>: The overall pipeline of the Visual Parser (ViP). Given an input image and the prototype parameters p 1 as input, we iteratively refine the features of the two levels with the proposed encoderdecoder process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part Encoder</head><p>The part encoder is responsible for extracting the part information based on the previous part-whole input. The encoder is implemented with an attention mechanism. Given the part representation p i?1 ? R N ?C of the last block i ? 1, we first normalize it with Layer Normalization <ref type="bibr" target="#b0">[1]</ref>, then use it as the query of the attention block. The whole feature map from the last block x i?1 ? R L?C serves as the key and value after the normalization. The information of the whole will be condensed into the part representations via attention, which can be formulated as:</p><formula xml:id="formula_0">p i?1 = p i?1 + Attention(p i?1 + d e , x i?1 + d w , x i?1 ),<label>(1)</label></formula><p>wherep i?1 ? R N ?C is the output of the attention block, and Attention(query, key, value) denotes the attention mechanism. d w ? R L?C , d e ? R N ?C are positional encodings for the whole and the part respectively. The positional encoding d w follows the sinusoidal design proposed in <ref type="bibr" target="#b6">[7]</ref>, and d e is a set of learnable weights. We follow the common practice of the classic attention calculation, which first outputs an affinity matrix M ? R N ?L between the query and key and then use it to select information lying in the value.</p><formula xml:id="formula_1">M = 1 ? C q(LN(p i?1 + d e )) ? k(LN(x i?1 + d w )) T ,<label>(2)</label></formula><p>where the functions q(?), k(?) denote the linear mappings for the inputs of query and key, LN is the Layer Normalization. Here we omit the learnable weights within LN for simplicity. The product of the query and the key is normalized by a temperature factor 1 ? C to avoid it being one-hot after the lateral softmax calculation. The softmax operation guarantees the sum of the affinity matrix on the whole dimension to be one, which can be formulated as:</p><formula xml:id="formula_2">M a,b = e M a,b l e M a,l .<label>(3)</label></formula><p>The normalized affinity matrix can be easy to explain, as it assigns an independent weight to each spatial location indicating where each part is lying on the spatial feature maps. To aggregate these weighted spatial locations into part vectors, we follow the classic attention that weighted averaging the values together with the affinity matrix:</p><formula xml:id="formula_3">Attention(p i?1 + d e , x i?1 + d w , x i?1 ) =M ? v(LN(x i?1 )),<label>(4)</label></formula><p>where v(?) is the linear mapping for values, andM is the affinity matrix after softmax.</p><p>Reasoning across different parts. After each part is filled with the information from the feature maps, we apply a part-wise reasoning module to enable information communication between parts. In order to save computational cost, we just apply a single linear projection with learnable weights W p ? R N ?N . An identity mapping and the normalization are also applied here as a residual block. The process of the part-wise reasoning can be formulated as:</p><formula xml:id="formula_4">p i?1 r =p i?1 + W p ? LN(p i?1 ),<label>(5)</label></formula><p>wherep i?1 r represents the output for the part-wise reasoning.  Patch Embedding, C 1</p><formula xml:id="formula_5">[C1, N 1 , G 1] ? B 1 Stage 2 HW 8 2 Patch Embedding, C 2 [C 2 , N 2 , G 2 ] ? B 2 Stage 3 HW 16 2 Patch Embedding, C 3 [C3, N 3 , G 3] ? B 3 Stage 4 HW 32 2 Patch Embedding, C 4 [C4, N 4 , G 4] ? B 4 Last Encoder: [C 4 , N 4 , G 4 ]</formula><p>1000, Linear Activating the part representations. The part representation learnt above may not be all meaningful since different objects may have different numbers of parts describing themselves. We thereby further apply a Multi-Layer Perceptron (MLP) that has two linear mappings with weight W f 1 , W f 2 ? R C?C and an activation function (GELU <ref type="bibr" target="#b25">[26]</ref>) ?(?) in its module. The activation function will only keep the useful parts to be active, while those identified to be less helpful will be squashed. In this way, we obtain the part representation p i for block i by:</p><formula xml:id="formula_6">p i =p i?1 r + MLP(p i?1 r ), MLP(p i?1 r ) = ?(LN(p i?1 r ) ? W f 1 ) ? W f 2 .<label>(6)</label></formula><p>The above process demonstrates that the part representation generated by the previous block will be used to initialize the parts of the next iteration. Thus the randomly initialized part representations will be gradually refined with the information from the whole within each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Whole Decoder</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, there are two inputs for the decoder, the part representation p i and the whole representation x i?1 . Interactions within the decoder can be divided into a part-whole global interaction between the parts and the feature maps, and a patch-based local attention between pixels in a local window of the whole feature maps.</p><p>Part-whole global interaction. We first apply the part-whole global attention to fill each pixel on the whole representations x i?1 with the global information encapsulated in the parts p i . The part-whole global attention completely follows the classic attention paradigm <ref type="bibr" target="#b61">[62]</ref>, which takes x i?1 as the query input, and p i as inputs of the key and value. Therefore each pixel of the whole representation can have a long-range interaction with the encoded parts. Before feeding into the attention, both part and whole representations will be normalized by Layer Normalization. An identity mapping and a MLP are also applied as what does in common practice. The process of the part-whole interaction in the decoder can be written as:</p><formula xml:id="formula_7">x i g = x i?1 + Attention(x i?1 + d w , p i + d d , p i ), x i = x i g + MLP(x i g ),<label>(7)</label></formula><p>where d d ? R N ?C is the positional encoding for parts in decoders, the definitions of the attention mechanism and MLP are identical to those defined in Eq. (1) (6). Note that d d is shared across all blocks of each stage. The axis that the softmax function normalizes on is the last dimension (specifically, the part dimension with N inputs).</p><p>Patch-based local attention. The above process, in both the encoder and the decoder, has completed the cross-level interactions for the i th iteration. In addition to the long-range modeling that the cross-level interaction provided, we also apply a local attention for fine-grained feature modeling. We divide the spatial feature maps into non-overlapping patches with size k ? k, then apply a multi-head self-attention module for all pixels within each patch. We denote the pixels of patch t as x i t ? R k 2 ?C , then the process of the local attention can be written as:</p><formula xml:id="formula_8">x i t = x i t + Attention(x i t , x i t + r i , x i t ), x i l = {x i 1 , ...,x i t , ...,x i Np }, x i =x i l + MLP(x i l ),<label>(8)</label></formula><p>where N p = L k 2 denotes the total number of patches, r i ? R k 2 ?C is the relative positional embedding. The implementation of the relative positional embedding follows the design in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54]</ref>. To save the computational cost, r i is factorized into two embeddings</p><formula xml:id="formula_9">r i h ? R (2k?1)? C 2 , r i w ? R (2k?1)? C 2</formula><p>for the dimension of height and width respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Architecture Specification.</head><p>In this paper, we design five kinds of different variants called ViP-Mobile (Mo), ViP-Tiny (Ti), ViP-Small (S), ViP-Medium (M), ViP-Base (B) respectively. These variants have some common features in design. <ref type="formula" target="#formula_0">(1)</ref> Architectures of all these models are divided into four stages according to the spatial resolution L of the feature map. Given an input with spatial size H ? W , the output spatial sizes of the feature maps for the four stages are</p><formula xml:id="formula_10">H 4 ? W 4 , H 8 ? W 8 , H 16 ? W 16 and H 32 ? W 32 . (2)</formula><p>The expansion rates of the MLP within the encoder and the decoder, which indicates the ratio between the number of channels of the hidden output and the input, are set to be 1 and 3 separately. <ref type="formula" target="#formula_2">(3)</ref> The patch size for the self-attention module of the decoder is set to {8, 7, 7, 7} for four different stages. (4) At the beginning of each stage, there is a patch embedding responsible for down-sampling and channel-wise expansion. We employ a separable convolution with normalization here with kernel size 3 ? 3 to perform the down-sampling operation for the whole representation. Since the number of channels of the part representation for each stage may vary, another linear operation is applied to align the number of channels between parts in different stages.</p><p>Apart from these common hyper-parameters, for a specific stage s, these variants mainly differ in the following aspects: (1) The number of channels C s , (2) The number of parts N s , (3) The number of blocks B s , (4) The number of groups (heads) G s for the attention mechanism. (5) For small models ViP-Mo, ViP-Ti and ViP-S, we employ a part encoder on top of the whole representation before the final global pooling and fully connected layer, while for ViP-M and ViP-B, we replace such encoder with a linear layer. The overall architecture of the ViP family is shown in <ref type="table" target="#tab_1">Table 1</ref>. The detailed specification of the four variants can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Convolutional Neural Networks (CNNs). Conventional CNNs <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b48">49]</ref> are prevalent in nearly all fields of computer vision since AlexNet <ref type="bibr" target="#b40">[41]</ref> demonstrates its power for image recognition on ImageNet <ref type="bibr" target="#b15">[16]</ref>. Now CNNs are still dominating nearly all major tasks in computer vision e.g. image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref>, object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44]</ref> and semantic segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b72">73]</ref>.</p><p>Part-whole hierarchies in visual representations. Tu et al. <ref type="bibr" target="#b60">[61]</ref> first devise a Bayesian framework to parse the image into a part-whole hierarchy for unifying all the major vision tasks. Capsule Networks (CapsNets) <ref type="bibr" target="#b51">[52]</ref> were first proposed to use a dynamic routing algorithm to dynamically allocate neurons to represent a small portion of the visual input. There are some other extensive works based on CapsNets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60]</ref> showing remarkable performance on some small datasets, however, these works cannot be well scaled onto large datasets. The recent proposal of GLOM <ref type="bibr" target="#b28">[29]</ref> gives an idea to build up a hierarchical representation with attention, but it gives no practical experiments. This paper borrows some ideas from these works to build a rather simple hierarchy with two levels for modeling basic visual representation. For example, the iterative attention mechanism in our model is similar to the dynamic routing designed in CapsNet <ref type="bibr" target="#b51">[52]</ref> or iterative attention mechanism <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Transformers and self-attention mechanism. With the success of Memory Networks <ref type="bibr" target="#b54">[55]</ref> and Transformers <ref type="bibr" target="#b61">[62]</ref> for natural language modeling <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b68">69]</ref>, lots of works in the field of computer vision attempted to migrate similar self-attention mechanism as an independent block    into CNNs for image classification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b65">66]</ref>, object detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b6">7]</ref> and video action recognition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Recent works tried to replace all convolutional layers in neural networks with local attention layers to build up self-attention-based networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b62">63]</ref>. To resolve the inefficiency problem, Vision Transformer (ViT) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b58">59]</ref> chose to largely reduce the image resolution and only retain the global visual tokens while processing. To aid the global token-based transformer with local inductive biases, there are several papers that incorporate convolution-like design into ViT <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b67">68]</ref>. Apart from the token-based approach, concurrent works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b63">64]</ref> that retain the spatial pyramids has also been proven to be effective. Different from the above existing works that extract either tokens or spatial feature maps for final prediction, ViP extracts both the token-based representations (the part) and spatial feature maps (the whole).</p><p>Token-based global attention mechanism. The interaction between the part and the whole is related to the token-based global attention mechanism. Recent works including <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b65">66]</ref> propose to tokenize the input feature map generated by the convolution block. Our work is different from theirs in three aspects: (1) The part representations are explicitly and iteratively refined in ViP, while in these works, the tokens are latent bi-product of each block. (2) We intend to design a hierarchy that can be used for final prediction while these works focus on designing a module then plug it into limited blocks of the network. (3) In detail, the token extraction and the bi-directional pathway designed in ViP are quite different from their pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Classification on ImageNet-1K</head><p>Experimental Settings. For image classification, we evaluate our models on ImageNet-1K <ref type="bibr" target="#b15">[16]</ref>, which consists of 1.28M training images and 50K validation images categorized into 1,000 classes. The network is trained for 300 epochs using AdamW <ref type="bibr" target="#b38">[39]</ref> and a half-cosine annealing learning rate RetinaNet 1?</p><p>RetinaNet 3?  We primarily follow the settings of the data augmentation adopted in <ref type="bibr" target="#b58">[59]</ref>, except for the repeat augmentation as we found it unhelpful towards the final prediction. Note that for all results for image classification on ImageNet reported in this paper, we did not use any external dataset for pre-training.</p><formula xml:id="formula_11">Backbone AP b AP b 50 AP b 75 AP b S AP b M AP b L AP b AP b 50 AP b 75 AP b S AP b M AP b L Params (M) FLOPS (G) ViP-</formula><p>ViP vs. CNNs. <ref type="table">Table 2</ref> compares ViP family with some of the state-of-the-art CNN models on ImageNet. The RegNet is also better tuned using training tricks in <ref type="bibr" target="#b58">[59]</ref>. ViP is both cost-efficient and parameter-efficient compared to these state-of-the-art models. For example, ViP-M can achieve a competitive 83.3% with only 49.6M parameters and 8.0G FLOPS. The counterpart BOTNet-T5 needs 25.5M more parameters and 11.3G FLOPS to achieve similar performance. When scaling the input to resolution 384 2 , ViP-B is able to further improve its top-1 accuracy to 84.2%.</p><p>ViP vs. ViT/DeiT. We first compare ViP to the token-based Vision Transformer (ViT), which radically reduces the image resolution at the beginning of the network. When both networks are trained from scratch on the training set of ImageNet-1K, ViP-Ti can outperform ViT-B by 1.1% with only about 1 7 of its number of parameters and a fraction of its FLOPS.</p><p>Another token-based vision transformer, DeiT, is also listed in <ref type="table">Table 2</ref> for comparison. The basic structure of DeiT is identical to what was proposed in ViT but is trained with more data augmentations and regularization techniques. When compare ViP with DeiT, we observe that ViP-Ti can surpass DeiT-Tiny by a significant 6.8%. As for small models like ViP-S, it can outperform DeiT-S by 2.1%, which is even better than a way larger variant DeiT-B of the DeiT family. The remarkable improvement on ImageNet demonstrates the importance of retaining the local features within the network for image recognition.</p><p>ViP vs. HaloNet. To the best of our knowledge, HaloNet <ref type="bibr" target="#b62">[63]</ref> is the current state-of-the-art network on ImageNet-1K. <ref type="figure">Figure 4</ref> shows the speed-accuracy Pareto curve of the ViP family compared to the HaloNet family. Note that the HaloNet is re-implemented by us on PyTorch. As shown in <ref type="figure">Figure 4</ref>, ViP achieves better speed-accuracy trade-off than HaloNet. Concretely, ViP-S is 6.3? faster than HaloNet-H4 with similar top-1 accuracy on ImageNet-1K.</p><p>ViP vs. other state-of-the-art Transformers. As shown in <ref type="table">Table 2</ref>, ViP consistently outperforms previous state-of-the-art Transformer-based models in terms of accuracy and model size. Especially, ViP-B achieves 83.8% ImageNet top-1 accuracy, which is 0.5% higher than Swin-B <ref type="bibr" target="#b45">[46]</ref> with fewer parameters and FLOPS. A similar trend can also be observed when scaled onto larger models, e.g. ViP-M achieves 83.3% top-1 accuracy, outperforming TNT-B <ref type="bibr" target="#b22">[23]</ref>, T2T-ViT-24 <ref type="bibr" target="#b70">[71]</ref>, PVT-Large <ref type="bibr" target="#b63">[64]</ref> by 0.4%, 1.0%, 1.6% respectively.  </p><formula xml:id="formula_12">Backbone AP b AP b 50 AP b 75 AP b S AP b M AP b L AP m AP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Detection and Instance Segmentation</head><p>Experimental settings. For object detection and instance segmentation, we evaluate ViP on the challenging MS COCO dataset <ref type="bibr" target="#b41">[42]</ref>, which contains 115k images for training (train-2017) and 5k images (val-2017) for validation. We train models on train-2017 and report the results on val-2017. We measure our results following the official definition of Average Precision (AP) metrics given by MS COCO, which includes AP 50 and AP 75 (averaged over IoU thresholds 50 and 75) and AP S , AP M , AP L (AP at scale Small, Medium and Large). Annotations of MS COCO include both bounding boxes and polygon masks for object detection and instance segmentation respectively. Experiments are implemented based on the open source mmdetection <ref type="bibr" target="#b7">[8]</ref> platform. All models are trained under two different training schedules 1? (12 epochs) and 3? (36 epochs) using the AdamW <ref type="bibr" target="#b38">[39]</ref> optimizer with the same weight decay set for image classification. After a 500 iteration's warming-up, the learning rate is initialized at 1 ? 10 ?4 then decayed by 0.1 after <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> and <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> epochs for 1? and 3? respectively. For data augmentation, we only apply random flipping with a probability of 0.5 and scale jittering from 640 to 800. The batch size for each GPU is 2 and we use 8 GPUs to train the network for all experiments. Stochastic Depth is also applied here as what proposed on ImageNet. We embed ViP into two popular frameworks for object detection and instance segmentation, RetinaNet <ref type="bibr" target="#b43">[44]</ref> and Cascade Mask-RCNN <ref type="bibr" target="#b5">[6]</ref>. When incorporating ViP into these frameworks, ViP serves as the backbone followed by a Feature Pyramid Network (FPN) <ref type="bibr" target="#b42">[43]</ref> refining the multi-scale whole representations. All weights within the backbone are first pre-trained on ImageNet-1K, while those outside the backbone are initialized with Xavier <ref type="bibr" target="#b21">[22]</ref>.</p><p>ViP can outperform ResNe(X)t with 4? less computational cost in RetinaNet. <ref type="table" target="#tab_6">Table 5</ref> exhibits the experimental results when embedding different backbones into RetinaNet. When trained under the 1? schedule, our ViP-Ti can outperform its counterpart ResNet-18 by 7.9, which is a large margin since it is even higher than the performance obtained by ResNet-101 (4? larger than ViP-Ti in terms of FLOPS and parameters). The ViP-S, which is just about the size of ResNet-50, can even outperforms the largest model ResNeXt-101-64?4d listed in <ref type="table" target="#tab_6">Table 5</ref> by a clear 2.0. For larger variant ViP-M, it can further boost the performance to a higher level 44.3. The performance of the ViP family can be steadily boosted by the longer 3? training schedule. As shown in <ref type="table" target="#tab_6">Table 5</ref>, all variants of the ViP family can retain their superiority compared with their ResNe(X)t and PVT counterparts.</p><p>ViP-Tiny can be comparable with the largest variant of ResNe(X)t family in Cascade Mask RCNN. <ref type="table" target="#tab_8">Table 6</ref> shows the results when incorporating different backbones into Cascade Mask RCNN <ref type="bibr" target="#b5">[6]</ref>. As shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>Number of parts: As shown in <ref type="table">Table 3</ref>, the number of parts N is crucial when the model is small. Concretely, for ViP-Ti, N =32 can lead to a remarkable 0.8% improvement on ImageNet compared to N =16. However, the improvement comes to be saturated when adding more parts into the network.</p><p>Effects of the part-wise linear. Different from the original design in Transformer <ref type="bibr" target="#b61">[62]</ref> that uses a self-attention module for part-wise communication, we replace it with a simple linear operation to save the computational cost. Introducing such a simple linear operation into ViP-S can lead to a 0.4% gain on ImageNet with only a fractional increase in parameters (0.03M) and FLOPS (0.02G).</p><p>Predicting on parts vs. wholes. As ViP has two levels, we can choose either of them for the final prediction. When using the parts for the final prediction, we employ an additional encoder on top of the whole before the final global pooling and fully connected layer to gather all parts obtained by the encoder. Otherwise, we replace the encoder with a linear projection. <ref type="table" target="#tab_4">Table 4</ref> compares the results of predicting on the part representation. For small models like ViP-Ti and ViP-small, to predict on parts can lead to remarkable improvements (+0.7% and +0.4%). However, predicting on part level encountered an overfitting problem when incorporated into ViP-M with a 0.6% drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>The visualization results of a pre-trained ViP-S are shown as an example in <ref type="figure" target="#fig_4">Figure 5</ref>. We average all heads of the affinity matrix M in Eq. (2) and then normalize it to [0, 255]. For each image, we visualize two parts in total for clearness. It can be observed that the attention maps tend to cover more area in the shallow blocks, and then gradually focus on salient objects through multiple iterations. The observation suggests that the part encoder can effectively aggregate features from a part of the image, and the part representations can be refined with the information from the whole. The visualization results show that a meaningful part-whole hierarchy is constructed by the proposed ViP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we construct a framework that includes different levels of representations called Visual Parser (ViP). ViP divides visual representations into the part level and the whole level with a novel encoder-decoder interaction. Extensive experiments demonstrate that the proposed ViP can achieve very competitive results on three major vision tasks. Particularly, ViP outperforms the previous state-of-the-art CNN backbones by a large margin on object detection and instance segmentation. Visualization results also indicate that the learned part representations are highly informative to the predicting classes. As the first step towards learning multi-level part-whole representations, our ViP is more explainable compared to previous architectures and shows great potential in visual modeling.</p><p>Acknowledgement. We would like to thank Pau de Jorge, Francesco Pinto, Hengshuang Zhao and Xiaojuan Qi for proof-reading and helpful comments. This work is supported by the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would also like to thank the Royal Academy of Engineering and FiveAI.  <ref type="table">Table 7</ref>: Detailed specification for ViP family. Contents in [?] represents the basic building block of ViP, and those in (?) are just part encoders. Note that we only apply a part decoder at the end of network to make the final prediction on part level in small models like ViP-Mo/Ti/S. The general specification of ViP is illustrated in Section 2.4. Here in <ref type="table">Table 7</ref> we show the detailed structure of different variants of the ViP family. Note that for small models including ViP-Mobile, ViP-Tiny, and ViP-Small, we apply another encoder at the end of the network but replacing MLP with the activation function GELU to predicting on the part level. While for larger models ViP-Medium and ViP-Base, we replace the encoder with a linear projection (with Batch Normalization <ref type="bibr" target="#b35">[36]</ref>) so that it can predict on the whole level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">More Visualization Results</head><p>Additional visualization results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. We follow the visualization method mentioned in Section 4.4, and it is obvious that the proposed part encoder is robust and works well even for the complex scene. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The basic building block of ViP. The symbol denotes element-wise summation. Here we omit the relative positional embedding r i for clear demonstration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization results about where the part representations attend on. Pixels rendered in different colors are associated to different parts. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>More visualized results. All results are visualized using the same method proposed in Section 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>General specification for ViP family. Building blocks of ViP are shown in brackets. The last encoder is only set for ViP-Mo/Ti/S.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :Table 3 :</head><label>23</label><figDesc>Results on ImageNet-1K. Effect of number of parts for ViP-Ti.</figDesc><table><row><cell cols="4">Figure 4: Speed-Accuracy comparison with HaloNet.</cell></row><row><cell></cell><cell>N</cell><cell cols="2">FLOPS (G)</cell><cell>Top-1 (%)</cell></row><row><cell></cell><cell cols="3">8 1.6</cell><cell>77.6</cell></row><row><cell>ViP-Ti</cell><cell cols="3">16 1.6 32 1.7</cell><cell>78.1 79.0</cell></row><row><cell></cell><cell cols="3">64 1.8</cell><cell>79.1</cell></row><row><cell cols="3">Predict</cell><cell>Predict</cell><cell>Top-1</cell></row><row><cell cols="3">on parts</cell><cell>on wholes</cell><cell>(%)</cell></row><row><cell>ViP-Ti</cell><cell></cell><cell></cell><cell>79.0 78.3</cell></row><row><cell>ViP-S</cell><cell></cell><cell></cell><cell>81.9 81.5</cell></row><row><cell>ViP-M</cell><cell></cell><cell></cell><cell>82.7 83.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Effect of predicting on part/whole level.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Mo 36.5 56.7 38.6 23.4 39.7 48.4 39.2 59.7 41.4 25.5 42.3 51.7 5.3 (15.2) 15 (166) R18 [44] 31.8 49.6 33.6 16.3 34.3 43.2 35.4 53.9 37.6 19.5 38.2 46.8 11.0 (21.3) 37 (189) PVT-T [64] 36.7(+4.9) 56.9 38.9 22.6 38.8 50.0 39.4(+4.0) 59.8 42.0 25.5 42.0 52.1 12.3 (23.0) 70 (221) ViP-Ti 39.7(+7.9) 60.6 42.2 23.9 42.9 53.0 41.6(+6.2) 62.6 44.0 27.2 45.1 54.2 11.2 (21.+1.4) 59.6 42.7 22.3 44.2 52.5 41.4(+0.5) 61.0 44.3 23.9 45.5 53.7 41.9 (56.4) 164 (319) PVT-M [64] 41.9(+3.4) 63.1 44.3 25.0 44.9 57.6 43.2(+2.3) 63.8 46.1 27.3 46.3 58.9 43.7 (54.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>4) 29 (181)</cell></row><row><cell>R50 [44]</cell><cell>36.5</cell><cell>55.4 39.1 20.4 40.3 48.1 39.0</cell><cell>58.4 41.8 22.4 42.8 51.6 23.3 (37.7) 84 (239)</cell></row><row><cell cols="4">PVT-S [64] 40.4(+3.9) 61.3 43.0 25.0 42.9 55.7 42.2(+3.2) 62.7 45.0 26.2 45.2 57.2 23.6 (34.2) 134 (286)</cell></row><row><cell>ViP-S</cell><cell cols="3">43.0(+6.5) 64.0 45.9 28.9 46.7 56.3 44.0(+5.0) 65.1 47.2 28.8 47.3 57.2 29.0 (39.9) 75 (227)</cell></row><row><cell>R101 [44]</cell><cell>38.5</cell><cell>57.8 41.2 21.4 42.6 51.1 40.9</cell><cell>60.1 44.0 23.7 45.0 53.8 42.3 (56.7) 160 (315)</cell></row><row><cell cols="4">X101-32 [44] 39.9(3) 222 (374)</cell></row><row><cell>ViP-M</cell><cell cols="3">44.3(+5.8) 65.9 47.4 30.7 48.0 57.9 45.3(+4.4) 66.4 48.5 29.7 48.6 59.3 48.8 (59.8) 135 (287)</cell></row><row><cell cols="2">X101-64 [44] 41.0</cell><cell>60.9 44.0 23.9 45.2 54.0 41.8</cell><cell>61.5 44.4 25.2 45.4 54.6 81.0 (95.5) 317 (473)</cell></row><row><cell cols="2">PVT-L [64] 42.6</cell><cell>63.7 45.4 25.8 46.0 58.4 43.4</cell><cell>63.6 46.1 26.1 46.0 59.5 60.9 (71.5) 324 (476)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Various backbones with RetinaNet. Here R and X are abbreviations for ResNet and ResNeXt. Parameters and FLOPS in black are for backbones, while those in (gray) are for the whole frameworks.</figDesc><table /><note>scheduler. The learning rate is warmed up for 20 epochs to reach the initial 1 ? 10 ?3 . Weight decays for ViP-Mo, ViP-Ti are set to be 0.03, while those for ViP-S, ViP-M are 0.05. The drop ratios of Stochastic Depth (a.k.a DropPath) [34] are linearly scaled from 0 to 0.1, 0.1, 0.2, 0.3 along the layer depth for each layer of ViP-Ti, ViP-S, ViP-M and ViP-B respectively. We do not apply Stochastic Depth to ViP-Mo during training. The total training batch size is set to be 1024 for all model variants.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Mo 42.6 61.8 46.0 27.1 44.9 56.4 37.7 59.2 40.1 22.5 39.8 51.0 5.9 (63.9) 16 (665) R18 38.7 56.2 41.3 21.3 40.8 52.9 34.0 53.5 36.4 17.4 36.0 48.1 11.0 (69.0) 37 (686) ViP-Ti 45.4(+6.7) 64.6 48.9 29.1 48.8 60.1 39.9(+5.9) 61.7 42.7 24.1 43.0 54.3 11.2 (69.2) 29 (678) R50 41.2 59.4 45.0 23.9 44.2 54.4 35.9 56.6 38.4 19.4 38.5 49.3 23.3 (82.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>m 50 AP m 75 AP m S AP m M AP m L</cell><cell>Params (M)</cell><cell>FLOPS (G)</cell></row><row><cell cols="5">ViP-0) 84 (739)</cell></row><row><cell>S50</cell><cell>45.4</cell><cell>64.1 49.2 28.3 49.1 58.8 39.5</cell><cell cols="2">61.4 42.5 23.1 43.0 52.8 25.1 (82.9) 110 (763)</cell></row><row><cell>ViP-S</cell><cell cols="4">48.5(+7.3) 67.5 52.5 31.9 51.8 63.2 42.2(+6.3) 64.8 45.7 25.9 45.5 56.7 29.0 (87.1) 75 (725)</cell></row><row><cell>R101</cell><cell>42.9</cell><cell>61.0 46.6 24.4 46.5 57.0 37.3</cell><cell cols="2">58.2 40.1 19.7 40.6 51.5 42.3 (101.0) 160 (815)</cell></row><row><cell cols="2">X101-32 44.3</cell><cell>62.7 48.4 25.4 48.4 58.1 38.3</cell><cell cols="2">59.7 41.2 20.6 42.0 52.3 41.9 (100.6) 164 (819)</cell></row><row><cell>S101</cell><cell>47.7</cell><cell>66.4 51.9 30.1 51.8 61.4 41.4</cell><cell cols="2">63.7 45.1 24.7 45.2 54.9 45.7 (104) 209 (862)</cell></row><row><cell cols="5">ViP-M 49.9(+7.0) 69.5 54.2 33.1 53.4 65.1 43.5(+6.2) 66.4 47.2 26.8 46.9 59.1 48.8 (107.0) 135 (785)</cell></row><row><cell cols="2">X101-64 45.3</cell><cell>63.9 49.6 26.7 49.4 59.9 39.2</cell><cell cols="2">61.1 42.2 21.6 42.8 53.7 81.0 (139.7) 317 (972)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Various backbones with Cascade Mask R-CNN. All results are trained under 1? schedule.Here S denotes ResNeSt<ref type="bibr" target="#b71">[72]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>, when trained under 1? schedule, all variants of the ViP family can achieve significantly better performance compared to their counterparts. Notably, as a tiny model with only 11.2M parameters and 29G FLOPS, ViP-Ti can achieve comparable performance obtained by the largest variant in ResNe(X)t family ResNeXt-101-64?4d which contains 81M parameters and 317G FLOPS. ViP also scales well with larger models. ViP-M further lifts the performance to 49.9 for object detection and 43.5 for instance segmentation. When compared with state-of-the-art variants</figDesc><table /><note>of ResNet family like ResNeSt [72], ViP can also outperform them by a clear margin. Specifically, ViP-S and ViP-M outperforms ResNeSt-50 and ResNeSt-101 by 3.1 and 2.2 respectively on object detection and 2.7 and 2.1 on instance segmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Patch Embedding, 48 Patch Embedding, 64 Patch Embedding, 96 Patch Embedding, 96 Patch Embedding, 128 C Patch Embedding, 96 Patch Embedding, 128 Patch Embedding, 192 Patch Embedding, 192 Patch Embedding, 256 C Patch Embedding, 192 Patch Embedding, 256 Patch Embedding, 384 Patch Embedding, 384 Patch Embedding, 512 C Patch Embedding, 384 Patch Embedding, 512 Patch Embedding, 768 Patch Embedding, 768 Patch Embedding, 1024 C</figDesc><table><row><cell cols="3">6 Appendix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">6.1 Network Specification of ViP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Stage L</cell><cell cols="2">ViP-Mobile</cell><cell cols="2">ViP-Tiny</cell><cell cols="2">ViP-Small</cell><cell cols="2">ViP-Medium</cell><cell>ViP-Base</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">7 ? 7, 64, Conv, stride 2; 3 ? 3 Max Pool, stride 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage 1</cell><cell>HW 4 2</cell><cell>1 = 48 N 1 = 16</cell><cell>? 1</cell><cell>C 1 = 64 N 1 = 32</cell><cell>? 1</cell><cell>C 1 = 96 N 1 = 64</cell><cell>? 1</cell><cell>C 1 = 96 N 1 = 64</cell><cell>? 1</cell><cell>C 1 = 128 N 1 = 64</cell><cell>? 1</cell></row><row><cell></cell><cell></cell><cell>G 1 = 1</cell><cell></cell><cell>G 1 = 1</cell><cell></cell><cell>G 1 = 1</cell><cell></cell><cell>G 1 = 1</cell><cell></cell><cell>G 1 = 1</cell><cell></cell></row><row><cell>Stage 2</cell><cell>HW 8 2</cell><cell>2 = 96 N 2 = 16</cell><cell>? 1</cell><cell>C 2 = 128 N 2 = 16</cell><cell>? 1</cell><cell>C 2 = 192 N 2 = 16</cell><cell>? 1</cell><cell>C 2 = 192 N 2 = 16</cell><cell>? 1</cell><cell>C 2 = 256 N 2 = 16</cell><cell>? 1</cell></row><row><cell></cell><cell></cell><cell>G 2 = 2</cell><cell></cell><cell>G 2 = 2</cell><cell></cell><cell>G 2 = 2</cell><cell></cell><cell>G 2 = 2</cell><cell></cell><cell>G 2 = 2</cell><cell></cell></row><row><cell>Stage 3</cell><cell>HW 16 2</cell><cell>3 = 192 N 3 = 16</cell><cell>? 1</cell><cell>C 3 = 256 N 3 = 32</cell><cell>? 2</cell><cell>C 3 = 384 N 3 = 64</cell><cell>? 3</cell><cell>C 3 = 384 N 3 = 64</cell><cell>? 8</cell><cell>C 3 = 512 N 3 = 128</cell><cell>? 8</cell></row><row><cell></cell><cell></cell><cell>G 3 = 4</cell><cell></cell><cell>G 3 = 4</cell><cell></cell><cell>G 3 = 12</cell><cell></cell><cell>G 3 = 12</cell><cell></cell><cell>G 3 = 16</cell><cell></cell></row><row><cell>Stage 4</cell><cell>HW 32 2</cell><cell>4 = 384 N 4 = 32</cell><cell>? 1</cell><cell>C 4 = 512 N 4 = 32</cell><cell>? 1</cell><cell>C 4 = 768 N 4 = 64</cell><cell>? 1</cell><cell>C 4 = 768 N 4 = 64</cell><cell>? 1</cell><cell>C 4 = 1024 N 4 = 128</cell><cell>? 1</cell></row><row><cell></cell><cell></cell><cell>G 4 = 8</cell><cell></cell><cell>G 4 = 8</cell><cell></cell><cell>G 4 = 24</cell><cell></cell><cell>G 4 = 24</cell><cell></cell><cell>G 4 = 32</cell><cell></cell></row><row><cell></cell><cell></cell><cell>C 4 = 384 N 4 = 32 G 4 = 8</cell><cell>? 1</cell><cell>C 4 = 512 N 4 = 32 G 4 = 8</cell><cell>? 1</cell><cell>C 4 = 768 N 4 = 64 G 4 = 24</cell><cell>? 1</cell><cell cols="2">768, Linear 768, BatchNorm</cell><cell cols="2">1024, Linear 1024, BatchNorm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Global Average Pool; 1000, Linear</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lambda networks: Modeling long-range interactions without attention. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11579</idno>
		<title level="m">Double attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Do we really need explicit position encodings for vision transformers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some demonstrations of the effects of structural descriptions in mental imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="250" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How to represent part-whole hierarchies in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12627</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The reviewing of object files: Object-specific integration of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="219" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Adam R Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06818</idno>
		<title level="m">Stacked capsule autoencoders</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15055</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09829</idno>
		<title level="m">Dynamic routing between capsules</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03495</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Capsules with inverted dot-product attention routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04764</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
