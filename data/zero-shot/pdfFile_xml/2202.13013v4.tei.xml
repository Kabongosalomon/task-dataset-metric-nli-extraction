<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIGN AND BASIS INVARIANT NETWORKS FOR SPECTRAL GRAPH REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
							<email>dereklim@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">MIT EECS &amp; MIT RLE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">MIT LIDS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIGN AND BASIS INVARIANT NETWORKS FOR SPECTRAL GRAPH REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce SignNet and BasisNet-new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if v is an eigenvector then so is ?v; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases. Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes. Our code is available at https://github.com/cptq/SignNet-BasisNet. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Numerous machine learning models process eigenvectors, which arise in various settings including principal component analysis, matrix factorizations, and operators associated to graphs or manifolds. An important example is the use of Laplacian eigenvectors to encode information about the structure of a graph or manifold <ref type="bibr" target="#b6">(Belkin &amp; Niyogi, 2003;</ref><ref type="bibr" target="#b84">Von Luxburg, 2007;</ref><ref type="bibr">L?vy, 2006)</ref>. Positional encodings that involve Laplacian eigenvectors have recently been used to generalize Transformers to graphs <ref type="bibr">(Kreuzer et al., 2021;</ref><ref type="bibr" target="#b29">Dwivedi &amp; Bresson, 2021)</ref>, and to improve the expressive power and empirical performance of graph neural networks (GNNs) <ref type="bibr" target="#b31">(Dwivedi et al., 2022)</ref>. Furthermore, these eigenvectors are crucial for defining spectral operations on graphs that are foundational to graph signal processing and spectral GNNs <ref type="bibr" target="#b55">(Ortega et al., 2018;</ref><ref type="bibr" target="#b13">Bruna et al., 2014)</ref>.</p><p>However, there are nontrivial symmetries that should be accounted for when processing eigenvectors, as has been noted in many fields <ref type="bibr" target="#b32">(Eastment &amp; Krzanowski, 1982;</ref><ref type="bibr" target="#b64">Rustamov et al., 2007;</ref><ref type="bibr" target="#b11">Bro et al., 2008;</ref><ref type="bibr" target="#b56">Ovsjanikov et al., 2008)</ref>. For instance, if v is an eigenvector, then so is ?v, with the same eigenvalue. More generally, if an eigenvalue has higher multiplicity, then there are infinitely many unit-norm eigenvectors that can be chosen. Indeed, a full set of linearly independent eigenvectors is only defined up to a change of basis in each eigenspace. In the case of sign invariance, for any k eigenvectors there are 2 k possible choices of sign. Accordingly, prior works on graph positional encodings randomly flip eigenvector signs during training in order to approximately learn sign invariance <ref type="bibr">(Kreuzer et al., 2021;</ref><ref type="bibr" target="#b30">Dwivedi et al., 2020;</ref><ref type="bibr">Kim et al., 2022)</ref>. However, learning all 2 k invariances is challenging and limits the effectiveness of Laplacian eigenvectors for encoding positional information. Sign invariance is a special case of basis invariance when all eigenvalues are distinct, but general basis invariance is even more difficult to deal with. In Appendix C.2, we show that higher dimensional eigenspaces are abundant in real datasets; for instance, 64% of molecule graphs in the ZINC dataset have a higher dimensional eigenspace.</p><p>In this work, we address the sign and basis ambiguity problems by developing new neural networks-SignNet and BasisNet. Under certain conditions, our networks are universal and can approximate any continuous function of eigenvectors with the proper invariances. Moreover, our networks are theoretically powerful for graph representation learning-they can provably approximate and go beyond both spectral graph convolutions and powerful spectral invariants, which allows our networks to express graph properties like subgraph counts that message passing neural networks cannot. Laplacian eigenvectors with SignNet and BasisNet can provably approximate many previously proposed graph positional encodings, so our networks are general and remove the need for choosing one of the many positional encodings in the literature. Experiments on molecular graph regression tasks, learning expressive graph representations, and texture reconstruction on triangle meshes illustrate the empirical benefits of our models' approximation power and invariances. For an n ? n symmetric matrix, let ? 1 ? . . . ? ? n be the eigenvalues and v 1 , . . . , v n the corresponding eigenvectors, which we may assume to form an orthonormal basis. For instance, we could consider the normalized graph Laplacian L = I ? D ?1/2 AD ?1/2 , where A ? R n?n is the adjacency matrix and D is the diagonal degree matrix of some underlying graph. For undirected graphs, L is symmetric. Nonsymmetric matrices can be handled very similarly, as we show in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SIGN AND BASIS INVARIANT NETWORKS</head><p>Motivation. Our goal is to parameterize a class of models f (v 1 , . . . , v k ) taking k eigenvectors as input in a manner that respects the eigenvector symmetries. This is because eigenvectors capture much information about data; Laplacian eigenvectors of a graph capture clusters, subgraph frequencies, connectivity, and many other useful properties <ref type="bibr" target="#b84">(Von Luxburg, 2007;</ref><ref type="bibr" target="#b25">Cvetkovi? et al., 1997)</ref>.</p><p>A major motivation for processing eigenvector input is for graph positional encodings, which are additional features appended to each node in a graph that give information about the position of that node in the graph. These additional features are crucial for generalizing Transformers to graphs, and also have been found to improve performance of <ref type="bibr">GNNs (Dwivedi et al., 2020;</ref>. <ref type="figure">Figure 2</ref> illustrates a standard pipeline and the use of our SignNet within it: the input adjacency, node features, and eigenvectors of a graph are used to compute a prediction about the graph. Laplacian eigenvectors are processed before being fed into this prediction model. Laplacian eigenvectors have been widely used as positional encodings, and many works have noted that sign and/or basis invariance should be addressed in this case <ref type="bibr" target="#b29">(Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr" target="#b5">Beaini et al., 2021;</ref><ref type="bibr" target="#b30">Dwivedi et al., 2020;</ref><ref type="bibr">Kreuzer et al., 2021;</ref><ref type="bibr">Mialon et al., 2021;</ref><ref type="bibr" target="#b31">Dwivedi et al., 2022;</ref><ref type="bibr">Kim et al., 2022)</ref>.</p><p>Sign invariance. For any eigenvector v i , the sign flipped ?v i is also an eigenvector, so a function f : R n?k ? R dout (where d out is an arbitrary output dimension) should be sign invariant:</p><formula xml:id="formula_0">f (v 1 , . . . , v k ) = f (s 1 v 1 , . . . , s k v k )<label>(1)</label></formula><p>for all sign choices s i ? {?1, 1}. That is, we want f to be invariant to the product group {?1, 1} k . This captures all eigenvector symmetries if the eigenvalues ? i are distinct and the eigenvectors are unit-norm.</p><p>Basis invariance. If the eigenvalues have higher multiplicity, then there are further symmetries. Let V 1 , . . . , V l be bases of eigenspaces-i.e., V i = v i1 . . . v i d i ? R n?di has orthonormal columns and spans the eigenspace associated with the shared eigenvalue ? i = ? i1 = . . . = ? i d i . Any other orthonormal basis that spans the eigenspace is of the form V i Q for some orthogonal</p><formula xml:id="formula_1">Q ? O(d i ) ? R di?di (see Appendix F.2).</formula><p>Thus, a function f : R n? l i=1 di ? R dout that is invariant to changes of basis in each eigenspace satisfies f (V 1 , . . . , V l ) = f (V 1 Q 1 , . . . , V l Q l ), Q i ? O(d i ).</p><p>(2)</p><p>In other words, f is invariant to the product group O(d 1 ) ? . . . ? O(d l ). The number of eigenspaces l and the dimensions d i may vary between matrices; we account for this in Section 2.2. As O(1) = {?1, 1}, sign invariance is a special case of basis invariance when all eigenvalues are distinct.</p><p>Permutation equivariance. For GNN models that output node features or node predictions, one typically further desires f to be invariant or equivariant to permutations of nodes, i.e., along the rows of each vector. Thus, for f : R n?d ? R n?dout , we typically require f (P V 1 , . . . , P V l ) = P f (V 1 , . . . , V l ) for any permutation matrix P ? R n?n . <ref type="figure" target="#fig_0">Figure 1</ref> illustrates all of the symmetries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Graph Model</head><p>Figure 2: Pipeline for using node positional encodings. After processing by our SignNet, the learned positional encodings from the Laplacian eigenvectors are added as additional node features of an input graph ([X, SignNet(V )] denotes concatenation). These positional encodings along with the graph adjacency and original node features are passed to a prediction model (e.g. a GNN). Not shown here, SignNet can also take in eigenvalues, node features and adjacency information if desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WARMUP: NEURAL NETWORKS ON ONE EIGENSPACE</head><p>Before considering the general setting, we design neural networks that take a single eigenvector or eigenspace as input and are sign or basis invariant. These single subspace architectures will become building blocks for the general architectures. For one subspace, a sign invariant function is merely an even function, and is easily parameterized. Proposition 1. A continuous function h : R n ? R dout is sign invariant if and only if</p><formula xml:id="formula_2">h(v) = ?(v) + ?(?v)<label>(3)</label></formula><p>for some continuous ? : R n ? R dout . A continuous h : R n ? R n is sign invariant and permutation equivariant if and only if (3) holds for a continuous permutation equivariant ? : R n ? R n .</p><p>In practice, we parameterize ? by a neural network. Any architecture choice will ensure sign invariance, while permutation equivariance can be achieved using elementwise MLPs, DeepSets <ref type="bibr" target="#b95">(Zaheer et al., 2017)</ref>, Transformers <ref type="bibr" target="#b80">(Vaswani et al., 2017)</ref>, or most GNNs.</p><p>Next, we address basis invariance for a single d-dimensional subspace, i.e., we aim to parameterize maps h : R n?d ? R n that are (a) invariant to right multiplication by Q ? O(d), and (b) equivariant to permutations along the row axis. For (a), we use the mapping V ? V V from V to the orthogonal projector of its column space, which is O(d) invariant. Mapping V ? V V does not lose information if we treat V as equivalent to V Q for any Q ? O(d). This is justified by the classical first fundamental theorem of O(d) <ref type="bibr">(Kraft &amp; Procesi, 1996)</ref>, which has recently been applied in machine learning by <ref type="bibr" target="#b83">Villar et al. (2021)</ref>.</p><p>Regarding (b), permuting the rows of V permutes rows and columns of V V ? R n?n . Hence, we desire the function ? : R n?n ? R n on V V to be equivariant to simultaneous row and column permutations: ?(P V V P ) = P ?(V V ). To parameterize such a mapping from matrices to vectors, we use an invariant graph network (IGN) <ref type="bibr">(Maron et al., 2018</ref>)-a neural network mapping to and from tensors of arbitrary order R n d 1 ? R n d 2 that has the desired permutation equivariance. We thus parameterize a family with the requisite invariance and equivariance as follows:</p><formula xml:id="formula_3">h(V ) = IGN(V V ).<label>(4)</label></formula><p>Proposition 2 states that this architecture universally approximates O(d) invariant and permutation equivariant functions. The full approximation power requires high order tensors to be used for the IGN; in practice, we restrict the tensor dimensions for efficiency, as discussed in the next section. Proposition 2. Any continuous,</p><formula xml:id="formula_4">O(d) invariant h : R n?d ? R dout is of the form h(V ) = ?(V V ) for a continuous ?. For a compact Z ? R n?d , maps of the form V ? IGN(V V ) universally approximate continuous h : Z ? R n?d ? R n that are O(d) invariant and permutation equivariant.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NEURAL NETWORKS ON MULTIPLE EIGENSPACES</head><p>To develop a method for processing multiple eigenvectors (or eigenspaces), we first prove a general decomposition theorem (see Appendix A for more details). Our result reduces invariance for a large product group G 1 ? . . . ? G k to the much simpler invariances for the smaller constituent groups G i .</p><formula xml:id="formula_5">Theorem 1 (informal). Let a product of groups G = G 1 ? . . . ? G k act on X 1 ? . . . ? X k . Under mild conditions, any continuous G-invariant function f can be written f (x 1 , . . . , x k ) = ?(? 1 (x 1 ), . . . , ? k (x k )), where ? i is G i invariant, and ? i and ? are continuous If X i = X j and G i = G j , then we can take ? i = ? j .</formula><p>For eigenvector data, the ith eigenvector (or eigenspace) is in X i , and its symmetries are described by G i . Thus, we can reduce the multiple-eigenspace case to the single-eigenspace case, and leverage the models we developed in the previous section.</p><p>SignNet. We parameterize our sign invariant network f :</p><formula xml:id="formula_6">R n?k ? R dout on eigenvectors v 1 , . . . , v k as: f (v 1 , . . . , v k ) = ? [?(v i ) + ?(?v i )] k i=1 ,<label>(5)</label></formula><p>where ? and ? are unrestricted neural networks, and [?] i denotes concatenation of vectors. The form ?(v i ) + ?(?v i ) induces sign invariance for each eigenvector. Since we do not yet impose permutation equivariance here, we term this model Unconstrained-SignNet.</p><p>To obtain a sign invariant and permutation equivariant f that outputs vectors in R n?dout , we restrict ? and ? to be permutation equivariant networks from vectors to vectors, such as elementwise MLPs, DeepSets <ref type="bibr" target="#b95">(Zaheer et al., 2017)</ref>, Transformers <ref type="bibr" target="#b80">(Vaswani et al., 2017)</ref>, or most standard GNNs. We name this permutation equivariant version SignNet. If desired, we can additionally use eigenvalues ? i and node features X ? R n?d feat by adding them as arguments to ?:</p><formula xml:id="formula_7">f (v 1 , . . . , v k , ? 1 , . . . , ? k , X) = ? [?(v i , ? i , X) + ?(?v i , ? i , X)] k i=1 .<label>(6)</label></formula><p>BasisNet. For basis invariance, let V i ? R n?di be an orthonormal basis of a d i dimensional eigenspace. Then we parameterize our Unconstrained-BasisNet f by</p><formula xml:id="formula_8">f (V 1 , . . . , V l ) = ? [? di (V i V i )] l i=1 ,<label>(7)</label></formula><p>where each ? di is shared amongst all subspaces of the same dimension d i , and l is the number of eigenspaces (i.e., number of distinct eigenvalues, which can differ from the number of eigenvectors k). As l differs between graphs, we may use zero-padding or a sequence model like a Transformer to parameterize ?. Again, ? di and ? are generally unrestricted neural networks. To obtain permutation equivariance, we make ? permutation equivariant and let ? di = IGN di : R n 2 ? R n be IGNs from matrices to vectors. For efficiency, we will only use matrices and vectors in the IGNs (that is, no tensors in R n p for p &gt; 2), i.e., we use 2- <ref type="bibr">IGN (Maron et al., 2018)</ref>. Our resulting BasisNet is</p><formula xml:id="formula_9">f (V 1 , . . . , V l ) = ? [IGN di (V i V i )] l i=1 .<label>(8)</label></formula><p>Expressive-BasisNet. While we restrict SignNet to only use vectors and BasisNet to only use vectors and matrices, higher order tensors are generally required for universally approximating permutation equivariant or invariant functions <ref type="bibr">(Keriven &amp; Peyr?, 2019;</ref><ref type="bibr">Maron et al., 2019;</ref><ref type="bibr">Maehara &amp; NT, 2019</ref>). Thus, we will consider a theoretically powerful but computationally impractical variant of our model, in which we replace ? and IGN di in BasisNet with IGNs of arbitrary tensor order. We call this variant Expressive-BasisNet. Universal approximation requires O(n n ) sized intermediate tensors <ref type="bibr" target="#b62">(Ravanbakhsh, 2020)</ref>. We study Expressive-BasisNet due to its theoretical interest, and to juxtapose with the computational efficiency and strong expressive power of SignNet and BasisNet.</p><p>In the multiple subspace case, we can prove universality for some instances of our models through our decomposition theorem-see Section A for details. For a summary of properties and more details about our models, see Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL POWER FOR GRAPH REPRESENTATION LEARNING</head><p>Next, we establish that our SignNet and BasisNet can go beyond useful basis invariant and permutation equivariant functions on Laplacian eigenvectors for graph representation learning, including: spectral graph convolutions, spectral invariants, and existing graph positional encodings. Expressive-BasisNet can of course compute these functions, but this section shows that the practical invariant architectures SignNet and BasisNet can compute them as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SIGNNET AND BASISNET STRICTLY GENERALIZE SPECTRAL GRAPH CONVOLUTION</head><p>For node features X ? R n?d feat and an eigendecomposition V ?V , a spectral graph convolution</p><formula xml:id="formula_10">takes the form f (V, ?, X) = n i=1 ? i v i v i X = V Diag(?)V X,</formula><p>for some parameters ? i , that may optionally be continuous functions h(? i ) = ? i of the eigenvalues <ref type="bibr" target="#b13">(Bruna et al., 2014;</ref><ref type="bibr" target="#b26">Defferrard et al., 2016)</ref>. This family includes important functions like heat kernels and generalized PageRanks on graphs <ref type="bibr" target="#b86">(Li et al., 2019)</ref>. A spectral GNN is defined as multiple layers of spectral graph convolutions and node-wise linear maps, e.g. V Diag(? 2 )V ? V Diag(? 1 )V XW 1 W 2 is a two layer spectral GNN. It can be seen (in Appendix H.1) that spectral graph convolutions are permutation equivariant and sign invariant, and if ? i = h(? i ) (i.e. the transformation applied to the diagonal elements is parametric) they are additionally invariant to a change of bases in each eigenspace.</p><p>Our SignNet and BasisNet can be viewed as generalizations of spectral graph convolutions, as our networks universally approximate all spectral graph convolutions of the above form. For instance, SignNet with ?(a 1 , . . . , a k ) = k i=1 a k and ?(v i , ? i , X) = 1 2 ? i v i v i X directly yields the spectral graph convolution. This is captured in Theorem 2, which we prove in Appendix H.1. In fact, we may expect SignNet to learn spectral graph convolutions well, according to the principle of algorithmic alignment <ref type="bibr" target="#b91">(Xu et al., 2020</ref>) (see Appendix H.1); this is supported by numerical experiments in Appendix J.3, in which our networks outperform baselines in learning spectral graph convolutions. Theorem 2. SignNet universally approximates all spectral graph convolutions. BasisNet universally approximates all parametric spectral graph convolutions.</p><p>In fact, SignNet and BasisNet are strictly stronger than spectral graph convolutions; there are functions computable by SignNet and BasisNet that cannot be approximated by spectral graph convolutions or spectral GNNs. This is captured in Proposition 3: our networks can distinguish bipartite graphs from non-bipartite graphs, but spectral GNNs cannot for certain choices of graphs and node signals. Proposition 3. There exist infinitely many pairs of non-isomorphic graphs that SignNet and BasisNet can distinguish, but spectral graph convolutions or spectral GNNs cannot distinguish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BASISNET CAN COMPUTE SPECTRAL INVARIANTS</head><p>Many works measure the expressive power of graph neural networks by comparing their power for testing graph isomorphism <ref type="bibr" target="#b90">(Xu et al., 2019;</ref><ref type="bibr" target="#b66">Sato, 2020)</ref>, or by comparing their ability to compute certain functions on graphs like subgraph counts <ref type="bibr" target="#b73">Tahmasebi et al., 2020)</ref>. These works often compare GNNs to combinatorial invariants on graphs, especially the k-Weisfeiler-Leman (k-WL) tests of graph isomorphism .</p><p>While we may also compare with these combinatorial invariants, as other GNN works that use spectral information have done <ref type="bibr" target="#b5">(Beaini et al., 2021)</ref>, we argue that it is more natural to analyze our networks in terms of spectral invariants, which are computed from the eigenvalues and eigenvectors of graphs. There is a rich literature of spectral invariants from the fields of spectral graph theory and complexity theory <ref type="bibr" target="#b25">(Cvetkovi? et al., 1997)</ref>. For a spectral invariant to be well-defined, it must be invariant to permutations and changes of basis in each eigenspace, a characteristic shared by our networks.</p><p>The simplest spectral invariant is the multiset of eigenvalues, which we give as input to our networks. Another widely studied, powerful spectral invariant is the collection of graph angles, which are defined as the values ? ij = V i V i e j 2 , where V i ? R n?di is an orthonormal basis for the ith adjacency matrix eigenspace, and e j is the jth standard basis vector, which is zero besides a one in the jth component. These are easily computed by our networks (Appendix H.3), so our networks inherit the strength of these invariants. We capture these results in the following theorem, which also lists a few properties that graph angles determine <ref type="bibr" target="#b24">(Cvetkovi?, 1991)</ref>. Theorem 3. BasisNet universally approximates the graph angles ? ij . The eigenvalues and graph angles (and thus BasisNet) can determine the number of length 3, 4, or 5 cycles, whether a graph is connected, and the number of length k closed walks from any vertex to itself.</p><p>Relation to WL and message passing. In contrast to this result, message passing GNNs are not able to express any of these properties (see <ref type="bibr" target="#b0">(Arvind et al., 2020;</ref><ref type="bibr">Garg et al., 2020)</ref> and Appendix H.3). Although spectral invariants are strong, <ref type="bibr" target="#b37">F?rer (2010)</ref> shows that the eigenvalues and graph angles-as well as some strictly stronger spectral invariants-are not stronger than the 3-WL test (or, equivalently, the 2-Folklore-WL test). Using our networks for node positional encodings in message passing GNNs allows us to go beyond graph angles, as message passing can distinguish all trees, but there exist non-isomorphic trees with the same eigenvalues and graph angles <ref type="bibr" target="#b37">(F?rer, 2010;</ref><ref type="bibr" target="#b23">Cvetkovi?, 1988)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SIGNNET AND BASISNET GENERALIZE EXISTING GRAPH POSITIONAL ENCODINGS</head><p>Many graph positional encodings have been proposed, without any clear criteria on which to choose for a particular task. We prove (in Appendix H.2) that our efficient SignNet and BasisNet can approximate many previously used graph positional encodings, as we unify these positional encodings by expressing them as either a spectral graph convolution matrix or the diagonal of a spectral graph convolution matrix. Proposition 4. SignNet and BasisNet can approximate node positional encodings based on heat kernels <ref type="bibr" target="#b34">(Feldman et al., 2022)</ref> and random walks <ref type="bibr" target="#b31">(Dwivedi et al., 2022)</ref>. BasisNet can approximate diffusion and p-step random walk relative positional encodings <ref type="bibr">(Mialon et al., 2021)</ref>, and generalized PageRank and landing probability distance encodings .</p><p>We note that diagonals of spectral convolutions are used as feature descriptors in the shape analysis literature, such as for the heat kernel signature <ref type="bibr" target="#b72">(Sun et al., 2009)</ref> and wave kernel signature <ref type="bibr" target="#b1">(Aubry et al., 2011)</ref>. In the language of recent works in graph machine learning, these are node positional encodings computed from a discrete Laplacian of a triangle mesh. This connection appears to be unnoticed in recent works on graph positional encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We demonstrate the strength of our networks in various experiments. Appendix B shows simple pseudo-code and <ref type="figure">Figure 2</ref> is a diagram detailing the use of SignNet as a node positional encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH REGRESSION</head><p>We study the effectiveness of SignNet for learning positional encodings (PEs) from the eigenvectors of the graph Laplacian on the ZINC dataset of molecule graphs ) (using the subset of 12,000 graphs from <ref type="bibr" target="#b30">Dwivedi et al. (2020)</ref>). We primarily consider three settings: 1) No positional encoding, 2) Laplacian PE (LapPE)-the k eigenvectors of the graph Laplacian with smallest eigenvalues are concatenated with existing node features, 3) SignNet positional featurespassing the eigenvectors through a SignNet and concatenating the output with node features. We parameterize SignNet by taking ? to be a GIN <ref type="bibr" target="#b90">(Xu et al., 2019)</ref> and ? to be an MLP. We sum over ? outputs before the MLP when handling variable numbers of eigenvectors, so the SignNet is of the form MLP l i=1 ?(v i ) + ?(?v i ) (see Appendix K.2 for further details). We consider four different base models that process the graph data and positional encodings: GatedGCN <ref type="bibr">(Bresson &amp;</ref>   Laurent, 2017), a Transformer with sparse attention only over neighbours <ref type="bibr">(Kreuzer et al., 2021)</ref>, PNA , and GIN <ref type="bibr" target="#b90">(Xu et al., 2019)</ref> with edge features (i.e. GINE) <ref type="bibr" target="#b47">(Hu et al., 2020b)</ref>. The total number of parameters of the SignNet and the base model is kept within a 500k budget. Results with GatedGCN show that these alternatives are not more effective than random sign flipping for learning positional encodings. We also consider an ablation of our SignNet architecture where we remove the sign invariance, using simply</p><formula xml:id="formula_11">MLP([?(v i )] k i=1 ).</formula><p>Although the resulting architecture is no longer sign invariant, ? still processes eigenvectors independently, meaning that only two invariances (?1) need be learned, significantly fewer than the 2 k total sign flip configurations. Accordingly, this non-sign invariant learned positional encoding achieves a test MAE of 0.148, improving over the Laplacian PE (0.198) but falling short of the fully sign invariant SignNet (0.121). In all cases, using all available eigenvectors in SignNet significantly improves performance over using a fixed number of eigenvectors; this is notable as generally people truncate to a fixed number of eigenvectors in other works. In Appendix J.1, we also show that SignNet improves performance when no edge features are included in the data.  Substructure counts (e.g. of cycles) and global graph properties (e.g. connectedness, diameter, radius) are important graph features that are known to be informative for problems in biology, chemistry, and social networks <ref type="bibr" target="#b44">Holland &amp; Leinhardt, 1977)</ref>. Following the setting of <ref type="bibr" target="#b99">Zhao et al. (2022)</ref>, we show that SignNet with Laplacian positional encodings boosts the ability of simple GNNs to count substructures and regress graph properties. We take a 4-layer GIN as the base model for all settings, and for SignNet we use GIN as ? and a Transformer as ? to handle variable numbers of eigenvectors (see Appendix K.4 for details). As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, Laplacian PEs with sign-flip data augmentation improve performance for counting substructures but not for regressing graph properties, while Laplacian PEs processed by SignNet significantly boost performance on all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NEURAL FIELDS ON MANIFOLDS</head><p>Discrete approximations to the Laplace-Beltrami operator on manifolds have proven useful for processing data on surfaces, such as triangle meshes <ref type="bibr">(L?vy, 2006)</ref>. <ref type="bibr">Recently, Koestler et al. (2022)</ref> propose intrinsic neural fields, which use eigenfunctions of the Laplace-Beltrami operator as positional encodings for learning neural fields on manifolds. For generalized eigenfunctions v 1 , . . . , v k , at a point p on the surface, they parameterize functions f (p) = MLP(v 1 (p), . . . , v k (p)). As these eigenfunctions have sign ambiguity, we use our SignNet to parameterize</p><formula xml:id="formula_12">f (p) = MLP( ?( [?(v i (p))+ ?(?v i (p))] i=1,.</formula><p>..,k ) ), with ? and ? being MLPs.  <ref type="bibr">(2022)</ref>. The total number of parameters in our SignNet-based model is kept below that of the original model. We see that the SignNet architecture improves over the original Intrinsic NF model and over other baselines -especially in the LPIPS (Learned Perceptual Image Patch Similarity) metric, which has been shown to be a typically better perceptual metric than PSNR or DSSIM <ref type="bibr" target="#b97">(Zhang et al., 2018a</ref>).</p><p>While we have not yet tested this, we believe that SignNet would allow even better improvements when learning over eigenfunctions of different models, as it could improve transfer and generalization. See Appendix D.1 for visualizations and Appendix K.5 for more details. To better understand SignNet, we plot the first principal component of ?(v) + ?(?v) for two eigenvectors on the cat model in <ref type="figure" target="#fig_2">Figure 4</ref>. We see that SignNet encodes bilateral symmetry and structural information on the cat model. See Appendix D for plots of more eigenvectors and further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">VISUALIZATION OF LEARNED POSITIONAL ENCODINGS</head><formula xml:id="formula_13">Eigvec 11 ?(v11) + ?(?v11) Eigvec 14 ?(v14) + ?(?v14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we review selected related work. A more thorough review is deferred to Appendix E.</p><p>Laplacian eigenvectors in GNNs. Various recently proposed methods in graph deep learning have directly used Laplacian eigenvectors as node positional encodings that are input to a message passing <ref type="bibr">GNN (Dwivedi et al., 2020;</ref>, or some variant of a Transformer that is adapted to graphs <ref type="bibr" target="#b29">(Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr">Kreuzer et al., 2021;</ref><ref type="bibr">Mialon et al., 2021;</ref><ref type="bibr" target="#b31">Dwivedi et al., 2022;</ref><ref type="bibr">Kim et al., 2022)</ref>. None of these methods address basis invariance, and they only partially address sign invariance for node positional encodings by randomly flipping eigenvector signs during training.  , and unsupervised node embedding methods . In particular,  use Laplacian eigenvectors for relative positional encodings in an invariant way, but they focus on robustness, so they have stricter invariances that significantly reduce expressivity (see Appendix E.2 for more details). These previously used positional encodings are mostly ad-hoc, less general since they can be provably expressed by SignNet and BasisNet (see Section 3.3), and/or are expensive to compute (e.g., all pairs shortest paths).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND DISCUSSION</head><p>SignNet and BasisNet are novel architectures for processing eigenvectors that are invariant to sign flips and choices of eigenspace bases, respectively. Both architectures are provably universal under certain conditions. When used with Laplacian eigenvectors as inputs they provably go beyond spectral graph convolutions, spectral invariants, and a number of other graph positional encodings. These theoretical results are supported by experiments showing that SignNet and BasisNet are highly expressive in practice, and learn effective graph positional encodings that improve the performance of message passing graph neural networks. Initial explorations show that SignNet and BasisNet can be useful beyond graph representation learning, as eigenvectors are ubiquitous in machine learning.</p><p>While we conduct experiments on graph machine learning tasks and a particular task on triangle meshes, SignNet and BasisNet should also be applicable to processing eigenvectors in other settings, such as recommender systems and tasks in shape analysis. Moreover, while we primarily consider eigenspaces in this work, sign invariance and basis invariance applies to any model that processes subspaces of a vector space; future work may explore our models on general subspaces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A UNIVERSALITY FOR MULTIPLE SPACES</head><p>While the networks introduced in the Section 2.2 possess the desired invariances, it is not immediately obvious whether they are powerful enough to express all functions with these invariances. Under certain conditions, the universality of our architectures follows as a corollary of the following general decomposition result, which may enable construction of universal architectures for other invariances as well.</p><p>Theorem 4 (Decomposition Theorem). Let X 1 , . . . , X k be topological spaces, and let G i be a group acting on X i for each i. We assume mild topological conditions on X i and G i hold. For any continuous f :</p><formula xml:id="formula_14">X = X 1 ? . . . ? X k ? R dout that is invariant to the action of G = G 1 ? . . . ? G k , there exists continuous ? i and a continuous ? : Z ? R a ? R dout such that f (v 1 , . . . , v k ) = ?(? 1 (v 1 ), . . . , ? k (v k )).<label>(9)</label></formula><p>Furthermore: (1) each ? i can be taken to be invariant to</p><formula xml:id="formula_15">G i , (2) the domain Z of ? is compact if each X i is compact, (3) if X i = X j and G i = G j , then ? i can be taken to be equal to ? j .</formula><p>This result says that when a product of groups G acts on a product of spaces X , for invariance to the product group G it suffices to individually process each smaller group G i on X i and then aggregate the results. Along with the proof of Theorem 4, the mild topological assumptions are explained in Appendix G.1. The assumptions hold for sign invariance and basis invariance, when not enforcing permutation equivariance. By applying this theorem, we can prove universality of some instances of our networks: This result shows that Unconstrained-SignNet, Unconstrained-BasisNet, and Expressive-BasisNet take the correct functional form for their respective invariances (proofs in Appendix G.2). Note that Expressive-BasisNet approximates all sign invariant functions as a special case, by treating all inputs as one dimensional eigenspaces. Further, note that we require Expressive-BasisNet's high order tensors to achieve universality when enforcing permutation equivariance. Universality under permutation equivariance is generally difficult to achieve when dealing with matrices with permutation symmetries <ref type="bibr">(Maron et al., 2019;</ref><ref type="bibr">Keriven &amp; Peyr?, 2019</ref>), but it may be possible that more efficient architectures can achieve it in our setting.</p><p>Accompanying the decomposition result, we show a corresponding universal approximation result (proof in Appendix G.3). Similarly to Theorem 4, the problem of approximating G = G 1 ? . . . ? G k invariant functions is reduced to approximating several G i -invariant functions. In <ref type="figure">Figure 2</ref>, we show a diagram that describes how SignNet is used as a node positional encoding for a graph machine learning task. In <ref type="table" target="#tab_7">Table 4</ref>, we compare and contrast properties of the neural architectures that we introduce. In <ref type="figure">Figure 5</ref>, we give pseudo-code of SignNet for learning node positional encodings with a GNN prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE DETAILS ON SIGNNET AND BASISNET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PyTorch-like pseudo-code for SignNet</head><p>class SignNetGNN(nn.Module):</p><formula xml:id="formula_16">def __init__(self, d, k, D1, D2, out_dim): self.phi = GIN(1, D1) # in dim=1, out dim=D1 self.rho = MLP(k * D1, D2) self.base_model = GNN(d+D2, out_dim)</formula><p>def forward(self, g, x, eigvecs): # g contains graph information # x shape: n x d # eigvecs shape: n x k n, k = eigvecs.shape eigvecs = eigvecs.reshape(n, k, 1) pe = self.phi(g, eigvecs) + self.phi(g, ?eigvecs) pe = pe.reshape(n, ?1) # n x k x D1 ?&gt; n x k * D1 pe = self.rho(pe) return self.base_model(g, x, pe) <ref type="figure">Figure 5</ref>: PyTorch-like pseudo-code for using SignNet with a GNN prediction model, where ? = GIN and ? = MLP as in the ZINC molecular graph regression experiments. Reshaping eigenvectors from n ? k to n ? k ? 1 allows ? to process each eigenvector (and its negation) independently in PyTorch-like deep learning libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 GENERALIZATION BEYOND SYMMETRIC MATRICES</head><p>In the main paper, we assume that the eigenspaces come from a symmetric matrix. This holds for many cases of practical interest, as e.g. the Laplacian matrix of an undirected graph is symmetric. However, we may also want to process directed graphs, or other data that have associated nonsymmetric matrices. Our SignNet and BasisNet generalize in a straightforward way to handle nonsymmetric diagonalizable matrices, as we detail here. Let A ? R n?n be a matrix with a diagonalization A = V ?V ?1 , where ? = Diag(? 1 , . . . , ? n ) contains the eigenvalues ? i , and the columns of V = [v 1 . . . v n ] are eigenvectors. Suppose we want to learn a function on the eigenvectors v 1 , . . . , v k . Unlike in the symmetric matrix case, the eigenvectors are not necessarily orthonormal, and both the eigenvalues and eigenvectors can be complex.</p><p>Real eigenvectors. First, we assume the eigenvectors v i are all real vectors in R n . We can take the eigenvectors to be real if A is symmetric, or if A has real eigenvalues (see Horn &amp; Johnson (2012) Theorem 1.3.29). Also, suppose that we choose the real numbers R as our base field for the vector space in which eigenvectors lie. Note that for any scaling factor c ? R \ {0} and eigenvector v, we have that cv is an eigenvector of the same eigenvalue. If the eigenvalues are distinct, then the eigenvectors of the form cv are the only other eigenvectors in the same eigenspace as v. Thus, we want a function to be invariant to scalings:</p><formula xml:id="formula_17">f (v 1 , . . . , v k ) = f (c 1 v 1 , . . . , c k v k ) c i ? R \ {0}.<label>(10)</label></formula><p>This can be handled by SignNet, by giving unit normalized vector inputs:</p><formula xml:id="formula_18">f (v 1 , . . . , v k ) = ? [?(v i / v i ) + ?(?v i / v i )] i=1,...,k .<label>(11)</label></formula><p>Now, say have bases of eigenspaces V 1 , . . . , V l with dimensions d 1 , . . . , d l . For a basis V i , we have that any other basis of the same space can be obtained as V i W for some W ? GL R (d i ), the set of real invertible matrices in R di?di . Indeed, the orthonormal projector for the space spanned by the columns of V i is given by</p><formula xml:id="formula_19">V i (V i V i ) ?1 V i . Thus, if Z ? R n?di is another basis for the column space of V i , we have that V i (V i V i ) ?1 V i = Z(Z Z) ?1 Z , so V i (V i V i ) ?1 V i Z = Z(Z Z) ?1 Z Z = Z,<label>(12)</label></formula><formula xml:id="formula_20">so let W = (V i V i ) ?1 V i Z ? R di?di . Note that W is invertible, because it has inverse (Z Z) ?1 Z V i , so indeed V i W = Z for W ? GL R (d i ). Thus, basis invariance in this case is of the form f (V 1 . . . , V l ) = f (V 1 W 1 , . . . , V l W l ) W i ? GL R (d i ).<label>(13)</label></formula><p>Note that the distinct eigenvalue invariance is a special case of this invariance, as G R (1) = R \ {0}.</p><p>We can again achieve this basis invariance by using a BasisNet, where the inputs to the ? di are orthogonal projectors of the corresponding eigenspace:</p><formula xml:id="formula_21">f (V 1 , . . . , V l ) = ? ? di (V i (V i V i ) ?1 V i ) i=1,...,l .<label>(14)</label></formula><p>Recall that if V i is an orthonormal basis, then the orthogonal projector is just V i V i , so this is a direct generalization of BasisNet in the symmetric case.</p><p>Complex eigenvectors. More generally, suppose V ? C n?n are complex eigenvectors, and we take the base field of the vector space to be C. The above arguments generalize to the complex case; in the case of distinct eigenvalues, we want</p><formula xml:id="formula_22">f (v 1 , . . . , v k ) = f (c 1 v 1 , . . . , c k v k ) c i ? C \ {0}.<label>(15)</label></formula><p>However, this symmetry can not be as easily reduced to a unit normalization and a discrete sign invariance, as it can be in the real case. Nonetheless, the basis invariant architecture directly generalizes, so we can handle the case of distinct eigenvalues by a more general basis invariant architecture as well. The basis invariance is</p><formula xml:id="formula_23">f (V 1 , . . . , V l ) = f (V 1 W 1 , . . . , V l W l ) W i ? GL C (d i ).<label>(16)</label></formula><p>The orthogonal projector of the image of</p><formula xml:id="formula_24">V i is V i (V * i V i ) ?1 V * i ,</formula><p>where there are now conjugate transposes replacing the transposes. Thus, BasisNet takes the form:</p><formula xml:id="formula_25">f (V 1 , . . . , V l ) = ? ? di (V i (V * i V i ) ?1 V * i ) i=1,...,l .<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 BROADER IMPACTS</head><p>We believe that our models and future sign invariant or basis invariant networks could be useful in a wide variety of applications. As eigenvectors arise in many domains, it is difficult to predict the uses of these models. We test on several molecular property prediction tasks, which have the potential for much positive impact, such as in drug discovery <ref type="bibr" target="#b71">(Stokes et al., 2020)</ref>. However, recent work has found that the same models that we use for finding beneficial drugs can also be used to design biochemical weapons <ref type="bibr" target="#b79">(Urbina et al., 2022)</ref>. Another major application of graph machine learning is in social network analysis, where positive (e.g. malicious node detection <ref type="bibr" target="#b57">(Pandit et al., 2007)</ref>) and negative (e.g. deanonymization <ref type="bibr" target="#b54">(Narayanan &amp; Shmatikov, 2009</ref>)) uses of machine learning are possible. Even if there is no negative intent, bias in learned models can differentially impact particular subgroups of people. Thus, academia, industry, and policy makers must be aware of such potential negative uses, and work towards reducing the likelihood of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MORE ON EIGENVALUE MULTIPLICITIES</head><p>In this section, we study the properties of eigenvalues and eigenvectors computed by numerical algorithms on real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 SIGN AND BASIS AMBIGUITIES IN NUMERICAL EIGENSOLVERS</head><p>When processing real-world data, we use eigenvectors that are computed by numerical algorithms. These algorithms return specific eigenvectors for each eigenspace, so there is some choice of sign or basis of each eigenspace. The general symmetric matrix eigensolvers numpy.linalg.eigh and scipy.linalg.eigh both call LAPACK routines. They both proceed as follows: for a symmetric matrix A, they first decompose it as A = QT Q for orthogonal Q and tridiagonal T , then they compute the eigendecomposition of T = W ?W , so the eigendecomposition of A is A = (QW )?(W Q ). There are multiple ambiguities here: for diagonal sign matrices S = Diag(s 1 , . . . , s n ) and S = Diag(s 1 , . . . , s n ), where s i , s i ? {?1, 1}, we have that A = QS(ST S)SQ is also a valid tridiagonalization, as QS is still orthogonal, SS = I, and ST S is still tridiagonal. Also, T = (W S )?(S W ) is a valid eigendecomposition of T , as W S is still orthogonal.</p><p>In practice, we find that the general symmetric matrix eigensolvers numpy.linalg.eigh and scipy.linalg.eigh differ between frameworks but are consistent with the same framework. More specifically, for a symmetric matrix A, we find that the eigenvectors computed with the default settings in numpy tend to differ by a choice of sign or basis from those that are computed with the default settings in scipy. On the other hand, the called LAPACK routines are deterministic, so the eigenvectors returned by numpy are the same in each call, and the eigenvectors returned by scipy are likewise the same in each call.</p><p>Eigensolvers for sparse symmetric matrices like scipy.linalg.eigsh are required for large scale problems. This function calls ARPACK, which uses an iterative method that starts with a randomly sampled initial vector. Due to this stochasticity, the sign and basis of eigenvectors returned differs between each call. <ref type="bibr" target="#b11">Bro et al. (2008)</ref> develop a data-dependent method to choose signs for each singular vector of a singular value decomposition. Still, in the worst case the signs chosen will be arbitrary, and they do not handle basis ambiguities in higher dimensional eigenspaces. Other works have made choices of sign, such as by picking the sign so that the eigenvector's entries are in the largest lexicographic order <ref type="bibr" target="#b74">(Tam &amp; Dunson, 2022)</ref>. This choice of sign may work poorly for learning on graphs, as it is sensitive to permutations on nodes. For some graph regression experiments in Section 4.1, we try a choice of sign that is permutation invariant, but we find it to work poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 HIGHER DIMENSIONAL EIGENSPACES IN REAL GRAPHS</head><p>Here, we investigate the normalized Laplacian eigenspace statistics of real-world graph data. For any graph that has distinct Laplacian eigenvalues, only sign invariance is required in processing eigenvectors. However, we find that graph data tends to have higher multiplicity eigenvalues, so basis invariance would be required for learning symmetry-respecting functions on eigenvectors.</p><p>Indeed, we show statistics for multi-graph datasets in <ref type="table" target="#tab_8">Table 5</ref> and for single-graph datasets with more nodes per graph in <ref type="table" target="#tab_9">Table 6</ref>. For multi-graph datasets, we consider : For single-graph datasets, we consider:</p><formula xml:id="formula_26">?</formula><p>? The 32 ? 32 image grid as in Section J.3</p><p>? Citation networks: Cora, Citeseer <ref type="bibr" target="#b68">(Sen et al., 2008)</ref> ? Co-purchasing graphs with Amazon Photo <ref type="bibr">(McAuley et al., 2015;</ref><ref type="bibr" target="#b69">Shchur et al., 2018)</ref>.</p><p>We see that these datasets all contain higher multiplicity eigenspaces, so sign invariance is insufficient for fully respecting symmetries. The majority of graphs in each multi-graph dataset besides COIL-DEL contain higher multiplicity eigenspaces. Also, the dimension of these eigenspaces can be quite large compared to the size of the graphs in the dataset. The single-graph datasets have a large proportion of their eigenvectors belonging to higher dimensional eigenspaces. Thus, basis invariance may play a large role in processing spectral information from these graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 RELATIONSHIP TO GRAPH AUTOMORPHISMS</head><p>Higher multiplicity eigenspaces are related to automorphism symmetries in graphs. For an adjacency matrix A, the permutation matrix P is an automorphism of the graph associated to A if P AP = A.</p><p>If P is an automorphism, then for any eigenvector v of A with eigenvalue ?, we have  so P v is an eigenvector of A with the same eigenvalue ?. If P v and v are linearly independent, then ? has a higher dimensional eigenspace. Thus, under certain additional conditions, automorphism symmetries of graphs lead to repeated eigenvalues <ref type="bibr" target="#b65">(Sachs &amp; Stiebitz, 1983;</ref><ref type="bibr" target="#b76">Teranishi, 2009</ref>).</p><formula xml:id="formula_27">AP v = P AP P v = P Av = P ?v = ?P v,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 MULTIPLICITIES IN RANDOM GRAPHS</head><p>It is known that almost all random graphs under the Erd?s-Renyi model have no repeated eigenvalues in the infinite number of nodes limit <ref type="bibr" target="#b75">(Tao &amp; Vu, 2017)</ref>. Likewise, almost all random graphs under the Erd?s-Renyi model are asymmetric in the sense of having no nontrivial automorphism symmetries <ref type="bibr" target="#b33">(Erdos &amp; R?nyi, 1963)</ref>. These results contrast sharply with the high eigenvalue multiplicities that we see in real-world data in Section C.2. Likewise, many types of real-world graph data have been found to possess nontrivial automorphism symmetries <ref type="bibr" target="#b4">(Ball &amp; Geyer-Schulz, 2018</ref>). This demonstrates a potential downside of using random graph models to study real-world data: the eigenspace dimensions and automorphism symmetries of random graphs may not agree with those of real-world data. In <ref type="figure" target="#fig_6">Figure 6</ref>, we plot the eigenvectors of the cotangent Laplacian on a cat model, as well as the first principal component of the corresponding learned ?(v) + ?(?v) from our SignNet model that was trained on the texture reconstruction task. Interestingly, this portion of our SignNet encodes bilateral symmetry; for instance, while some eigenvectors differ between left feet and right feet, this portion of our SignNet gives similar values for the left and right feet. This is useful for the texture reconstruction task, as the texture regression target has bilateral symmetry. We also show principal components of outputs for the full SignNet model in <ref type="figure" target="#fig_7">Figure 7</ref>. This is not as interpretable, as the outputs are high frequency and appear to be close to the texture that is the regression target. If instead we trained the network on a task involving eigenvectors of multiple models, then we may expect the SignNet to learn more structurally interpretable mappings (as in the case of the molecule tasks).</p><formula xml:id="formula_28">D VISUALIZATION OF SIGNNET OUTPUT D.1 CAT MODEL VISUALIZATION Eigenvector 1 ?(v1) + ?(?v1) Eigenvector 9 ?(v9) + ?(?v9) Eigenvector 11 ?(v11) + ?(?v11) Eigenvector 14 ?(v14) + ?(?v14) Eigenvector 1023 ?(v1023) + ?(?v1023)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 MOLECULE VISUALIZATION</head><p>To better understand SignNet, in <ref type="figure" target="#fig_10">Figure 9</ref> we visualize the learned positional encodings of a SignNet with ? = GIN, ? = MLP (with a summation to handle variable eigenvector numbers) trained on ZINC as in Section 4.1. SignNet learns interesting structural information such as min-cuts (PC 3) and appendage atoms (PC 2) that qualitatively differ from any single eigenvector of the graph. Various graph positional encodings have been proposed, which have been motivated for increasing expressive power or practical performance of graph neural networks, and for generalizing Transformers to graphs. Positional encodings are related to so-called position-aware network embeddings <ref type="bibr" target="#b15">(Chami et al., 2020)</ref>, which capture distances between nodes in graphs. These include network embedding methods like Deepwalk <ref type="bibr" target="#b60">(Perozzi et al., 2014)</ref> and node2vec <ref type="bibr" target="#b41">(Grover &amp; Leskovec, 2016)</ref>, which have been recently integrated into GNNs that respect their invariances by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 EIGENVECTOR SYMMETRIES IN GRAPH REPRESENTATION LEARNING</head><p>Many works that attempt to respect the invariances of eigenvectors solely focus on sign invariance (by using data augmentation) <ref type="bibr" target="#b30">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b29">Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr" target="#b31">Dwivedi et al., 2022;</ref><ref type="bibr">Kreuzer et al., 2021)</ref>. This may be reasonable for continuous data, where eigenvalues of associated matrices may be usually distinct and separated (e.g. <ref type="bibr" target="#b61">Puny et al. (2022)</ref> finds that this empirically holds for covariance matrices of n-body problems). However, discrete graph Laplacians are known to have higher multiplicity eigenvalues in many cases, and in Appendix C.2 we find this to be true in various types of real-world graph data. Graphs without higher multiplicity eigenspaces are easier to deal with; in fact, graph isomorphism can be tested in polynomial time on graphs of bounded </p><formula xml:id="formula_29">v i ) + ?(?v i )] i=1,...,n ) of SignNet.</formula><p>multiplicity for adjacency matrix eigenvalues <ref type="bibr" target="#b2">(Babai et al., 1982;</ref><ref type="bibr">Leighton &amp; l. Miller, 1979)</ref>, with a time complexity that is lower for graphs with lower maximum multiplicities.</p><p>A recent work of  proposes full orthogonal group invariance for functions that process positional encodings. In particular, for positional encodings Z ? R n?k , they parameterize functions f (Z) such that f (Z) = f (ZQ) for all Q ? O(k). This indeed makes sense for network embeddings like node2vec <ref type="bibr" target="#b41">(Grover &amp; Leskovec, 2016)</ref>, as their objective functions are based on inner products and are thus orthogonally invariant. While they prove stability results when enforcing full orthogonal invariance for eigenvectors, this is a very strict constraint compared to our basis invariance. For instance, when k = n and all eigenvectors are used in V , the condition f (V ) = f (V Q) implies that f is a constant function on orthogonal matrices, since any orthogonal matrix W can be obtained as W = V Q for Q = V W ? O(n). In other words, for bases of eigenspaces V 1 , . . . , V l and V = [V 1 . . . V l ],  enforces V Q ? = V , while we enforce V Diag(Q 1 , . . . , Q l ) ? = V . While the columns of V Diag(Q 1 , . . . , Q l ) are still eigenvectors, the columns of V Q generally are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 GRAPH SPECTRA AND LEARNING ON GRAPHS</head><p>More generally, graph spectra are widely used in analyzing graphs, and spectral graph theory <ref type="bibr" target="#b20">(Chung, 1997)</ref> studies the connection between graph properties and graph spectra. Different graph kernels have been defined based on graph spectra, which use robust and discriminative notions of generalized spectral distance <ref type="bibr" target="#b82">(Verma &amp; Zhang, 2017)</ref>, the spectral density of states <ref type="bibr" target="#b48">(Huang et al., 2021)</ref>, random walk return probabilities <ref type="bibr" target="#b98">(Zhang et al., 2018b)</ref>, or the trace of the heat kernel <ref type="bibr" target="#b78">(Tsitsulin et al., 2018)</ref>. Graph signal processing relies on spectral operations to define Fourier transforms, frequencies, convolutions, and other useful concepts for processing data on graphs <ref type="bibr" target="#b55">(Ortega et al., 2018)</ref>. The closely related spectral graph neural networks <ref type="bibr" target="#b89">(Wu et al., 2020;</ref><ref type="bibr" target="#b3">Balcilar et al., 2020)</ref> parameterize neural architectures that are based on similar spectral operations. F DEFINITIONS, NOTATION, AND BACKGROUND F.1 BASIC TOPOLOGY AND ALGEBRA DEFINITIONS We will use some basic topology and algebra for our theoretical results. A topological space (X , ? ) is a set X along with a family of subsets ? ? 2 X satisfying certain properties, which gives useful notions like continuity and compactness. From now on, we will omit mention of ? , and refer to a topological space as the set X itself. For topological spaces X and Y, we write X ? = Y and say that X is homeomorphic to Y if there exists a continuous bijection with continuous inverse from X to Y. We will say X = Y if the underlying sets and topologies are equal as sets (we will often use this notion of equality for simplicity, even though it can generally be substituted with homeomorphism). For a function f : X ? Y between topological spaces X and Y, the image imf is the set of values that f takes, imf = {f (x) : x ? X }. This is also denoted f (X ). A function f : X ? Y is called a topological embedding if it is a homeomorphism from X to its image.</p><p>A group G is a set along with a multiplication operation G ? G ? G, such that multiplication is associative, there is a multiplicative identity e ? G, and each g ? G has a multiplicative inverse g ?1 .</p><p>A topological group is a group that is also a topological space such that the multiplication and inverse operations are continuous.</p><p>A group G may act on a set X by a function ? : G ? X ? X . We usually denote g ? x as gx. A topological group is said to act continuously on a topological space X if ? is continuous. For any group G and topological space X , we define the coset Gx = {gx : g ? G}, which can be viewed as an equivalance class of elements that can be transformed from one to another by a group element. The quotient space X /G = {Gx : x ? X } is the set of all such equivalence classes, with a topology induced by that of X . The quotient map ? : X ? X /G is a surjective continuous map that sends x to its coset, ?(x) = Gx.</p><p>For x ? R d , x 2 denotes the standard Euclidean norm. By the ? norm of functions f :</p><formula xml:id="formula_30">Z ? R d from a compact Z to a Euclidean space R d , we mean f ? = sup z?Z f (z) 2 . F.2 BACKGROUND ON EIGENSPACE INVARIANCES Let V = [v 1 . . . v d ] and W = [w 1 . . . w d ] ? R n?d</formula><p>be two orthonormal bases for the same d dimensional subspace of R n . Since V and W span the same space, their orthogonal projectors are the same, so V V = W W . Also, since V and W have orthonormal columns, we have</p><formula xml:id="formula_31">V V = W W = I ? R d?d . Define Q = V W . Then Q is orthogonal because Q Q = W V V W = W W W W = I<label>(19)</label></formula><p>Moreover, we have that  <ref type="formula" target="#formula_32">(2020)</ref> Chapter 5 for more information on this. We will use this relationship in our proofs of universal representation.</p><formula xml:id="formula_32">V Q = V V W = W W W = W<label>(</label></formula><p>When we consider permutation invariance or equivariance, the permutation acts on dimensions of size n. Then a tensor X ? R n k ?d is called an order k tensor with respect to this permutation symmetry, where order 0 are called scalars, order 1 tensors are called vectors, and order 2 tensors are called matrices. Note that this does not depend on d; in this work, we only ever consider vectors and scalars with respect to the O(d) action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PROOFS OF UNIVERSALITY</head><p>We begin by proving the two propositions for the single subspace case from Section 2.1. Proposition 1. A continuous function h : R n ? R dout is sign invariant if and only if</p><formula xml:id="formula_33">h(v) = ?(v) + ?(?v)<label>(3)</label></formula><p>for some continuous ? : R n ? R dout . A continuous h : R n ? R n is sign invariant and permutation equivariant if and only if (3) holds for a continuous permutation equivariant ? : R n ? R n . </p><formula xml:id="formula_34">Proof. If h(v) = ?(v) + ?(?v), then h is obviously sign invariant. On the other hand, if h is sign invariant, then letting ?(v) = h(v)/2 gives that h(v) = ?(v) + ?(?v), and ? is of course continuous. If h(v) = ?(v) + ?(?v) for a permutation equivariant ?, then h(?P v) = ?(?P v) + ?(P v) = P ?(?v) + P ?(v) = P (?(v) + ?(?v)) = P h(v),</formula><formula xml:id="formula_35">Z ? R n?n ? R n such that h(V ) = ?(V V ).</formula><p>Since h is permutation equivariant, for any permutation matrix P we have that</p><formula xml:id="formula_36">h(P V ) = P ? h(V ) (21) ?(P V V P ) = P ? ?(V V ),<label>(22)</label></formula><p>so ? is a continuous permutation equivariant function from matrices to vectors. Then note that Keriven &amp; Peyr? <ref type="formula" target="#formula_0">(2019)</ref> show that invariant graph networks (of generally high tensor order in hidden layers) universally approximate continuous permutation equivariant functions from matrices to vectors on compact sets of matrices. Thus, an IGN can -approximate ?, and hence V ? IGN(V V ) can -approximate h. Here, we give the formal statement of Theorem 4, which provides the necessary topological assumptions for the theorem to hold. In particular, we only require the G i be a topological group that acts continuously on X i for each i, and that there exists a topological embedding of each quotient space into some Euclidean space. That the group action is continuous is a very mild assumption, and it holds for any finite or compact matrix group, which all of the invariances we consider in this paper can be represented as.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 PROOF OF DECOMPOSITION THEOREM</head><formula xml:id="formula_37">X 1 ? . . . ? X k (X 1 /G 1 ) ? . . . ? (X k /G k ) R dout Z = im(?) ? R a ? = ? 1 ? . . . ? k f =f ? ? ? = ? ? ? ? = ? 1 ? . . . ? ? kf ? ?1 ? =f ? ? ?1</formula><p>A topological embedding of the quotient space into a Euclidean space is desired, as we know how to parameterize neural networks with Euclidean outputs and inputs, whereas dealing with a quotient space is generally difficult. Many different conditions can guarantee existence of such an embedding. For instance, if the quotient space is a smooth manifold, then the Whitney Embedding Theorem (Lemma 5) guarantees such an embedding. Also, if the base space X i is a Euclidean space and G i is a finite or compact matrix Lie group, then a map built from G-invariant polynomials gives such an embedding <ref type="bibr" target="#b40">(Gonz?lez &amp; de Salas (2003)</ref> Lemma 11.13).</p><p>Figure 10 provides a commutative diagram representing the constructions in our proof.</p><p>Theorem 4 (Decomposition Theorem). Let X 1 , . . . , X k be topological spaces, and let G i be a topological group acting continuously on X i for each i. Assume that there is a topological embedding ? i : X i /G i ? R ai of each quotient space into a Euclidean space R ai for some dimension a i . Then, for any continuous function f : X = X 1 ? . . . ? X k ? R dout that is invariant to the action of G = G 1 ? . . . ? G k , there exists continuous functions ? i : X i ? R ai and a continuous function</p><formula xml:id="formula_38">? : Z ? R a ? R dout , where a = i a i such that f (v 1 , . . . , v k ) = ?(? 1 (v 1 ), . . . , ? k (v k )).<label>(23)</label></formula><p>Furthermore: (1) each ? i can be taken to be invariant to G i , (2) the domain Z is compact if each X i is compact, (3) if X i = X j and G i = G j , then ? i can be taken to be equal to ? j .</p><p>Proof. Let ? i : X i ? X i /G i denote the quotient map for X i /G i . Since each G i acts continuously, Lemma 3 gives that the quotient of the product space is the product of the quotient spaces, i.e. that</p><formula xml:id="formula_39">(X 1 ? . . . ? X k )/(G 1 ? . . . G k ) ? = (X 1 /G 1 ) ? . . . ? (X k /G k ),<label>(24)</label></formula><p>and the corresponding quotient map ? : X /G is given by</p><formula xml:id="formula_40">? = ? 1 ? . . . ? ? k , ?(x 1 , . . . , x k ) = (? 1 (x 1 ), . . . , ? k (x k )).<label>(25)</label></formula><p>By passing to the quotient (Lemma 1), there exists a continuousf :</p><formula xml:id="formula_41">X /G ? R dout on the quotient space such that f =f ? ?. By Lemma 4, each X i /G i is compact if X i is compact. Defining the image Z i = ? i (X i /G i ) ? R ai , we thus know that Z i is compact if X i is compact.</formula><p>Moreover, as ? i is a topological embedding, it has a continuous inverse ? ?1 i on its image Z i . Further, we have a topological embedding ? :</p><formula xml:id="formula_42">X /G ? Z = Z 1 ? . . . ? Z k given by ? = ? 1 ? . . . ? ? k , with continuous inverse ? ?1 = ? ?1 1 ? . . . ? ? ?1 k . Note that f =f ? ? = (f ? ? ?1 ) ? (? ? ?).<label>(26)</label></formula><p>So we define</p><formula xml:id="formula_43">? =f ? ? ?1 ? : Z ? R dout (27) ? i = ? i ? ? i ? i : X i ? Z i (28) ? = ? ? ? = ? 1 ? . . . ? ? k ? : X ? Z (29) Thus, f = ? ? ? = ? ? (? 1 ? . . . ? ? k )</formula><p>, so equation <ref type="formula" target="#formula_14">(9)</ref> holds. Moreover, the ? and ? i are continuous, as they are compositions of continuous functions. Furthermore, (1) holds as each ? i is invariant</p><formula xml:id="formula_44">to G i because each ? i is invariant to G i . Since each Z i is compact if X i is compact, the product Z = Z 1 ? . . . ? Z k is compact if each X i is compact, thus proving (2).</formula><p>To show the last statement (3), note simply that if X i = X j and G i = G j , then the quotient maps are equal, i.e. ? i = ? j . Moreover, we can choose the embeddings to be equal, so say ? i = ? j . Then, ? i = ? i ? ? i = ? j ? ? j = ? j , so we are done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 UNIVERSALITY OF SIGNNET AND BASISNET</head><p>Here, we prove Corollary 1 on the universal representation and approximation capabilities of our Unconstrained-SignNets, Unconstrained-BasisNets, and Expressive-BasisNets. We proceed in several steps, first proving universal representation of continuous functions when we do not require permutation equivariance, then proving universal approximation when we do require permutation equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.1 SIGN INVARIANT UNIVERSAL REPRESENTATION</head><p>Recall that S n?1 denotes the unit sphere in R n . As we normalize eigenvectors to unit norm, the domain of our functions on k eigenvectors are on the compact space (S n?1 ) k .</p><p>Corollary 2 (Universal Representation for SignNet). A continuous function f : <ref type="figure" target="#fig_0">1}</ref>, if and only if there exists a continuous ? : R n ? R 2n?2 and a continuous ? :</p><formula xml:id="formula_45">(S n?1 ) k ? R dout is sign invariant, i.e. f (s 1 v 1 , . . . , s k v k ) = f (v 1 , . . . , v k ) for any s i ? {?1,</formula><formula xml:id="formula_46">R (2n?2)k ? R dout such that f (v 1 , . . . , v k ) = ? [?(v i ) + ?(?v i )] k i=1 .<label>(30)</label></formula><p>Proof. It can be directly seen that any f of the above form is sign invariant.</p><p>Thus, we show that any sign invariant f can be expressed in the above form. First, we show that we can apply the general Theorem 4. The group G i = {1, ?1} acts continuously and satisfies that S n?1 /{1, ?1} = RP n?1 , where RP n?1 is the real projective space of dimension n ? 1. Since RP n?1 is a smooth manifold of dimension n ? 1, Whitney's embedding theorem states that there exists a (smooth) topological embedding ? i : RP n?1 ? R 2n?2 (Lemma 5).</p><p>Thus, we can apply the general theorem to see that f = ? ?? k for some continuous ? and? k . Note that each? i =? is the same, as each X i = S n?1 and G i = {1, ?1} is the same. Also, Theorem 4 says that we may assume that? is sign invariant, so?(x) =?(?x). Letting ?(x) =?(x)/2, we are done with the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.2 SIGN INVARIANT UNIVERSAL REPRESENTATION WITH EXTRA FEATURES</head><p>Recall that we may want our sign invariant functions to process other data besides eigenvectors, such as eigenvalues or node features associated to a graph. Here, we show universal representation for when we have this other data that does not possess sign symmetry. The proof is a simple extension of Corollary 2, but we provide the technical details for completeness. Then f is sign invariant for the inputs on the sphere, i.e.</p><formula xml:id="formula_47">f (s 1 v 1 , . . . , s k v k , x 1 , . . . , x k ) = f (v 1 , . . . , v k , x 1 , . . . , x k ) s i ? {1, ?1},<label>(31)</label></formula><p>if and only if there exists a continuous ? : R n+d ? R 2n?2+d and a continuous ? :</p><formula xml:id="formula_48">R (2n?2+d)k ? R dout such that f (v 1 , . . . , v k ) = ? (?(v 1 , x 1 ) + ?(?v 1 , x 1 ), . . . , ?(v k , x k ) + ?(?v k , x k )) .<label>(32)</label></formula><p>Proof. Once again, the sign invariance of any f in the above form is clear.</p><p>We follow very similar steps to the proof of Corollary 2 to show that we may apply Theorem 4. We can view ? as a quotient space, after quotienting by the trivial group that does nothing, ? ? = ?/{1}. The corresponding quotient map is id ? , the identity map. Also, ? trivially topologically embeds in R d by the inclusion map.</p><p>As G i = {?1, 1} ? {1} acts continuously, by Lemma 3 we have that</p><formula xml:id="formula_49">(S n?1 ? ?)/({1, ?1} ? {1}) ? = (S n?1 /{1, ?1}) ? (?/{1}) ? = RP n?1 ? ?,<label>(33)</label></formula><p>with corresponding quotient map ? ? id ? , where ? is the quotient map to RP n?1 .</p><p>Letting? be the embedding of RP n?1 ? R 2n?2 guaranteed by Whitney's embedding theorem (Lemma 5), we have that ? =? ? id ? is an embedding of RP n?1 ? ? ? R 2n?2+d . Thus, we can apply Theorem 4 to write</p><formula xml:id="formula_50">f = ? ?? k for? = (? ? id ? ) ? (? ? id ? ), s? ?(v i , x i ) = (?(v i ), x i ),<label>(34)</label></formula><formula xml:id="formula_51">where?(v i , x i ) =?(?v i , x i ). Letting ?(v i , x i ) =?(v i , x i )/2, we are done.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.3 BASIS INVARIANT UNIVERSAL REPRESENTATION</head><p>Recall that <ref type="figure">St(d, n)</ref> is the Stiefel manifold of d-tuples of vectors (v 1 , . . . , v d ) where v i ? R n and v 1 , . . . , v d are orthonormal. This is where our inputs lie, as our eigenvectors are unit norm and orthogonal. We will also make use of the <ref type="figure">Grassmannian Gr(d, n)</ref>, which consists of all d-dimensional subspaces in R n . This is because the Grassmannian is the quotient space for the group action we want, <ref type="bibr" target="#b38">Gallier &amp; Quaintance, 2020)</ref>.</p><formula xml:id="formula_52">Gr(d, n) ? = St(d, n)/O(d), where Q ? O(d) acts on V ? St(d, n) ? R n?d by mapping V to V Q (</formula><p>Corollary 4 (Universal Representation for BasisNet) <ref type="figure" target="#fig_0">. For dimensions d 1 , .</ref> . . , d l ? n let f be a continuous function on <ref type="figure" target="#fig_0">St(d 1 , n</ref> <ref type="figure">n)</ref> by multiplication on the right.</p><formula xml:id="formula_53">) ? . . . ? St(d l , n). Further assume that f is invariant to O(d 1 ) ? . . . ? O(d l ), where O(d i ) acts on St(d i ,</formula><p>Then there exist continuous ? :</p><formula xml:id="formula_54">R l i=1 2di(n?di) ? R dout and continuous ? i : St(d i , n) ? R 2di(n?di) such that f (V 1 , . . . , V l ) = ? (? 1 (V 1 ), . . . , ? l (V l )) ,<label>(35)</label></formula><p>where the ? i are O(d i ) invariant functions, and we can take</p><formula xml:id="formula_55">? i = ? j if d i = d j .</formula><p>Proof. Letting X i = St(d i , n) and G i = O(d i ), it can be seen that G i acts continuously on X i . Also, we have that the quotient space</p><formula xml:id="formula_56">St(d i , n)/O(d i ) = Gr(d i , n) is the Grassmannian of d i dimensional subspaces in R n , which is a smooth manifold of dimension d i (n ? d i ).</formula><p>Thus, the Whitney embedding theorem (Lemma 5) gives a topological embedding ? i :</p><formula xml:id="formula_57">Gr(d i , n) ? R 2di(n?di) .</formula><p>Hence, we may apply Theorem 4 to obtain continuous</p><formula xml:id="formula_58">O(d i ) invariant ? i : St(d i , n) ? R 2di(n?di) and continuous ? : R l i=1 2di(n?di) ? R dout , such that f = ? ? (? 1 ? . . . ? ? l ). Also, if d i = d j , then X i = X j and G i = G j , so we can take ? i = ? j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.4 BASIS INVARIANT AND PERMUTATION EQUIVARIANT UNIVERSAL APPROXIMATION</head><p>With the restriction that f (V 1 , . . . , V l ) : R n? i di ? R n be permutation equivariant and basis invariant, we need to use the impractically expensive Expressive-BasisNet to approximate f . Universality of permutation invariant or equivariant functions from matrices to scalars or matrices to vectors is difficult to achieve in a computationally tractable manner <ref type="bibr">(Maron et al., 2019;</ref><ref type="bibr">Keriven &amp; Peyr?, 2019;</ref><ref type="bibr">Maehara &amp; NT, 2019)</ref>. One intuitive reason to expect this is that universally approximating such functions allows solution of the graph isomorphism problem <ref type="bibr" target="#b17">(Chen et al., 2019b)</ref>, which is a computationally difficult problem. While we have exact representation of basis invariant functions by continuous ? and ? i when there is no permutation equivariance constraint, we can only achieve approximation up to an arbitrary &gt; 0 when we require permutation equivariance. Corollary 5 (Universal Approximation for Expressive-BasisNets). Let f (V 1 , . . . , V l ) : <ref type="figure" target="#fig_0">St(d 1 , n</ref></p><formula xml:id="formula_59">) ? . . . ? St(d l , n) ? R n be continuous, O(d 1 ) ? . . . ? O(d l ) invariant,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and permutation equivariant.</head><p>Then f can be -approximated by an Expressive-BasisNet.</p><p>Proof. By invariance, Corollary 4 of the decomposition theorem shows that f can be written as </p><formula xml:id="formula_60">f (V 1 , . . . , V l ) = ? (? d1 (V 1 ), . . . , ? d l (V l ))<label>(36)</label></formula><formula xml:id="formula_61">V i ) = ? di (V i V i ) for some continuous ? di . Let Z = {(V 1 V 1 , . . . , V l V l ) : V i ? St(d i , n)} ? R n 2 ?l ,<label>(37)</label></formula><p>which is compact as it is the image of the compact space St(d 1 , n)?. . .?St(d l , n) under a continuous function. Define h : Z ? R n 2 ?l ? R n by</p><formula xml:id="formula_62">h(V 1 V 1 , . . . , V l V l ) = ? ? d1 (V 1 V 1 ), . . . , ? d l (V l V l ) .<label>(38)</label></formula><p>Then note that h is continuous and permutation equivariant from matrices to vectors, so it can be -approximated by an invariant graph network (Keriven &amp; Peyr?, 2019), call it IGN. If we define ? = IGN and IGN di (V i V i ) = V i V i (this identity operation is linear and permutation equivariant, so it can be exactly expressed by an IGN), then we have -approximation of f by</p><formula xml:id="formula_63">IGN(V 1 V 1 , . . . , V l V l ) =? IGN d1 (V 1 V 1 ), . . . , IGN d l (V l V l ) .<label>(39)</label></formula><p>G.3 PROOF OF UNIVERSAL APPROXIMATION FOR GENERAL DECOMPOSITIONS Theorem 5. Consider the same setup as Theorem 4, where X i are also compact. Let ? i be a family of G i -invariant functions that universally approximate G i -invariant continuous functions X i ? R ai , and let R be a set of continuous function that universally approximate continuous functions Z ? R a ? R dout for every compact Z, where a = i a i . Then for any ? &gt; 0 and any G-invariant continuous function f :</p><formula xml:id="formula_64">X 1 ? . . . ? X k ? R dout there exists ? ? ? and ? ? R such that f ? ?(? 1 , . . . , ? k ) ? &lt; ?.</formula><p>Proof. Consider a particular G-invariant continuous function f : X 1 ? . . . ? X k ? R dout . By Theorem 4 there exists G i -invariant continuous functions ? i : X i ? R ai and a continuous function</p><formula xml:id="formula_65">? : Z ? R a ? R dout (where a = i a i ) such that f (v 1 , . . . , v k ) = ? (? 1 (v 1 ), . . . , ? k (v k )).</formula><p>Now fix an ? &gt; 0. For any ? ? R and any ? i ? ? i (i = 1, . . . k) we may bound the difference from f as follows (suppressing the v i 's for brevity),</p><formula xml:id="formula_66">f ? ?(? 1 , . . . , ? k ) ? = ? (? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) ? = ? (? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) + ?(? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) ? ? ? (? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) ? + ?(? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) ? = I + II Now let K = k i=1 im? i .</formula><p>Since each ? i is continuous and defined on a compact set X i we know that im? i is compact, and so the product K is also compact. Since K is compact, it is contained in a closed ball B(r) of radius r &gt; 0 centered at the origin. Let K be the closed ball B(r + 1) of radius r + 1 centered at the origin, so K contains K and a ball of radius 1 around each point of K . We may extend ? continuously to K as needed, so assume ? : K ? R dout . By universality of R we may pick a particular ? : K ? R dout , ? ? R such that</p><formula xml:id="formula_67">I = sup {vi?Xi} k i=1 ? (? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) ? ? sup z?K ? (z) ? ?(z) 2 &lt; ?/2.</formula><p>Keeping this choice of ?, it remains only to bound II. As ? is continuous on a compact domain, it is in fact uniformly continuous. Thus, we can choose a ? &gt; 0 such that if y ? z 2 ? ? , then ?(y) ? ?(z) ? &lt; /2, and then we define ? = min(? , 1).</p><formula xml:id="formula_68">Since ? i universally approximates ? i we may pick ? i ? ? i such that ? i ? ? i ? &lt; ?/ ? k, and thus (? 1 , . . . , ? k ) ? (? 1 , . . . ? k ) ? ? ?. With this choice of ? i , we know that k i=1 im? i ? K (because each ? i (x i ) is within distance 1 of ? i (x i ))</formula><p>. Thus, ?(? 1 (x 1 ), . . . , ? k (x k )) is well-defined, and we have</p><formula xml:id="formula_69">II = ?(? 1 , . . . , ? k ) ? ?(? 1 , . . . , ? k ) ? = sup {xi?Xi} k i=1 ?(? 1 (x 1 ), . . . , ? k (x k )) ? ?(? 1 (x 1 ), . . . , ? k (x k )) 2 &lt; ?/2</formula><p>due to our choice of ?, which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H BASIS INVARIANCE FOR GRAPH REPRESENTATION LEARNING H.1 SPECTRAL GRAPH CONVOLUTION</head><p>In this section, we consider spectral graph convolutions, which for node features X ? R n?d feat take the form f (V, ?, X) = n i=1 ? i v i v i X for some parameters ? i . We can optionally take ? i = h(? i ) for some continuous function h : R ? R of the eigenvalues. This form captures most popular spectral graph convolutions in the literature <ref type="bibr" target="#b13">(Bruna et al., 2014;</ref><ref type="bibr" target="#b42">Hamilton, 2020;</ref><ref type="bibr" target="#b12">Bronstein et al., 2017)</ref>; often, such convolutions are parameterized by taking h to be some analytic function such as a simple affine function <ref type="bibr">(Kipf &amp; Welling, 2017)</ref>, a linear combination in a polynomial basis <ref type="bibr" target="#b26">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b19">Chien et al., 2021)</ref>, or a parameterization of rational functions <ref type="bibr">(Levie et al., 2018;</ref><ref type="bibr" target="#b8">Bianchi et al., 2021)</ref>.</p><p>First, it is well known and easy to see that spectral graph convolutions are permutation equivariant, as for a permutation matrix P we have</p><formula xml:id="formula_70">f (P V, ?, P X) = i ? i P v i v i P P X = i ? i P v i v i X = P f (V, ?, X).<label>(40)</label></formula><p>Also, it is easy to see that they are sign invariant, as (?v i )(?v i ) = v i v i . However, if the ? i do not depend on the eigenvalues, then the spectral graph convolution is not necessarily basis invariant. For instance, if v 1 and v 2 are in the same eigenspace, and we change basis by permuting v 1 = v 2 and v 2 = v 1 , then if ? 1 = ? 2 the spectral graph convolution will generally change as well.</p><p>On the other hand, if ? i = h(? i ) for some function h : R ? R, then the spectral graph convolution is basis invariant. This is because if v i and v j belong to the same eigenspace, then Another way to see this basis invariance is with a simple computation. Let V 1 , . . . , V l be the eigenspaces of dimension d 1 , . . . , d l , where V i ? R n?di . Let the corresponding eigenvalues be ? 1 , . . . , ? l . Then for any orthogonal matrices</p><formula xml:id="formula_71">? i = ? j so h(? i ) = h(? j ). Thus, if v i1 , . . . , v i d are eigenvectors of the same eigenspace with eigenvalue ?, we have that d l=1 h(? i l )v i l v i l = h(?) d l=1 v i l v i l . Now, note that d l=1 v i l v i</formula><formula xml:id="formula_72">Q i ? O(d i ), we have n i=1 h(? i )v i v i = l j=1 V j h(? j )I dj V j (41) = l j=1 V j h(? j )I dj Q j Q j V j (42) = l j=1 (V j Q j )h(? j )I dj (V j Q j ) ,<label>(43)</label></formula><p>so the spectral graph convolution is invariant to substituting V j Q j for V j . Now, we give the proof that shows SignNet and BasisNet can universally approximate spectral graph convolutions. Theorem 2 (Learning Spectral Graph Convolutions). Suppose the node features X ? R n?d feat take values in compact sets. Then SignNet can universally approximate any spectral graph convolution, and both BasisNet and Expressive-BasisNet can universally approximate any parametric spectral graph convolution.</p><p>Proof. Note that eigenvectors and eigenvalues of normalized Laplacian matrices take values in compact sets, since the eigenvalues are in [0, 2] and we take eigenvectors to have unit-norm. Thus, the whole domain of the spectral graph convolution is compact.</p><p>Let ? &gt; 0. First, consider a spectral graph convolution f (V, ?, X) = n i=1 ? i v i v i X. For SignNet, let ?(v i , ? i , X) approximate the function?(v i , ? i , X) = ? i v i v i X to within ?/n error, which DeepSets can do since this is a continuous permutation equivariant function from vectors to vectors <ref type="bibr" target="#b67">(Segol &amp; Lipman, 2019</ref>) (note that we can pass ? i as a vector in R n by instead passing ? i 1, where 1 is the all ones vector). Then ? = n i=1 is a linear permutation equivariant operation that can be exactly expressed by DeepSets, so the total error is within ?. The same argument applies when ? i = h(? i ) for some continuous function h.</p><p>For the basis invariant case, consider a parametric spectral graph convolution f (V, ?,</p><formula xml:id="formula_73">X) = n i=1 h(? i )v i v i X.</formula><p>Note that if the eigenspace bases are V 1 , . . . , V l with eigenvalues ? 1 , . . . , ? l , we can write the f (V, ?, X) = l i=1 h(? j )V j V j X. Again, we will let ? = l i=1 be a sum function, which can be expressed exactly by DeepSets. Thus, it suffices to show that h(? j )V j V j X can be /n approximated by a 2-IGN (i.e. an IGN that only uses vectors and matrices).</p><p>Note that since h is continuous, we can use an elementwise MLP (which IGNs can learn) to approximate f 1 (?11 , V V , X) = (h(?)11 , V V , X) to arbitrary precision (note that we represent the eigenvalue ? as a constant matrix ?11 ). Also, since a 2-IGN can learn matrix vector multiplication <ref type="bibr" target="#b14">(Cai &amp; Wang (2022)</ref> Lemma 10), we can approximate</p><formula xml:id="formula_74">f 2 (h(?)11 , V V , X) = (h(?)11 , V V X), as V i V i ? R n 2</formula><p>is a matrix and X ? R n?d feat is a vector with respect to permutation symmetries. Finally, we use an elementwise MLP to approximate the scalar-vector multiplica-</p><formula xml:id="formula_75">tion f 3 (h(?)11 , V V , X) = h(?)V V X. Since f 3 ? f 2 ? f 1 (?11 , V V , X) = h(?)V V X,</formula><p>and since 2-IGNs universally approximate each f i , applying Lemma 6 shows that a 2-IGN can approximate h(?)V V X to /n accuracy, so we are done. Since Expressive-BasisNet is stronger than BasisNet, it can also universally approximate these functions.</p><p>From the proof, we can see that SignNet and BasisNet need only learn simple functions for the ? and ? when h is simple, or when the filter is non-parametric and we need only learn ? i . <ref type="bibr" target="#b91">Xu et al. (2020)</ref> propose the principle of algorithmic alignment, and show that if separate modules of a neural network each need only learn simple functions (that is, functions that are well-approximated by low-order polynomials with small coefficients), then the network may be more sample efficient. If we do not require permutation equivariance, and parameterize SignNet and BasisNet with simple MLPs, then algorithmic alignment may suggest that our models are sample efficient. Indeed, ? = is a simple linear function with coefficients 1, and ?(V, ?, X) = h(?)V V X is quadratic in V and linear in X, so it is simple if h is simple. Proposition 3. There exist infinitely many pairs of non-isomorphic graphs that SignNet and BasisNet can distinguish, but spectral graph convolutions or spectral GNNs cannot distinguish.</p><p>Proof. The idea is as follows: we will take graphs G and give them the node feature matrix X G = D 1/2 1, i.e. each node has as feature the square root of its degree. Then any spectral graph convolution (or, the first layer of any spectral GNN) will map V Diag(?)V X to something that only depends on the degree sequence and number of nodes. Thus, any spectral graph convolution or spectral GNN will have the same output (up to permutation) for any such graphs G with node features X G and the same number of nodes and same degree sequence. On the other hand, SignNet and BasisNet can distinguish between infinitely many pairs of graphs (G (1) , G (2) ) with node features (X G (1) , X G (2) ) and the same number of nodes and degree sequence; this is because SignNet and BasisNet can tell when a graph is bipartite.</p><p>For each n ? 5, we will define G (1) and G (2) as connected graphs with n nodes, with the same degree sequence. Also, we define G (1) to have node features X</p><formula xml:id="formula_76">(1) i = d (1) i , where d (1)</formula><p>i is the degree of node i in G (1) , and similarly G (2) has node features X</p><formula xml:id="formula_77">(2) i = d (2)</formula><p>i . Now, note that X (1) is an eigenvector of the normalized Laplacian of G (1) , and it has eigenvalue 0. As we take the eigenvectors to be orthonormal (since the normalized Laplacian is symmetric), for any spectral graph convolution we have that</p><formula xml:id="formula_78">n i=1 ? i v i v i X (1) = ? 1 v 1 v 1 X (1) = ? 1 D 1/2 1 1(D 1/2 1 1) D 1/2 1 1 = ? 1 n j=1 (d (1) j )D 1/2 1 1.<label>(44)</label></formula><p>Where D 1 is the diagonal degree matrix of G (1) . Likewise, any spectral graph convolution outputs ? 1 j (d</p><p>j )D 1/2 2 1 for G (2) . Since D 1 and D 2 are the same up to a permutation, we have that any spectral graph convolution has the same output for G (1) and G (2) , up to a permutation. In fact, this <ref type="figure" target="#fig_0">Figure 11</ref>: Illustration of our constructed G (1) and G (2) for n = 5, as used in the proof of Proposition 3. <ref type="figure" target="#fig_0">Figure 12</ref>: Illustration of our constructed G (1) and G (2) for n = 6, as used in the proof of Proposition 3. also holds for spectral GNNs, as the first layer will always have the same output (up to a permutation) on G (1) and G (2) , so the latter layers will also have the same output up to a permutation. Now, we concretely define G (1) and G (2) . This is illustrated in <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref>. For n = 5, let G (1) contain a triangle with nodes w 1 , w 2 , w 3 , and have a path of length 2 coming out of one of the nodes in the triangle, say w 1 connects to w 4 , and w 4 connects to w 5 . This is not bipartite, as there is a triangle. Let G (2) be a bipartite graph that has 2 nodes on the left (v 1 , v 2 ) and 3 nodes on the right (v 3 , v 4 , v 5 ). Connect v 1 with all nodes on the right, and connect v 2 with v 3 and v 4 .</p><formula xml:id="formula_80">w 1 w 2 w 3 w 4 w 5 G (1) v 1 v 2 v 3 v 4 v 5 G (2)</formula><formula xml:id="formula_81">w 1 w 2 w 3 w 4 w 5 w 6 G (1) v 1 v 2 v 3 v 4 v 5 v 6 G (2)</formula><p>Note that both G (1) and G (2) have the same number of nodes and the same degree sequence {3, 2, 2, 2, 1}. Thus, spectral graph convolutions or spectral GNNs cannot distinguish them. However, SignNet and BasisNet can distinguish them, as they can tell whether a graph is bipartite by checking the highest eigenvalue of the normalized Laplacian. This is because the multiplicity of the eigenvalue 2 is the number of bipartite components. In particular, SignNet can approximate the function ?(v i , ? i , X) = ? i and ? ? max n i=1 . Likewise, BasisNet can approximate the function</p><formula xml:id="formula_82">? di (V i V i , ? i ) = ? i and ? ? max l i=1</formula><p>. This in fact gives an infinite family of graphs that SignNet / BasisNet can distinguish, but spectral graph convolutions or spectral graph GNNs cannot. To see why, suppose we have G (1) and G (2) for some n ? 5. Then we construct a pair of graphs on n + 1 nodes with the same degree sequence. To do this, we add another node to the path of G (1) , thus giving it degree sequence {3, 2, . . . , 2, 1}. For G (2) , we add a node v n+1 to the side that v n is not contained on (e.g. for n = 5, we add v 6 to the left side, as v 5 was on the right), then connect v n to v n+1 to also give a degree sequence {3, 2, . . . , 2, 1}. Note that the non-bipartiteness of G (1) and bipartiteness of G (2) are preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 EXISTING POSITIONAL ENCODINGS</head><p>Here, we show that our SignNets and BasisNets universally approximate various types of existing graph positional encodings. The key is to show that these positional encodings are related to spectral graph convolution matrices and the diagonals of these matrices, and to show that our networks can approximate these matrices and diagonals.</p><p>Proposition 5. If the eigenvalues take values in a compact set, SignNets and BasisNets universally approximate the diagonal of any spectral graph convolution matrix f</p><formula xml:id="formula_83">(V, ?) = diag n i=1 h(? i )v i v i .</formula><p>BasisNets can additionally universally approximate any spectral graph convolution matrix f</p><formula xml:id="formula_84">(V, ?) = n i=1 h(? i )v i v i .</formula><p>Proof. Note that the v i come from a compact set as they are of unit norm. The ? i are from a compact set by assumption; this assumption holds for the normalized Laplacian, as ? i ? [0, 2]. Also, as diag is linear, the spectral graph convolution diagonal can be written</p><formula xml:id="formula_85">n i=1 h(? i )diag(v i v i ). Let &gt; 0. For SignNet, let ? = n i=1</formula><p>, which can be exactly expressed as it is a permutation equivariant linear operation from vectors to vectors. Then ?(v i , ? i ) can approximate the function ? i diag(v i v i ) to arbitrary precision, as it is a permutation equivariant function from vectors to vectors <ref type="bibr" target="#b67">(Segol &amp; Lipman, 2019)</ref>. Thus, letting ? approximate the function to /n accuracy, SignNet can approximate f to accuracy.</p><p>Let l be the number of eigenspaces</p><formula xml:id="formula_86">V 1 , . . . , V l , so f (V, ?) = l i=1 h(? i )V i V i .</formula><p>For BasisNet, we need only show that it can approximate the spectral graph convolution matrix to /l accuracy, as a 2-IGN can exactly express the diag function in each ? di , since it is a linear permutation equivariant function from matrices to vectors. A 2-IGN can universally approximate the function f</p><formula xml:id="formula_87">1 (? i , V i V i ) = (h(? i ), V i V i )</formula><p>, as it can express any elementwise MLP. Also, a 2-IGN can universally approximate the scalar-matrix multiplication <ref type="figure" target="#fig_6">6</ref> shows that a single 2-IGN can approximate this composition to /l accuracy, so we are done.</p><formula xml:id="formula_88">f 2 (h(? i ), V i V i ) = h(? i )V i V i by another elementwise MLP. Since h(? i )V i V i = f 2 ? f 1 (? i , V i V i ), Lemma</formula><p>for some function f 3 . We restrict to continuous f 3 here; shortest path distances can be obtained by a discontinuous f 3 that we discuss below. Their generalized PageRank based distance encodings can be obtained by</p><formula xml:id="formula_89">n i=1 ? ? k?1 ? k (1 ? ? i ) k ? ? v i v i<label>(47)</label></formula><p>for some ? k ? R, so this is a spectral graph convolution. They also define so-called landing probability based positional encodings, which take the form</p><formula xml:id="formula_90">n i=1 (1 ? ? i ) k v i v i ,<label>(48)</label></formula><p>for some choices of integer k. Thus, BasisNets can approximate these distance encoding matrices.</p><p>Another powerful class of positional encodings is based on shortest path distances between nodes in the graph . Shortest path distances can be expressed in a form similar to the spectral graph convolution, but require a highly discontinuous function. If we define f 3 (x 1 , . . . , x n ) = min i:xi =0 i to be the lowest index such that x i is nonzero, then we can write the shortest path distance matrix as f</p><formula xml:id="formula_91">3 (D ?1 A, (D ?1 A) 2 , . . . , (D ?1 A) n ), where f 3 is applied elementwise to return an n ? n matrix. As (D ?1 A) k = n i=1 (1 ? ? i ) k v i v i ,</formula><p>BasisNets can learn the inside arguments, but cannot learn the discontinuous function f 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 SPECTRAL INVARIANTS</head><p>Here, we consider the graph angles ? ij = V i V i e j 2 , for i = 1, . . . , l where l is the number of eigenspaces, and j = 1, . . . , n. It is clear that graph angles are permutation equivariant and basis invariant. These graph angles have been extensively studied, so we cite a number of interesting properties of them. That graph angles determine the number of length 3, 4 and 5 cycles, the connectivity of a graph, and the number of length k closed walks is all shown in Chapter 4 of <ref type="bibr" target="#b25">Cvetkovi? et al. (1997)</ref>. Other properties may be of use for graph representation learning as well. For instance, the eigenvalues of node-deleted subgraphs of a graph G are determined by the eigenvalues and graph angles of G; this may be useful in extending recent graph neural networks that are motivated by node deletion and the reconstruction conjecture <ref type="bibr" target="#b22">(Cotta et al., 2021;</ref><ref type="bibr" target="#b7">Bevilacqua et al., 2022;</ref><ref type="bibr">Papp et al., 2021;</ref><ref type="bibr" target="#b73">Tahmasebi et al., 2020)</ref>. Now, we prove that BasisNet can universally approximate the graph angles. The graph properties we consider in the theorem are all integer valued (e.g. the number of cycles of length 3 in a graph is an integer). Thus, any two graphs that differ in these properties will differ by at least 1, so as long as we have approximation to ? &lt; 1/2, we can distinguish any two graphs that differ in these properties. Recall the statement of Theorem 3. Theorem 3. BasisNet can universally approximate the graph angles ? ij . The eigenvalues and graph angles (and thus BasisNets) can determine the number of length 3, 4, and 5 cycles, whether a graph is connected, and the number of length k closed walks from any vertex to itself.</p><p>Proof. Note that the graph angles satisfy</p><formula xml:id="formula_92">? ij = V i V i e j 2 = e j V i V i V i V i e j = e j V i V i e j ,<label>(49)</label></formula><p>where V i is a basis for the ith adjacency matrix eigenspace, and e j V i V i e j is the (j, j)-entry of V i V i . These graph angles are just the elementwise square roots of the diagonals of the matrices V</p><formula xml:id="formula_93">i V i . As f 1 (V i V i ) = diag(V i V i )</formula><p>is a permutation equivariant linear function from matrices to vectors, 2-IGN on V i V i can exactly compute this with 0 error. Then a 2-IGN can learn an elementwise MLP to approximate the elementwise square root f 2 (diag(V i V i )) = diag(V i V i ) to arbitrary precision. Finally, there may be remaining operations f 3 that are permutation invariant or permutation equivariant from vectors to vectors; for instance, the ? ij are typically gathered into a matrix of size l ? n where the columns are lexicographically sorted (l is the number of eigenspaces) <ref type="bibr" target="#b25">(Cvetkovi? et al., 1997)</ref>, or we may have a permutation invariant readout to compute a subgraph count. A</p><p>DeepSets can approximate f 3 without any higher order tensors besides vectors <ref type="bibr" target="#b95">(Zaheer et al., 2017;</ref><ref type="bibr" target="#b67">Segol &amp; Lipman, 2019)</ref>.</p><p>As 2-IGNs can approximate each f i individually, a single 2-IGN can approximate f 3 ? f 2 ? f 1 by Lemma 6. Also, since the graph properties considered in the theorem are integer-valued, BasisNet can distinguish any two graphs that differ in one of these properties.</p><p>To see that message passing graph neural networks (MPNNs) cannot determine these quantities, we use the fact that MPNNs cannot distinguish between two graphs that have the same number of nodes and where each node (in both graphs) has the same degree. For k ? 3, let C k denote the cycle graph of size k, and C k + C k denote the graph that is the union of two disjoint cycle graphs of size k. MPNNs cannot distinguish between C 2k and C k + C k for k ? 3, because they have the same number of nodes, and each node has degree 2. Thus, MPNNs cannot tell whether a graph is connected, as C 2k is but C k + C k is not. Also, it cannot count the number of 3, 4, or 5 cycles, as C k + C k has two k cycles while C 2k has no k cycles. Likewise, any node in C k + C k has more length k closed walks than any node in C 2k . This is because any length k closed walk in C 2k has an analogous closed walk in C k + C k , but the nodes in C k + C k also have a closed walk that completely goes around a cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I USEFUL LEMMAS</head><p>In this section, we collect useful lemmas for our proofs. These lemmas generally only require basic tools to prove. Our first lemma is a crucial property of quotient spaces. Lemma 1 (Passing to the quotient). Let X and Y be topological spaces, and let X /G be a quotient space, with corresponding quotient map ?. Then for every continuous G-invariant function f : X ? Y, there is a unique continuousf :</p><formula xml:id="formula_94">X /G ? Y such that f =f ? ?.</formula><p>Proof. For z ? X /G, by surjectivity of ? we can choose an x z ? X such that ?(x z ) = z. Defin? f :</p><formula xml:id="formula_95">X /G ? Y byf (z) = f (x z )</formula><p>. This is well-defined, since if ?(x z ) = ?(x) for any other x ? X , then gx z = x for some g ? G, so</p><formula xml:id="formula_96">f (x) = f (gx z ) = f (x z ) =f (z),<label>(50)</label></formula><p>where the second equality uses the G-invariance of f . Note thatf is continuous by the universal property of quotient spaces. Also,f is the unique function such that f =f ? ?; if there were another function h :</p><formula xml:id="formula_97">X /G ? Y with h(z) =f (z), then h(z) = f (x z ), so h(?(x z )) = h(z) = f (x z ).</formula><p>Next, we give the First Fundamental Theorem of O(d), a classical result that has been recently used for machine learning by <ref type="bibr" target="#b83">Villar et al. (2021)</ref>. This result shows that an orthogonally invariant f (V ) can be expressed as a function h(V V ). We give a proof that if f is continuous, then h is also continuous. </p><formula xml:id="formula_98">Lemma 2 (First Fundamental Theorem of O(d)). A continuous function f : R n?d ? R dout is orthogonally invariant, i.e. f (V Q) = f (V ) for all Q ? O(d), if and only if f (V ) = h(V V ) for some continuous h. Proof. If f (V ) = h(V V ), then we have f (V Q) = h(V QQ V ) = h(V V ) so f is</formula><formula xml:id="formula_99">V V ) = h(p ? ?(V )) =f ? ?(V ) = f (V ), so we are done.</formula><p>The next lemma allows us to decompose a quotient of a product space into a product of smaller quotient spaces. Lemma 3. Let X 1 , . . . , X k be topological spaces and G 1 , . . . , G k be topological groups such that each G i acts continuously on X i . Denote the quotient maps by ? i : X i ? X i /G i . Then the quotient of the product is the product of the quotient, i.e.</p><formula xml:id="formula_100">(X 1 ? . . . ? X k )/(G 1 ? . . . ? G k ) ? = (X 1 /G 1 ) ? . . . ? (X k /G k ),<label>(51)</label></formula><p>and ? 1 ? . . . ? ? k :</p><formula xml:id="formula_101">X 1 ? . . . X k ? (X 1 /G 1 ) ? . . . ? (X k /G k ) is quotient map.</formula><p>Proof. First, we show that ? 1 ? . . . ? ? k is a quotient map. This is because </p><formula xml:id="formula_102">X 1 ? . . . ? X k ? (X 1 ? . . . ? X k )/(G 1 ? . . . ? G k )</formula><p>denote the quotient map for this space, it is easily seen that q(x 1 , . . . , x k ) = q(y 1 . . . , y k ) if and only if ? 1 ? . . . ? ? k (x 1 , . . . , x k ) = ? 1 ? . . . ? ? k (y 1 , . . . , y k ), since either of these is true if and only if there exist g i ? G i such that x i = g i y i for each i. Thus, we have an isomorphism of these quotient spaces.</p><p>The following lemma shows that quotients of compact spaces are also compact, which is useful for universal approximation on quotient spaces. Lemma 4 (Compactness of quotients of compact spaces). Let X be a compact space. Then the quotient space X /G is compact.</p><p>Proof. Denoting the quotient map by ? : X ? X /G and letting {U ? } ? be an open cover of X /G, we have that {? ?1 (U ? )} ? is an open cover of X . By compactness of X , we can choose a finite subcover {? ?1 (U ?i )} i=1,...,n . Then {?(? ?1 (U ?i ))} i=1,...,n = {U ?i } i=1,...,n by surjectivity, and {U ?i } i=1,...,n is thus an open cover of X /G.</p><p>The Whitney embedding theorem gives a nice condition that we apply to show that the quotient spaces X /G that we deal with embed into Euclidean space. It says that when X /G is a smooth manifold, then it can be embedded into a Euclidean space of double the dimension of the manifold. The proof is outside the scope of this paper. Lemma 5 (Whitney Embedding Theorem <ref type="bibr" target="#b87">(Whitney, 1944)</ref>). Every smooth manifold M of dimension n &gt; 0 can be smoothly embedded in R 2n .</p><p>Finally, we give a lemma that helps prove universal approximation results. It says that if functions f that we want to approximate can be written as compositions f = f L ? . . . ? f 1 , then it suffices to universally approximate each f i and compose the results to universally approximate the f . This is especially useful for proving universality of neural networks, as we may use some layers to approximate each f i , then compose these layers to approximate the target function f . Lemma 6 (Layer-wise universality implies universality </p><formula xml:id="formula_103">? = {? L ? . . . ? ? 1 : ? i ? ? i } universally approximates F.</formula><p>Proof. Let f = f L ? . . . ? f 1 ? F. LetZ 1 = Z, and then for i ? 2 letZ i = f i?1 (Z i?1 ). Then each Z i is compact by continuity of the f i . For 1 ? i &lt; L, let Z i =Z i , and for i = L let Z L be a compact set containingZ L such that every ball of radius one centered at a point inZ L is still contained in Z L .</p><p>Let &gt; 0. We will show that there is a ? ? ? such that f ? ? ? &lt; by induction on L. This holds trivially for L = 1, as then ? = ? 1 . Now, let L ? 2, and suppose it holds for L ? 1. By universality of ? L , we can choose a ? L :</p><formula xml:id="formula_104">Z L ? R d L ? ? L such that ? L ? f L ? &lt; /2.</formula><p>As ? L is continuous on a compact domain, it is also uniformly continuous, so we can choose a? &gt; 0 such that y ? z 2 &lt;? =? ? L (y) ? ? L (z) 2 &lt; /2.</p><p>Let ? = min(?, 1). By induction, we can choose   In <ref type="table" target="#tab_16">Table 8</ref>, we compare our model against methods that have domain-specific information about molecules built into them: HIMP <ref type="bibr" target="#b36">(Fey et al., 2020)</ref> and CIN <ref type="bibr" target="#b9">(Bodnar et al., 2021)</ref>. We see that SignNet is better than HIMP and CIN-small on these tasks, and is within a standard deviation of CIN. The SignNet models are the same as the ones reported in <ref type="table" target="#tab_2">Table 2</ref>. Once again, we emphasize that SignNet is domain-agnostic. To numerically test the ability of our basis invariant networks for learning spectral graph convolutions, we follow the experimental setups of <ref type="bibr" target="#b3">Balcilar et al. (2020)</ref>; . We take the dataset of 50 images in  (originally from the Image Processing Toolbox of MATLAB), and resize them from 100?100 to 32?32. Then we apply the same spectral graph convolutions on them as in , and train neural networks to learn these as regression targets. As in prior work, we report sum of squared errors on the training set to measure expressivity.</p><formula xml:id="formula_105">? L?1 ? . . . ? ? 1 , ? i ? ? i such that ? L?1 ? . . . ? ? 1 ? f L?1 ? . . . ? f 1 ? &lt; ?. (52) Note that ? L?1 ? . . . ? ? 1 (Z) ? Z L , because for each x ? Z, ? L?1 ? . . . ? ? 1 (x) is within ? ? 1 Euclidean distance to f L?1 ? . . . ? f 1 (x) ?Z L , so it is contained in Z L by construction. Thus, we may define ? = ? L ? . . . ? ? 1 : Z ? R d L , and compute that ? ? f ? ? ? ? ? L ? f L?1 ? . . . ? f 1 ? + ? L ? f L?1 ? . . . ? f 1 ? f ? (53) &lt; ? ? ? L ? f L?1 ? . . . ? f 1 ? + /2,<label>(54)</label></formula><formula xml:id="formula_106">since ? L ? f L ? &lt; /2. To bound this other term, let x ? Z, and for y = ? L?1 ? . . . ? ? 1 (x) and z = f L?1 ? . . . ? f 1 (x), we know that y ? z 2 &lt; ?, so ? L (y) ? ? L (z) 2 &lt; /2 by uniform continuity. As this holds for all x, we have ? ? ? L ? f L?1 ? . . . ? f 1 ? ? /2, so ? ? f ? &lt;<label>and</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3 LEARNING SPECTRAL GRAPH CONVOLUTIONS</head><p>We compare against message passing <ref type="bibr">GNNs (Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b81">Veli?kovi? et al., 2018)</ref> and spectral GNNs <ref type="bibr" target="#b19">(Chien et al., 2021;</ref><ref type="bibr" target="#b8">Bianchi et al., 2021;</ref><ref type="bibr" target="#b26">Defferrard et al., 2016;</ref>. Also, we consider standard Transformers with only node features, with eigenvectors and sign flip augmentation, and with absolute values of eigenvectors. These models are all approximately sign invariant (they either use eigenvectors in a sign invariant way or do not use eigenvectors). We use DeepSets <ref type="bibr" target="#b95">(Zaheer et al., 2017)</ref> in SignNet and 2- <ref type="bibr">IGN (Maron et al., 2018)</ref> in BasisNet for ?, use a DeepSets for ? in both cases, and then feed the features into another DeepSets or a standard Transformer <ref type="bibr" target="#b80">(Vaswani et al., 2017)</ref> to make the final predictions. That is, we are only given graph information through the eigenvectors and eigenvalues, and we do not use message passing. <ref type="table" target="#tab_17">Table 9</ref> displays the results, which validate our theoretical results in Section 3.1. Without any message passing, SignNet and BasisNet allow DeepSets and Transformers to perform strongly, beating the spectral GNNs GPR-GNN and ARMA on all tasks. Also, our networks outperform all other methods on the band-rejection and comb filters, and are mostly close to the best model on the other filters.</p><p>As they appear to be freely available with permissive licenses or no licenses, we do not ask for permission from the creators or hosts of the data.</p><p>We do not believe that any of this data contains offensive content or personally identifiable information.</p><p>The 50 images used in the spectral graph convolution experiments are mostly images of objects, with a few low resolution images of humans that do not appear to have offensive content. The only other human-related data appears to be the human mesh, which appears to be from a 3D scan of a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2 GRAPH REGRESSION DETAILS</head><p>ZINC. In Section 4.1 we study the effectiveness of SignNet for learning positional encodings to boost the expressive power, and thereby generalization, on the graph regression problem ZINC. In all cases we take our ? encoder to be an 8 layer GIN with ReLU activation. The input eigenvector v i ? R n , where n is the number of nodes in the graph, is treated as a single scalar feature for each node. In the case of using a fixed number of eigenvectors k, the aggregator ? is taken to be an 8 layer MLP with batch normalization and ReLU activation. The aggregator ? is applied separately to the concatenatation of the k different embeddings for each node in a graph, resulting in one single embedding per node. This embedding is concatenated to the node features for that node, and the result passed as input to the base (predictor) model. We also consider using all available eigenvectors in each graph instead of a fixed number k. Since the total number of eigenvectors is a variable quantity, equal to the number of nodes in the underlying graph, an MLP cannot be used for ?. To handle the variable sized input in this case, we take ? to be an MLP preceded by a sum over the ? outputs. In other words, the SignNet is of the form MLP k i=1 ?(v i ) + ?(?v i ) in this case.</p><p>As well as testing SignNet, we also checked whether simple transformations that resolve the sign ambiguity of the Laplacian eigenvectors p = (v 1 , . . . , v k ) could serve as effective positional encoding. We considered three options. First is to randomly flip the sign of each ?v i during training. This is a common heuristic used in prior work on Laplacian positional encoding <ref type="bibr">(Kreuzer et al., 2021;</ref><ref type="bibr" target="#b30">Dwivedi et al., 2020)</ref>. Second, take the element-wise absolute value |v i |. This is a non-injective map, creating sign invariance at the cost of destroying positional information. Third is a different canonicalization that avoids stochasticity and use of absolute values by selecting the sign of each v i so that the majority of entries are non-negative, with ties broken by comparing the 1 -norm of positive and negative parts. When the tie-break also fails, the sign is chosen randomly. Results for GatedGCN base model on ZINC in <ref type="table" target="#tab_1">Table 1</ref> show that all three of these approaches are significantly poorer positional encodings compared to SignNet.</p><p>Our training pipeline largely follows that of <ref type="bibr" target="#b31">Dwivedi et al. (2022)</ref>, and we use the GatedGCN and PNA base models from the accompanying implementation (see https://github.com/ vijaydwivedi75/gnn-lspe). The Sparse Transformer base model architecture we use, which like GAT computes attention only across neighbouring nodes, is introduced by Kreuzer et al. <ref type="bibr">(2021)</ref>. Finally, the GINE implementation is based on the PyTorch Geometric implementation <ref type="bibr" target="#b35">(Fey &amp; Lenssen, 2019)</ref>. For the state-of-the-art comparison, all baseline results are from their respective papers, except for GIN, which we run.</p><p>ZINC-full. We also run our method on the full ZINC dataset, termed ZINC-full. The result we report for SignNet is a larger version of the GatedGCN base model with a SignNet that takes in all eigenvectors. This model has 994,113 parameters in total. All baseline results are from their respective papers, except for GIN, which is from <ref type="bibr" target="#b9">(Bodnar et al., 2021)</ref>.</p><p>Alchemy. We run our method and compare with the state-of-the-art on Alchemy (with 10,000 training graphs). We use the same data split as <ref type="bibr" target="#b51">Morris et al. (2020b)</ref>. Our base model is a GIN that takes in edge features (i.e. a GINE). The SignNet consists of GIN for ? and a Transformer for ?, as in the counting substructures and graph property regression experiments in Section 4.2. The model has 907,371 parameters in total. Our training setting is very similar to that of <ref type="bibr" target="#b53">Morris et al. (2022)</ref>, as we build off of their code. We train with an Adam optimizer (Kingma &amp; Ba, 2014) with a starting learning rate of .001, and a minimum learning rate of .000001. The learning rate schedule cuts the learning rate in half with a patience of 20 epochs, and training ends when we reach the minimum learning rate. All baseline results are from their respective papers, except for GIN, which is from <ref type="bibr" target="#b53">(Morris et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.3 SPECTRAL GRAPH CONVOLUTION DETAILS</head><p>In Appendix J.3, we conduct node regression experiments for learning spectral graph convolutions. The experimental setup is mostly taken from . However, we resize the 100 ? 100 images to 32 ? 32. Thus, each image is viewed as a 1024-node graph. The node features X ? R n are the grayscale pixel intensities of each node. Just as in , we only train and evaluate on nodes that are not connected to the boundary of the grid (that is, we only evaluate on the 28 ? 28 middle section). For all experiments we limit each model to 50,000 parameters. We use the Adam (Kingma &amp; Ba, 2014) optimizer for all experiments. For each of the GNN baselines (GCN, GAT, GPR-GNN, ARMA, ChebNet, BernNet), we select the best performing out of 4 hyperparameter settings: either 2 or 4 convolution layers, and a hidden dimension of size 32 or D, where D is just large enough to stay with 50,000 parameters (for instance, D = 128 for GCN, GPR-GNN, and BernNet).</p><p>We use DeepSets or standard Transformers as our prediction network. This takes in the output of SignNet or BasisNet and concatenates it with the node features, then outputs a scalar prediction for each node. We use a 3 layer output network for DeepSets SignNet, and 2 layer output networks for all other configurations. All networks use ReLU activations.</p><p>For SignNet, we use DeepSets for both ? and ?. Our ? takes in eigenvectors only, then our ? takes the outputs of ? and the eigenvalues. We use three layers for ? and ?.</p><p>For BasisNet, we use the same DeepSets for ? as in SignNet, and 2-IGNs for the ? di . There are three distinct multiplicities for the grid graph (1, 2, and 32), so we only need 3 separate IGNs. Each IGN consists of an R n 2 ?1 ? R n?d layer and two R n?d ? R n?d layers, where the d are hidden dimensions. There are no matrix to matrix operations used, as the memory requirements are intensive for these ? 1000 node graphs. The ? di only take in V i V i from the eigenspaces, and the ? takes the output of the ? di as well as the eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.4 SUBSTRUCTURES AND GRAPH PROPERTIES REGRESSION DETAILS</head><p>We use the random graph dataset from  for counting substructures and the synthetic dataset from  for regressing graph properties. For fair comparison we fix the base model as a 4-layer GIN model with hidden size 128. We choose ? as a 4-layer GIN (independently applied to every eigenvector) and ? as a 1-layer Transformer (independently applied to every node). Combined with proper batching and masking, we have a SignNet that takes Laplacian eigenvectors V ? R n?n and outputs fixed size sign-invariant encoding node features f (V, ?, X) ? R n?d , where n varies between graphs but d is fixed. We use this SignNet in our experiments and compare with other methods of handling PEs. We closely follow the experimental setting of <ref type="bibr">Koestler et al. (2022)</ref> for the texture reconstruction experiments. In this work, we use the cotangent Laplacian <ref type="bibr" target="#b64">(Rustamov et al., 2007)</ref> of a triangle mesh with the lowest 1023 eigenvectors besides the trivial eigenvector of eigenvalue 0. We implemented SignNet in the authors' original code, which was privately shared with us. Both ? and ? are taken to be MLPs. Hyperparameter settings and number of parameters are given in <ref type="table" target="#tab_1">Table 10</ref>. We chose hyperparameters so that the total number of parameters in the SignNet model was no larger than that of the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.5 TEXTURE RECONSTRUCTION DETAILS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Symmetries of eigenvectors of a symmetric matrix with permutation invariances (e.g. a graph Laplacian). A neural network applied to the eigenvector matrix (middle) should be invariant or equivariant to permutation of the rows (left product with a permutation matrix P ) and invariant to the choice of eigenvectors in each eigenbasis (right product with a block diagonal orthogonal matrix Diag(Q 1 , Q 2 , Q 3 )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Counting substructures and regressing graph properties (lower is better). With Laplacian PEs, SignNet improves performance, while sign flip data augmentation (LapPE) is less consistent. Mean and standard deviations are reported on 3 runs. All runs use the same 4-layer GIN base model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Cotangent Laplacian eigenvectors of the cat model and first principal component of ?(v) + ?(?v) from our trained SignNet. Our SignNet encodes bilateral symmetry, which is useful for reconstruction of the bilaterally symmetric texture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Graph positional encodings. Other recent methods use positional encodings besides Laplacian eigenvectors. These include positional encodings based on random walks<ref type="bibr" target="#b31">(Dwivedi et al., 2022;</ref>  Mialon et al., 2021;, diffusion kernels on graphs(Mialon et al., 2021;<ref type="bibr" target="#b34">Feldman et al., 2022)</ref>, shortest paths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Corollary 1 .</head><label>1</label><figDesc>Unconstrained-SignNet can represent any sign invariant function and Unconstrained-BasisNet can represent any basis invariant function. Expressive-BasisNet is a universal approximator of functions that are both basis invariant and permutation equivariant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Molecule graphs: ZINC<ref type="bibr" target="#b30">Dwivedi et al., 2020)</ref>, ogbg-molhiv<ref type="bibr" target="#b88">(Wu et al., 2018;</ref><ref type="bibr" target="#b46">Hu et al., 2020a)</ref> ? Social networks: IMDB-M, COLLAB<ref type="bibr" target="#b92">(Yanardag &amp; Vishwanathan, 2015;</ref> 2020a),? Bioinformatics graphs: PROTEINS<ref type="bibr" target="#b50">(Morris et al., 2020a)</ref> ? Computer vision graphs: COIL-DEL<ref type="bibr" target="#b63">(Riesen &amp; Bunke, 2008;</ref><ref type="bibr" target="#b50">Morris et al., 2020a)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>(Left) Cotangent Laplacian eigenvectors of the cat model. (Right) First principal component of ?(v) + ?(?v) from our trained SignNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>First three principal components of the full SignNet output on the cat model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>For</head><label></label><figDesc>this visualization we use a SignNet trained with a GatedGCN base model on ZINC, as in Section 4.1. This SignNet uses GIN as ? and ? as an MLP (with a sum before it to handle variable numbers of eigenvectors), and takes in all eigenvectors of each graph. See Figure 8 for all of the eigenvectors of fluorescein. E MORE RELATED WORK E.1 GRAPH POSITIONAL ENCODINGS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>. Further, Li  et al. (2020)  studies the theoretical and practical benefits of incorporating distance features into graph neural networks.<ref type="bibr" target="#b31">Dwivedi et al. (2022)</ref> proposes a method to inject learnable positional encodings into each layer of a graph neural network, and uses a simple random walk based node positional encoding.<ref type="bibr" target="#b94">You et al. (2021)</ref> proposes a node positional encoding diag(A k ), which captures the number of closed walks from a node to itself.<ref type="bibr" target="#b30">Dwivedi et al. (2020)</ref> propose to use Laplacian eigenvectors as positional encodings in graph neural networks, with sign ambiguities alleviated by sign flipping data augmentation.<ref type="bibr" target="#b70">Srinivasan &amp; Ribeiro (2019)</ref> theoretically analyze node positional embeddings and structural representations in graphs, and show that most-expressive structural representations contain the information of any node positional embedding.While positional encodings in sequences as used for Transformers<ref type="bibr" target="#b80">(Vaswani et al., 2017)</ref> are able to leverage the canonical order in sequences, there is no such useful canonical order for nodes in a graph, due in part to permutation symmetries. Thus, different permutation equivariant positional encodings have been proposed to help generalize Transformers to graphs.<ref type="bibr" target="#b29">Dwivedi &amp; Bresson (2021)</ref> directly add in linearly projected Laplacian eigenvectors to node features before processing these features with a graph Transformer.Kreuzer et al. (2021)  propose an architecture that uses attention over Laplacian eigenvectors and eigenvalues to learn node or edge positional encodings. Mialon et al. (2021) uses spectral kernels such as the diffusion kernel to define relative positional encodings that modulate the attention matrix. Ying et al. (2021) achieve state-of-the-art empirical performance All normalized Laplacian eigenvectors of the fluorescein graph. The first principal components of SignNet's learned positional encodings do not exactly match any eigenvectors. with simple Transformers that incorporate shortest-path based relative positional encodings. Zhang et al. (2020) also utilize shortest-path distances for positional encodings in their graph Transformer. Kim et al. (2021) develop higher-order transformers (that generalize invariant graph networks), which interestingly perform well on graph regression using sparse higher-order transformers without positional encodings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Normalized Laplacian eigenvectors and learned positional encodings for the graph of fluorescein. (Top row) From left to right: smallest and second smallest nontrivial eigenvectors, then second largest and largest eigenvectors. (Bottom row) From left to right: first four principal components of the output ?([?(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>20) Thus, for any orthonormal bases V and W of the same subspace, there exists an orthogonal Q ? O(d) such that V Q = W . For another perspective on this, define the Grassmannian Gr(d, n) as the smooth manifold consisting of all d dimensional subspaces of R n . Further define the Stiefel manifold St(d, n) as the set of all orthonormal tuples [v 1 . . . v d ] ? R n?d of d vectors in R n . Letting O(d) act by right multiplication, it holds that St(d, n)/O(d) ? = Gr(d, n). This implies that any O(d) invariant function on St(d, n) can be viewed as a function on subspaces. See e.g. Gallier &amp; Quaintance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Commutative diagram for our proof of Theorem 4. Black arrows denote functions from topological constructions, and red dashed lines denote functions that we parameterize by neural networks (? = ? 1 ? . . . ? ? k and ?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Corollary 3 (</head><label>3</label><figDesc>Universal Representation for SignNet with features). For a compact space of features ? ? R d , let f (v 1 , . . . , v k , x 1 , . . . , x k ) be a continuous function f : (S n?1 ? ?) k ? R dout .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>l is the orthogonal projector onto the eigenspace (Trefethen &amp; Bau III, 1997). A change of basis does not change this orthogonal projector, so such spectral graph convolutions are basis invariant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>we are done. J FURTHER EXPERIMENTS J.1 GRAPH REGRESSION WITH NO EDGE FEATURES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Adjacency Matrix Node Features Laplacian Eigenvectors SignNet Prediction Model (e.g. GNN, Transformer) Compute Eigvecs</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the ZINC dataset with a 500k parameter budget. All models use edge features. Numbers are the mean and standard deviation over 4 runs, each with different seeds.</figDesc><table><row><cell>Base model</cell><cell>Positional encoding</cell><cell>k</cell><cell cols="2">#param Test MAE (?)</cell></row><row><cell></cell><cell cols="2">No PE N/A</cell><cell>492k</cell><cell>0.252?0.007</cell></row><row><cell></cell><cell>LapPE (flip)</cell><cell>8</cell><cell>492k</cell><cell>0.198?0.011</cell></row><row><cell></cell><cell>LapPE (abs.)</cell><cell>8</cell><cell>492k</cell><cell>0.204?0.009</cell></row><row><cell>GatedGCN</cell><cell>LapPE (can.)</cell><cell>8</cell><cell>505k</cell><cell>0.298?0.019</cell></row><row><cell></cell><cell>SignNet (?(v) only)</cell><cell>8</cell><cell>495k</cell><cell>0.148?0.007</cell></row><row><cell></cell><cell>SignNet</cell><cell>8</cell><cell>495k</cell><cell>0.121?0.005</cell></row><row><cell></cell><cell>SignNet</cell><cell>All</cell><cell cols="2">491k 0.100?0.007</cell></row><row><cell></cell><cell cols="2">No PE N/A</cell><cell>473k</cell><cell>0.283?0.030</cell></row><row><cell>Sparse Transformer</cell><cell>LapPE (flip) SignNet</cell><cell>16 16</cell><cell>487k 479k</cell><cell>0.223?0.007 0.115?0.008</cell></row><row><cell></cell><cell>SignNet</cell><cell>All</cell><cell cols="2">486k 0.102?0.005</cell></row><row><cell></cell><cell cols="2">No PE N/A</cell><cell>470k</cell><cell>0.170?0.002</cell></row><row><cell>GINE</cell><cell>LapPE (flip) SignNet</cell><cell>16 16</cell><cell>470k 470k</cell><cell>0.178?0.004 0.147?0.005</cell></row><row><cell></cell><cell>SignNet</cell><cell>All</cell><cell cols="2">417k 0.102?0.002</cell></row><row><cell></cell><cell cols="2">No PE N/A</cell><cell>474k</cell><cell>0.133?0.011</cell></row><row><cell>PNA</cell><cell>LapPE (flip) SignNet</cell><cell>8 8</cell><cell>474k 476k</cell><cell>0.132?0.010 0.105?0.007</cell></row><row><cell></cell><cell>SignNet</cell><cell>All</cell><cell cols="2">487k 0.084?0.006</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with SOTA methods on graph-level regression tasks. Numbers are test MAE, so lower is better. Best models within a standard deviation are bolded.</figDesc><table><row><cell></cell><cell cols="3">ZINC (10K) ? ZINC-full ? Alchemy (10k) ?</cell></row><row><cell>GIN (Xu et al., 2019)</cell><cell>.170?.002</cell><cell>.088?.002</cell><cell>.180?.006</cell></row><row><cell>?-2-GNN (Morris et al., 2020b)</cell><cell>.374?.022</cell><cell>.042?.003</cell><cell>.118?.001</cell></row><row><cell cols="2">?-2-LGNN (Morris et al., 2020b) .306?.044</cell><cell>.045?.006</cell><cell>.122?.003</cell></row><row><cell>SpeqNet (Morris et al., 2022)</cell><cell>-</cell><cell>-</cell><cell>.115?.001</cell></row><row><cell>GNN-IR (Dupty &amp; Lee, 2022)</cell><cell>.137?.010</cell><cell>-</cell><cell>.119?.002</cell></row><row><cell>PF-GNN (Dupty et al., 2021)</cell><cell>.122?.01</cell><cell>-</cell><cell>.111?.01</cell></row><row><cell>Recon-GNN (Cotta et al., 2021)</cell><cell>.170?.006</cell><cell>-</cell><cell>.125?.001</cell></row><row><cell>SignNet (ours)</cell><cell>.084?.006</cell><cell>.024?.003</cell><cell>.113?.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows the results. For all 4 base models, the PE learned with SignNet yields the best test MAE</cell></row><row><cell>(mean absolute error)-lower MAE is better. This includes the cases of PNA and GINE, for which</cell></row><row><cell>Laplacian PE with simple random sign flipping was unable to improve performance over using no PE</cell></row><row><cell>at all. Our best performing model is a PNA model combined with SignNet, which achieves 0.084</cell></row><row><cell>test MAE. Besides SignNet, we consider two non-learned approaches to resolving eigenvector sign</cell></row><row><cell>ambiguity-canonicalization and taking element-wise absolute values (see Appendix K.2 for details).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test results for texture reconstruction experiment on cat and human models, following the experimental setting of(Koestler et al., 2022). We use 1023 eigenvectors of the cotangent Laplacian.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Human</cell></row><row><cell cols="3">Method</cell><cell cols="6">Params PSNR ? DSSIM ? LPIPS ?</cell><cell cols="2">PSNR ? DSSIM ? LPIPS ?</cell></row><row><cell cols="3">Intrinsic NF</cell><cell>329k</cell><cell>34.25</cell><cell>.099</cell><cell cols="2">.189</cell><cell></cell><cell cols="2">32.29</cell><cell>.119</cell><cell>.330</cell></row><row><cell cols="3">Absolute value</cell><cell>329k</cell><cell>34.67</cell><cell>.106</cell><cell cols="2">.252</cell><cell></cell><cell cols="2">32.42</cell><cell>.132</cell><cell>.363</cell></row><row><cell cols="3">Sign flip</cell><cell>329k</cell><cell>23.15</cell><cell>1.28</cell><cell cols="2">2.35</cell><cell></cell><cell cols="2">21.52</cell><cell>1.05</cell><cell>2.71</cell></row><row><cell cols="3">SignNet</cell><cell>324k</cell><cell>34.91</cell><cell>.090</cell><cell cols="2">.147</cell><cell></cell><cell cols="2">32.43</cell><cell>.125</cell><cell>.316</cell></row><row><cell cols="11">Efficiency. These significant performance improvements from SignNet come with only a slightly</cell></row><row><cell cols="11">higher computational cost. For example, GatedGCN with no PE takes about 8.2 seconds per training</cell></row><row><cell cols="11">iteration on ZINC, while GatedGCN with 8 eigenvectors and SignNet takes about 10.6 seconds;</cell></row><row><cell cols="11">this is only a 29% increase in time, for a reduction of test MAE by over 50%. Also, eigenvector</cell></row><row><cell cols="11">computation time is neglible, as we need only precompute and save the eigenvectors once, and it only</cell></row><row><cell cols="8">takes 15 seconds to do this for the 12,000 graphs of ZINC.</cell><cell></cell><cell></cell></row><row><cell cols="11">Comparison with SOTA. In Table 2, we compare SignNet with other domain-agnostic state-of-the-</cell></row><row><cell cols="11">art methods on graph-level molecular regression tasks on ZINC (10,000 training graphs), ZINC-full</cell></row><row><cell cols="11">(about 250,000 graphs), and Alchemy (Chen et al., 2019a) (10,000 training graphs). SignNet</cell></row><row><cell cols="11">outperforms all methods on ZINC and ZINC-full. Our mean score is the second best on Alchemy,</cell></row><row><cell cols="11">and is within a standard deviation of the best. We perform much better on ZINC (.084) than other</cell></row><row><cell cols="11">state-of-the-art positional encoding methods, like GNN-LSPE (.090) (Dwivedi et al., 2022), SAN</cell></row><row><cell cols="11">(.139) (Kreuzer et al., 2021), and Graphormer (.122) (Ying et al., 2021).</cell></row><row><cell cols="11">4.2 COUNTING SUBSTRUCTURES AND REGRESSING GRAPH PROPERTIES</cell></row><row><cell>Test MAE</cell><cell>0.0 0.1 0.2 0.3</cell><cell>Triangle</cell><cell cols="2">Tailed Tri. Counting Substructures 4-Cycle</cell><cell>Star NoPE LapPE SignNet</cell><cell>log 10 (Test MSE)</cell><cell>?4 ?2 0</cell><cell cols="2">IsConnected</cell><cell>Diameter Graph properties</cell><cell>Radius</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>shows our results for texture reconstruction experiments on all models from Koestler et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Nicolas Keriven and Gabriel Peyr?. Universal invariant and equivariant graph neural networks. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019. Jinwoo Kim, Saeyoon Oh, and Seunghoon Hong. Transformers generalize deepsets and can be extended to graphs &amp; hypergraphs. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021. Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. arXiv preprint arXiv:2207.02505, 2022.</figDesc><table /><note>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Int. Conference on Learning Representations (ICLR), volume 5, 2017. Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, and Zorah L?hner. Intrinsic neural fields: Learning functions on manifolds. arXiv preprint arXiv:2203.07967, 2022. Hanspeter Kraft and Claudio Procesi. Classical invariant theory, a primer. Lecture Notes., 1996. Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L?tourneau, and Prudencio Tossou. Re- thinking graph transformers with spectral attention. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021. John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds. Springer, 2013.F. Thomson Leighton and Gary l. Miller. Certificates for graphs with distinct eigen values. Orginal Manuscript, 1979. Ron Levie, Federico Monti, Xavier Bresson, and Michael M Bronstein. Cayleynets: Graph con- volutional neural networks with complex rational spectral filters. IEEE Transactions on Signal Processing, 67(1):97-109, 2018. Bruno L?vy. Laplace-beltrami eigenfunctions towards an algorithm that" understands" geometry. In IEEE International Conference on Shape Modeling and Applications 2006 (SMI'06), pp. 13-13. IEEE, 2006. Pan Li, Eli Chien, and Olgica Milenkovic. Optimizing generalized pagerank methods for seed- expansion community detection. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019. Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design prov- ably more powerful neural networks for graph representation learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 4465-4478, 2020. Takanori Maehara and Hoang NT. A simple proof of the universality of invariant/equivariant graph neural networks. arXiv preprint arXiv:1910.03802, 2019. Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In Int. Conference on Learning Representations (ICLR), volume 6, 2018. Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In Int. Conference on Machine Learning (ICML), pp. 4363-4371. PMLR, 2019. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pp. 43-52, 2015. Gr?goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. GraphiT: Encoding graph structure in transformers. In preprint arXiv:2106.05667, 2021.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>-BasisNet</cell></row></table><note>Properties of our architectures: Unconstrained-SignNet, SignNet, Unconstrained-BasisNet, and Expressive-BasisNet. The properties are: permutation equivariance, universality (for the proper class of continuous invariant functions), and computational tractability.Unconstr.-SignNet SignNet Unconstr.-BasisNet BasisNet Expr.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Eigenspace statistics for datasets of multiple graphs. From left to right, the columns are: dataset name, number of graphs, range of number of nodes per graph, largest multiplicity, and percent of graphs with an eigenspace of dimension &gt; 1.DatasetGraphs # Nodes Max. Mult % Graphs mult. &gt; 1</figDesc><table><row><cell>ZINC</cell><cell>12,000</cell><cell>9-37</cell><cell>9</cell><cell>64.1</cell></row><row><cell>ZINC-full</cell><cell>249,456</cell><cell>6-38</cell><cell>10</cell><cell>63.8</cell></row><row><cell cols="2">ogbg-molhiv 41,127</cell><cell>2 -222</cell><cell>42</cell><cell>68.0</cell></row><row><cell>IMDB-M</cell><cell>1,500</cell><cell>7 -89</cell><cell>37</cell><cell>99.9</cell></row><row><cell>COLLAB</cell><cell>5,000</cell><cell>32 -492</cell><cell>238</cell><cell>99.1</cell></row><row><cell>PROTEINS</cell><cell>1,113</cell><cell>4 -620</cell><cell>20</cell><cell>77.3</cell></row><row><cell>COIL-DEL</cell><cell>3,900</cell><cell>3 -77</cell><cell>4</cell><cell>4.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Eigenspace statistics for single graphs. From left to right, the columns are: dataset name, number of nodes, distinct eigenvalues (i.e. distinct eigenspaces), number of unique multiplicities, largest multiplicity, and percent of eigenvectors belonging to an eigenspace of dimension &gt; 1.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Nodes Distinct ? # Mult. Max Mult. % Vecs mult. &gt; 1</cell></row><row><cell cols="2">32 ? 32 image 1,024</cell><cell>513</cell><cell>3</cell><cell>32</cell><cell>96.9</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>2,187</cell><cell>11</cell><cell>300</cell><cell>19.7</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>1,861</cell><cell>12</cell><cell>491</cell><cell>44.8</cell></row><row><cell cols="2">Amazon Photo 7,650</cell><cell>7,416</cell><cell>8</cell><cell>136</cell><cell>3.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>so h is permutation equivariant and sign invariant. If h is permutation equivariant and sign invariant, then define ?(v) = h(v)/2 again; it is clear that ? is continuous and permutation equivariant.Proposition 2. Any continuous, O(d) invariant h : R n?d ? R dout is of the form h(V ) = ?(V V ) for a continuous ?.For a compact domain Z ? R n?d , maps of the form V ? IGN(V V ) universally approximate continuous functions h : Z ? R n?d ? R n that are O(d) invariant and permutation equivariant. {V V : V ? Z} and let &gt; 0. Note that Z is compact, as it is the continuous image of a compact set. Since h is O(d) invariant, the first fundamental theorem of O(d) shows that there exists a continuous function ? :</figDesc><table><row><cell>Proof. The case without permutation equivariance holds by the First Fundamental Theorem of O(d)</cell></row><row><cell>(Lemma 2).</cell></row><row><cell>For the permutation equivariant case, let Z =</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>for some continuous O(d i ) invariant ? di and continuous ?. By the first fundamental theorem of O(d) (Lemma 2), each ? di can be written as ? di (</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>For the other direction, invariant theory shows that the O(d) invariant polynomials are generated by the inner productsv i v j , where v i ? R d are the rows of V (Kraft &amp; Procesi, 1996). Let p : R n?d ? R n?n be the map p(V ) = V V . Then<ref type="bibr" target="#b40">Gonz?lez &amp; de Salas (2003)</ref> Lemma 11.13 shows that the quotient space R</figDesc><table><row><cell>orthogonally</cell></row><row><cell>invariant.</cell></row></table><note>n?d /O(d) is homeomorphic to a closed subset p(R n?d ) = Z ? R n?n . Letp refer to this homeomorphism, and note thatp ? ? = p by passing to the quotient (Lemma 1). Then any continuous O(d) invariant f passes to a unique continuousf : R n?d /O(d) ? R dout (Lemma 1), so f =f ? ? where ? is the quotient map. Define h : Z ? R dout by h =f ?p ?1 , and note that h is a composition of continuous functions and hence continuous. Finally, we have that h(</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>1. the quotient map of any continuous group action is an open map, so each ? i is an open map, 2. the product of open maps is an open map, so ? 1 ? . . . ? ? k is an open map and 3. a continuous surjective open map is a quotient map, so ? 1 ? . . . ? ? k , which is continuous and surjective, is a quotient map.</figDesc><table><row><cell>Now, we need only apply the theorem of uniqueness of quotient spaces to show (51) (see e.g. Lee</cell></row><row><cell>(2013), Theorem A.31). Letting q :</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>). Let Z ? R d0 be a compact domain, let F 1 , . . . , F L be families of continuous functions where F i consists of functions fromR di?1 ? R di for some d 1 , . . . , d L . Let F be the family of functions {f L ? . . . f 1 : Z ? R d L , f i ? F i } that are compositions of functions f i ? F i .For each i, let ? i be a family of continuous functions that universally approximates F i . Then the family of compositions</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Results on the ZINC dataset with 500k parameter budget and no edge features. Numbers are the mean and standard deviation over 4 runs each with different seeds.All graph regression models inTable 1use edge features for learning and inference. To show that SignNet is also useful when no edge features are available, we ran ZINC experiments without edge features as well. The results are displayed inTable 7. In this setting, SignNet still significantly improves the performance over message passing networks without positional encodings, and over Laplacian positional encodings with sign flipping data augmentation.J.2 COMPARISON WITH DOMAIN SPECIFIC MOLECULAR GRAPH REGRESSION MODELS</figDesc><table><row><cell cols="2">Base model Positional encoding</cell><cell>k</cell><cell cols="2">#params Test MAE (?)</cell></row><row><cell></cell><cell cols="2">No PE 16</cell><cell>497k</cell><cell>0.348?0.014</cell></row><row><cell>GIN</cell><cell cols="2">LapPE (flip) 16</cell><cell>498k</cell><cell>0.341?0.011</cell></row><row><cell></cell><cell cols="2">SignNet 16</cell><cell cols="2">500k 0.238?0.012</cell></row><row><cell></cell><cell cols="2">No PE 16</cell><cell>501k</cell><cell>0.464?0.011</cell></row><row><cell>GAT</cell><cell cols="2">LapPE (flip) 16</cell><cell>502k</cell><cell>0.462?0.013</cell></row><row><cell></cell><cell cols="2">SignNet 16</cell><cell cols="2">499k 0.243?0.008</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>Comparison with domain specific methods on graph-level regression tasks. Numbers are test MAE, so lower is better. Best models within a standard deviation are bolded.</figDesc><table><row><cell></cell><cell cols="2">ZINC (10K) ? ZINC-full ?</cell></row><row><cell>HIMP  ? (Fey et al., 2020)</cell><cell>.151?.006</cell><cell>.036?.002</cell></row><row><cell cols="2">CIN-small  ? (Bodnar et al., 2021) .094?.004</cell><cell>.044?.003</cell></row><row><cell>CIN  ? (Bodnar et al., 2021)</cell><cell>.079?.006</cell><cell>.022?.002</cell></row><row><cell>SignNet (ours)</cell><cell>.084?.006</cell><cell>.024?.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Sum of squared errors for spectral graph convolution regression (with no test set). Lower is better. Numbers are mean and standard deviation over 50 images from.</figDesc><table><row><cell></cell><cell>Low-pass</cell><cell>High-pass</cell><cell cols="2">Band-pass Band-rejection</cell><cell>Comb</cell></row><row><cell>GCN</cell><cell cols="3">.111?.068 3.092?5.11 1.720?3.15</cell><cell cols="2">1.418?1.03 1.753?1.17</cell></row><row><cell>GAT</cell><cell>.113?.065</cell><cell cols="2">.954?.696 1.105?.964</cell><cell>.543?.340</cell><cell>.638?.446</cell></row><row><cell>GPR-GNN</cell><cell>.033?.032</cell><cell>.012?.007</cell><cell>.137?.081</cell><cell>.256?.197</cell><cell>.369?.460</cell></row><row><cell>ARMA</cell><cell>.053?.029</cell><cell>.042?.024</cell><cell>.107?.039</cell><cell>.148?.089</cell><cell>.202?.116</cell></row><row><cell>ChebNet</cell><cell>.003?.002</cell><cell>.001?.001</cell><cell>.005?.003</cell><cell>.009?.006</cell><cell>.022?.016</cell></row><row><cell>BernNet</cell><cell>.001?.002</cell><cell>.001?.001</cell><cell>.000?.000</cell><cell>.048?.042</cell><cell>.027?.019</cell></row><row><cell>Transformer</cell><cell cols="3">3.662?1.97 3.715?1.98 1.531?1.30</cell><cell cols="2">1.506?1.29 3.178?1.93</cell></row><row><cell>Transformer Eig Flip</cell><cell cols="3">4.454?2.32 4.425?2.38 1.651?1.53</cell><cell cols="2">2.567?1.73 3.720?1.94</cell></row><row><cell>Transformer Eig Abs</cell><cell cols="3">2.727?1.40 3.172?1.61 1.264?.788</cell><cell cols="2">1.445?.943 2.607?1.32</cell></row><row><cell>DeepSets SignNet</cell><cell>.004?.013</cell><cell>.086?.405</cell><cell>.021?.115</cell><cell>.008?.037</cell><cell>.003?.016</cell></row><row><cell>Transformer SignNet</cell><cell>.003?.016</cell><cell>.004?.025</cell><cell>.001?.004</cell><cell>.006?.023</cell><cell>.093?.641</cell></row><row><cell>DeepSets BasisNet</cell><cell>.009?.018</cell><cell>.003?.015</cell><cell>.008?.030</cell><cell>.004?.011</cell><cell>.015?.060</cell></row><row><cell>Transformer BasisNet</cell><cell>.079?.471</cell><cell>.014?.038</cell><cell>.005?.018</cell><cell>.006?.016</cell><cell>.014?.051</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Parameter settings for the texture reconstruction experiments. Params Base MLP width Base MLP layers ? out dim ? out dim ?, ? width</figDesc><table><row><cell cols="2">Intrinsic NF 328,579</cell><cell>128</cell><cell>6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SignNet</cell><cell>323,563</cell><cell>108</cell><cell>6</cell><cell>4</cell><cell>64</cell><cell>8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank anonymous reviewers of earlier versions of this work, especially those of the Topology, Algebra, and Geometry Workshop at ICML 2022, for providing useful feedback and suggestions. We thank Leonardo Cotta for a discussion about automorphism symmetries in real-world and random graphs (Appendix C.3 and C.4). We thank Truong Son Hy for sending us some useful PyTorch codes for invariant graph networks. Stefanie Jegelka and Suvrit Sra acknowledge support from NSF CCF-2112665 (TILOS AI Research Institute). Stefanie Jegelka also acknowledges support from NSF Award 2134108 and NSF Convergence Accelerator Track D 2040636 and NSF C-ACCEL D636 -CRIPT Phase 2. Suvrit Sra acknowledges support from NSF CAREER grant (IIS-1846088). Joshua Robinson is partially supported by a Two Sigma fellowship.</p><p>Proposition 4. SignNet and BasisNet can approximate node positional encodings based on heat kernels <ref type="bibr" target="#b34">(Feldman et al., 2022)</ref> and random walks <ref type="bibr" target="#b31">(Dwivedi et al., 2022)</ref>. BasisNet can approximate diffusion and p-step random walk relative positional encodings (Mialon et al., 2021), and generalized PageRank and landing probability distance encodings .</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. We will show that we can apply the above Proposition 5, by showing that all of these positional encodings are spectral graph convolutions. The heat kernel embeddings are of the form diag n i=1 exp(?t? i )v i v i for some choices of the parameter t, so they can be approximated by SignNets or BasisNets. Also, the diffusion kernel <ref type="bibr">(Mialon et al., 2021)</ref> is just the matrix of this heat kernel, and the p-step random walk kernel is</p><p>BasisNets can universally approximate both of these.</p><p>For the other positional encodings, we let v i be the eigenvectors of the random walk Laplacian I ? D ?1 A instead of the normalized Laplacian I ? D ?1/2 AD ?1/2 . The eigenvalues of these two Laplacians are the same, and if? i is an eigenvector of the normalized Laplacian then D ?1/2? i is an eigenvector of the random walk Laplacian with the same eigenvalue <ref type="bibr" target="#b84">(Von Luxburg, 2007)</ref>.</p><p>Then with v i as the eigenvectors of the random walk Laplacian, the random walk positional encodings (RWPE) in <ref type="bibr" target="#b31">Dwivedi et al. (2022)</ref> take the form</p><p>for any choices of integer k.</p><p>The distance encodings proposed in  take the form f 3 (AD ?1 , (AD ?1 ) 2 , (AD ?1 ) 3 , . . .),</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On weisfeiler-leman invariance: Subgraph counts and related graph properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikraman</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Fuhlbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2020" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The wave kernel signature: A quantum mechanical approach to shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Schlickewei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1626" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Isomorphism of graphs with bounded eigenvalue multiplicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Grigoryev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mount</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth annual ACM symposium on Theory of computing</title>
		<meeting>the fourteenth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="310" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How symmetric are real-world graphs? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geyer-Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Equivariant subgraph aggregation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2625" to="2640" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Residual gated graph ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Resolving the sign ambiguity in the singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evrim</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics: A Journal of the Chemometrics Society</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="140" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convergence of invariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10129</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09427</idno>
		<title level="m">A quantum chemistry dataset for benchmarking ai models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1589" to="15902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10383" to="10395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reconstruction for powerful graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constructing trees with given eigenvalues and angles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drago?</forename><surname>Cvetkovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some comments on the eigenspaces of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drago?</forename><surname>Cvetkovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Inst. Math.(Beograd)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">64</biblScope>
			<biblScope unit="page" from="24" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eigenspaces of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drago?</forename><surname>Cvetkovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rowlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Simic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Number 66 in Encyclopedia of Mathematics and its Applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Mohammed Haroon Dupty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09141</idno>
		<title level="m">Graph representation learning with individualization and refinement</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pf-gnn: Differentiable particle filtering based approximation of universal graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Mohammed Haroon Dupty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Sun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Deep Learning on Graphs: Methods and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph neural networks with learnable structural and positional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-validatory choice of the number of components from a principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ht Eastment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krzanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="77" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Asymmetric graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfr?d</forename><surname>R?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Math. Acad. Sci. Hungar</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Weisfeiler and leman go infinite: Spectral and combinatorial pre-colorings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Boyarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13410</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop on Representation Learning on Graphs and Manifolds)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Gin</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the power of combinatorial and spectral invariants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>F?rer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="page" from="2373" to="2380" />
		</imprint>
	</monogr>
	<note>Linear algebra and its applications</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Differential geometry and Lie groups: a computational perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Gallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Quaintance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer Nature</publisher>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navarro</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan B Sancho De</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1824</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">C ? -differentiable spaces</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A method for detecting structure in sociometric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social networks</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1977" />
			<biblScope unit="page" from="411" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R Johnson</forename><surname>Horn</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Density of states graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bindel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2021 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan G</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21824" to="21840" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Weisfeiler and leman go machine learning: The story so far</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09992</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Speqnets: Sparsityaware permutation-equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13913</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">De-anonymizing social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE symposium on security and privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Kova?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="808" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Global intrinsic symmetries of shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1341" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dropgnn: random dropouts increase the expressiveness of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolis</forename><surname>P?l Andr?s Papp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Martinkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Frame averaging for invariant and equivariant network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Universal equivariant multilayer perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7996" to="8006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Iam graph database repository for graph based pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Laplace-beltrami eigenfunctions for deformation invariant shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rustamov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on geometry processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automorphism group and spectrum of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stiebitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in pure mathematics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983" />
			<biblScope unit="page" from="587" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On universal equivariant set networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimrod</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Relational Representation Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On the equivalence between positional node embeddings and structural graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><forename type="middle">M</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Craig R Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohar</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bloom-Ackermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A concise and provably informative multi-scale signature based on heat diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer graphics forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Counting substructures with higher-order graph neural networks: Possibility and impossibility results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03174</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Multiscale graph comparison via the embedded laplacian distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edric</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dunson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12064</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Random matrices have simple spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="539" to="553" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Eigenvalues and automorphisms of a graph. Linear and Multilinear Algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuo</forename><surname>Teranishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Bau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>SIAM</publisher>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>Numerical linear algebra</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Netlsd: hearing the shape of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2347" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dual use of artificial-intelligencepowered drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Urbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippa</forename><surname>Lentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Invernizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ekins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="191" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scalars are universal: Equivariant machine learning, structured like classical physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Storey-Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Blum-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Equivariant and stable positional encoding for more powerful graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haorui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The self-intersections of a smooth n-manifold in 2n-space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassler</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of Mathematics</title>
		<imprint>
			<date type="published" when="1944" />
			<biblScope unit="page" from="220" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">What can neural networks reason about?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Identity-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10737" to="10745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Graph-BERT: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">RetGK: Graph kernels based on return probabilities of random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijian</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arye</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3964" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">From stars to subgraphs: Uplifting any GNN with local structure awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Further</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Details</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">HARDWARE, SOFTWARE, AND DATA DETAILS All experiments could fit on one GPU at a time. Most experiments were run on a server with 8</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">We run all of our experiments in Python, using the PyTorch (Paszke et al., 2019) framework (license URL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Nvidia Rtx 2080 Ti Gpus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Apache License 2.0), and PyTorch Geometric (PyG) (Fey &amp; Lenssen, 2019) (MIT License) for experiments with graph data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>We also make use of Deep Graph Library (DGL</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">MIT License), the images dataset used by Balcilar et al. (2020) (GNU General Public License v3.0), the cat mesh from free3d. com/3d-model/cat-v1--522281.html (Personal Use License), and the human mesh from turbosquid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irwin</surname></persName>
		</author>
		<ptr target="com/3d-models/water-park-slides-3d-max/1093267" />
	</analytic>
	<monogr>
		<title level="m">The data we use are all freely available online. The datasets we use are ZINC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>TurboSquid 3D Model License. If no license is listed, this means that we cannot find a license for the dataset</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
