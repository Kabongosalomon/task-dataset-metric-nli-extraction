<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Efficient Representations for Keyword Spotting with Triplet Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vygon</surname></persName>
							<email>rvygon@ntr.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Higher IT School</orgName>
								<orgName type="institution">Tomsk State University</orgName>
								<address>
									<postCode>634050</postCode>
									<settlement>Tomsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">NTR Labs</orgName>
								<address>
									<postCode>129594</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Efficient Representations for Keyword Spotting with Triplet Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Keyword Spotting</term>
					<term>Spoken Term Detection</term>
					<term>Triplet Loss</term>
					<term>kNN</term>
					<term>Representation Learning</term>
					<term>Audio Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past few years, triplet loss-based metric embeddings have become a de-facto standard for several important computer vision problems, most notably, person reidentification. On the other hand, in the area of speech recognition the metric embeddings generated by the triplet loss are rarely used even for classification problems. We fill this gap showing that a combination of two representation learning techniques: a triplet loss-based embedding and a variant of kNN for classification instead of cross-entropy loss significantly (by 26% to 38%) improves the classification accuracy for convolutional networks on a Li-briSpeech-derived LibriWords datasets. To do so, we propose a novel phonetic similarity based triplet mining approach. We also improve the current best published SOTA for Google Speech Commands dataset V1 10+2 -class classification by about 34%, achieving 98.55% accuracy, V2 10+2-class classification by about 20%, achieving 98.37% accuracy, and V2 35-class classification by over 50%, achieving 97.0% accuracy. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of keyword spotting is to detect a relatively small set of predefined keywords in a stream of user utterances, usually in the context of small-footprint device <ref type="bibr" target="#b0">[1]</ref>. Keyword spotting (KWS for short) is a critical component for enabling speech-based user interactions for such devices <ref type="bibr" target="#b1">[2]</ref>. It is also important from an engineering perspective for a wide range of applications <ref type="bibr" target="#b2">[3]</ref>. In this article we show how the use of the triplet loss-based embeddings allows us to improve the classification accuracy of the existing small-footprint neural network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous work on KWS</head><p>The first work on KWS was most likely published in 1967 <ref type="bibr" target="#b3">[4]</ref>. Over years, a number of machine learning architectures for small-footprint KWS have been proposed (see, for example <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>. With the renaissance of neural networks, they become the architecture class of choice for KWS systems (see, for example, <ref type="bibr" target="#b0">[1]</ref>[2] <ref type="bibr" target="#b9">[10]</ref>[11] <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref>). Probably, the only -but notable -exception from this trend is the very recent work of Lei et al. <ref type="bibr" target="#b14">[15]</ref> that uses Tsetlin machines for keyword spotting for their extremely low power consumption. Publication of the Google Speech Command dataset <ref type="bibr" target="#b15">[16]</ref> have provided a common ground for KWS system evaluation and allowed for accelerating research. Further, we denote V1 and V2 versions 1 and 2 of this dataset, respectively. When publishing the dataset, Warden <ref type="bibr" target="#b15">[16]</ref> have also provided a baseline model based on the convolutional architecture of Sainath and Parada <ref type="bibr" target="#b10">[11]</ref>, achieving the accuracy of 85.4% and 88.2% on V1 and V2, respectively. The related Kaggle competition winner has achieved 91% accuracy on V1.</p><p>Since the publication of the Google Speech Command dataset led to a vast corpus of work appearing in the past three years, we will only briefly discuss the most relevant recent work. Jansson <ref type="bibr" target="#b16">[17]</ref> suggested an interesting fully-convolutional model working out of raw waveforms, but, probably, a bit ahead of time and did not improve on previous results. de Andrade et al. <ref type="bibr" target="#b2">[3]</ref> have proposed an attention-based recurrent network architecture and achieved the SOTA on 2, 10, 20-word and full-scale versions of the dataset. Majumdar and Ginsburg <ref type="bibr" target="#b17">[18]</ref> have published a lightweight separable convolution residual network architecture MatchboxNet, achieving the new SOTA of 97.48% on V1 and 97.63% on V2. Mordido et al. <ref type="bibr" target="#b18">[19]</ref> have suggested an interesting improvement to MatchboxNet model, replacing 1x1-convolutions in 1D time-channel separable convolutions by constant, sparse random ternary matrices with weights in {-1; 0; +1}.</p><p>Rybakov, Kononenko et al. <ref type="bibr" target="#b19">[20]</ref> tested many of the existing models and proposed a multihead attention-based recurrent neural network architecture, achieving a new SOTA of 98% on V2. Wei et al. <ref type="bibr" target="#b20">[21]</ref> proposed a new architecture, EdgeCRNN, which is based on depthwise separable convolution and residual structure, apparently drawing inspiration from MatchboxNet <ref type="bibr" target="#b17">[18]</ref> and Attention RNN <ref type="bibr" target="#b2">[3]</ref>, to achieve a slight improvement in accuracy and a SOTA of 98.05%. Tang et al. <ref type="bibr" target="#b21">[22]</ref> have released Howl -a productionalized, open-source wakeword detection toolkit, explored a number of models and achieved nearly-SOTA accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.2</head><p>Previous work on the use of triplet loss for the metric embedding learning</p><p>The goal of metric embedding learning is to learn a function : ? , which maps semantically similar points from the data manifold in onto metrically close points in , and semantically different points in onto metrically distant points in <ref type="bibr" target="#b22">[23]</ref>. The triplet loss for this problem was most likely first introduced in <ref type="bibr" target="#b23">[24]</ref> in the framework of image ranking:</p><formula xml:id="formula_0">( , , ) = 0, + ( ), ( ) ? ( ), ( )<label>(1)</label></formula><p>where , , are the anchor image, positive image, and negative image, respectively, is a gap parameter that regularizes the gap between the distance of the two image pairs: ( , ) and ( , ), and is a distance function that can be, for example, Euclidean distance in the image embedding space:</p><formula xml:id="formula_1">( ), ( ) = ? ( ) ? ( )?<label>(2)</label></formula><p>A similar loss function was earlier proposed by Chechik et al. in <ref type="bibr" target="#b24">[25]</ref>, but the real traction came to the triplet loss in the area of face re-identification after the works of Schroff et al. on FaceNet <ref type="bibr" target="#b25">[26]</ref> and Hermans et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>In the speech domain, the use of triplet loss is more limited, but there still are several important works to mention. In particular, Huang J. et al <ref type="bibr" target="#b26">[27]</ref>, Ren et al. <ref type="bibr" target="#b27">[28]</ref>, Kumar et al. <ref type="bibr" target="#b28">[29]</ref>, and Harvill et al. <ref type="bibr" target="#b29">[30]</ref> use triplet loss with varied neural network architectures for the task of the speech emotion recognition. Bredin <ref type="bibr" target="#b30">[31]</ref> and Song et al. <ref type="bibr" target="#b31">[32]</ref> use triplet-loss based learning approaches for the speaker diarization, and Zhang and Koshida <ref type="bibr" target="#b32">[33]</ref> and Li et al. <ref type="bibr" target="#b33">[34]</ref> -for the related task of speaker verification. Turpault et al. <ref type="bibr" target="#b34">[35]</ref> propose a strategy for augmenting data with transformed samples, in line with more recent works in varied machine learning areas.</p><p>The most similar works to ours are probably <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and <ref type="bibr" target="#b39">[40]</ref>, but there are important differences with each of these works:</p><p>? Sacchi et al. <ref type="bibr" target="#b35">[36]</ref> operate in the open-vocabulary setting, which required the authors to design a system with a common embedding for text and speech, while we concentrate on improving the quality of existing low-footprint architectures for closed-vocabulary keyword spotting ? Shor et al. <ref type="bibr" target="#b36">[37]</ref> concentrate on building an unified embedding that works well for non-semantic tasks, while we concentrate on the semantic task of keyword spotting ? Yuan et al. <ref type="bibr" target="#b37">[38]</ref> operate in a two-stage detection / classification framework and use a BLSTM network with a mix of triplet, reverse triplet and hinge loss ? Huh et al. <ref type="bibr" target="#b38">[39]</ref> start from the same res15 model as we do, but primarily focus on detection metrics and use SVM for classification, so our classification metrics are significantly better ? Huang et. al <ref type="bibr" target="#b39">[40]</ref> concentrate on Query-by-Example KWS application and adopt the softtriple loss -a combination of triplet loss and softmax loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our contributions</head><p>Our contributions in this work are the following:</p><p>? We show that combining two representation learning methods: triplet-loss based metric embeddings and a kNN classifier allows us to significantly improve the accuracy of CNN-based models that use cross-entropy to classify audio information and achieve the SOTA for the Google Speech Commands dataset ? We propose a novel batch sampling approach based on phonetic similarity that allows to improve F1 metric when classifying highly imbalanced datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architectures</head><p>Most of the current state-of-the-art keyword spotting architectures are present in the work of Rybakov et al. <ref type="bibr" target="#b19">[20]</ref>, with the best model to date being the Bidirectional GRUbased Multihead Attention RNN. It takes a mel-scale spectrogram and convolves it with a set of 2D convolutions. Then two bidirectional GRU layers are used to capture twoway long term dependencies. The feature in the center of the bidirectional LSTM's output sequence is projected using a dense layer and is used as a query vector for the multi-head attention (4 heads) mechanism. Finally, the weighted (by attention score) average of the bidirectional GRU output is processed by a set of fully connected layers for classification. We have mostly experimented with ResNet-based models res8 <ref type="bibr" target="#b21">[22]</ref>[1] and res15 <ref type="bibr" target="#b40">[41]</ref>[1]. The initial experiments have shown that RNN-based architectures show significantly worse results when trained for the triplet loss, so they were discarded in our later work. We used the encoder part of each of the models above to generate triplet-loss based embeddings, that are later classified using the K-Nearest Neighbor (kNN) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Input Preprocessing 64-dimensional (for LibriWords) or 80-dimensional (for Google Speech Commands dataset) mel-spectrograms are constructed and stacked using a 25-millisecond window size and a 10-millisecond frame shift. Our implementation stacks all such windows within the one-second sample of Google Speech Commands. LibriWords samples are constrained to have a duration of 0.1-3 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Resnet architecture</head><p>Our resnet implementation is taken directly from <ref type="bibr" target="#b40">[41]</ref> with very minor code changes and is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. When working with triplet loss, the softmax layer is removed.  3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and tasks</head><p>SpeechCommands. Google Speech Commands dataset Version 1 has 65K utterances from various speakers, each utterance 1 second long. Each of these utterances belongs to one of 30 classes corresponding to common words like "Go", "Stop", "Left", "Down", etc. Version 2 has 105K utterances, each 1 second long, belonging to one of 35 classes. The sampling rate of both datasets is 16kHz. In our experiments we have considered the following tasks based on these datasets <ref type="bibr" target="#b2">[3]</ref>[16]:</p><p>? Recognition of all 35 words using Google Speech Dataset V2 ? Recognition of 10 words ("Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop", and "Go") and additional labels for "Unknown" and "Silence" using either V1 or V2 datasets.</p><p>For these tasks and each architecture studied we have measured top-1 classification accuracy.</p><p>LibriWords Datasets. To further explore the possibilities of triplet loss models we needed a dataset that consists of a large number of different words to classify. Thus, we have used LibriSpeech [42] -a collection of 1,000 hours of read English speech. The dataset was split on the word level by Lugosch et al. <ref type="bibr" target="#b42">[43]</ref>. Since LibriSpeech is aligned on sentence level only, the Montreal Forced Aligner <ref type="bibr" target="#b43">[44]</ref> was used to obtain intervals for individual words. The alignments are available online <ref type="bibr" target="#b42">[43]</ref>. Further we call this derived dataset LibriWords. We have created four different versions of the dataset (LibriWords10, LibriWords100, LibriWords1000, LibriWords10000) that correspond to the first 10, 100 etc. words by popularity in the LibriSpeech 1000h corpus. For example, the LibriWords10 words are: "the", "and", "of", "to", "a", "in", "he", "I", "that", "was".</p><p>Durations of the words range from 0.03 seconds to 2.8 seconds, with mean duration of 0.28 seconds. The details on the datasets metrics are available in the Appendix 1. We have split the dataset into train\val\test in in 8:1:1 proportion, and tried to make sure this proportion holds for each word in the dataset. We release NeMo-like manifests for ease of use and reproduction. Since the motivation behind the dataset is to model reallife speech recognition scenarios, there was no further quality assurance on the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Approach to training models Batch sampling. When working with Speech Commands and LibriWords10 datasets, to ensure a meaningful representation of the anchor-positive distances, following <ref type="bibr" target="#b25">[26]</ref>, we sample an equal number of objects from all the classes available. For unbalanced datasets with a large number of words, we also needed an efficient class-sampling method, otherwise the network will often train on irrelevant batches where embeddings of the words are already far from each other. To achieve better class selection we have used three sampling approaches:</p><p>? Uniform: sample batch_size classes randomly from a uniform distribution. ? Proportional: sample batch_size classes randomly from a distribution proportional to the word distribution in the dataset. Motivation behind this approach is twofold. First, the popular words are short (the, a, I)) so they are not easy to distinguish from the rest. Second, if you equally train on them, there will be the same amount of errors, and that's a lot in terms of the absolute value. (If we classify 2% of a popular word incorrectly, this would significantly spoil the metric for the entire dataset). ? Phonetic: Calculate a matrix of phonetic similarity for all the words in the dataset, sample batch_size/2 classes, then, for each sampled class add three random phonetically similar words (equally distributed) to the batch. Similarity score is calculated using SoundEx, Caverphone, Metaphone and NYSIIS algorithms <ref type="bibr" target="#b44">[46]</ref>.</p><p>Comparing phonetic distance methods. To compare the phonetic similarity algorithms, a model was trained using phonetic sampling only, while the similarity matrix was calculated with each of the methods separately. For each phonetic similarity algorithm, we have trained a model for two epochs on LibriWords10000 dataset. The results are listed in the <ref type="table" target="#tab_3">Table 4</ref>. While the difference between the algorithms is not large, Metaphone leads in both the accuracy and F1. Phonetic distance. On LibriWords dataset, we used a weighted average of distances calculated using all 4 algorithms weighted as follows:</p><formula xml:id="formula_2">= * 0.2 + * 0.2 + * 0.5 + * 0.1</formula><p>The weights reasonably reflect the efficiency of each method as per <ref type="table" target="#tab_1">Table 2</ref>. The optimal use of these algorithms is a matter of future research, for example, we had to adjust manually the distances of a handful of pairs of words: e.g. the pair "know-no" had a large distance while being similar. The problem was found while analyzing the confusion matrix.</p><p>We have evaluated these three triplet mining approaches alone and in combinations, mixing them with equal probabilities. Thus, for example, Uniform+Phounetic in the <ref type="table" target="#tab_2">Table 3</ref> below means 50% probability to sample with the Uniform approach, and 50% with the Phonetic approach, and Uniform+Proportional +Phonetic means 1/3 probability to sample with the Uniform approach, 1/3 with the Phonetic approach, and 1/3 with the Proportional approach.</p><p>The results in the <ref type="table" target="#tab_2">Table 3</ref> show that the proportional sampling method improves the accuracy by increasing the score of more popular words while the phonetic sampling method improves the F1 metric due to better classification of difficult pairs like "at"-"ate", "an"-"anne". Uniform sampling usage is essential as one of the sampling strategies, as it provides the proper class coverage. Triplet selection. An important part of TL models is the selection of triplets used to calculate the loss, since taking all possible triplets from a batch is computationally expensive. We have used a randomized approach to the online batch triplet mining based on <ref type="bibr" target="#b22">[23]</ref>, where the negative sample to a hard pair of the anchor and a positive sample is selected randomly from the set of negative samples resulting in non-zero loss. Our initial experiments have shown that this modification of the online batch triplet mining performs better than hard or semi-hard batch loss options.</p><p>Optimization and training process. Baseline models were trained until they reached a plateau on a validation set. We monitored the validation accuracy of triplet loss models each 1k batches and stopped the training process if the accuracy didn't increase for more than .1% for 3 consecutive times. The number of epochs is listed in the <ref type="table" target="#tab_3">Table 4</ref> below. The decrease in epochs for larger datasets is due to class-imbalance -triplet models sample classes directly, so instead of seeing all objects in the dataset it sees the same number of objects, but distributed more evenly between classes. The baseline, crossvalidation based models converge to predict the most popular words well, while ignoring the rest. One can see this from the low F1 metric on LibriWords10000 dataset. The batch size was 35*10 for TL-res8, 35*4 for TL-res15 and 128 for the baseline models.</p><p>Training was done using the Novograd <ref type="bibr" target="#b45">[47]</ref> algorithm with initial learning rate of 0.001 and cosine annealing decay to 1e-4.</p><p>Influence of kNN. We have tested kNN for several values of k, and have found that for LibriWords the best performing value varies depending on the dataset size, while for Speech Commands the best performing value was k=5 (see <ref type="table" target="#tab_4">Table 5</ref>).</p><p>As the model size is of a great concern for the keyword spotting application, and for the larger datasets kNN part of the model can take a lot of memory, we have also studied the effect of kNN quantization available from <ref type="bibr" target="#b11">[12]</ref> on the size, speed and accuracy of the resulting model, varying the number of segments for the Product Quantizer. For each dataset/task there is an optimal number of segments that reduces accuracy by 1.6% -13.6%, and reduces the memory consumption by a factor of 7 to 13.</p><p>We should note that the use of kNN is essential for the accuracy we achieve. We have tried to replace kNN with a two-layer fully-connected network with ReLU between the layers, but the results were drastically worse. Specifically, we experimented with intermediary dimensions of 64, 128 and 256 between the two fully connected layers (see <ref type="table" target="#tab_5">Table 6</ref>). We have frozen the same triplet loss based encoder as used with kNN and optimized each fully-connected decoder with a cross-entropy loss using Novograd optimizer for 30 epochs with cosine annealing from 1e-3 to 8e-5. The resulting accuracy was around 90% and F1 around 82% independently of the intermediary dimension. This means that the embeddings generated by the triplet loss are not linearly separable and using kNN is really critical for high-quality decoding the triplet loss embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The results below were obtained by training a model for 3 different runs in each scenario and averaging the results to avoid the "lucky seed" effect. We can see that triplet loss + kNN based models provide better accuracy than baseline ones, achieve state of the art results on Speech Commands dataset, while being more lightweight and faster in convergence than the mh-att-rnn <ref type="bibr" target="#b19">[20]</ref> model. In particular, triplet loss + kNN based models improve the accuracy on the datasets studied by 25% to 38% and F1 measure by 16% to 57% compared to extremely strong crossentropy based baselines (see <ref type="table" target="#tab_6">Table 7</ref>). The bigger the number of classes in the dataset, the bigger the difference between crossentropy and triplet loss based classifiers. Our res15 network trained with triplet loss and kNN classifier achieves state of the art on Google Speech Commands datasets V2/35, V2/12 and V1/12, improving the best previously published results <ref type="bibr" target="#b2">[3]</ref>[21] <ref type="bibr" target="#b21">[22]</ref> by 50%, 16% and 34% respectively ( <ref type="table" target="#tab_7">Table 8</ref>).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>res* architecture (from [1])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 1</head><label>11</label><figDesc>Encoder model sizes for the key models studied. above compares the model sizes for the main models studied.</figDesc><table><row><cell></cell><cell>Embedding dimension</cell><cell>Model encoder size, [K]</cell></row><row><cell>Mh-Att-RNN</cell><cell>256</cell><cell>743</cell></row><row><cell>res8</cell><cell>128</cell><cell>885</cell></row><row><cell>res15</cell><cell>45</cell><cell>237</cell></row><row><cell>Att-RNN</cell><cell>128</cell><cell>202</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Phonetic similarity metric comparison</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>F1</cell></row><row><cell>CaverPhone</cell><cell>64.0</cell><cell>41.0</cell></row><row><cell>NYSIIS</cell><cell>63.8</cell><cell>40.5</cell></row><row><cell>Soundex</cell><cell>64.8</cell><cell>43.2</cell></row><row><cell>Metaphone</cell><cell>65.3</cell><cell>43.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The effects of the different sampling strategies for triplet loss of res15 model on LibriWords10000</figDesc><table><row><cell>Method(s)</cell><cell>Accuracy</cell><cell>F1</cell></row><row><cell>Uniform</cell><cell>79.4</cell><cell>0.72</cell></row><row><cell>Proportional</cell><cell>77.1</cell><cell>0.61</cell></row><row><cell>Phonetic</cell><cell>76.9</cell><cell>0.73</cell></row><row><cell>Uniform+Phonetic</cell><cell>78.9</cell><cell>0.76</cell></row><row><cell>Uniform+Proportional</cell><cell>81.2</cell><cell>0.74</cell></row><row><cell>Proportional+Phonetic</cell><cell>80.0</cell><cell>0.72</cell></row><row><cell>Uniform+Proportional +Phonetic</cell><cell>80.8</cell><cell>0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The number of epochs models were trained for</figDesc><table><row><cell>TL, epochs</cell><cell>Baseline, epochs</cell><cell></cell></row><row><cell>Speech Commands</cell><cell>30</cell><cell>30</cell></row><row><cell>Libri10</cell><cell>10</cell><cell>30</cell></row><row><cell>Libri100</cell><cell>5</cell><cell>10</cell></row><row><cell>Libri1000</cell><cell>5</cell><cell>7</cell></row><row><cell>Libri10000</cell><cell>3</cell><cell>5</cell></row><row><cell>Three augmentation techniques were used:</cell><cell></cell><cell></cell></row><row><cell>1. Shifting samples in range (-100ms; +100ms).</cell><cell></cell><cell></cell></row><row><cell>2. SpecAugment.</cell><cell></cell><cell></cell></row></table><note>3. Adding background noise from audio files in Google Speech Commands Dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Classification accuracy for res15 model triplet loss embeddings with kNN classification for various k</figDesc><table><row><cell>k</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>30</cell></row><row><cell cols="2">Speech Commands V2 / 12 98.18</cell><cell>98.37</cell><cell>98.27</cell><cell>98.29</cell></row><row><cell>LW10</cell><cell>89.91</cell><cell>91.48</cell><cell>91.74</cell><cell>91.72</cell></row><row><cell>LW100</cell><cell>83.93</cell><cell>86.53</cell><cell>86.9</cell><cell>86.98</cell></row><row><cell>LW1000</cell><cell>80.43</cell><cell>83.82</cell><cell>84.29</cell><cell>84.37</cell></row><row><cell>LW10000</cell><cell>77.57</cell><cell>80.82</cell><cell>81.17</cell><cell>80.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Classification accuracy and F1 for res15 model triplet loss embeddings with kNN and fully-connected network classifiers</figDesc><table><row><cell>Metric</cell><cell>kNN</cell><cell>FC64</cell><cell>FC128</cell><cell>FC256</cell></row><row><cell>Accuracy</cell><cell>98.37</cell><cell>90.04</cell><cell>89.88</cell><cell>90.16</cell></row><row><cell>F1</cell><cell>0.98</cell><cell>0.822</cell><cell>0.821</cell><cell>0.825</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of accuracy and F1 measure of triplet loss and crossentropy loss based res15 models</figDesc><table><row><cell>Task</cell><cell cols="2">Triplet Loss Accuracy, %</cell><cell>F1</cell><cell cols="2">Crossentropy Accuracy, % F1</cell><cell cols="2">Relative improve-ment Accuracy, % F1,%</cell></row><row><cell>Speech Commands V2 35</cell><cell>97.0</cell><cell cols="2">0.965</cell><cell>95.96</cell><cell>0.955</cell><cell>25.74</cell><cell>22.22</cell></row><row><cell>Speech Commands V2 12</cell><cell>98.37</cell><cell cols="2">0.980</cell><cell>97.8</cell><cell>0.963</cell><cell>25.91</cell><cell>45.95</cell></row><row><cell>Speech Commands V1 12</cell><cell>98.56</cell><cell cols="2">0.978</cell><cell>97.7</cell><cell>0.967</cell><cell>37.39</cell><cell>33.33</cell></row><row><cell>LibriWords10</cell><cell>91.7</cell><cell></cell><cell>0.90</cell><cell>88.8</cell><cell>0.88</cell><cell>26.25</cell><cell>16.67</cell></row><row><cell>LibriWords100</cell><cell>86.9</cell><cell></cell><cell>0.87</cell><cell>82.3</cell><cell>0.81</cell><cell>25.99</cell><cell>31.58</cell></row><row><cell>LibriWords 1000</cell><cell>84.3</cell><cell></cell><cell>0.86</cell><cell>78.2</cell><cell>0.78</cell><cell>27.94</cell><cell>36.36</cell></row><row><cell>LibriWords 10000</cell><cell>81.2</cell><cell></cell><cell>0.75</cell><cell>69.3</cell><cell>0.41</cell><cell>38.66</cell><cell>57.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Model accuracy comparison on Google Speech Commands dataset tasks</figDesc><table><row><cell>Model</cell><cell>Loss</cell><cell>Model Size, KB</cell><cell>V2 35 accuracy</cell><cell>V2 12 accuracy</cell><cell>V1 12 accuracy</cell></row><row><cell>res8 (ours)</cell><cell>Triplet Crossentropy</cell><cell>901 885</cell><cell>95.33 95.25</cell><cell>97.48 97.39</cell><cell>96.03</cell></row><row><cell>res15 (ours)</cell><cell>Triplet Crossentropy</cell><cell>252 237</cell><cell>97.0 95.96</cell><cell>98.37 97.8</cell><cell>98.56 97.7</cell></row><row><cell>EdgeCRNN [21]</cell><cell>Crossentropy</cell><cell></cell><cell></cell><cell>98.05</cell><cell></cell></row><row><cell>Mh-Att-RNN [20]</cell><cell>Crossentropy</cell><cell>743</cell><cell></cell><cell>98.0</cell><cell></cell></row><row><cell cols="2">Attention RNN [3] Crossentropy</cell><cell>202</cell><cell>93.9</cell><cell></cell><cell></cell></row><row><cell>res8 (Howl) [22]</cell><cell>Crossentropy</cell><cell></cell><cell></cell><cell></cell><cell>97.8</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/roman-vygon/triplet_loss_kws</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to ? colleagues at NTR Labs Machine Learning Research group for the discussions and support; ? Prof. Sergey Orlov and Prof. Oleg Zmeev for the computing facilities provided; ? Nikolay Shmyrev for pointing out to the works <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>ArXiv 1710.10361</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>arXiv 1711.07128</idno>
		<title level="m">Hello Edge: Keyword Spotting on Microcontrollers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno>arXiv 1808.08929</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experimental, limited vocabulary, speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kellett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Focht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="130" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Continuous hidden Markov modeling for speakerindependent word spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rohlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="627" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Phoneme based acoustics keyword spotting in informal continuous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Szoke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<idno type="DOI">10.1007/11551874_39</idno>
		<editor>Matousek V., Mautner P., Pavelka T. Text, Speech and Dialogue</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="302" to="309" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved mandarin keyword spotting using confusion garbage model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="page" from="3700" to="3703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech Keyword Spotting with Rule Based Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greibus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Telksnys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information and Software Technologies. ICIST 2013</title>
		<editor>Skersys T., Butleris R., Butkiene R.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An integrated system for voice command recognition and emergency detection based on audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Principi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Squartini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bonfigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piazza</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2015.02.036</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5668" to="5683" />
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05390</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Max-pooling loss training of long short-term memory networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="474" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Streaming Small-Footprint Keyword Spotting using Sequence-to-Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09617</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Low-Power Audio Keyword Spotting using Tsetlin Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2101.11336" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speech commands: A public dataset for single-word speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Single-word speech recognition with Convolutional Neural Networks on raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jansson</surname></persName>
		</author>
		<imprint>
			<pubPlace>Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information technology, ARCADA University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Degree Thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ginsburg</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531</idno>
		<title level="m">MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Compressing 1D Time-Channel Separable Convolutions using Sparse Random Ternary Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mordido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Keirsbilck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2103.17142" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenzo</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06720</idno>
		<title level="m">Streaming keyword spotting on mobile devices</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EdgeCRNN: an edge-computing oriented model of acoustic feature enhancement for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Humaniz. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Howl: A Deployed, Open-Source Wake Word Detection System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bicking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09606</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermans_A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leibe</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning finegrained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4661</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philbin</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="3673" to="3677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-modal Correlated Network for emotion recognition in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="150" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end Triplet Loss based Emotion Embedding System for Speech Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06200</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retrieving speech samples with similar emotional content using a Triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harvill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotfian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8683273</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7400" to="7404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04301</idno>
		<title level="m">Tristounet: triplet loss for speaker turn embedding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Willi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01535</idno>
		<title level="m">Triplet Network with Attention for Speaker Diarization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1608</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1487" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02304</idno>
		<title level="m">Deep Speaker: an Endto-End Neural Speaker Embedding System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised triplet loss based learning of ambient audio embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">2025824</biblScope>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Open-Vocabulary Keyword Spotting with Audio and Text Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cer?ak</surname></persName>
		</author>
		<idno type="DOI">3362-3366.10.21437/Interspeech.2019-1846</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Havivy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12764</idno>
		<title level="m">Towards Learning a Universal Non-Semantic Representation of Speech</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Verifying Deep Keyword Spotting Detection with Acoustic Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU46091.2019.9003781</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Autom. Speech Recognit. Underst. Work. ASRU 2019 -Proc</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="613" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Metric learning for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.087762020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Query-by-Example Keyword Spotting system using Multi-head Attention and Softtriple Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2102.07061" />
		<imprint>
			<date type="published" when="2021-03-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Honk: A PyTorch reimplementation of convolutional neural networks for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.06554" />
		<imprint>
			<date type="published" when="2017-10-17" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv. arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2015-08" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Speech model pre-training for endto-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</title>
		<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="814" to="818" />
		</imprint>
	</monogr>
	<note>2019-Sept</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Montreal forced aligner: Trainable text-speech alignment using kaldi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Socolof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mihuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sonderegger</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1386</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="498" to="502" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Do your resources sound similar?: On the impact of using phonetic similarity in link discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C N</forename><surname>Ngomo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3360901.3364426</idno>
	</analytic>
	<monogr>
		<title level="m">K-CAP 2019 -Proceedings of the 10th International Conference on Knowledge Capture</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.11286" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbdata.2019.2921572</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
