<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
							<email>zouyanyan6@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Zhang</surname></persName>
							<email>zhanghainan1990@163.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
							<email>dingzhuoye@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caixia</forename><surname>Yuan</surname></persName>
							<email>yuancx@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
							<email>xjwang@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unlike well-structured text, such as news reports and encyclopedia articles, dialogue content often comes from two or more interlocutors, exchanging information with each other. In such a scenario, the topic of a conversation can vary upon progression and the key information for a certain topic is often scattered across multiple utterances of different speakers, which poses challenges to abstractly summarize dialogues. To capture the various topic information of a conversation and outline salient facts for the captured topics, this work proposes two topic-aware contrastive learning objectives, namely coherence detection and sub-summary generation objectives, which are expected to implicitly model the topic change and handle information scattering challenges for the dialogue summarization task. The proposed contrastive objectives are framed as auxiliary tasks for the primary dialogue summarization task, united via an alternative parameter updating strategy. Extensive experiments on benchmark datasets demonstrate that the proposed simple method significantly outperforms strong baselines and achieves new state-of-the-art performance. The code and trained models are publicly available via https://github.com/Junpliu/ConDigSum. * Work done at JD.com. Julia: Where are you? Hania: That's a good question, haha ?? Hania: Don't even tell me, I have been on the road for 3 hours already Julia: I know how you feel love, I am sick of trains already :( Hania: I will be there around 7pm I guess :( Julia: I will be waiting! :* Hania: Great! Julia: You must be starving, I am gonna make some food. What would you like? ?? Hania: Or actually maybe we will order some takeaway? Julia: Sounds like a plan :) pizza or burgers? Hania: Pizza always :D ?? (t 1 ) Hania has been traveling for 3 hours already. (t 2 ) She will get there around 7pm. (t 3 ) Julia will order takeaway pizza for her.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online conversations have become an indispensable manner of communication in our daily work and life. In the era of information explosion, it is paramount to present the most salient facts of conversation content, rather than lengthy utterances, which is useful for online customer service <ref type="bibr" target="#b26">(Liu et al., 2019a)</ref> and meeting summary <ref type="bibr" target="#b44">(Zhao et al., 2019)</ref>. This work focuses on abstractive dialogue summarization. To summarize dialogues, one simple way is to directly apply existing document summarization models to dialogues <ref type="bibr" target="#b37">(Shang et al., 2018;</ref><ref type="bibr"></ref> Figure 1: A dialogue and its paired summary. S 1 , S 2 , and S 3 stands for referred topic snippets, current situation, time of arrival and food to eat, respectively. The corresponding summary consists of three sentences t 1 , t 2 and t 3 . Each t i corresponds to one snippet S i (i = 1, 2, 3). S 4 and S 5 are inter-topic snippets. <ref type="bibr" target="#b12">Gliwa et al., 2019)</ref> or to employ hierarchical models to capture features from different turns of different speakers <ref type="bibr" target="#b44">(Zhao et al., 2019;</ref><ref type="bibr" target="#b47">Zhu et al., 2020b)</ref>. However, succinctly summarizing the dialogue is much more challenging.</p><p>The well-structured textual descriptions, such as news reports <ref type="bibr" target="#b36">(See et al., 2017)</ref> and academic papers <ref type="bibr" target="#b31">(Nikolov et al., 2018)</ref>, often come from one single speaker or writer where the information flow is more natural and clearer with paragraphs and sections. Differently, consisting of multiple utterances from two or more interlocutors, the conversational content is in a complicated flow with information exchange and the focused topic can vary upon the conversation progression. On the other hand, the salient information for a specific topic is often scattered across multiple utterances arXiv:2109.04994v1 [cs.CL] 10 Sep 2021 and can be presented separately. Exemplified by <ref type="figure">Figure 1</ref>, this dialogue touches three topics, current situation, time of arrival and food to eat, where the corresponding topic snippets are S 1 , S 2 and S 3 , respectively. The central ideas of each topic is summarized with one sentence, covering information from multiple utterances, i.e., t 1 for S 1 , t 2 for S 2 , and t 3 for S 3 . We also observe that utterances residing in the same topic (e.g., S 1 , S 2 and S 3 ) is inherently more coherent than those coming from different topics (e.g., the inter-topic snippet S 4 and S 5 ), which reveals the underlying relationships between topic and utterance coherence, also demonstrated by <ref type="bibr" target="#b11">Glava? and Somasundaran (2020)</ref>.</p><p>Recent studies involves intrinsic information of dialogues to handle the challenges for summarizing dialogues, such as topic segment features <ref type="bibr" target="#b29">(Liu et al., 2019b;</ref><ref type="bibr" target="#b7">Li et al., 2019;</ref><ref type="bibr" target="#b1">Chen and Yang, 2020)</ref>, dialogue acts <ref type="bibr" target="#b13">(Goo and Chen, 2018)</ref> and conversation stages <ref type="bibr" target="#b1">(Chen and Yang, 2020)</ref>. Although such existing models have demonstrated the effectiveness of the dialogue analysis on generating summaries, additional human efforts in data annotations or extra topic segmentation algorithms are necessary. For example, <ref type="bibr" target="#b13">Goo and Chen (2018)</ref>; <ref type="bibr" target="#b26">Liu et al. (2019a)</ref> require extensive expert annotations on dialogue acts, while the knowledge of visual focus of each speaker and topic segment is a must for <ref type="bibr" target="#b7">Li et al., 2019'</ref>s work, which are both expensive and sometimes hard to obtain. <ref type="bibr" target="#b29">Liu et al. (2019b)</ref>; <ref type="bibr" target="#b1">Chen and Yang (2020)</ref> need extra algorithms to obtain topic segment information, which works with the primary summarization model in a pipeline manner and thus may cause error propagation. Different from the structured text where a paragraph or a section can be treated as natural topic segment, it is difficult to accurately segment topics of dialogues.</p><p>Recall the inherent relationships between the topic and utterance coherence, this work proposes to implicitly capture the dialogue topic information by modeling the utterance coherence in a contrastive way. The coherence detection objective is constructed to push the model to focus more on snippets that are more coherent and likely contain salient information from the same topics. Further, since we aim to generate better summaries for each topic in a dialogue, we also introduce the subsummary generation objective, which is expected to force the model to identify the most salient information and generate corresponding summaries. Note that both objectives are constructed in a con- In this work, we frame the abstractive summarization task as a sequence-to-sequence learning problem. The sequence-to-sequence Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017</ref>) is adopted as our backbone architecture, where the model takes as input the dialogue utterances and generates a corresponding summary. Specifically, given a dialogue D = (u 1 , u 2 , ..., u |D| ), consisting of |D| utterances, coupled with its corresponding summary T D = (y 1 , y 2 , ..., y |T D | ) in the length of |T D |, the goal is to learn the optimal model parameters ? and to minimize the negative log-likelihood:</p><formula xml:id="formula_0">L D,T D = |T D | i=1 ? log p(y i |y 1:i?1 , D; ?) (1)</formula><p>where y 1:i?1 denotes the first i ? 1 tokens of the output sequence (i.e., y 1:i?1 = (y 1 , y 2 , ..., y i?1 )).</p><p>For a certain batch of dialogue-summary pairs B = ( D 1 , T D 1 , D 2 , T D 2 , . . . , D |B| , T D |B| ), the negative log-likelihood is calculated as:</p><formula xml:id="formula_1">L B main = 1 |B| D,T D ?B L D,T D<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Objectives</head><p>In this section, we introduce two contrastive objectives, coherence detection and sub-summary examination objectives, which can be considered as auxiliary tasks during training phase and reinforce the primary dialogue summarization task.</p><p>Coherence Detection Objective. The access to topic labels of dialogues often requires extra expert annotations or additional topic segment algorithms, which is expensive or may introduce error propagation. Considering the observation that text coherence is inherently related to the text topic (refer to Section 1), instead, we obtain the topical information of a dialogue by modeling the coherence change among utterances. The assumption behind this is that utterances within the same topic are more coherent than those spanning across different topics, based on which we construct the contrastive coherence detection objective.</p><p>To conduct contrastive learning, we construct positive-negative pairs with self-supervision. Recall that a dialogue consists of |D| utterances, i.e., D = (u 1 , u 2 , . . . , u |D| ). We introduce a window comprising a subsequence of k (k &lt; |D|) utterances of a dialogue D, as a snippet, denoted as S D k . For instance, (u j , u j+1 , . . . , u j+k ) is an example snippet for dialogue D where j ? [1, |D| ? k] is an integer utterance index. Such a snippet is regarded as a positive example, while the corresponding negative snippet S D k is constructed by shuffling the order of sentences inside S D k . Given a pair of positive and negative examples, denoted as P D co = (S D k , S D k ), the contextual representations of each snippet can be obtained through the last layer of the Transformer encoder, denoted as</p><formula xml:id="formula_2">E S D k , E S D k ,</formula><p>individually. Then we can calculate the coherence scores within a snippet by:</p><formula xml:id="formula_3">y S D k = w 1 * E S D k + b 1 ; y S D k = w 1 * E S D k + b 1</formula><p>where w 1 ? R d and b 1 ? R are trainable parameters besides the original Transformer architecture, as depicted as Coherence Regressor in <ref type="figure">Figure 2</ref>. The normalization with a softmax layer is conducted to obtain the final coherence score:</p><formula xml:id="formula_4">[co(S D k ), co( S D k )] = sof tmax([y S D k , y S D k ])</formula><p>For a dialogue D, there exist at least |D ? k| contrastive snippet pairs, while, for simplicity, we randomly select N co &lt; |D?k| pairs for each epoch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Snippet selection for a sub-summary</head><p>Input: A sub-summary t i ? T , a dialogue D containing |D| utterances, sliding window size interval [a, b] Output:</p><formula xml:id="formula_5">(S i pos , S i neg ) for t i W = ? for w = a to b do for j = 1 to |D| ? w do cand = D j,j+w r(j, w) ? ROUGE(cand, t i ) W ? W ? cand j ? j + w/2 w ? w + 1 j best , w best ? arg max j,w r(j, w) S i pos ? D j best ,(j best +w best ) S i neg ? W \ S i pos</formula><p>during training. The contrastive margin-based coherence loss is then calculated as:</p><formula xml:id="formula_6">L D co = 1 N co Nco n=1 max(0, ? co ? (co(S D k,n ) ? co( S D k,n )))</formula><p>where ? co is a margin coefficient by which we expect that the coherence score for the positive snippet is larger than the score for the negative one. k, N co and ? co are hyperparameters. For a certain batch of dialogue-summary pairs B = ( D 1 , T D 1 , D 2 , T D 2 , . . . , D |B| , T D |B| ), the margin-based contrastive loss is calculated as:</p><formula xml:id="formula_7">L B co = 1 |B| D,T D ?B L D co<label>(3)</label></formula><p>In this setting, we only use the dialogue while the summary is untouched. The coherence loss can be used to update the parameters in the encoder.</p><p>Sub-summary Generation Objective. The summary of a long dialogue always consists of multiple sentences each of which is regarded as a sub-summary. Considering the fact that one dialogue may contain more than one topics, we assume that each sub-summary is related to one topic. Hence, we introduce the contrastive sub-summary generation objective.</p><p>It is straightforward to obtain the sub-summaries by dividing the whole summary into single sentences via period symbols 1 . For simple illustra-tion, here we denote the corresponding target summary of a dialogue D = (u 1 , u 2 , . . . , u |D| ) as T D = (t 1 , t 2 , ..., t m ), where m is the number of sentences and each t i is considered as a subsummary. Given a sub-summary t i , we can retrieve the most related snippet S i pos from the dialogue D according to the ROUGE-2 recall score <ref type="bibr" target="#b25">(Lin, 2004)</ref>. The detailed selection algorithm is presented in Algorithm 1. Given an integer window size w ? [a, b] (0 &lt; a ? b &lt; |D|), we can slide the window over the dialogue D in the stride of half window size and obtain a set of candidate snippets W. Enumerating each snippet candidate in W and calculating the ROUGE-2 recall score with the sub-summary t i , we can get the optimal snippet scored the highest, which is selected as the most related snippet and regarded as the positive example S i pos . The corresponding negative example is randomly picked from the rest snippets in W, denoted as S i neg . Now, we have constructed the contrastive sub-summary generation</p><formula xml:id="formula_8">pairs {(S i pos , t i ), (S i neg , t i )}.</formula><p>Like the primary dialogue summarization task, we also model the subsummary generation objective as a sequence-tosequence learning problem. Following Equation 1, the negative log-likelihoods are calculated as:</p><formula xml:id="formula_9">L t i pos = ? log( |t i | j=1 p(t i j |t i 1:j?1 , S i pos ; ?)) L t i neg = ? log( |t i | j=1 p(t i j |t i 1:j?1 , S i neg ; ?))</formula><p>where t i j refers to the j th token in t i and t i 1:j?1 stands for all preceding tokens before position j.</p><p>The normalized scores after the softmax layer can be regarded as the irrelevance score to show how irrelevant a snippet is to a sub-summary:</p><formula xml:id="formula_10">[su(S i pos ), su(S i neg )] = sof tmax([L t i pos , L t i neg ])</formula><p>For a dialogue D paired with its summary T D , at least m contrastive pairs can be constructed, while, similar to the coherence case, we randomly select N su &lt; m pairs for each epoch during training phase. Thus, we can construct a contrastive marginbased loss for dialogue D:</p><formula xml:id="formula_11">L D,T D su = 1 N su Nsu n=1 max(0, ? su ? (su(S n neg ) ? su(S n pos )))</formula><p>Algorithm 2 Alternating Updating Strategy</p><formula xml:id="formula_12">Input: A batch of dialogue-summary instances B Coherence Task 1: L B co = 1 |B| D,T D ?B L D co 2: ? ? ? ? ?w co ?L B co ?? Sub-summary Task 3: L B su = 1 |B| D,T D ?B L D,T D su 4: ? ? ? ? ?w su ?L B su ?? Main Task 5: L B main = ? 1 |B| D,T D ?B L D,T D 6: ? ? ? ? ?w main ?L B main ??</formula><p>where ? su is a margin coefficient by which we would like the relevance score between a positive snippet and a sub-summary to be at least larger than the relevance score of the negative pair. a, b, N su and ? su are hyperparameters. For a certain batch of dialogue-summary pairs</p><formula xml:id="formula_13">B = ( D 1 , T D 1 , D 2 , T D 2 , . . . , D |B| , T D |B| )</formula><p>, the negative log-likelihood is calculated as:</p><formula xml:id="formula_14">L B su = 1 |B| D,T D ?B L D,T D su<label>(4)</label></formula><p>The sub-summary objective can be used to update the parameters in the encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Task Learning</head><p>The proposed two contrastive objectives can contribute to the primary dialogue summarization task during training phase, acting as auxiliary tasks. There are two options to combine the primary and auxiliary tasks: 1) summing the three objectives as a single one and update the model parameters using the summation loss; 2) alternatively update the model parameters using one of three objectives at each time. The empirical studies (Section 3.3) show that the alternating updating strategy performs better. Thus, in this work, we adopt the alternating parameter updating strategy, as shown in Algorithm 2. For a certain batch of dialogue-summary pairs, three objectives are adopted to update parameters in sequence. We first update the model parameters using the coherence objective, followed by the sub-summary and the primary generation objectives. The three objectives share the same learning rate ?. Since the main focus is to generate better dialogue summaries with the help of auxiliary contrastive objectives, we give more attentions to the primary task. Inspired by <ref type="bibr" target="#b6">Dasgupta and Namboodiri, 2016</ref>, to drive the auxiliary tasks to contribute to the primary one yet not to be dominate, we also introduce task-wise coefficients to each task, denoted as w co , w su and w main , individually. Following experiments demonstrate the effectiveness of the alternating strategy and the introduced task-wise coefficients.</p><p>3 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>SAMSum contains natural message-like dialogues in English written by linguists, each of which is annotated with summary by language experts <ref type="bibr" target="#b12">(Gliwa et al., 2019)</ref>. There are 14,732 dialogue-summary pairs for training, 818 and 819 instances for validation and test, respectively.</p><p>MediaSum is a large-scale dataset for dialogue summarization, containing interview transcripts collected from National Public Radio (NPR) 2 and CNN 3 , where the overview descriptions or discussion guidelines, coming with the transcripts, are considered as corresponding abstractive summaries <ref type="bibr" target="#b45">(Zhu et al., 2021)</ref>. The whole corpus contains 463.6K instances, with 10K each for validation and testing individually, and the rest is for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>As mentioned in Section 2.1, the sequence-tosequence Transformer model is adopted as our backbone architecture, implemented using Fairseq toolkit 4 <ref type="bibr" target="#b33">(Ott et al., 2019)</ref>. To be specific, our model is initialized with a pre-trained sequenceto-sequence, i.e., BART <ref type="bibr" target="#b23">(Lewis et al., 2020</ref> rate ? for SAMSum is 4e-5, 2e-5 for MediaSum. The maximum number of tokens for a certain batch is 800 and 1100 for SAMSum and MediaSum, individually. The margin coefficients ? co and ? su for the two contrastive objectives are always set to 1.</p><p>Other hyper-parameters of our methods, including w co , w su , k, a, b are tuned on the validation set. More implementation details and sensitivity tests for hyper-parameters are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>To evaluate our models, we utilized the ROUGE <ref type="bibr" target="#b25">(Lin, 2004)</ref> to measure the quality of summary output generated by different models. We adopted the files2rouge 5 package based on the official ROUGE-1.5.5.pl perl script to get full-length ROUGE-1, ROUGE-2 and ROUGE-L F-measure scores. The recent popular automatic evaluation metric for text generation, BERTScore <ref type="bibr" target="#b41">(Zhang et al., 2020b)</ref>, is also presented for comparisons 6 . For simiplicity, we use R-1, R-2, R-L and BERTS to refer to ROUGE-1, ROUGE-2, ROUGE-L and BERTScore, respectively.</p><p>Baselines Lead3 is a commonly adopted method in the news summarization task, which simply takes the first three leading sentences of text as its summary. PTGen <ref type="bibr" target="#b36">(See et al., 2017)</ref>  2018) first selects pivot sentences and then generates abstract summary with reinforcement learning. DynamicConv + GPT-2/News <ref type="bibr" target="#b39">(Wu et al., 2019)</ref> proposes a lightweight dynamic convolutions to replace the self-attention modules in the Transformer layers. UniLM <ref type="bibr" target="#b7">(Dong et al., 2019)</ref> is a unified language model which can be used for both natural language understanding and generation tasks.</p><p>BART <ref type="bibr" target="#b23">(Lewis et al., 2020</ref>) is a pre-trained encoderdecoder Transformer model, with two versions BART BASE and BART LARGE . For simplicity, we use BART to denote BART LARGE . Multiview BART (Chen and Yang, 2020) incorporates mutli-view features to summarize dialogues, including global, discrete, topic and stage information of dialogues. BART ORI finetunes the BART LARGE with its original pre-training tasks (i.e., sentence shuffling and text infilling) <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref>, acted as auxiliary tasks like this work.</p><p>Results on SAMSum. The results on SAMSum dataset are listed in <ref type="table">Table 1</ref>. Results of Lead3, PT-Gen, DynamicConv + GPT-2/News, and FastAbs-RL are taken from <ref type="bibr" target="#b12">Gliwa et al., 2019.</ref> Others are based on our implementations (see the appendix).</p><p>As we can see that, according to ROUGE script our model CONDIGSUM significantly outperforms previous state-of-the-art models in the first block (p &lt; 0.05), indicated by * , with regard to both ROUGE and BERT scores, which demonstrates the effectiveness of the proposed contrastive objectives. Comparing BART LARGE against BART ORI , it is interesting to observe that treating the original pre-training objectives as auxiliary tasks during fine-tuning also leads to performance gains. However, our proposed contrastive objectives are more effective.</p><p>We also conducted an ablation study on the SAMSum dataset. The ROUGE-2 score drops 0.7   points after removing the coherence detection objective, while the performance drops 1 point by ignoring the sub-summary generation objective. Such a phenomenon indicates both proposed contrastive objectives help generate better summaries, while the sub-summary generation objective contributes more to the primary task, compared to the coherence detection objective. One reason is that the sub-summary generation objective and the primary summary task are both sequence-to-sequence learning problems, yet the coherence detection objective only affects the encoder part.</p><p>Results on MediaSum. <ref type="table">Table 2</ref> shows the results on the MediaSum dataset. Results of PTGen and UniLM are reported by <ref type="bibr" target="#b45">Zhu et al., 2021</ref>. Similar to SAMSum, CONDIGSUM also outperforms all the baseline models. The ablation study on the MediaSum dataset shows both auxiliary tasks contribute to the primary task and the results of them are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Different Multi-Task Combination</head><p>Strategies. <ref type="table" target="#tab_4">Table 3</ref> listed the performance on SAMSum dataset adopting either the alternating parameter updating or the summation objective strategy. Compared to the BART LARGE baseline in <ref type="table">Table  1</ref>, both strategies result in performance gains, while the alternating parameter updating strategy is more helpful. Hence, this work adopts the alternating parameter updating strategy.</p><p>Human Evaluation. Since the automatic evaluation mainly focuses on the semantic matching between the generated output and the ground truth, while the generated summaries may be disfluent or ungrammatical, we thus also elicit feedback from human efforts. We compared our proposed model with the human references, as well as two base- lines, BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref> and Multiview BART (Chen and Yang, 2020) 7 . 100 dialogues are randomly selected from the test split of SAM-Sum dataset. 10 participants are presented with a dialogue and its paired candidate summaries, including human references, generated outputs by three models. For each selected dialogue, they are asked to rank the candidate output from the best to worst with regard to fluency (is the summary fluent/grammatically correct?), informativeness (does the summary contains the most informative pieces of the dialogue?), and succinctness (does the summary express in an abstractive way?). <ref type="table" target="#tab_5">Table 4</ref> listed the proportions of different system rankings and mean rank (lower is better). The output of our CONDIGSUM is ranked as the most appropriate summary for 26% of all cases. Overall, we obtain lower mean rank than the other two systems but still lags behind the Gold one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Study and analysis</head><p>How do coherence and sub-summary objectives work? Firstly, we compared the coherence scores predicted by our CONDIGSUM model of intra-topic snippets and inter-topic snippets. Taking the dialogue in <ref type="figure">Figure 1</ref> from the test split of SAMSum dataset as an example, coherence scores of intratopic snippets S 1 , S 2 and S 3 are 1.37, 2.17 and 3.12, respectively, while the scores of inter-topic snippets S 4 and S 5 are much lower <ref type="bibr">(-0.15 and -5.64, individually)</ref>. 8 This indicates that the coherence detection objective does help the model capture the topical information of the dialogue. On the other hand, we tried to find out how the sub-summary generation objective affects the generation of summaries. For the same dialogue, we calculated 7 Outputs are publicly available at https://github.com/GT-SALT/Multi-View-Seq2Seq 8 Refer to the appendix for the illustration.  <ref type="figure">Figure 4</ref>: Impact of different values of task-coefficients of coherence detection (left) and sub-summary generation (right) objectives on the validation loss of the primary dialogue summarization task.</p><p>the sequence-to-sequence loss of snippet-summary pairs {(S i , t j ), i, j ? {1, 2, 3} } by feeding each snippet-summary pair into the trained model (the snippet for encoder and the summary for decoder). The log-likelihood loss was then transformed to represent the correlation score between a snippet and a sub-summary (a lower loss means a higher correlation). <ref type="figure" target="#fig_0">Figure 3</ref> visualizes how much one subsummary is related to different snippets (i.e., every column). The results of our CONDIGSUM model were more concentrated on the diagonal than those of BART, which proves that our sub-summary generation objective indeed forces the model to pay more attention to the most salient fact and generate more relevant summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">How does task-wise coefficients affect primary task?</head><p>In order to make it easier to observe how the taskwise coefficients affect the primary task, we only consider one contrastive objective at each time, by removing either the sub-summary generation objective or the coherence detection objective as well as scaling down and up the optimal values of the taskcoefficients, w co and w su , based on the optimal values (denoted as ?1). The values of the primary summrization loss on SAMSum dataset with different task-coefficients are depicted in <ref type="figure">Figure 4</ref>. We can observe that the primary loss increases with either larger or smaller task-coefficients. Assigning larger weights to the auxiliary tasks will encourage the model to prefer auxiliary tasks and ignore the primary task, where the primary task converges to the sub-optimal point. However, auxiliary tasks assigned by too small weight numbers will fail to assist the model to capture the dialogue topic information.</p><p>Is the coherence detection objective actually topical-related? To quickly investigate the relationship between coherence detection objective and discourse structures, we constructed a set of 70 contrastive examples. Each example is constructed as follows: For a snippet s 1 , consisting of utterances from the same topic in a dialogue, we randomly select an utterance u in s 1 and replace it with another utterance v from other topics, where the dialogue act types for u and v are the same. Therefore, we get a new snippet s 2 . The encoder of our model is used to get the coherence scores for s 1 and s 2 , respectively. We found that the average coherence scores(-0.73) for the original snippets(s 1 ) are higher than the scores for their counterparts(s 2 ) with replacements(-1.02). Though the two examples have the same dialogue act types, the coherence scores are different. From this, we think the coherence detection objective does capture topic-related information. Moreover, from our understanding, a dialogue's topic and its discourse structure can be interlaced. The coherence score distribution of dialogue can reflect the topic change and also correlate to the discourse flow, while our work mainly focuses on the first point.</p><p>Relation between the quality of summary and complexity of dialogues. We further investigated the relation between the quality of generated summaries with regard to the number of subsummaries residing in a dialogue summary. The test split of SAMSum dataset was divided into two sets: a) One: the dialogue summaries that only contain one sub-summary; b) More: the dialogue summaries consisting of more than one sub-summaries. For each set, we calculated the averaged ROUGE-2 score over all elements. We include CONDIG-SUM, BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref> and Multiview BART <ref type="bibr" target="#b1">(Chen and Yang, 2020)</ref> for comparison, as listed in <ref type="figure">Figure 5</ref>. Our model performs better than two baselines under both circumstances. In addition, under the One situation, CONDIGSUM outperforms Multiview BART by 0.41 ROUGE-2 point, yet the difference is expanded to 1.28 points under More. This increment indicates that our model significantly improves the quality of generated summaries when the dialogue summary comprises of more than one sub-summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Document Summarization. Automatic document summarization aims to condense a wellstructured document into its shorter form where the important information preserved. This task can be categorized into extractive and abstractive document summarization. The extractive summarizer  <ref type="figure">Figure 5</ref>: ROUGE-2 score of generated summaries for dialogues containing one or more sub-summaries.</p><p>learns to find the informative sentences from the input document as its summary, which can be viewed as a sentence problem <ref type="bibr" target="#b22">(Kupiec et al., 1995;</ref><ref type="bibr" target="#b5">Conroy and O'leary, 2001)</ref>. The features can be learned from LSTMs, CNNs or Transformers <ref type="bibr" target="#b4">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b30">Nallapati et al., 2017;</ref><ref type="bibr" target="#b27">Liu and Lapata, 2019)</ref>. The abstractive summarization task learns to generate summaries by rewriting the input document, which is a typical sequence-to-sequence learning problem. Sequence-to-sequence attentive LSTMs (Hochreiter and Schmidhuber, 1997; <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> and its extensions with copy mechanism <ref type="bibr" target="#b14">(Gu et al., 2016)</ref>, coverage mechanism <ref type="bibr" target="#b36">(See et al., 2017)</ref> and reinforcement learning <ref type="bibr" target="#b34">(Paulus et al., 2018)</ref> have shown effectiveness on summarizing the document. Recent studies have investigated the pretrained transformer models, like BERTAbs <ref type="bibr" target="#b27">(Liu and Lapata, 2019)</ref>, BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref>, PEGASUS <ref type="bibr" target="#b40">(Zhang et al., 2020a)</ref> and STEP <ref type="bibr" target="#b48">(Zou et al., 2020)</ref>. The extractive and abstractive methods can be combined with reinforcement learning <ref type="bibr" target="#b3">(Chen and Bansal, 2018)</ref>, attention mechanisms <ref type="bibr" target="#b10">(Gehrmann et al., 2018;</ref><ref type="bibr" target="#b18">Hsu et al., 2018)</ref> or in a pipeline manner <ref type="bibr" target="#b35">(Pilault et al., 2020)</ref>, while this work focuses on summarizing dialogue utterances from a sequenceto-sequence learning perspective.</p><p>Dialogue Summarization. The dialogue summarization task aims to summarize the dialogue content consisting of utterances from multiple speakers. <ref type="bibr" target="#b37">Shang et al. (2018)</ref> proposed a simple multi-sentence compression technique to summarize meetings in an unsupervised fashion. <ref type="bibr" target="#b44">Zhao et al. (2019)</ref>; <ref type="bibr">Zhu et al. (2020a)</ref> designed hierarchical model structures to capture features of conversational utterances from different turns.</p><p>The conversational analysis can also be unitized to generate the summaries for dialogue content. <ref type="bibr" target="#b29">Liu et al. (2019b)</ref>; <ref type="bibr" target="#b7">Li et al. (2019)</ref> introduced the topical information to the summarization process, while  took use of the key utterances and <ref type="bibr" target="#b13">Goo and Chen (2018)</ref> leveraged the dialogue acts. Chen and Yang (2020) explicitly modeled conversational structures from four different views and then design a multi-view decoder to incorporate features from such four views to generate dialogue summaries. However, the additional information of conversational topics, key utterances, dialogue acts, and conversational structures requires human annotations, which is quite expensive or requires extra segment algorithms. Without requiring extra human effort or algorithms, this work proposes to introduce two contrastive learning objectives as auxiliary tasks during training.</p><p>Contrastive Learning. The application of contrastive learning for various tasks has been investigated recently, mainly in computer vision domain. The contrastive predictive coding <ref type="bibr" target="#b32">(Oord et al., 2018)</ref> has been studied for data-efficient image recognition <ref type="bibr" target="#b16">(Henaff, 2020)</ref>. Without using specialized architectures or a memory bank, learning visual representations in a contrastive manner outperforms various baselines with self-supervised, semi-supervised and transfer learning . <ref type="bibr" target="#b19">Khosla et al. (2020)</ref> proposed a fullysupervised contrastive loss which achieved new state-of-the-art results on the image classification task, surpassing the cross-entropy loss. This work also demonstrates that, compared to the traditional cross-entropy loss, the proposed supervised contrastive loss performs more stably to different hyperparameter settings, like data augmentations and optimizers. Moreover, <ref type="bibr" target="#b21">Klein and Nabi (2020)</ref> introduced contrastive margin as regularizer for commonsense reasoning where a pairwise contrastive auxiliary prediction task is constructed. <ref type="bibr" target="#b8">Fang et al. (2020)</ref> proposed to pre-train language models with contrastive self-supervised learning at the sentence level, which learns to predict whether two sentences originate the same one. <ref type="bibr">Gunel et al. (2020)</ref> proposed a supervised contrastive learning objective which allows to work with cross-entropy and lead to significant performance gains. The contrastive learning is also introduced to learn the sentence embeddings <ref type="bibr" target="#b9">(Gao et al., 2021)</ref>. The above applications of contrastive learning are for computer vision or natural language understanding domains, while, in this work, we introduce the contrastive learning to the abstractive dialogue summarization task, which is a typical generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Recent research progresses have present the effectiveness of dialogue studies (e.g., topical information and dialogue acts) on summarizing dialogues, while additional expert annotations or extra algorithms are required to obtain the knowledge. This work proposes a simple yet effective method, CONDIGSUM, that implicitly captures the topical knowledge residing in dialogue content by modeling the text coherence, yet no additional human annotations or segment algorithms are needed. We design two contrastive objectives as auxiliary task, i.e., coherence detection and sub-summary generation objectives,working together with the primary summarization task during training. An alternating parameter update strategy is employed to cooperate the primary and auxiliary tasks. Experiments on two benchmark datasets demonstrate the efficacy of the proposed model. Future directions include learning structured representations of information flow residing in dialogues and leveraging knowledge graphs to generate better dialogue summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ethical Considerations</head><p>Our simple yet effective abstractive dialogue summarization system could be used where there exists dialogue systems (two or multi-party dialogues). For example, it could be used for grasping the key points quickly or recapping on the salient information of online office meeting. In addition, the system can also be used for customer service, requiring employees to summarize the conversation records of customers' inquiries, complaints and suggestions.</p><p>The daily dialogue and media interview datasets used in this work are publicly available, and only for research purpose. There may exist biased views in them, and the content of them should be viewed with discretion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset</head><p>We also show detailed statistics about such two datasets, SAMSum <ref type="bibr" target="#b12">(Gliwa et al., 2019)</ref> and Media-Sum <ref type="bibr" target="#b45">(Zhu et al., 2021)</ref>, with regard to average tokens, utterances and speakers, as showed in <ref type="table" target="#tab_9">Table 5</ref>. It is straightforward that the dialogue in MediaSum is much longer than the one in SAMSum, yet the corresponding summary is much shorter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>The sequence-to-sequence Transformer model is adopted as our backbone architecture, implemented using Fairseq toolkit 9 <ref type="bibr" target="#b33">(Ott et al., 2019)</ref>. To be specific, our model is initialized with a pre-trained sequence-to-sequence, i.e., BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref>. Thus they share the same architectures, 6layer encoder-decoder Transformer for BART BASE and 12-layer Transformer for BART LARGE . Each layer in BART BASE has 16 attention heads, and the hidden size and feed-forward filter size is 1024 and 4096, respectively, resulting in 140M trainable parameters. Each layer in BART LARGE has 16 attention heads, and the hidden size and feed-forward filter size is 1024 and 4096, respectively, resulting in 400M trainable parameters. The dropout rates for all layers are set to 0.1. The optimizer is Adam (Kingma and Ba, 2015) with warmup.</p><p>For SAMSum dataset, the learning rate ? is 4e-5, and the maximum number of tokens in each batch is 800. The model is trained for 3 epochs. Each epoch takes around 0.7 hours on single Tesla P40 GPU. The window size k of the coherence detection objective is tuned over 5 to 15, with a stride of 2. The optimal value is 14. The lower bound of sliding window size for the sub-summary generation objective is selected from <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>, with the difference between lower and upper bounds set to 20. The optimal values for w co and w su are 0.005 and 0.0001 individually. The number of contrastive pairs for each sample, i.e., N co and N su , is equal to 2.</p><p>For MediaSum dataset, the learning rate ? is 2e-5, and the maximum number of tokens in one batch is 1100. The model is trained for 4 epochs, each of which takes around 15 hours on four Tesla V100 GPUs. Similar to SAMSum, for the coherence detection objective, the window size k is 10, and the <ref type="bibr">9</ref> We empirically observed that different frameworks (e.g. Fairseq and Huggingface Transformer) may obtain different results under the same hyperparameter settings. task-wise coefficient w co is 0.00005. The sliding window size interval of the sub-summary generation objective is <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>, with the task-wise coefficient w su of 0.00005. For simplicity, the number of contrastive pairs for each sample, i.e., N co and N su , is equal to 1. Following <ref type="bibr" target="#b45">Zhu et al. (2021)</ref>, we add interlocutors information before concatenating utterances, and then truncate the dialogues to keep only first 1024 tokens as input. All experiments were conducted on either Tesla P40 GPUs (24GB) or Tesla V100 GPUs (16GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Construction of Sub-summary</head><p>All sub-summaries are constructed from groundtruth summaries following a pre-processing procedure. We only consider dialogues whose groundtruth summary consists of at least two sentences and filtered the sentences in ground-truth summaries that have no good match with any snippets in original dialogues in terms of ROUGE score. We also tried to take BertScore as the selection metric of snippets, but ROUGE was finally adopted because there is barely any difference between them and the cost of BertScore was much larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Results</head><p>The output of MultiviewBART <ref type="bibr" target="#b1">(Chen and Yang, 2020)</ref> is publicly available at https://github.com/GT-SALT/Multi-View-Seq2Seq.</p><p>Since the ROUGE scores may vary due to different toolkits, to make fair comparisons with our model, we recalculated the ROUGE scores on the output of MultiviewBART using the files2rouge 10 , same as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Performance on the Validation Set</head><p>The performance on the validation split of SAM-Sum and MediaSum is listed in <ref type="table" target="#tab_10">Table 6</ref> and 7, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Sensitivity tests</head><p>To explore the effects of the hyper-parameters of our methods, we conducted sensitivity tests on validation split of SAMSum. Generally, there is an optimal value reaching at highest ROUGE scores, while too small or too large values hamper performance. * indicates the best setting according to the validation set.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Case Study</head><p>A complete example showing coherence scores of different snippets and the generation loss of one sub-summary with respect to different snippets is shown in <ref type="figure">Figure 6</ref>.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of how much a sub-summary is related to different snippets (the sum of every column is equal to 1). The result of CONDIGSUM is more concentrated on diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Thus they share the same architectures, 6-layer encoder-decoder Transformer for BART BASE and 12-layer Transformer for BART LARGE . Each layer in BART BASE has 16 attention heads, and the hidden size and feed-forward filter size is 1024 and 4096, respectively, resulting in 140M trainable parameters. Each layer in BART LARGE has 16 attention heads, and the hidden size and feed-forward filter size is 1024 and 4096, respectively, resulting in 400M trainable parameters. The dropout rates for all layers are set to 0.1. The optimizer is Adam (Kingma and Ba, 2015) with warmup. The learning</figDesc><table><row><cell>Model</cell><cell cols="2">R-1 R-2 R-L BERTS</cell></row><row><cell>* Lead3</cell><cell>31.4 8.7 29.4</cell><cell>-</cell></row><row><cell>* PTGen</cell><cell>40.1 15.3 36.6</cell><cell>-</cell></row><row><cell cols="2">* DynamicConv + GPT-2 41.8 16.4 37.6</cell><cell>-</cell></row><row><cell>* FastAbs-RL</cell><cell>42.0 18.1 39.2</cell><cell>-</cell></row><row><cell cols="2">* DynamicConv + News 45.4 20.7 41.5</cell><cell>-</cell></row><row><cell>Multiview BART</cell><cell>53.9 28.4 44.4</cell><cell>53.6</cell></row><row><cell>* BART BASE</cell><cell>46.1 22.3 36.4</cell><cell>44.8</cell></row><row><cell>* BART</cell><cell>52.6 27.0 42.1</cell><cell>52.1</cell></row><row><cell>* BART ORI</cell><cell>52.6 27.2 42.7</cell><cell>52.3</cell></row><row><cell>CONDIGSUM BASE</cell><cell>48.1 24.0 39.2</cell><cell>48.0</cell></row><row><cell>CONDIGSUM</cell><cell>54.3 29.3 45.2</cell><cell>54.0</cell></row><row><cell>w/ow/o Sub-summary</cell><cell>53.8 28.3 44.1</cell><cell>53.5</cell></row><row><cell>w/ow/o Coherence</cell><cell>53.9 28.6 44.2</cell><cell>53.5</cell></row><row><cell cols="3">Table 1: Results on SAMSum test split.  *  indicates that</cell></row><row><cell cols="3">the results are significantly different from ours (p &lt;</cell></row><row><cell>0.05).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of multi-task combination strategies.</figDesc><table><row><cell>Systems</cell><cell>1st</cell><cell>2nd 3rd</cell><cell>4th MR</cell></row><row><cell>BART</cell><cell cols="3">0.14 0.12 0.31 0.43 3.03</cell></row><row><cell cols="4">Multiview BART 0.19 0.27 0.25 0.29 2.64</cell></row><row><cell>CONDIGSUM</cell><cell cols="3">0.26 0.32 0.23 0.19 2.35</cell></row><row><cell>Gold</cell><cell cols="3">0.41 0.29 0.21 0.09 1.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Human evaluation on SAMSum: proportions of rankings. MR: mean rank (the lower the better).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Data statistics of dialogue summarization datasets. DialogToken, UtterToken and SummaryToken stand for the average number of tokens in dialogues, utterances and summaries, respectively. DialogUtter is the average number of utterances in dialogues. The last column lists the average number of speakers in dialogues.</figDesc><table><row><cell>Model</cell><cell>R-1 R-2 R-L</cell></row><row><cell>BART BASE</cell><cell>48.7 25.2 39.1</cell></row><row><cell>BART</cell><cell>54.0 28.8 44.0</cell></row><row><cell>BART ORI</cell><cell>53.7 28.2 43.5</cell></row><row><cell>CONDIGSUM BASE</cell><cell>50.7 26.9 41.6</cell></row><row><cell>CONDIGSUM</cell><cell>55.3 30.5 45.5</cell></row><row><cell cols="2">w/ow/o Sub-summary 54.9 29.5 44.6</cell></row><row><cell>w/ow/o Coherence</cell><cell>54.8 29.6 44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results on SAMSum validation split.</figDesc><table><row><cell>Model</cell><cell>R-1 R-2 R-L</cell></row><row><cell>BART</cell><cell>34.9 17.8 31.0</cell></row><row><cell>BART ORI</cell><cell>35.0 17.8 31.0</cell></row><row><cell>CONDIGSUM</cell><cell>35.6 18.7 31.9</cell></row><row><cell cols="2">w/ow/o Sub-summary 35.4 18.5 31.8</cell></row><row><cell>w/ow/o Coherence</cell><cell>35.3 18.4 31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results on MediaSum validation split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Sensitivity test of the coherence window k.</figDesc><table><row><cell>a</cell><cell>R-1 R-2 R-L</cell></row><row><cell>1</cell><cell>54.4 29.2 44.6</cell></row><row><cell>3</cell><cell>54.7 29.4 44.7</cell></row><row><cell cols="2">5 *  55.3 30.5 45.5</cell></row><row><cell>7</cell><cell>54.8 29.3 44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Sensitivity test of the sub-summary window's lower bound a.</figDesc><table><row><cell cols="2">N co R-1 R-2 R-L</cell></row><row><cell>1</cell><cell>54.8 29.7 45.0</cell></row><row><cell>2 *</cell><cell>55.3 30.5 45.5</cell></row><row><cell>3</cell><cell>55.0 29.8 45.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Sensitivity test of the number of contrastive pairs for each sample N co .</figDesc><table><row><cell cols="2">N su R-1 R-2 R-L</cell></row><row><cell>1</cell><cell>54.9 29.8 45.0</cell></row><row><cell>2 *</cell><cell>55.3 30.5 45.5</cell></row><row><cell>3</cell><cell>54.7 29.5 45.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Sensitivity test of the number of contrastive pairs for each sample N su .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More details are in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">www.npr.org 3 www.transcripts.cnn.com4  We empirically observed that different frameworks (e.g. Fairseq and Huggingface Transformer) may obtain different results under the same hyperparameter settings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/pltrdy/files2rouge Note that the ROUGE scores might vary with different tookits.6  We use version 0.3.8, with default English setting (roberta-large_L17_no-idf_version=0.3.8(hug_trans=4.4.0)rescaled).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/pltrdy/files2rouge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Yanyan Zou and Xiaojie Wang are the corresponding authors. We would like to thank anonymous reviewers for their suggestions and comments. The work was supported by the National Natural Science Foundation of China (NSFC62076032) and the Cooperation Project with Beijing SanKuai Technology Co., Ltd. We would also like to thank all annotators who have contributed to the case study, especially Rizhongtian Lu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>score: -5.64 <ref type="figure">Figure 6</ref>: Coherence scores of snippets and visualization of how the sub-summary t 3 is related to different snippets S i (i ? {1, 2, 3}). Darker background means a smaller loss and higher correlation between one snippet and the sub-summary t 3 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Hyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text summarization via hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne P O&amp;apos;</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging multiple tasks to regularize fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3476" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Twolevel transformer and auxiliary coherence modeling for improved text segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gliwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iwona</forename><surname>Mochol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Biesek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Wawer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstractive dialogue summarization with sentencegated modeling optimized by dialogue acts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wen</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Alexis Conneau, and Ves Stoyanov. 2020. Supervised contrastive learning for pretrained language model fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01403</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unified model for extractive and abstractive summarization using inconsistency loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Ting</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Kai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerui</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Jing Tang, and Min Sun</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrastive selfsupervised learning for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7517" to="7523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A trainable document summarizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 18th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Keep meeting summaries on topic: Abstractive multi-modal meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic dialogue summary generation for customer service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reading turn by turn: Hierarchical attention architecture for spoken dialogue comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topic-aware pointer-generator networks for summarizing spoken conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheldon</forename><forename type="middle">Lee</forename><surname>Shao Guang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiti</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="814" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Data-driven summarization of scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nikola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">H R</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahnloser</surname></persName>
		</author>
		<idno>abs/1804.08875</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On extractive and abstractive neural document summarization with transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised abstractive meeting summarization with multisentence compression and budgeted submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Lorr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural latent extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization via hierarchical adaptive segmental network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3455" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MediaSum: A large-scale media interview dataset for dialogue summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.474</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5927" to="5934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">2020a. End-to-end abstractive summarization for meetings. arXiv e-prints</title>
		<imprint>
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A hierarchical network for abstractive meeting summarization with cross-domain pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pre-training for abstractive document summarization by reinstating source text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. (t 3 ) Julia will order takeaway pizza for her</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing. (t 3 ) Julia will order takeaway pizza for her</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hania: I will be there around 7pm I guess</title>
	</analytic>
	<monogr>
		<title level="m">Julia: I will be waiting! :* Hania: Great!</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
