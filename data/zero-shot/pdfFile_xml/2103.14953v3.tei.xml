<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Class Learned Encoder-Decoder Network with Adversarial Context Masking for Novelty Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Taylor</forename><surname>Jewell</surname></persName>
							<email>jjewell6@uwo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Western University London</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vahid</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Western University London</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khazaie</surname></persName>
							<email>vkhazaie@uwo.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Western University London</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalda</forename><surname>Mohsenzadeh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Western University London</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One-Class Learned Encoder-Decoder Network with Adversarial Context Masking for Novelty Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code available at https://github.com/jewelltaylor/OLED.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Novelty detection is the task of recognizing samples that do not belong to the distribution of the target class. During training, the novelty class is absent, preventing the use of traditional classification approaches. Deep autoencoders have been widely used as a base of many novelty detection methods. In particular, context autoencoders have been successful in the novelty detection task because of the more effective representations they learn by reconstructing original images from randomly masked images. However, a significant drawback of context autoencoders is that random masking fails to consistently cover important structures of the input image, leading to suboptimal representations -especially for the novelty detection task. In this paper, to optimize input masking, we introduce a Mask Module that learns to generate optimal masks and a Reconstructor that aims to reconstruct masked images. The networks are trained in an adversarial setting in which the Mask Module seeks to maximize the reconstruction error that the Reconstructor is minimizing. When applied to novelty detection, the proposed approach learns semantically richer representations compared to context autoencoders and enhances novelty detection at test time through more optimal masking. Novelty detection experiments on the MNIST and CIFAR-10 image datasets demonstrate the proposed approach's superiority over cutting-edge methods. In a further experiment on the UCSD video dataset for novelty detection, the proposed approach achieves a frame-level Area Under the Curve (AUC) of 99.02% and an Equal Error Rate (EER) of 5.4%, exceeding recent state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Novelty detection involves determining whether or not an unknown sample belongs to the distribution of the training data. In the case the sample is similar to the training data, it is referred to as an inlier or normal sample. Alternatively, if the sample does not follow the distribution defined in the training examples, it is referred to as an outlier or anomaly. Novelty detection differs from other machine learning tasks in that the outlier class is poorly sampled or nonexistent. Due to the unavailability of outlier samples, traditional classification approaches are not suitable.</p><p>Within computer vision, novelty detection is ubiquitous with subtasks that have widespread applications such as marker discovery in biomedical data <ref type="bibr" target="#b0">[1]</ref> and video surveillance <ref type="bibr" target="#b1">[2]</ref>. Anomaly detection in images is one such task that involves identifying whether an image is an inlier or an outlier based on training data that mostly consists of inlier images. To compensate for the unavailability of outlier samples, one-class classification methods aim to model the distribution of the inlier data <ref type="bibr" target="#b2">[3]</ref>. New samples that do not conform to the target distribution are considered outliers. However, it is often hard to model the distribution of image data with conventional methods because of the high dimensionality in which the data points reside <ref type="bibr" target="#b2">[3]</ref>.</p><p>With the advent of deep learning, methods have been proposed that are able to effectively produce representations for high dimensional data <ref type="bibr" target="#b3">[4]</ref>. Autoencoders (AE) are an unsupervised class of approaches that are well suited for modeling image data <ref type="bibr" target="#b4">[5]</ref>. At a high level, an AE consists of two modules: an encoder and a decoder. The encoder learns a mapping from an image to a lower-dimensional latent space, and the decoder learns a mapping from the latent space back to the original image. In this way, AEs are trained in an unsupervised manner by minimizing the error between the original image and the reconstruction.</p><p>As a powerful unsupervised method for learning representations, AEs are the basis of many one-class classification approaches <ref type="bibr" target="#b5">[6]</ref>. To detect anomalous images, the AE is first trained on a set of primarily normal images. At test time, the reconstruction error of a sample is used as an anomaly score. The underlying intuition is that the reconstruction error will be lower for inlier samples than outlier samples <ref type="bibr" target="#b6">[7]</ref>. This follows from the fact that the AE is <ref type="bibr">Figure 1</ref>. An overview of the architecture in OLED. The Mask Module adversarially learns to cover the important parts of the input image; it consists of an autoencoder that generates an activation map and a threshold unit to produce the binary mask. The Reconstructor aims to minimize the reconstruction error and the Mask Module aims to maximize the reconstruction error. trained solely on inlier samples. However, this assumption is often violated, and the AE generalizes well to construct images outside of the distribution of the training data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. This is especially evident in cases where anomalous images share similar compositional patterns as inlier images.</p><p>Recent methods introduce additional complexity into the autoencoders reconstruction task so that outliers are not reconstructed well <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. To this end, denoising autoencoders (DAE) have been used. DAEs learn to reconstruct unperturbed images from images that have been perturbed by noise <ref type="bibr" target="#b12">[13]</ref>. Beyond yielding more robust representations, the denoising task of the AE has been shown to induce a reconstruction error that approximates the local derivative of the log-density with respect to the input <ref type="bibr" target="#b13">[14]</ref>. Thus, a sample's global reconstruction error reflects the norm of the derivative of the log-density with respect to the input. In this way, DAEs provide a more interpretable and theoretically grounded anomaly score.</p><p>Context autoencoders (CAE) <ref type="bibr" target="#b14">[15]</ref>, a specific type of DAE, have shown strong performance in the anomaly detection task <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. Instead of being perturbed by noise, input images are subjected to random masking. Consequently, the CAE learns to inpaint a randomly masked region of the input image in conjunction with the reconstruction task. This random masking is similar to using salt-and-pepper noise, which has been shown to yield better representations by implicitly enforcing the AE to learn semantic information about the distribution of the training data <ref type="bibr" target="#b14">[15]</ref>. Despite these strengths, in some cases CAEs suffer from suboptimal representations leading to poor performance in the anomaly detection task.</p><p>Inspired by the drawbacks of CAEs <ref type="bibr" target="#b14">[15]</ref>, we proposed One-Class Learned Encoder-Decoder (OLED) Network with Adversarial Context Masking. OLED introduces a Mask Module M M that produces masks applied to images input into the Reconstructor R. The masks generated by M M are optimized to cover the most important parts of the input image, resulting in a comparable reconstruction score across samples. The underlying intuition is that the loss of the masked region will be low in the case of inlier images and high in the case of outlier images. This stems from the fact that the Reconstructor learns to inpaint masked regions using mostly inlier samples. Thus, the inpainted regions of outlier images will consist of patterns present in the inlier images, yielding a high reconstruction error.</p><p>At a high level, the Mask Module is a convolutional autoencoder, and the Reconstructor is a convolutional encoder-decoder. They are trained in an adversarial manner, where the Mask Module is trying to generate masks that yield higher reconstruction errors, and the Reconstructor is trying to minimize the reconstruction error of the masked image. The architecture of the proposed approach is shown in <ref type="figure">Figure 1</ref>. We applied OLED to several benchmark datasets for anomaly detection in addition to providing a formal analysis of the efficacy of the Mask Module. Experimental results demonstrate that OLED is able to outperform a variety of recent state-of-the-art methods and hints at the broader usefulness of the mask module in other core computer vision tasks. In this paper our contributions are the following:</p><p>? We proposed a novel approach for finding the most important parts of images for novelty detection.</p><p>? Our framework is optimized through adversarial setting which yields more efficient representations for novelty detection.</p><p>? Our method provides several anomaly scores which capture different aspects of normality ? Due to effectiveness of our method in masking important parts of the image, we can leverage it at the test time which yields better anomaly scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>One-class classification is primarily associated with the domain of novelty, outlier, and anomaly detection. In these types of problems, a model attempts to capture the distribution of the inlier class to finally detect the unknown outliers or novel concepts. The conventional methods in the anomaly detection field utilized one-class SVM <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and Principal Component Analysis (PCA) and its variations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> to find a subspace that best represents the distribution of normal samples. Unsupervised clustering techniques like k-means <ref type="bibr" target="#b2">[3]</ref> and Gaussian Mixture Models (GMM) <ref type="bibr" target="#b20">[21]</ref> also have been used to formulate the distribution of normal data for identifying the anomalies, but they normally fail in dealing with high-dimensional data.</p><p>Several other proposed methods benefit from selfrepresentation learning, such as reconstruction-based approaches. They usually rely on the hypothesis that the outlier samples cannot be reconstructed precisely by a model that only learned the distribution of inlier samples. For example, Cong et al. <ref type="bibr" target="#b21">[22]</ref> suggested a model for video anomaly and outlier detection by learning sparse representations for distinguishing between inlier and outlier samples. In <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, test samples are reconstructed using the representations learned from inlier samples, and the reconstruction error is employed as a metric for novelty detection. Most of the deep learning-based models with encoderdecoder architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref> also used this score to detect anomalies. Although effective, these methods are limited by the under-designed representation of their latent space. Gong et al. <ref type="bibr" target="#b8">[9]</ref> proposed a deep autoencoder augmented with a memory module to encode the input to a latent space with the encoder. The resulting latent vector is used as a query to retrieve the most relevant memory item for reconstruction with the decoder.</p><p>In <ref type="bibr" target="#b0">[1]</ref>, a deep convolutional generative adversarial network (GAN) is leveraged to learn a manifold of normal images with a novel anomaly score based on the mapping from image space to a random distribution. Sabokrou et al. <ref type="bibr" target="#b28">[29]</ref> took advantage of Generative Adversarial Networks (GAN) <ref type="bibr" target="#b29">[30]</ref> along with denoising autoencoders to use the discriminator's score for the reconstructed images for the novelty detection task. Zaheer et al. <ref type="bibr" target="#b30">[31]</ref> redefined the adversarial one-class classifier training setup by modifying the role of the discriminator to distinguish between good and bad quality reconstructions and improved the results even further. Perera et al. used denoising auto-encoder networks to enforce the normal samples to be distributed uniformly across the latent space <ref type="bibr" target="#b10">[11]</ref>. Abati et al. suggested a deep autoencoder model with a parametric density estimator that learns the probability distribution underlying its latent representations through an autoregressive procedure <ref type="bibr" target="#b9">[10]</ref>.</p><p>Some recent works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> have tried to leverage pretrained deep neural networks by distilling the knowledge. In <ref type="bibr" target="#b31">[32]</ref>, they utilized a VGG16 <ref type="bibr" target="#b33">[34]</ref> to compute a multilevel loss for training the student network to calculate the anomaly score and perform anomaly segmentation. Even though these methods could achieve high performance, they benefit from the knowledge attained by training on millions of labeled images and also may not work well on other modalities of data. As our proposed method does not leverage pretrained networks, we consider our work complimentary, and thus do not compare against this class of approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Previous works have demonstrated that the reconstruction error of an Autoencoder (AE) acts as a good indicator of whether or not a sample conforms to the distribution defined in the training examples <ref type="bibr" target="#b6">[7]</ref>. As such, AEs are commonly used for anomaly detection. To this end, Denoising Auotoencoders (DAE) have often been used because of the more robust representations they offer <ref type="bibr" target="#b13">[14]</ref>. Context Autoencoders (CAEs), a subclass of DAEs, have been particularly successful in the anomaly detection task by offering representations that capture the semantics of the underlying training distribution <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, CAEs have a number of disadvantages. The first drawback of CAEs is that they learn suboptimal representations by failing to consistently mask important parts of the image during training. Furthermore, they perform poorly at test time if they include random masking. This is because the mask placement is closely related to the reconstruction score. An outlier with a simple part of the image masked may have a lower reconstruction error than an inlier image with a difficult part of the image masked. Thus, random masking cannot be effectively used at test time for more robust anomaly detection. Conversely, our approach avoids these drawbacks by learning to mask intelligently. Experimental results from the ablation study in section 4.6 support this conclusion. In order to mitigate these shortcomings while leveraging the benefits offered by CAEs, we propose a One-Class Learned Encoder-Decoder Network with adversarial context masking, which we call OLED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overview</head><p>Our proposed framework, OLED, consists of two modules: the Reconstructor R and the Mask Module M M . An overview of the architecture is available in 1. R and M M are trained in an adversarial manner, where R seeks to reconstruct images that have been covered by masks generated by M M . Masks have the same spatial resolution as input images with a single channel of 0 or 1 activations. As such, a masked image is easily obtained by taking an element-wise product of an image and its corresponding mask.</p><p>Through the adversarial training process, R learns representations that encode semantic information of the train-ing distribution through the inpainting task. Alternatively, M M learns to mask the most important parts of the input image by maximizing the reconstruction error of R. At test time, new samples are subjected to masks generated by M M and fed to R where the reconstruction error is used as an anomaly score. Accordingly, the reconstruction error will be low for the inlier class because R is trained to reconstruct and inpaint inlier samples. However, in the case of anomalies, the reconstruction error will be higher primarily. This stems from the fact that R learns to reconstruct and inpaint masked regions using mostly inlier samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reconstructor</head><p>R is a convolutional encoder-decoder network that is trained to reconstruct masked images. Following some of the previous works <ref type="bibr" target="#b15">[16]</ref>, a dense bottleneck is used. The full connectivity of the dense layer is helpful for the inpainting task, especially for shallow networks with low receptive fields. Moreover, R does not include max-pooling layers for greater stability in training. To further promote stability, Leaky ReLU and batch normalization are used in each convolutional block. The values after the last convolution layer are clipped to in between -1 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mask Module</head><p>M M consists of a mask generator M G followed by a threshold unit T that generates masks of the same resolution as the input image. These masks are applied to the corresponding input image prior to being fed into R. M M seeks to produce a mask that maximizes the reconstruction error of the input. In this way, it learns to mask the most optimal parts of the image. Thus, masks generated by M M yield more comparable anomaly scores across samples in contrast to random masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Mask Generator</head><p>M G is a convolutional autoencoder that takes an input image and generates a corresponding activation map. This activation map is input into the threshold unit to produce a binary mask. Similar to R, M G avoids the use of max pooling. Additionally, batch normalization and Leaky ReLU are used in each convolutional block, with the exception of the last convolution block that uses ReLU. In contrast to R, M G has a spatial bottleneck and contains much fewer parameters. This reflects the fact that R has a substantially more complex task than M G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Threshold Unit</head><p>Activation maps generated by M G are input into T to generate a mask. T requires a threshold hyperparameter that determines what percentage of the pixels in the image will not be masked. In this way, the same amount of pixels are masked in each image, ensuring that the reconstruction errors are comparable between samples.</p><p>For each activation map, pixels with activations in the top 1 -t percent are set to 0. The final mask is obtained by setting the remaining activations to one. More formally, given an activation map A and a scalar s that represents the numeric value of the pixel with the t highest activations:</p><formula xml:id="formula_0">A ij = 0, if A ij ? s 1, otherwise<label>(1)</label></formula><p>As it stands, this is a discontinuous function, which is known to have less stable optimization. In order to eliminate this problem, the threshold operation is reformulated in terms of continuous ReLU activation function:</p><formula xml:id="formula_1">A ij = max(A ij * ?1 + s, 0) max(A ij * ?1 + s, 0) + ?<label>(2)</label></formula><p>where max(A ij , 0) represents the ReLU activation, and ? is an infinitesimal positive scalar. The above formulation ensures continuity over the entire domain of the function enabling backpropagation through T into M G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Masking Procedure</head><p>M G and T sequentially process an input image to create a mask. Masks generated by M M are single-channel binary images with the same spatial resolution as input images. The masked image is obtained by applying the mask to its corresponding image for each channel. More precisely, given an image x, the corresponding masked image x m is defined as:</p><formula xml:id="formula_2">x m = x ? M M (x)<label>(3)</label></formula><p>where ? denotes element-wise multiplication. In this way, activations in the mask that are 0 set the corresponding pixel in the input image to 0, otherwise the pixel remains unchanged. It is important to note that input images, and thus the reconstructions generated by R, are scaled between -1 and 1. Because of this, masked pixels are set to the midpoint of the color range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Adversarial Training</head><p>Adversarial training is a learning mechanism in which two networks compete in a minmax game that iteratively enhances the ability to model the underlying distribution of the data. Following this intuition, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b29">[30]</ref> have been proposed and shown immense success in generating samples with similar distribution of the training data. In order to do so, a generator network G and discriminator network D are trained in this manner. G takes as input a noise vector and seeks to produce samples that follow the distribution of the training data. Alternatively, D takes as input real samples from the training set along with fake samples generated by G and seeks to discriminate between the two. More formally, given an image x sampled from p data and a random latent vector z sampled from p z the objective of a GAN is:</p><formula xml:id="formula_3">min G max D E x?pdata(x) [log D(x)]+ E z?pz(z) [log(1 ? D(G(z))]<label>(4)</label></formula><p>G(z) is a sample generated by G with input z. D(x) and D(G(z)) are the discriminator's classification scores for a real and generated sample, respectively.</p><p>Similarly, we train M M and R adversarially. M M seeks to generate masks that yield the highest reconstruction error from R. The total reconstruction error L tot consists of an L2 loss of the masked image L mask , contextual loss of the masked region L cont and an L2 loss of an unperturbed image L recon . Given an inlier image x and the corresponding masked image x m , L mask , L cont and L recon are defined as:</p><formula xml:id="formula_4">L mask = ?x ? R(x m )? 2 (5) L cont = ?x c ? R(x c )? (6) L rec = ?x ? R(x)? 2<label>(7)</label></formula><p>where x c is the masked region of the input image and R(x c ) is the reconstruction of the masked region. R(x m ) denotes the reconstruction of the masked image x m . R(x) is the reconstruction of the intact image x with the Reconstructor. The following are the components of the objective:</p><p>? L mask : Forces the network to form a semantic understanding of characteristic elements of inlier samples.</p><p>? L cont : Emphasizes that the masked region of the image is reconstructed properly to avoid blurry reconstructions of the masked region.</p><p>? L rec : Helps the network learn the distribution of unmasked inliers.</p><p>As such, the objective function of OLED is given by:</p><formula xml:id="formula_5">min R max M M L mask + ?L cont + ?L rec<label>(8)</label></formula><p>where ? and ? are hyperparameters that weigh L cont and L rec , respectively. Since M M has no bearing on L rec , it is not included in the error of M M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Anomaly Scoring</head><p>The three distinct loss terms in the OLED objective present the opportunity for three anomaly scores to be defined: s mask , s cont and s rec . s mask , s cont and s rec are obtained through scaling L mask , L cont and L rec between 0 and 1. By virtue of being derived from the respective losses, each anomaly score captures a different element of normality. s cont and s mask capture normality local to the masked region which tends to cover the most characteristic parts of the image. s rec captures the global normality of the image, taking into account how good the entire reconstruction of the image is. s avg is obtained by taking the average of s mask , s cont and s rec .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section contains a detailed analysis of the proposed method, OLED. In particular, we evaluated OLED on three datasets that are benchmarks in the novelty/anomaly detection literature, and the results are compared to recent stateof-the-art methods. Additionally, we presented a formal analysis exploring the effectiveness of masks generated by M M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>OLED is implemented in Python using the TensorFlow <ref type="bibr" target="#b34">[35]</ref>. A detailed overview of the architecture of R and M M is available in Section 3.3 and Section 3.4, respectively. t, ? and ? are set to 87.5, 1 and 50 respectively. These hyperparameters were set based on experimentation and an ablation study showing the stability of the performance across different settings. The threshold parameter can be adjusted based on the difficulty of the dataset; where larger values of the threshold are more suitable for more complex datasets. The weights of the the loss function listed as the defaults are to balance out the effect of reconstruction losses since they are on different scales. R and M M use an Adam optimizer with a learning rate of 5e ?4 , b 1 of .5 and b 2 of .9. The networks are trained for 300 epochs. Following <ref type="bibr" target="#b28">[29]</ref>, a small validation set containing 150 samples from inliers and 150 samples from outliers from the training set is used to determine the best epoch to select models R and M M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>The three datasets chosen for the experiments are MNIST <ref type="bibr" target="#b35">[36]</ref>, CIFAR-10 <ref type="bibr" target="#b36">[37]</ref> and UCSD <ref type="bibr" target="#b37">[38]</ref>. These particular datasets were chosen based on their popularity as benchmarks in the anomaly detection literature. The setups were chosen in a way that enables OLED to be compared to a variety of recent state-of-the-art methods.</p><p>MNIST: MNIST is a dataset that contains 60,000 images of handwritten digits from 0 to 9. Images in MNIST are grayscale with a resolution of 28 x 28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUCROC</head><p>OCSVM <ref type="bibr" target="#b16">[17]</ref> 0.9499 AE <ref type="bibr" target="#b4">[5]</ref> 0.9619 VAE <ref type="bibr" target="#b39">[40]</ref> 0.9643 PixCNN <ref type="bibr" target="#b40">[41]</ref> 0.6141 DSEBM <ref type="bibr" target="#b25">[26]</ref> 0.9554 MemAE <ref type="bibr" target="#b8">[9]</ref> 0.9751 OLED (Ours) s rec 0.9772 OLED (Ours) s mask 0.9851 OLED (Ours) s cont 0.9650 OLED (Ours) s avg 0.9845 CIFAR-10: CIFAR-10 is a dataset that contains 60,000 natural images of objects from across ten classes. Images in CIFAR-10 are RGB with a resolution of 32 x 32. Similar to MNIST, CIFAR-10 is also used widely as a benchmark in the anomaly detection literature. However, CIFAR-10 presents more of a challenge because images differ substantially across classes, and the background of images are not aligned.</p><p>UCSD: This dataset <ref type="bibr" target="#b38">[39]</ref> consists of two subsets (Ped1 and Ped2) with different outdoor scenes. Available objects in the frames are pedestrians, cars, skateboarders, wheelchairs, and bicycles. Pedestrians are dominant in nearly all frames and considered as the normal class, while other objects are anomalies. We assessed our method on Ped2, which includes 2,550 frames in 16 training and 2,010 frames in 12 test videos, all with a resolution of 240?360 pixels. Following <ref type="bibr" target="#b30">[31]</ref>, we calculated frame-level area under the receiver operating characteristic (AUCROC) and Equal Error Rate (EER) to evaluate performance and compare against both patch-based and full-frame setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Novelty Detection in Image Datasets</head><p>MNIST: OLED is evaluated on MNIST using the protocol defined in <ref type="bibr" target="#b8">[9]</ref>. This protocol involves dividing the dataset into ten different anomaly detection datasets corresponding to the ten predefined classes in MNIST. In each anomaly detection dataset, the inliers are sampled from 1 class, and the outliers are sampled from the remaining 9 classes. The normal data is split into train and test sets with a ratio of 2:1, and the anomaly proposition is set to be 30%. Following <ref type="bibr" target="#b8">[9]</ref>, AUCROC is the evaluation metric for this experiment.</p><p>Given the protocol in <ref type="bibr" target="#b8">[9]</ref>, OLED is compared against MemAE <ref type="bibr" target="#b8">[9]</ref> and other methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref>. The results are reported in <ref type="table" target="#tab_0">Table 1</ref>. OLED yields excellent results, surpassing MemAE and other approaches. In particular, s rec , s mask and s avg exceed all other identified approaches, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method AUCROC</head><p>OCSVM <ref type="bibr" target="#b16">[17]</ref> 0.5856 DAE <ref type="bibr" target="#b12">[13]</ref> 0.5358 VAE <ref type="bibr" target="#b39">[40]</ref> 0.5833 PixCNN <ref type="bibr" target="#b40">[41]</ref> 0.5506 GAN <ref type="bibr" target="#b0">[1]</ref> 0.5916 AND <ref type="bibr" target="#b9">[10]</ref> 0.6172 AnoGAN <ref type="bibr" target="#b0">[1]</ref> 0.6179 DSVDD <ref type="bibr" target="#b41">[42]</ref> 0.6481 OCGAN <ref type="bibr" target="#b10">[11]</ref> 0.6566 OLED (Ours) s rec 0.6622 OLED (Ours) s mask 0.6711 OLED (Ours) s avg 0.6683 OLED (Ours) s cont 0.6673 <ref type="table">Table 2</ref>. One-class novelty detection Average AUCROC results on CIFAR-10 image dataset following the protocol in <ref type="bibr" target="#b10">[11]</ref>.</p><p>recording an AUCROC of 0.977, 0.985 and 0.984, respectively. A visualization of OLED applied to both inlier and outlier samples for MNIST is available in <ref type="figure" target="#fig_0">Figure 2</ref>. Additionally, in <ref type="figure" target="#fig_1">Figure 3</ref>, the reconstructions of OLED are compared to that of a normal AE, further demonstrating the superiority of the representations offered by OLED for the anomaly detection task. CIFAR-10: OLED is evaluated on CIFAR-10 using the protocol defined in <ref type="bibr" target="#b10">[11]</ref>. This protocol involves dividing the dataset into ten different anomaly detection datasets corresponding to the ten predefined classes in CIFAR-10. In each anomaly detection dataset, the inliers are sampled from 1 class, and the outliers are sampled from the remaining 9 classes. The predefined train and test splits are used to conduct the experiments. Testing data of all classes are used for testing. Following <ref type="bibr" target="#b10">[11]</ref>, AUCROC is the evaluation metric for this experiment.</p><p>OLED is compared to OCGAN <ref type="bibr" target="#b10">[11]</ref> and other recently Method AUCROC EER TSC <ref type="bibr" target="#b42">[43]</ref> 0.922 -FRCN action <ref type="bibr" target="#b43">[44]</ref> 0.922 -AbnormalGAN <ref type="bibr" target="#b44">[45]</ref> 0.935 0.13 MemAE <ref type="bibr" target="#b8">[9]</ref> 0.941 -GrowingGas <ref type="bibr" target="#b45">[46]</ref> 0.941 -FFP <ref type="bibr" target="#b46">[47]</ref> 0.954 -ConvAE+UNet <ref type="bibr" target="#b47">[48]</ref> 0.962 -STAN <ref type="bibr" target="#b48">[49]</ref> 0.965 -Object-centric <ref type="bibr" target="#b49">[50]</ref> 0.978 -Ravanbakhsh <ref type="bibr">[</ref>  proposed methods for anomaly detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The results are reported in <ref type="table">Table 2</ref>. OLED outperforms the compared methods, including OCGAN, by a considerable margin. Particularly, s rec , s mask , s avg and s cont exceed all other identified approaches, recording an AUCROC of 0.662, 0.671, 0.6683 and 0.667, respectively. A visualization of OLED applied to both inlier and outlier samples for CIFAR-10 is available in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Video Novelty Detection</head><p>One of the common use cases of one-class classification is in the domain of novelty detection for surveillance purposes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. Nonetheless, this task is more difficult in the video domain because of the variations of mobile objects across the frames. In this experiment, each frame of the dataset is divided into patches of size 30?30 pixels following <ref type="bibr" target="#b28">[29]</ref>. Training patches only include scenes of walking pedestrians, while in the testing phase, patches are extracted from outlier frames that contain abnormal as well as normal objects. Frame-level AUROC and EER are the two metrics used to compare our method with state-ofthe-art methods in recent years. As depicted in <ref type="table" target="#tab_2">Table 3</ref>, our method outperforms recent state-of-the-art models in the video novelty detection task. More specifically, our approach achieves an AUCROC performance of 99.02% and an EER of 5.4%. The visualization in <ref type="figure" target="#fig_2">Figure 4</ref> demonstrates the separability of anomaly scores for inliers and outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Mask Module Evaluation</head><p>The results from the experiments in Section 4.3 and Section 4.4 are a clear indication that OLED is a strong method for anomaly detection. In every case, anomaly scores that leveraged masking, and by extension M M , yielded the highest performance. Visual results in <ref type="figure" target="#fig_0">Figure  2</ref> and 3 support the initial hypothesis that M M generates masks that cover important structures in the input image. Furthermore, this is the case for both inlier and outlier images. The following section seeks to solidify these observations more formally.</p><p>To quantitatively assess the effectiveness of M M in masking important parts of images, M M is re-purposed to perform a binary segmentation task that involves identifying whether or not each pixel in the input image is important. Specifically, the activation maps A generated by M M serve as the predicted semantic maps for images. A is used instead of M M (x) to avoid the threshold constraint imposed by T . Using A and the ground truth semantic maps, the pixelwise AUCROC score is computed for both inlier and outlier images.</p><p>The aforementioned analysis is realized by evaluating the M M trained on digit class 8 from the MNIST experiments in Section 4.3 on the corresponding test set. MNIST is well suited for this experiment because we are able to make the assumption that nonzero pixels are part of the digit and thus important. The ground truth semantic maps for the test set are obtained by setting non zero activations to 1 otherwise 0. The former signals the pixel corresponds to part of the written digit, and the latter signals the pixel is part of the background.</p><p>The results for the experiment are displayed in <ref type="table">Table 5</ref>. M M is able to segment important pixels in both inlier and outlier images with a high degree of accuracy with no modifications to the original architecture. This is a testament to the usefulness of M M in the anomaly detection task and hints at broader use cases in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>In order to further assess the value of the proposed learned masking approach, OLED is compared to the baseline method context autoencoders (CAE). As CAEs employ random masking during training, the following section seeks to compare the learned masking proposed by OLED with random masking utilized in CAEs. To realize this comparison, a CAE was implemented and evaluated on the MNIST dataset using the protocol outlined in Section 4.3. The CAE shared the same architecture as R. The CAE is given input images with a random 10 x 10 region cropped out during training, keeping the number of masked pixels relatively consistent with R.</p><p>The results from the above experiment are displayed in 4. Similar to OLED, s rec , s mask , s avg and s cont are reported for the CAE. OLED is able to substantially outperform CAE, despite having identical architectures for the base reconstruction module. This is a clear indication that  the learned masking approach proposed in OLED outperforms random masking for the anomaly detection task. Additionally, masking at test time enhances the performance of OLED but substantially decreases the performance of the CAE. This supports our intuition that the wrong placement of the masks by CAEs leads to suboptimal representations and introduce unwanted variations in the reconstruction error of samples that is detrimental to novelty detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The results presented in Section 4 are a clear indication of the effectiveness of OLED for the anomaly detection task. In all three anomaly detection experiments on MNIST, CIFAR-10 and UCSD, OLED outperformed state-of-the-art methods by a large margin. Additional experiments evalu- ating the performance of M M demonstrated strong performance in segmenting the most important parts of samples for both the inlier and outlier class.</p><p>As initially hypothesized, OLED is able to reconstruct samples from the inlier class with ease but struggles to reconstruct samples from the outlier class. This addresses one of the fundamental problems AE face when applied to the anomaly detection task; reconstructing outliers too well. OLED accomplishes this by offering representations that are optimized for reconstructing important parts of the inlier samples through the adversarial training of R and M M . Beyond this, anomaly detection is enhanced through the use of masking at test time.</p><p>OLED also presents the benefit of being trained endto-end, resulting in a less cumbersome training procedure than some of the identified methods. In this way, M M can be included seamlessly into existing AE-based anomaly detection methods. There are also no constraints that prevent OLED from being applied to other modalities of data. Furthermore, the core innovation proposed in this research, learned optimal masking, has the potential to be applied to other tasks in computer vision and beyond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed an adversarial framework for novelty detection in both images and videos. More specifically, our method includes a Mask Module and a Reconstructor; the Mask Module is a convolutional autoencoder that learns to cover the most important parts of images, and the Reconstructor is a convolutional encoder-decoder that strives to reconstruct the masked images. The mask module will learn to mask the parts of input in a way to increase the reconstruction loss while the Reconstructor tries to minimize it. The proposed approach allows semantically rich representations and improves novelty detection at test time by covering the most important parts of the context. We have applied our method to a variety of tasks, including outlier and anomaly detection in images and videos. The results illustrate the superiority of OLED in identifying samples related to the outlier class compared to recent state-ofthe-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>OLED Reconstructions. For both MNIST and CIFAR-10, the original image, perturbed image after applying the mask generated by M M (masks are illustrated in gray) and the final reconstruction are shown. Inlier samples are in the top two rows and outlier samples are in the bottom two rows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>AE vs OLED Reconstructions for the MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Sample of anomaly scores for both the inlier and outlier class for the UCSD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average AUCROC values on all 10 classes sampled from MNIST image dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Frame-level AUCROC and EER comparison on UCSD dataset with state-of-the-art methods.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data. Statistical Analysis and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep learning for anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03407</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Context-encoding variational autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What regularized autoencoders learn from the data-generating distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3563" to="3593" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Autoencoders for unsupervised anomaly segmentation in brain mr images: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Denner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">101952</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support vector novelty detection applied to jet engine vibration spectra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hayton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Anuzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="946" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel pca for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="874" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Group anomaly detection using flexible genre models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Neural Information Processing Systems</title>
		<meeting>the 24th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localisation based on the sparsity and reconstruction error of auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Hoseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1122" to="1124" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takehisa</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</title>
		<meeting>the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Randy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Haur</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Old is gold: Redefining the adversarially learned one-class classifier training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ha</forename><surname>Muhammad Zaigham Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14183" to="14193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiresolution knowledge distillation for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Baselizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad H Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamid R Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14902" to="14912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anomaly detection in video via selfsupervised and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Barbalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12742" to="12752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Fernanda Vi?gas</publisher>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ucsd pedestrian dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="926" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stan: Spatiotemporal adversarial networks for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1323" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training adversarial discriminators for crosschannel abnormal event detection in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1896" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1992" to="2004" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
