<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegStereo: Exploiting Semantic Information for Disparity Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong 3 SenseTime Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong 3 SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SegStereo: Exploiting Semantic Information for Disparity Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>disparity estimation ? semantic cues ? semantic feature em- bedding ? softmax loss regularization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Disparity estimation for binocular stereo images finds a wide range of applications. Traditional algorithms may fail on featureless regions, which could be handled by high-level clues such as semantic segments. In this paper, we suggest that appropriate incorporation of semantic cues can greatly rectify prediction in commonly-used disparity estimation frameworks. Our method conducts semantic feature embedding and regularizes semantic cues as the loss term to improve learning disparity. Our unified model SegStereo employs semantic features from segmentation and introduces semantic softmax loss, which helps improve the prediction accuracy of disparity maps. The semantic cues work well in both unsupervised and supervised manners. SegStereo achieves stateof-the-art results on KITTI Stereo benchmark and produces decent prediction on both CityScapes and FlyingThings3D datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Examples of prediction of unsupervised models on KITTI Stereo dataset. Left: input stereo images. Top-middle and top-right: colorized disparity and error maps predicted without semantic clues. Bottom-middle and bottom-right: colorized disparity and error maps predicted by SegStereo. With the guidance of semantic cues, disparity estimation of SegStereo is more accurate especially on the local ambiguous areas.</p><p>or resort to unsupervised learning to form photometric loss for disparity prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>. Recently, with the development of deep neural networks, the performance of disparity estimation is significantly improved <ref type="bibr" target="#b42">[43]</ref>. The deep feature extracted from networks can exploit inherent global information in paired input compared to traditional methods, therefore benefits from a large number of training data either in supervised or unsupervised manner.</p><p>Although deep learning based methods produce impressive feature representation given its large receptive field, it is still difficult to overcome local ambiguity, which is a common problem in disparity estimation. For example, in <ref type="figure">Fig. 1</ref>, the disparity prediction in the center of road and vehicle area is not correct. It is because the matching clues for disparity estimation on those ambiguous areas are not enough to guide the model to seek correct direction for convergence, which is however the central objective for both supervised and unsupervised stereo learners.</p><p>Human can perform binocular alignment well at ambiguous areas by exploiting more cues such as global perception of foreground and background, scaling relative to the known size of familiar objects, and semantic consistency for individuals. Such ambiguous areas in disparity estimation always locate in the central region given a big target. They are easy to deal with by semantic classification.</p><p>Based on the above-mentioned observation, we design an unified model called SegStereo that incorporates semantic cues into backbone disparity estimation network. Basically, we use the ResNet <ref type="bibr" target="#b18">[19]</ref> with correlation operation <ref type="bibr" target="#b10">[11]</ref> as the encoder and several deconvolutional blocks as decoder to regress a full-size disparity map. The correlation operation is designed in <ref type="bibr" target="#b10">[11]</ref> to compute matching cost volumes based on pairs of feature maps. A segmentation sub-network is employed in our model to extract semantic features that are connected to the disparity branch as the semantic feature embedding. Moreover, we propose the warped semantic consistency via semantic loss regularization, which further enhances robustness of disparity estimation. Both semantic and disparity evaluation is fully-convolutional so that the proposed SegStereo enables end-to-end training.</p><p>Our SegStereo model with semantic clues embedded benefits both unsupervised and supervised training. In the unsupervised training, both photometric consistency loss and semantic softmax loss are computed and propagated backward. Both the semantic feature embedding and semantic softmax loss can introduce beneficial constraints of semantic consistency. The results evaluated on KITTI Stereo dataset <ref type="bibr" target="#b32">[33]</ref> demonstrate the effectiveness of our strategies. We also apply the unsupervised model to CityScapes dataset <ref type="bibr" target="#b9">[10]</ref>. It yields better performance than classical SGM method <ref type="bibr" target="#b20">[21]</ref>. For the supervised training scheme, we adopt the supervised regression loss instead of unsupervised photometric consistency loss to train the model, which achieves state-of-the-art results on KITTI Stereo benchmark. We further apply the SegStereo model to FlyingThings3D dataset <ref type="bibr" target="#b30">[31]</ref>. It reaches high accuracy with normal fine-tuning.</p><p>Our main contribution and achievement are summarized below.</p><p>-We propose a unified framework called SegStereo that incorporates semantic segmentation information into disparity estimation pipeline, where semantic consistency becomes an active guidance for disparity estimation. -The semantic feature embedding strategy and semantic guidance softmax loss help train the system in both unsupervised and supervised manner. -Our method achieves state-of-the-art results on KITTI Stereo datasets. The results on CityScapes and FlyingThings3D dataset also manifest the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Supervised Stereo Matching Traditional methods design local descriptors to compute local matching cost <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>, followed by some global optimization steps <ref type="bibr" target="#b20">[21]</ref>. Zbontar and LeCun <ref type="bibr" target="#b42">[43]</ref> are the first to use CNN for matching cost computation. Luo et al. <ref type="bibr" target="#b29">[30]</ref> designed a siamese network that extracts marginal distributions over all possible disparities for each pixel. Chen et al. <ref type="bibr" target="#b7">[8]</ref> presented a multi-scale deep embedding model that fuses feature vectors learned within different scale-spaces. Shaked and Wolf <ref type="bibr" target="#b37">[38]</ref> proposed a highway network architecture with a hybrid loss that conducts multi-level comparison of image patches. Inspired by other pixel-wise labeling tasks, the fully-convolution network (FCN) <ref type="bibr" target="#b28">[29]</ref> was used to enable end-to-end learning of disparity maps. Mayer et al. <ref type="bibr" target="#b30">[31]</ref> raised DispNet with a correlation module to encode matching cues instead of picking corresponding pairs from stereo images. Kendall et al. <ref type="bibr" target="#b24">[25]</ref> proposed the GC-Net framework that combines contextual information by means of 3D convolutions over a cost volume. A three-stage network of Gidaris and Komodakis <ref type="bibr" target="#b15">[16]</ref> implements a pipeline to detect, replace, and refine disparity errors respectively. Pang et al. <ref type="bibr" target="#b33">[34]</ref> presented a cascade network where the second stage learned the residual between initial result and ground-truth values.</p><p>Yu et al. <ref type="bibr" target="#b41">[42]</ref> designed a two-stream network for generation and selection of cost aggregation proposals respectively. Liang et al. <ref type="bibr" target="#b27">[28]</ref> integrated disparity estimation and refinement into one network. It reaches state-of-the-art performance on KITTI benchmark <ref type="bibr" target="#b32">[33]</ref>. Chang and Chen <ref type="bibr" target="#b5">[6]</ref> exploited context information for finding correspondence by a pyramid stereo matching network. In contrast, our method concentrates on combining semantic information to improve disparity estimation by semantic feature embedding.</p><p>Unsupervised Stereo Matching In recent years, a number of unsupervised learning methods based on spatial transformation were proposed for view synthesis, depth prediction, optical flow and disparity estimation. Unsupervised methods get rid of the dependence of ground-truth labels, which are always expensive to access. Flynn et al. <ref type="bibr" target="#b11">[12]</ref> presented an image synthesis network called DeepStereo that learns a cost volume combined with a separate conditional color model. Xie et al. <ref type="bibr" target="#b39">[40]</ref> designed a Deep3D network that minimizes pixel-wise reconstruction loss to generate right-view images.</p><p>Garg et al. <ref type="bibr" target="#b12">[13]</ref> proposed an end-to-end framework to learn single-view depth by optimizing the projection errors in a calibrated stereo environment. The improved method <ref type="bibr" target="#b16">[17]</ref> introduces a fully-differentiable structure and an extra leftright consistency check that leads to better results. A semi-supervised approach was proposed by Kuznietsov et al. <ref type="bibr" target="#b26">[27]</ref> where supervised and unsupervised alignment loss are used to train the network for depth estimation. Yu et al. <ref type="bibr" target="#b22">[23]</ref> focused on unsupervised learning of optical flow via photometric constancy and motion smoothness. Meister et al. <ref type="bibr" target="#b31">[32]</ref> defined a bidirectional census loss to train optical flow. An iterative unsupervised learning network presented by Zhou et al. <ref type="bibr" target="#b44">[45]</ref> adopts left-right checking to pick suitable matching pairs. Compared with these unsupervised methods, our model applies warping reconstruction to both photometric image and semantic maps, along with additional semantic feature embedding, to reliably estimate disparity.</p><p>Semantic-Guided Algorithms Compared to disparity estimation, semantic segmentation is a high-level classification task where each pixel in the image is assigned to a class <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44]</ref>. Several methods apply scene parsing information to other tasks. Guney and Geiger <ref type="bibr" target="#b17">[18]</ref> leveraged object knowledge in MRF formulation to resolve stereo ambiguity. Bai et al. <ref type="bibr" target="#b1">[2]</ref> tackled instance-level segmentation and epipolar constraints to reduce the uncertainty of optical flow estimation. A cascaded classification framework of Ren et al. <ref type="bibr" target="#b34">[35]</ref> iteratively refines semantic masks, stereo correspondence and optical flow fields. Behl et al. <ref type="bibr" target="#b3">[4]</ref> integrated the instance recognition cues into a CRF-based model for scene flow estimation.</p><p>With similar motivation to ours, Cheng et al. <ref type="bibr" target="#b8">[9]</ref> designed an end-to-end trainable network called SegFlow, which enables joint learning for video object segmentation and optical flow. This model contains a segmentation branch and a flow branch whose feature maps concatenate. We differently focus on disparity estimation, where objects in the scene are captured at the same time so that stable structural information can be exploited. In addition, our SegStereo model also propagates softmax loss back to disparity branch by warping, which makes semantic information effective in the whole training process. In addition, our model enables unsupervised learning of disparity with photometric loss and semantic-aware constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Our SegStereo framework. We extract intermediate features F l and F r from stereo input. We calculate the cost volume F c via the correlation operator. The left segmentation feature map F s l is aggregated into disparity branch as semantic feature embedding. The right segmentation feature map F s r is warped to left view for per-pixel semantic prediction with softmax loss regularization. Both steps incorporate semantic information to improve disparity estimation. The SegStereo framework enables both unsupervised and supervised learning, using photometric loss L p or disparity regression loss L r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>In this section, we describe our SegStereo disparity estimation architecture, suitable for both unsupervised and supervised learning. We first present the basic network for disparity regression. Then we detail our incorporation strategies of semantic cues, including semantic feature embedding and semantic loss regularization. Both of them are effective to rectify disparity prediction. Finally, we show how disparity estimation is achieved under unsupervised and supervised conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Network Architecture</head><p>Our overall SegStereo network is shown in <ref type="figure">Fig. 2</ref>. The backbone network is ResNet-50 <ref type="bibr" target="#b18">[19]</ref>. Instead of directly computing disparity on raw pixels, we adopt the shallow part of ResNet-50 model to extract image features F l and F r on the paired input I l and I r , which is known as robust to local context information encoding.</p><p>The cost volume features for stereo matching F c are computed by correlation layer between F l and F r , similar to that of DispNetC <ref type="bibr" target="#b30">[31]</ref>. To preserve detail information on left stereo feature, we apply a convolution block on F l and obtain transformed feature F t l . Meanwhile, a segmentation network is utilized to compute semantic features F s l and F s r for left and right images respectively, sharing shallow layer representation with disparity network. The left transformed disparity features F t l , the correlated features F c and the left semantic features F s l are concatenated as hybrid feature representation F h . Here, semantic cues are preliminarily introduced to the disparity network as Semantic Feature Embedding.</p><p>After feature embedding, we feed F h into the disparity encoder-decoder to get full-size disparity map D. The disparity map is further used to warp right semantic feature F s r to left under Semantic Loss Regularization, detailed in Section 3.3. They constitute the key components of our framework. We describe more setting details in Section 4.1 and in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Feature Embedding</head><p>The basic disparity estimation frameworks work well on image patches with edges and corners where clear matching cues are located. It can be optimized with photometric loss in an unsupervised system or guided by supervised 1 norm regularization otherwise. The remaining major issue is on flat regions, as shown in the first row of <ref type="figure">Fig. 1</ref>. We use semantic cues to help prediction and rectify the final disparity map. As a result, we first incorporate the cues by embedding of semantic feature.</p><p>Our semantic feature embedding combines information from left disparity features F t l , the correlated features F c and the left semantic features F s l . It contains the following advantages. (1) The employed segmentation branch shares the shallow computation with backbone disparity network for efficient computation and effective representation. (2) The semantic feature F s l gives more consistent representations on those flat regions compared to the disparity feature F t l , which introduce object-level prior knowledge. (3) The low-level features and high-level recognition information are fused explicitly via the aggregation of F t l , F c and F s l . The experiments in Sections 4.5 and 4.6 further manifest that our semantic embedding helps disparity branch predict more convincing results in both unsupervised and supervised learning. In addition, the right semantic features F s r are reserved for the following semantic feature warping and loss regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic Loss Regularization</head><p>The semantic information cues can also guide learning of disparity as a loss term. As shown in <ref type="figure">Fig. 2</ref>, based on the predictive disparity map D, we employ feature warping on the right segmentation map F s r to get the reconstructed left segmentation mapF s l , and use left segmentation ground truth labels as guidance to learn a per-pixel classifier. Finally, the semantic cues guidance loss L seg is measured between classified warped maps and ground-truth labels.</p><p>When training the disparity network, the semantic loss L seg is propagated back to disparity branch through semantic convolutional classifier and feature warping layer. Along with basic photometric loss L p or regression loss L r , semantic loss L seg imposes extra object-aware constraints to guide disparity training. The experiments prove that semantic loss regularization can effectively resolve the local disparity ambiguities, especially in the unsupervised learning period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective Function</head><p>The semantic information detailed above can be used in both unsupervised and supervised systems. Here we detail the loss functions in these two conditions. Unsupervised Manner One image in a stereo pair can be reconstructed from the other with estimated disparity, which should be close to the original raw input. We utilize this property as photometric consistency to help learn the disparity in an unsupervised manner. Given estimated disparity D, we apply image warping ? on the right image I r and get the warped left image reconstruction as? l . Then we adopt 1 norm to regularize the photometric consistency with photometric loss L p expressed as</p><formula xml:id="formula_0">L p = 1 N i,j ? p i,j ? l i,j ? I l i,j 1 ,<label>(1)</label></formula><p>where N is the number of pixels. ? p i,j is a mask indicator to avoid outlier as image boarder or occluded regions, where no pixel correspondence exists. If the resulting photometric difference on position (i, j) is greater than a threshold , ? p i,j is 0, otherwise, it is 1.</p><p>The photometric consistency enables disparity learning in an unsupervised manner. If there is no regularization term in L p to enforce local smoothness of the estimated disparity, local disparity may be incoherent. To remedy this issue, we apply 1 penalty to disparity gradients ?D with the smoothness loss L s defined as</p><formula xml:id="formula_1">L s = 1 N i,j [? s (D i,j ? D i+1,j ) + ? s (D i,j ? D i,j+1 )],<label>(2)</label></formula><p>where ? s (?) is the spatial smoothness penalty implemented as generalized Charbonnier function <ref type="bibr" target="#b2">[3]</ref>.</p><p>With the semantic feature embedding and semantic loss, the overall loss in our unsupervised system is L unsup , containing the photometric loss L p , smoothness loss L s , and the semantic cues loss L seg . We note that disparity labels are not involved in loss computation so that disparity estimation is considered as an unsupervised learning process here. To balance learning of different loss branches, we introduce loss weights ? p for L p , ? s for L s , and ? seg for L seg . Thus the total loss L unsup is expressed as</p><formula xml:id="formula_2">L unsup = ? p L p + ? s L s + ? seg L seg .<label>(3)</label></formula><p>Supervised Manner The proposed semantic cues for disparity prediction also works in supervised training, where the ground truth disparity mapD is provided. We directly adopt the 1 norm to regularize prediction where the disparity regression loss L r is</p><formula xml:id="formula_3">L r = 1 N V i,j?V D i,j ?D i,j 1 ,<label>(4)</label></formula><p>where V is the set of valid disparity pixels inD and N V is the number of valid pixels. For utilizing the semantic cues, both feature embedding and semantic softmax loss are adopted as described in Sections 3.2 and 3.3. Loss weight ? r is used for regression term L r . The overall loss function L sup becomes</p><formula xml:id="formula_4">L sup = ? r L r + ? s L s + ? seg L seg .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we evaluate key components in the SegStereo model. We mainly pretrain the model on CityScapes dataset <ref type="bibr" target="#b9">[10]</ref> and evaluate it on KITTI Stereo 2015 dataset <ref type="bibr" target="#b32">[33]</ref>. We also compare the performance of our method with other disparity estimation methods on KITTI benchmark <ref type="bibr" target="#b32">[33]</ref>. Further, we apply our SegStereo model to FlyingThings3D dataset <ref type="bibr" target="#b30">[31]</ref> to assess performance on different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Specification</head><p>PSPNet-50 <ref type="bibr" target="#b43">[44]</ref> is employed as a segmentation network due to its high performance. The layers (from "conv1 1" to "conv3 1) of PSPNet-50 are used as the shallow part. The extracted features F l and F r have a 1/8 spatial size to raw images. We select the output of "conv5 4" layer of PSPNet-50 as semantic features. The weights in the shallow part and segmentation network are fixed when training SegStereo. For cost volume computation, we perform 1D-correlation <ref type="bibr" target="#b30">[31]</ref> between F l and F r according to epipolar constraints. Both max displacement and padding size are set to 24 so that the channel number of correlated features F c is 25. For left feature transformation, the kernel size of transformed convolutional layer is 1 ? 1 ? 256. All of F c , F t l and F s l have the same spatial size. We directly concatenate them to form the hybrid feature map F h . Disparity encoder behind hybrid features F h contains 12 residual blocks. Several common convolutional operations in residual blocks are replaced with dilation patterns <ref type="bibr" target="#b43">[44]</ref> to integrate wider context information. Disparity decoder consists of 3 deconvolutional blocks and 1 convolutional regression layer to output full-size disparity map. We provide more details in supplementary material.</p><p>The right segmentation map F s r is of 1/8 size to the raw image, while the estimated disparity map D is of full size. To perform feature warping, we first upsample F s r to the full size. We afterwards downsample warped feature map to 1/8 size and get the final reconstructed left segmentation feature map asF s l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Model Excluding Semantic Information</head><p>To validate the effect of incorporating semantic cues, we design a baseline model called ResNetCorr without any semantic information. The hybrid features F h in ResNetCorr is concatenated with the correlated features F c and left transformed features F t l . The rest encoder-decoders are attached behind F h , as that of SegStereo. The softmax L seg term is excluded in loss computation. We provide the structural definition of ResNetCorr model in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets and Evaluation Metrics</head><p>The CityScapes dataset <ref type="bibr" target="#b9">[10]</ref> is released for urban scene understanding. It provides rectified stereo image pairs and corresponding disparity maps pre-computed by SGM algorithm <ref type="bibr" target="#b20">[21]</ref>. It contains 5,000 high quality pixel-level finely annotated maps for left-view. These images are split into sets with numbers 2, 975, 500 and 1, 525 for training, validation and testing. In addition, this dataset provides 19, 997 stereo images and their SGM labels in extra training set. We will use these extra data for model pretraining.</p><p>The KITTI Stereo 2015 dataset <ref type="bibr" target="#b32">[33]</ref> contains 200 training and 200 testing image pairs. The 200 training images also has semantic labels <ref type="bibr" target="#b0">[1]</ref>. We mainly use the dataset for fine tuning and evaluation. The KITTI Stereo 2012 dataset <ref type="bibr" target="#b13">[14]</ref> also provides disparity maps, which contain 194 training and 195 testing image pairs.</p><p>The FlyingThings3D dataset <ref type="bibr" target="#b30">[31]</ref> is a virtual dataset for scene matching including optical flow estimation and disparity prediction. This dataset is rendered by computer graphics techniques with background objects and 3D models. It provides 22,390 images for training and 4,370 images for testing.</p><p>To evaluate the results, we apply the end-point-error (EPE), which measures the average pixel deviation and the bad pixel error (D1). The latter calculates the percentage of disparity errors below a threshold. Both the errors in non-occluded region (Noc) and all pixels (All) are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>Our implementation of the SegStereo model is based on a customized Caffe <ref type="bibr" target="#b23">[24]</ref>. We use the "poly" learning rate policy where current learning rate equals to the base one multiplying (1 ? iter max iter ) power . Such learning policy is also adopted in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref> for better performance. When training on CityScapes dataset, we set base learning rate to 0.01, power to 0.9. Momentum and weight decay are set to 0.9 and 0.0001, respectively. These parameters of learning policy are kept on supervised fine-tuning process.</p><p>For data augmentation, we adopt random resizing, color shift and contrast brightness adjustment. The random factor is between 0.5 to 2.0. The maximum color shift along RGB axes is set to 10 and the maximum brightness shift is set to 5. The contrast multiplier is between 0.8 and 1.2. The "cropsize" is set to 513 ? 513 and batch size is set to 16. In unsupervised training, the loss weights ? p , ? s and ? seg for photometric, softmax and smoothness terms are set to 1.0, 10.0, 0.1, respectively. The threshold in photometric loss is set to 10. When switching to supervised training, if providing semantic labels, the loss weights for ? r , ? s and ? seg for regression, softmax and smoothness term are set to 1.0, 1.0, 0.1. If no semantic labels are provided, the loss weight of softmax term is set to 0. The Charbonnier parameters ?, ? and in smoothness loss term are 0.21, 5.0 and 0.001 as described in <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Unsupervised Learning</head><p>Semantic Feature Embedding The first experiment in Tab. 1 compares the errors between ResNetCorr and SegStereo models. We observe that with semantic features from PSPNet-50, EPE is improved by 20% and the D1 error is reduced by 15% when only adopting photometric loss. When combining the photometric and smoothness losses to train the models, EPE is improved by 12% and the D1 error is reduced by 13%. It shows that semantic feature embedding significantly reduces the disparity errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Loss Regularization</head><p>The second experiment in Tab. 1 is to validate the effect of softmax loss regularization. Based on photometric loss, we use smoothness loss to penalize discontinuity on disparity maps, which reduces EPE from 2.72 to 2.17 and the D1 error from 12.08 to 10.53 on all pixels. With additional softmax loss to constrain semantic consistency, EPE decreases from 2.17 to 1.89 and the D1 error decreases from 10.53 to 10.03. Thus, the regularization of softmax loss reduces EPE by 13% and the D1 error by 5%, respectively. <ref type="figure">Fig. 3</ref> shows results of different loss combinations (with or without softmax loss). We observe that the gain of softmax loss mainly arises on big objects, such as road and car, which directly help enhance disparity prediction on local ambiguous regions. <ref type="figure">Fig. 3</ref>: Qualitative examples of unsupervised SegStereo models on KITTI Stereo 2015 dataset <ref type="bibr" target="#b32">[33]</ref>. With the guidance of softmax regularization and additional fine-tune process, the accuracy of disparity is improved. <ref type="figure">Fig. 4</ref>: Qualitative examples of unsupervised-learning version of the SegStereo model on CityScapes validation set <ref type="bibr" target="#b9">[10]</ref>. From left to right: left input images, disparity maps predicted by SGM algorithm <ref type="bibr" target="#b20">[21]</ref>, and our disparity maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetune on KITTI Stereo dataset</head><p>We compare our approach with other unsupervised methods in the third experiment as listed in Tab. 1. To adapt our model to KITTI dataset, we finely tune SegStereo on the 200 images of KITTI 2015 training set. We set the maximum iteration number to 500 and batch size to 16, so that 40 epochs are conducted. All photometric loss, smoothness loss and softmax loss are used in this process. Qualitative results in <ref type="figure">Fig. 3</ref> show that prediction errors are further reduced by fine-tuning. Our model outperforms the other two unsupervised methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b16">17]</ref> on KITTI 2015 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CityScapes Results</head><p>We adapt the unsupervised SegStereo model to CityScapes dataset <ref type="bibr" target="#b9">[10]</ref>. In <ref type="figure">Fig. 4, we</ref> give several examples to visualize quality on the validation set. Compared to the results of SGM algorithm <ref type="bibr" target="#b20">[21]</ref>, our method yields better structures in term of global scene information and details of objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Supervised Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Results</head><p>In supervised learning, the ground-truth disparity maps are directly applied to train our SegStereo model. As KITTI stereo dataset is too small, we pre-train our model on CityScapes dataset. Although the disparity maps computed by SGM algorithm contain errors and holes, they are useful for our model to get reasonable accuracy. The maximum iteration is set to 90K. Different from unsupervised training, here the disparity regression loss L r plays the major role. We also compare the performance between ResNetCorr and SegStereo. The first experiment in Tab. 2 shows that disparity error rate is slightly reduced by semantic feature embedding when we pretrain the models on CityScapes dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>In the second experiment, we fuse the extra training set in CityScapes and training set in FlyingThings3D dataset to pretrain ResNetCorr and SegStereo. Since there is no semantic labels in such two datasets, we do not compute softmax loss. The weights in segmentation branch of SegStereo is pretrained on CityScapes training set and fixed. We extend the maximum iterations to 500K. Compared to the first experiment, with more training data, both ResNetCorr and SegStereo achieve higher accuracy. And the performance of SegStereo is still better than ResNetCorr.</p><p>In the third experiment, we use KITTI Stereo 2012 and 2015 datasets to finely tune our pretrained models from the second experiment. We set the maximum iteration to 90K and base learning rate to 0.01. To facilitate performance comparison, we split Stereo 2015 training set <ref type="bibr" target="#b29">[30]</ref> so that 40 images are randomly selected for validation and the remaining 160 images are used for training. Tab. 2 lists errors on both training and validation sets. Compared to the ResNetCorr model, semantic feature embedding prevents overfitting and brings a certain improvement on disparity estimates.</p><p>To exploit more detailed matching cues on fine scales, we redesign SegStereo, where the shallow part is end with the "conv1 3" layer of PSPNet-50. To adapt to the increased feature map size, the maximum displacement and padding size of <ref type="table">Table 3</ref>: Comparison with other disparity estimation methods on the test set of KITTI 2015 <ref type="bibr" target="#b32">[33]</ref>. Our method achieves state-of-the-art results on this benchmark.  <ref type="bibr" target="#b32">[33]</ref>. By incorporating semantic information, our method is able to estimate accurate disparity. From left to right, we show left input images, disparity predictions of SegStereo, and error maps the correlation layer are both set to 96. We also up-sample the semantic feature maps from "conv5 4" layer for semantic feature embedding. This redesigned model is also pretrained on the fusion set of CityScapes and FlyingThings3D, followed by fine-tuning on KITTI Stereo dataset. The new SegStereo model (with remark "corr13" in Tab. 2) outperforms general SegStereo by leveraging more detail information. Tab. 3 compares our model to other approaches on KITTI 2015 benchmark <ref type="bibr" target="#b32">[33]</ref>. Our method achieves state-of-the-art results. <ref type="figure">Fig. 5</ref> gives several visual examples on KITTI 2015 test set. By incorporating semantic information, our SegStereo model is able to handle challenging scenarios. In supplementary material, we also provide results on KITTI 2012 benchmark <ref type="bibr" target="#b13">[14]</ref> and segmentation results.  FlyingThings3D Results To illustrate that our SegStereo model can adapt to other datasets, we test the supervised-training ResNetCorr and SegStereo on FlyingThings3D dataset <ref type="bibr" target="#b30">[31]</ref>. Here, we directly select the pretrained models from the second experiments of Tab. 2. The two models are compared with other methods on the validation set of FlyingThings3D in Tab. 4. With the guidance of semantic information, the SegStereo model outperforms ResNetCorr and becomes state-of-the-art, which indicates that segmentation modules is effective and general for disparity estimation across various datasets. <ref type="figure" target="#fig_0">Fig. 6</ref> shows several visual examples on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a unified model SegStereo, which integrates semantic feature maps into disparity prediction pipeline. A softmax loss is combined with common photometric loss or disparity regression loss to enable training in both unsupervised and supervised manners. Our SegStereo leads to more reliable results, especially on ambiguous areas. Experiments on KITTI Stereo datasets demonstrate the effectiveness of the semantic-guided strategy. Our method achieves state-of-the-art performance on this benchmark. Results on CityScapes and FlyingThings3D datasets further manifest its adaptability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative examples of ResNetCorr and SegStereo model on FlyingTh-ings3D validation set<ref type="bibr" target="#b30">[31]</ref>. From left to right, left images, ground-truth, ResNet-Corr results and SegStereo results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of unsupervised training models on KITTI Stereo 2015<ref type="bibr" target="#b32">[33]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">Noc pixels EPE D1 Error</cell><cell cols="2">All pixels EPE D1 Error</cell></row><row><cell cols="3">1. Evaluation of semantic feature embedding</cell><cell></cell><cell></cell></row><row><cell>ResCorr (photometric loss)</cell><cell>2.46</cell><cell>12.78</cell><cell>3.36</cell><cell>14.08</cell></row><row><cell>SegStereo (photometric loss)</cell><cell>1.98</cell><cell>10.76</cell><cell>2.72</cell><cell>12.08</cell></row><row><cell>ResCorr (photometric loss + smooth loss)</cell><cell>2.13</cell><cell>11.05</cell><cell>2.43</cell><cell>12.16</cell></row><row><cell>SegStereo (photometric loss + smooth loss)</cell><cell>1.87</cell><cell>9.39</cell><cell>2.17</cell><cell>10.53</cell></row><row><cell cols="3">2. Evaluation of softmax loss regularization</cell><cell></cell><cell></cell></row><row><cell>SegStereo (photometric)</cell><cell>1.98</cell><cell>10.76</cell><cell>2.72</cell><cell>12.08</cell></row><row><cell>SegStereo (photometric + smooth)</cell><cell>1.87</cell><cell>9.39</cell><cell>2.17</cell><cell>10.53</cell></row><row><cell cols="2">SegStereo (photometric + smooth + softmax) 1.61</cell><cell>8.95</cell><cell>1.89</cell><cell>10.03</cell></row><row><cell cols="3">3. Comparison to other unsupervised methods</cell><cell></cell><cell></cell></row><row><cell>Zhou [45]</cell><cell>-</cell><cell>8.61</cell><cell>-</cell><cell>9.91</cell></row><row><cell>Godard [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.19</cell></row><row><cell>SegStereo (pretrain on Cityscapes dataset)</cell><cell>1.61</cell><cell>8.95</cell><cell>1.89</cell><cell>10.03</cell></row><row><cell>SegStereo (ft on KITTI Stereo dataset)</cell><cell>1.46</cell><cell>7.70</cell><cell>1.84</cell><cell>8.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of supervised-training models evaluated on KITTI Stereo 2015<ref type="bibr" target="#b32">[33]</ref> </figDesc><table><row><cell></cell><cell></cell><cell cols="2">Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>EPE Noc</cell><cell>All</cell><cell>Noc</cell><cell>D1</cell><cell>All</cell><cell cols="2">EPE Noc</cell><cell>All</cell><cell>Noc</cell><cell>D1</cell><cell>All</cell></row><row><cell></cell><cell cols="6">1. Pretrained on Cityscapes dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNetCorr</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>1.43</cell><cell></cell><cell>1.46</cell><cell>7.33</cell><cell></cell><cell>7.64</cell></row><row><cell>SegStereo</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>1.39</cell><cell cols="2">1.41</cell><cell>7.01</cell><cell></cell><cell>7.34</cell></row><row><cell></cell><cell cols="9">2. Pretrained on Cityscapes extra set and FlyingThings3D dataset</cell><cell></cell></row><row><cell>ResNetCorr</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>1.19</cell><cell></cell><cell>1.21</cell><cell>5.46</cell><cell></cell><cell>5.64</cell></row><row><cell>SegStereo</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>1.15</cell><cell cols="2">1.17</cell><cell>5.20</cell><cell></cell><cell>5.38</cell></row><row><cell></cell><cell cols="8">3. Finetune on KITTI stereo 2012 and 2015 dataset</cell><cell></cell><cell></cell></row><row><cell>ResNetCorr</cell><cell>0.40</cell><cell>0.41</cell><cell>0.68</cell><cell></cell><cell>0.76</cell><cell>0.73</cell><cell></cell><cell>0.76</cell><cell>2.13</cell><cell></cell><cell>2.40</cell></row><row><cell>SegStereo</cell><cell>0.40</cell><cell>0.41</cell><cell>0.65</cell><cell></cell><cell>0.70</cell><cell>0.73</cell><cell></cell><cell>0.75</cell><cell>2.11</cell><cell></cell><cell>2.30</cell></row><row><cell cols="2">SegStereo (corr13) 0.39</cell><cell>0.40</cell><cell>0.65</cell><cell></cell><cell>0.70</cell><cell>0.66</cell><cell cols="2">0.70</cell><cell>1.96</cell><cell></cell><cell>2.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with other disparity estimation methods on the test set of FlyingThings3D<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell cols="8">Model SGM [21] DispNetC [31] GC-Net [25] CRL [34] iResNet [28] ResNetCorr SegStereo</cell></row><row><cell>EPE</cell><cell>7.29</cell><cell>2.33</cell><cell>1.84</cell><cell>1.67</cell><cell>1.27</cell><cell>3.50</cell><cell>1.45</cell></row><row><cell>D1</cell><cell>16.18</cell><cell>10.04</cell><cell>9.67</cell><cell>6.70</cell><cell>4.90</cell><cell>8.45</cell><cell>3.50</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Augmented reality meets deep learning for car instance segmentation in urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03077</idno>
		<title level="m">A more general robust loss function</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3d scene flow estimation in autonomous driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A deep visual correspondence embedding model for stereo matching costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2012)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient large-scale stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Detect, replace, refine: Deep structured prediction for pixel wise labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2017) 1, 4</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Displets: Resolving stereo ambiguities using object knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast dense stereo correspondences by binary locality sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation of stereo matching costs on images with radiometric differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM MM</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method for learning matching errors for stereo computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2016) 3, 5, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2015) 3, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A twostage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cascaded scene flow prediction using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepmatching: Hierarchical deformable dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patch based confidence prediction for dense disparity map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved stereo matching with constant highway networks and reflective confidence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep stereo matching with explicit cost aggregation sub-architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised learning of stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
