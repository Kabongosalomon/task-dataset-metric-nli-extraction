<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Ding</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>5 SenseTime</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language-queried video actor segmentation aims to predict the pixel-level mask of the actor which performs the actions described by a natural language query in the target frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatiotemporal feature for the target frame. Though 3D convolutions are amenable to recognizing which actor is performing the queried actions, it also inevitably introduces misaligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccurate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which contains a 3D temporal encoder over the video clip to recognize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to flexibly integrate spatial and temporal features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial-and temporal-relevant linguistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new stateof-the-art performance on two popular benchmarks with less computational overhead than previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep models have achieved notable progress in computer vision and other fields <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref>. Languagequeried video actor segmentation <ref type="bibr" target="#b11">[12]</ref> is an emerging task * Equal contribution ? Corresponding author The input video clip. (c) The spatial encoder can generate fine segmentation but may misidentify other actors due to weak action recognition ability. (d) The temporal encoder can recognize which actor is performing the queried action but may introduce misaligned spatial feature into the target frame, yielding inaccurate segmentation. (e) By integrating spatial and temporal encoders, the correct actor in the target frame can be well segmented. whose goal is to predict pixel-level mask for the actor performing some actions in a video described by a natural language query. Different from language-queried video spatial or temporal localization <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b47">48]</ref>, this task requires more fine-grained spatial-temporal modeling and visuallinguistic interaction to generate pixel-level prediction, thus is more challenging. At the intersection of computer vision and natural language processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref>, this task enjoys a wide range of applications such as languagedriven video editing <ref type="bibr" target="#b20">[21]</ref>, intelligent surveillance video processing <ref type="bibr" target="#b34">[35]</ref> and human-robot interaction <ref type="bibr" target="#b29">[30]</ref>.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, given an input query "a white and brown cat is jumping backward" and an input video clip (we show 3 frames for brevity where the target frame is in the middle), language-queried video actor segmentation aims to segment the queried cat on the target frame. Since the output is based on the context of the whole video clip, we claim that both temporal modeling over the video clip and spatial modeling over the target frame are essential to solve this task. On one hand, as there are two white and brown cats in the target frame, spatial modeling cannot identify the correct cat by exploiting only appearance information. It instead inclines to producing fine but falsepositive predictions on other cats. Therefore, the queried action needs to be recognized by incorporating information from adjacent frames to distinguish the jumping cat from the sitting one, leading to the necessity of temporal modeling over the video clip. On the other hand, the jumping cat has various poses and locations in 3 frames. Features of these spatially-misaligned pixels from adjacent frames will disturb the feature representation of the target frame during temporal modeling. The correspondence between the feature of the target frame and its ground-truth mask is hence broken. Thus, spatial modeling over the target frame is also necessary to provide precise spatial feature.</p><p>However, existing approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29]</ref> conduct only temporal modeling over the video clip. Concretely, they first feed the video clip into a temporal encoder (3D CNN) to extract cross-frame video features, then apply temporal pooling over the time dimension to obtain a mixed feature of the target frame. As discussed above, mixing multi-frame spatial information will result in confused spatial feature of the target frame, leading to inaccurate segmentation. To tackle this limitation, we propose a collaborative spatial-temporal framework which contains two encoders to conduct spatial modeling over the target frame and temporal modeling over the video clip respectively. For the temporal encoder, we adopt a 3D CNN to identify the actor performing the queried action, which can be regarded as the coarse localization of the correct actor by temporal modeling. For the spatial encoder, we adopt a 2D CNN to extract precise spatial feature of the target frame, which serves as the fine segmentation of the correct actor by spatial modeling. To effectively integrate features from the two encoders, we introduce a Language-Guided Feature Selection (LGFS) module in the decoder to combine the two features with flexible channel selection weights, which are generated from the linguistic feature. Thus, language query serves as a selector to form comprehensive spatial-temporal feature for accurate segmentation.</p><p>In addition, language query contains both spatialrelevant information (appearance words, e.g., "white and brown") and temporal-relevant information (action words, e.g., "jumping backward"). When interacting with vi-sual feature from the spatial encoder, features of spatialrelevant words should play a more important role than temporal-relevant words and vice versa. Therefore, we also propose a Cross-Modal Adaptive Modulation (CMAM) module which dynamically recombines linguistic features by cross-modal attention, yielding spatial-or temporalrelevant linguistic features to adaptively modulate corresponding visual features. By densely inserting our CMAM module into each stage of the two encoders, visual features can interact with linguistic features hierarchically and dynamically to highlight regions of the correct actor in spatial and temporal aspects.</p><p>The main contributions of our paper are summarized as follows: 1) We propose a collaborative spatial-temporal framework which contains a temporal encoder to recognize the queried action and a spatial encoder to generate accurate segmentation of the actor. A Language-Guided Feature Selection (LGFS) module is proposed in the decoder to aggregate spatial and temporal features comprehensively. 2) We also propose a Cross-Modal Adaptive Modulation (CMAM) module to conduct spatial-and temporal-relevant multimodal interaction dynamically in each stage of the two encoders. 3) Extensive experiments on two popular benchmarks show our method outperforms previous stateof-the-art methods with less computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Actor and Action Video Segmentation</head><p>To simultaneously infer various types of actors undergoing various actions, Xu et al. <ref type="bibr" target="#b43">[44]</ref> collect a video dataset (A2D) in which both actors and actions in each video are annotated with pixel-level labels, and introduce a new task named actor and action video segmentation. They propose to formulate actors and actions using supervoxels and exploit a trilayer model to reason their relationships for joint labeling. Later in <ref type="bibr" target="#b42">[43]</ref>, they propose a graphical model to enable long-range interaction modeling among video parts. Yan et al. <ref type="bibr" target="#b45">[46]</ref> explore the actor-action segmentation task in a weakly supervised setting with a multitask ranking model. As deep models shown their powerful representation learning ability, Kalogeiton et al. <ref type="bibr" target="#b18">[19]</ref> perform joint actor-action video segmentation via a "detectionsegmentation" approach. Gavrilyuk et al. <ref type="bibr" target="#b11">[12]</ref> further extend the A2D dataset with natural language descriptions and propose a new task called language-queried video actor segmentation. They exploit dynamic filters predicted from language features to convolve with video features and conduct segmentation based on the convolved heatmaps. Based on <ref type="bibr" target="#b11">[12]</ref>, Wang et al. <ref type="bibr" target="#b38">[39]</ref> incorporate deformable convolutions <ref type="bibr" target="#b5">[6]</ref> into dynamic filters to capture geometric variations. ACGA <ref type="bibr" target="#b39">[40]</ref> explores co-attention mechanism between video and language features to extract multimodal </p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ?</formula><p>A white and brown cat is jumping backward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow of Visual Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow of Text Features</head><p>CNNx Conv Layers GRU <ref type="figure">Figure 2</ref>. Overall architecture of our method. Spatial and temporal encoders extract features of the target frame and the video clip respectively, aided by CMAM which dynamically interacts multimodal features in each stage.</p><p>LGFS is also densely applied in each stage of the decoder to flexibly fuse spatial and temporal features.</p><p>context for feature enhancement. Capsule networks <ref type="bibr" target="#b32">[33]</ref> are exploited in <ref type="bibr" target="#b27">[28]</ref> to encode video and language features for more effective representations that convolutions. PRPE <ref type="bibr" target="#b28">[29]</ref> proposes a polar positional encoding method to better localize the actor queried in the video. Different from the above works which only use 3D CNNs to extract mixed spatio-temporal feature, we introduce a 2D spatial encoder to collaborate with 3D temporal encoder for compensating the spatial misalignment brought by the temporal encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Language-Queried Video Actor Localization</head><p>Some works have explored the alignment between visual and linguistic modalities by localizing actors and actions in the video by bounding boxes with language queries. For 1D temporal localization, Chen et al. <ref type="bibr" target="#b2">[3]</ref> propose a cross-gated attended recurrent network to match video sequence and sentence, and perform cross-frame matching with a selfinteractor. Local and global video feature integration is explored in <ref type="bibr" target="#b0">[1]</ref> to match with input sentence more comprehensively. For 2D spatial localization, Yamaguchi et al. <ref type="bibr" target="#b44">[45]</ref> first detect persons with spatio-temporal tubes and then conduct matching between tubes and textual descriptions. For 3D spatial-temporal localization, Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose a interesting task of localizing a spatial-temporal tube in the video corresponding to the given sentence in a weakly-supervised manner. Different from the above works, we identify the actors and actions more precisely with segmentation masks, providing fine-grained multimodal understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Spatio-Temporal Modeling</head><p>Spatio-temporal modeling <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref> is the key to solve video-related tasks. A direct way of spatio-temporal modeling is to use 3D CNNs such as C3D <ref type="bibr" target="#b36">[37]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref>, etc. To reduce the computational budget of 3D convolutions, (2+1)D ConvNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31]</ref> is proposed to decompose 3D convolution. SlowFast <ref type="bibr" target="#b7">[8]</ref> proposes a slow path and a fast path to model spatial and motion information respectively. TSM <ref type="bibr" target="#b26">[27]</ref> proposes a temporal shift module which shifts a portion of feature channels along the time dimension, and this operation can be regarded as a special case of 1D temporal convolution. In this paper, our model shares the same spirit with SlowFast where a spatial encoder and a temporal encoder are combined to collaboratively extract finer spatio-temporal context for pixel-level classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall architecture of our method is illustrated in <ref type="figure">Figure 2</ref>. The input video clip and query are processed by visual and linguistic encoders respectively (i.e., CNNs <ref type="bibr" target="#b35">[36]</ref> for video data and GRU <ref type="bibr" target="#b4">[5]</ref> for sentence data). For visual modality, a spatial encoder and a temporal encoder are designed to extract spatial-and temporal-aware visual features respectively. In each stage of spatial and temporal visual encoders, visual features and linguistic features are fed into our proposed Cross-Modal Adaptive Modulation (CMAM) module to dynamically highlight visual features matched with the linguistic features. Then in the decoder, we propose a Language-Guided Feature Selection (LGFS) module to selectively fuse features of the spatial and temporal encoders from each stage. By progressive fusion and upsampling, our decoder produces a feature map of the same size as the input target frame to predict the segmentation mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual and Linguistic Encoders</head><p>Given a video clip with T frames where the target frame annotated with ground-truth mask is in the middle, we adopt Inception V3 <ref type="bibr" target="#b35">[36]</ref> as the spatial encoder to process the target frame, and I3D <ref type="bibr" target="#b1">[2]</ref> as the temporal encoder to process the whole video clip. We denote features of the i-th stage (i ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>) from the spatial and temporal visual encoders</p><formula xml:id="formula_1">as V i S ? R H i ?W i ?C i V and V i T ? R T i ?H i ?W i ?C i V respec- tively, where T i , H i , W i , and C i</formula><p>V are the frame number, height, width and channel number of the i-th visual feature. We also adopt an 8-dimensional coordinate feature to encode relative position information of each pixel following <ref type="bibr" target="#b39">[40]</ref>. Since the coordinate feature is densely fused with visual features in each stage of the encoders, we omit its denotation in the following formulas for ease of presentation. For the input textual query with N words, we utilize GRU <ref type="bibr" target="#b4">[5]</ref> to extract the linguistic feature which is denoted as L ? R N ?C L where C L denotes the channel number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Modal Adaptive Modulation</head><p>Our CMAM aims to enable visual and linguistic features to interact with each other for highlighting visual features which are matched with the corresponding linguistic clues. We insert the proposed CMAM module into each stage of the spatial and temporal encoders. To clearly elaborate the multimodal interaction process in CMAM, we take the i-th stage of our spatial encoder as an example and omit the superscript i for simplicity. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, given the visual feature V S ? R H?W ?C V of the target frame and the linguistic feature L ? R N ?C L of the sentence, we first conduct cross-modal attention between V S and L to compute an attention map A ? R N ?HW which measures the feature relevance between each word and the target frame. Concretely, V S and L are first transformed to the same subspace by convolutions:</p><formula xml:id="formula_2">V S = Conv 2d (V S ),<label>(1)</label></formula><formula xml:id="formula_3">L S = Conv 1d (L),<label>(2)</label></formula><p>where V S ? R H?W ?C M , L S ? R N ?C M . Then, V S is reshaped to R HW ?C M to match the matrix dimensions. We further perform matrix product between V S and L S to obtain attention map A as follows:</p><formula xml:id="formula_4">A = L S ? V S T<label>(3)</label></formula><p>where ? denotes matrix product.</p><p>Here A ? R N ?HW measures the relevance between each word and each spatial location. Then we add all the values on the HW dimension and normalize it as follows:</p><formula xml:id="formula_5">? = HW j=1 A j , ? = Sof tmax( ? ? 2 ),<label>(4)</label></formula><p>where ? 2 denotes the L 2 norm of a vector, A j ? R N is the feature relevance between the j-th spatial location and N words, and? ? R N is the normalized global feature relevance between each word and the whole target frame. Therefore, we can use these N weights to linearly re-combine features of N words to attain adaptive sentence feature l S = N k=1 (? k L k ) ? R C L which contains more spatial information matched with the spatial feature V S of the target frame.</p><p>Afterwards, a linear layer and sigmoid function are adopted to transform l S to R C V dimensions and generate channel-wise modulation weightsl S ? R C V :</p><formula xml:id="formula_6">l S = ?(Linear(l S )),<label>(5)</label></formula><p>where ? denotes sigmoid function. Inspired by SENet <ref type="bibr" target="#b13">[14]</ref>, we multiplyl S with feature of the target frame V S to highlight sentence-relevant visual feature channels and add the modulated feature with original V S to ease optimization:</p><formula xml:id="formula_7">V S = V S + V S l S ,<label>(6)</label></formula><p>where denotes elementwise product,? S is the output of the CMAM module and serves as the input feature of the next stage in the spatial visual encoder. For the temporal visual encoder, the same operations are applied on all the T frames to highlight sentence-relevant temporal visual features in each stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Language-Guided Feature Selection</head><p>In this section, we will elaborate our spatio-temporal decoder which progressively incorporates modulated features from spatial and temporal encoders to recover the feature resolution to the size of the input frame for mask prediction. The key problem is which encoder we should trust more for prediction during feature incorporation. To solve this problem, we propose a Language-Guided Feature Selection (LGFS) module to select the ratio of spatial and temporal features in the incorporation process under the guidance of linguistic feature. Our decoder contains 5 stages for consistency with the encoders and we take the i-th stage (i ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>) as an example to detail our LGFS module.</p><p>Concretely, we conduct average pooling on the linguistic features L ? R N ?C L to obtain feature of the whole sentence l ? R C L . Since l contains both spatial-and temporalrelevant linguistic information, we can use it to capture channel dependencies between spatial and temporal visual features inspired by SKNet <ref type="bibr" target="#b21">[22]</ref>, which proves to be effective on image classification. Two linear layers are applied on l to generate raw selection weights of each pair of channels from spatial and temporal visual features as follows:</p><formula xml:id="formula_8">g S = Linear(l), g T = Linear(l),<label>(7)</label></formula><p>where g S ? R C V and g T ? R C V have the same channel number with visual features. We apply Sof tmax over each pair of channels of g S and g T to produce the normalized channel selection weightsg S andg T . The incorporated feature V i F is obtained as follows:</p><formula xml:id="formula_9">V i F =? i S g S +? i T g T ,<label>(8)</label></formula><p>where denotes elementwise product using broadcasting rule. We slightly abuse the notation of? i T to denote the modulated feature of the target frame from the temporal encoder for simplicity. Finally, the output feature of the i-th stage in our decoder V i D ? R H i ?W i ?C i V is defined as:</p><formula xml:id="formula_10">V i D = V i F , i = 5, V i F + U psample(V i+1 D ), 1 ? i ? 4.<label>(9)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We conduct experiments on two popular languagequeried video actor segmentation benchmarks including A2D Sentences <ref type="bibr" target="#b11">[12]</ref> and J-HMDB Sentences <ref type="bibr" target="#b11">[12]</ref>. We adopt Overall IoU, Mean IoU and Precision@X (P@X) as metrics to evaluate our model following prior works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref>. Overall IoU calculates the ratio of the accumulated intersection area over the accumulated union area between predictions and ground-truth masks on all the test samples, while Mean IoU calculates the averaged IoU over all the test samples. Precision@X measures the percentage of test samples whose IoU are higher than a predefined threshold X, where X ? [0.5, 0.6, 0.7, 0.8, 0.9]. We also compute the Average Precision (AP) over the section of [0.50 : 0.05 : 0.95].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Following prior works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29]</ref>, we use the I3D <ref type="bibr" target="#b1">[2]</ref> networks pretrained on Kinetics400 <ref type="bibr" target="#b1">[2]</ref> dataset as our temporal visual encoder. For spatial visual encoder, we adopt Inception V3 <ref type="bibr" target="#b35">[36]</ref> pretrained on ImageNet <ref type="bibr" target="#b6">[7]</ref> dataset. We adopt two GRUs for the two visual encoders to extract linguistic features respectively. The maximum length of the input sentence is set as 20. We sample T = 8 RGB frames as the video input to our model where the annotated target frame is in the middle. The input frames are resized and padded to 320 ? 320. Adam <ref type="bibr" target="#b19">[20]</ref> is utilized as the optimizer and the training process is divided into two stages. First, we train the spatial and temporal networks (both encoders and decoders) respectively on A2D Sentence dataset for 12 epochs with batch size 8 and learning rate 5e ?4 (divided by 10 every 8 epochs). Then, the two pretrained encoders are combined with a random-initialized decoder and fixed during finetuning the decoder for another 4 epochs with the same learning rate 5e ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>We conduct experiments on A2D Sentences and J-HMDB Sentences to compare our method with pervious state-of-the-arts. As illustrated in <ref type="table">Table 1</ref>, our method outperforms pervious state-of-the-arts on A2D Sentences test set, indicating the effectiveness of collaborative learning of spatial and temporal encoders and adaptive visual-linguistic interaction. Comparing with CMDy <ref type="bibr" target="#b38">[39]</ref> and PRPE <ref type="bibr" target="#b28">[29]</ref>, our method achieves 3.0% and 3.2% absolute improvements on Mean IoU respectively. For the most rigorous metric P@0.9, our method is also superior than the previous performances of CMDy and PRPE, demonstrating that our method can not only accurately identify the correct actor through cross-modal alignment, but also generate complete mask to cover the actor. Since Overall IoU favors large actors while Mean IoU treating actors of different scales equally, our improvements on IoU metrics also show that our method can well handle the scale variation of actors.</p><p>We further verify the generalization ability of our method on J-HMDB Sentences test set. Following prior works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, we use the best model pretrained on A2D Sentences dataset to directly evaluate all the test samples in J-HMDB Sentences without finetuning. For each testing video, 3 frames are uniformly sampled to evaluate the performance. As shown in the <ref type="table">Table 2</ref>, our method accomplishes significant performance gains over previous state-of-the-arts, indicating that our method can excavate richer multimodal information through the mutual enhancement of spatial and temporal encoders, so as to obtain stronger generalization ability. Noted that all the methods including ours produce approximate zero performance on P@0.9, which is probably because models without training on J-HMDB Sentences cannot generate particularly fine </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We conduct ablation studies on the A2D Sentences dataset to evaluate different design of our framework.</p><p>Component Analysis. We summarize the ablation results of our proposed encoders and modules in <ref type="table" target="#tab_3">Table 3a</ref>. The 1st and 2nd row denote our spatial-and temporalonly baselines where multimodal interactions only occurs in the decoders by visual and linguistic feature concatenation. Spatial and temporal baselines outperform each other on P@0.9 and P@0.5 respectively, which indicates spatial encoder can yield finer segmentation while temporal encoder can identify the actors more accurately. When simply combining the two encoders together in the 3rd row, we can observe an obvious performance boost in all metrics, which well demonstrates the complementarity of spatial and temporal encoders. Incorporating our LGFS module is able to further improve the performance, which shows that using language information as guidance can select effective spatial and temporal features more flexibly. In the 5th row, our CMAM module also brings large performance gain over a strong result in the 4th row. We insert CMAM into each stage of the spatial and temporal encoders to conduct multimodal feature interaction and remove the feature concatenation in our baselines. Results of CMAM prove the effectiveness of modulating visual features by dynamically recombine spatial-and temporal-relevant linguistic features.</p><p>Spatial-Temporal Feature Fusion. <ref type="table" target="#tab_3">Table 3b</ref> presents results of different spatial-temporal feature fusion methods without CMAM module. Elementwise addition and maximization produce similar results. However, these simple fusion methods usually lack the ability to select appropriate spatial and temporal visual information according to the needs of language information. Our LGFS compensates for this deficiency and outperforms the above two operations, demonstrating the effectiveness of language guidance.</p><p>Inserting Positions of CMAM. We evaluate different inserting positions of CMAM and summarize the results in <ref type="table" target="#tab_3">Table 3c</ref>. Inserting CMAM into the 5th and 4th stages of our spatial-temporal encoders can bring relatively significant improvements, which is probably because features from deep layers usually contain more high-level semantic information and are beneficial to the segmentation. As we insert CMAM into the shallow layers of encoders, segmentation performance is also constantly improved, showing the Spatial Temporal</p><p>LGFS  Backbones. <ref type="table" target="#tab_3">Table 3d</ref> shows the results of different backbone selections for our spatial and temporal encoders. We conduct experiments on ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and S3D <ref type="bibr" target="#b41">[42]</ref>, which have similar capacities with their Inception and I3D counterparts. From the table we can observe that models with different backbones exhibit stable and high performances, which demonstrates that our collaborative spatialtemporal framework can well adapt to different backbones. For fair comparison with previous approaches, we adopt I3D and Inception V3 (which has moderate computational overhead comparing with ResNet-50) as our temporal and spatial encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Size GFLOPs AP ACGA <ref type="bibr" target="#b39">[40]</ref> 16 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computational Overhead</head><p>We calculate the computational overhead of previous methods and ours in <ref type="table" target="#tab_4">Table 4</ref>. Since CMDy has not released code, we estimate its computational overhead according to the details in their paper, including I3D backbone and the same input size as reported in the paper of ACGA. Both CMDy and ACGA rely on large input size to retain their performances. However, our method outperforms theirs with significant margins using 3? less GFLOPs and much smaller input size, showing that our method is more efficient in excavating useful information from multimodal features. Noted that our introduced spatial encoder only takes up 9.2% computational overhead of our full framework but brings considerable performance improvement, which well demonstrates the effectiveness of our collaborative spatialtemporal modeling framework. <ref type="figure">Figure 4</ref> presents the visualization results of our model on target frames, which provides qualitative analysis on the complementarity of our spatial and temporal encoders. As shown in the 2nd row, using only spatial encoder tends to make false-positive predictions on other actors (e.g., the man) which are irrelevant with the description due to the unawareness of action information, albeit the generated masks are relatively accurate. When only temporal encoder is used, the woman who is rolling can be correctly located but the segmentation result lacks some local details. For example, part of the woman's legs is misclassified as background. Incorporating both the spatial and temporal encoders yields precise segmentation on the correct actor. Similar phenomenon also appears in other examples of <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Analysis</head><p>We also visualize the attention maps between words and target frames in CMAM from spatial and temporal encoders in <ref type="figure">Figure 5</ref>. In the 1st row, spatial-relevant words "small" and "baby" yield highly-responsive attention maps on body of the two babies in the spatial encoder, while temporalrelevant word "crawling" mainly focuses on the moving "A car is leading another car in a race" "Player rolling on the mat"  hands and heads of the two babies in the temporal encoder. The word "running" in the 3rd row also focuses on the moving black dog to capture actions in the temporal encoder. These results show CMAM can well associate visual and linguistic features using spatial and temporal information.</p><p>Noted that for actor who only appears in a part of the video, our method can handle such case. For each frame, through our spatial and temporal encoders, intra-frame visual information and global cues from the whole video clip are collected and fused together. Then, the final segment per frame is guided by linguistic cues. Thus, our model does not explicitly leverage any causal information or frame-byframe propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we explore the language-queried video actor segmentation task. We propose a collaborative encoderdecoder framework containing a 3D temporal encoder to recognize the queried actions and a 2D spatial encoder to well segment the actors, which alleviates the spatial misalignment issue brought by 3D CNNs in prior works. An LGFS module is introduced in the decoder to flexibly fuse spatial-temporal features. In addition, we also propose a CMAM module to dynamically recombine linguistic features for more adaptive multimodal feature interaction in each encoder. Our method outperforms previous methods by large margins on two popular benchmarks with 3? less computational overhead. In the future, we hope to accelerate current framework by designing more lightweight spatial and temporal encoders.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our motivation. (a) The target frame. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of CMAM module. Linguistic feature is dynamically recombined based on the relevance with visual feature. "S" and "T" subscripts in notations are omitted to denote general spatial or temporal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in the middle is sitting in front of camera"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 ."Figure 5 .</head><label>45</label><figDesc>Qualitative analysis on target frames. (a) Target frame. (b) Results of our model using spatial encoder only. (c) Results of our model using temporal encoder only. (d) Results of our model using spatial and temporal encoders. (e) Ground-truth. "Small baby crawling on the right" The toddler in a yellow shirt is walking a black lab" Visualization of attention maps between words and frames in CMAM. (a) Target frame. (b) Attention maps in spatial encoder. (c) Attention maps in temporal encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Spatial Encoder Temporal Encoder Decoder ? Target Frame Prediction T frames</head><label></label><figDesc></figDesc><table><row><cell>CMAM</cell><cell>CMAM</cell><cell>CMAM</cell><cell>CMAM</cell><cell>CMAM</cell></row><row><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>CNN4</cell><cell>CNN5</cell></row><row><cell>LGFS</cell><cell>LGFS</cell><cell>LGFS</cell><cell>LGFS</cell><cell>LGFS</cell></row><row><cell>Conv&amp;Up2x</cell><cell>Up2x</cell><cell>Up2x</cell><cell>Up2x</cell><cell>Up2x</cell></row><row><cell>CMAM</cell><cell>CMAM</cell><cell>CMAM</cell><cell>CMAM</cell><cell>CMAM</cell></row><row><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>CNN4</cell><cell>CNN5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean Hu et al. Comparison with state-of-the-art methods on A2D Sentences test set. Our method significantly outperforms previous methods using only RGB input. ? denotes utilizing additional optical flow input. Comparison with state-of-the-art methods on J-HMDB Sentences test set using the best model trained on A2D Sentences without finetuning. Our method shows notable generalization ability. ? denotes training more layers of I3D backbone on A2D Sentences.</figDesc><table><row><cell cols="2">Method P@0.5 [15] ECCV2016 34.8</cell><cell>23.6</cell><cell>Precision 13.3</cell><cell>3.3</cell><cell>0.1</cell><cell>AP 13.2</cell><cell>47.4</cell><cell>IoU</cell><cell>35.0</cell></row><row><cell>Li et al. [23] CVPR2017</cell><cell>38.7</cell><cell>29.0</cell><cell>17.5</cell><cell>6.6</cell><cell>0.1</cell><cell>16.3</cell><cell>51.5</cell><cell></cell><cell>35.4</cell></row><row><cell>Gavrilyuk et al. [12] CVPR2018</cell><cell>47.5</cell><cell>34.7</cell><cell>21.1</cell><cell>8.0</cell><cell>0.2</cell><cell>19.8</cell><cell>53.6</cell><cell></cell><cell>42.1</cell></row><row><cell>Gavrilyuk et al. [12] ? CVPR2018</cell><cell>50.0</cell><cell>37.6</cell><cell>23.1</cell><cell>9.4</cell><cell>0.4</cell><cell>21.5</cell><cell>55.1</cell><cell></cell><cell>42.6</cell></row><row><cell>ACGA [40] ICCV2019</cell><cell>55.7</cell><cell>45.9</cell><cell>31.9</cell><cell>16.0</cell><cell>2.0</cell><cell>27.4</cell><cell>60.1</cell><cell></cell><cell>49.0</cell></row><row><cell>VT-Capsule [28] CVPR2020</cell><cell>52.6</cell><cell>45.0</cell><cell>34.5</cell><cell>20.7</cell><cell>3.6</cell><cell>30.3</cell><cell>56.8</cell><cell></cell><cell>46.0</cell></row><row><cell>CMDy [39] AAAI2020</cell><cell>60.7</cell><cell>52.5</cell><cell>40.5</cell><cell>23.5</cell><cell>4.5</cell><cell>33.3</cell><cell>62.3</cell><cell></cell><cell>53.1</cell></row><row><cell>PRPE [29] IJCAI2020</cell><cell>63.4</cell><cell>57.9</cell><cell>48.3</cell><cell>32.2</cell><cell>8.3</cell><cell>38.8</cell><cell>66.1</cell><cell></cell><cell>52.9</cell></row><row><cell>Ours</cell><cell>65.4</cell><cell>58.9</cell><cell>49.7</cell><cell>33.3</cell><cell>9.1</cell><cell>39.9</cell><cell>66.2</cell><cell></cell><cell>56.1</cell></row><row><cell>Method</cell><cell cols="9">Precision P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean AP IoU</cell></row><row><cell>Hu et al. [15] ECCV2016</cell><cell>63.3</cell><cell>35.0</cell><cell>8.5</cell><cell>0.2</cell><cell>0.0</cell><cell>17.8</cell><cell>54.6</cell><cell></cell><cell>52.8</cell></row><row><cell>Li et al. [23] CVPR2017</cell><cell>57.8</cell><cell>33.5</cell><cell>10.3</cell><cell>0.6</cell><cell>0.0</cell><cell>17.3</cell><cell>52.9</cell><cell></cell><cell>49.1</cell></row><row><cell>Gavrilyuk et al. [12] CVPR2018</cell><cell>69.9</cell><cell>46.0</cell><cell>17.3</cell><cell>1.4</cell><cell>0.0</cell><cell>23.3</cell><cell>54.1</cell><cell></cell><cell>54.2</cell></row><row><cell>Gavrilyuk et al. [12] ? CVPR2018</cell><cell>71.2</cell><cell>51.8</cell><cell>26.4</cell><cell>3.0</cell><cell>0.0</cell><cell>26.7</cell><cell>55.5</cell><cell></cell><cell>57.0</cell></row><row><cell>ACGA [40] ICCV2019</cell><cell>75.6</cell><cell>56.4</cell><cell>28.7</cell><cell>3.4</cell><cell>0.0</cell><cell>28.9</cell><cell>57.6</cell><cell></cell><cell>58.4</cell></row><row><cell>VT-Capsule [28] CVPR2020</cell><cell>67.7</cell><cell>51.3</cell><cell>28.3</cell><cell>5.1</cell><cell>0.0</cell><cell>26.1</cell><cell>53.5</cell><cell></cell><cell>55.0</cell></row><row><cell>CMDy [39] AAAI2020</cell><cell>74.2</cell><cell>58.7</cell><cell>31.6</cell><cell>4.7</cell><cell>0.0</cell><cell>30.1</cell><cell>55.4</cell><cell></cell><cell>57.6</cell></row><row><cell>PRPE [29] IJCAI2020</cell><cell>69.0</cell><cell>57.2</cell><cell>31.9</cell><cell>6.0</cell><cell>0.1</cell><cell>29.4</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Ours</cell><cell>78.3</cell><cell>63.9</cell><cell>37.8</cell><cell>7.6</cell><cell>0.0</cell><cell>33.5</cell><cell>59.8</cell><cell></cell><cell>60.4</cell></row></table><note>masks on unseen samples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Inserting positions of CMAM. I j denotes the j-th stage in Inception and I3D.</figDesc><table><row><cell>1 2 3 4 5</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ?</cell><cell>CMAM ?</cell><cell>P@0.5 53.0 54.4 58.5 60.3 65.4</cell><cell>P@0.6 45.7 45.5 51.8 53.6 58.9</cell><cell>Precision P@0.7 35.1 33.7 42.3 44.0 49.7</cell><cell>P@0.8 20.3 18.1 27.9 29.1 33.3</cell><cell>P@0.9 4.4 2.9 7.5 7.9 9.1</cell><cell>AP 0.5:0.95 29.0 28.1 34.5 36.0 39.9</cell><cell>IoU Overall 56.7 58.2 62.2 62.8 66.2</cell><cell>Mean 47.7 48.2 51.7 52.9 56.1</cell></row><row><cell cols="13">(a) Component analysis. Verifying the effectiveness of each component in our encoder-decoder framework. "Spatial" and "Temporal"</cell></row><row><cell cols="5">denote spatial and temporal encoders respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ST-Fusion Add Max LGFS (Ours) (b) Spaital and temporal feature fusion. AP IoU 0.5:0.95 Overall Mean 34.5 62.2 51.7 34.8 62.6 51.6 36.0 62.8 52.9</cell><cell cols="9">Position {I 5 } {I 5 , I 4 } {I 5 , I 4 , I 3 } {I 5 , I 4 , I 3 , I 2 } {I 5 , I 4 , I 3 , I 2 , I 1 } (c) Backbone AP IoU 0.5:0.95 Overall Mean 36.9 63.1 53.3 38.5 65.2 55.0 39.0 65.4 55.6 39.4 65.9 55.7 39.9 66.2 56.1 R50 + S3D R50 [13] + I3D IV3 + S3D [42] IV3 + I3D (d) Backbones for our spatial and temporal AP IoU 0.5:0.95 Overall Mean 38.5 67.1 55.1 39.5 66.4 56.4 38.3 66.7 55.0 39.9 66.2 56.1 encoder. R50: ResNet-50. IV3: Inception V3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies. Models are trained on A2D Sentences train split and evaluated on test split.</figDesc><table /><note>dynamically recombined linguistic features can modulate visual features of different abstraction levels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Computational overhead analysis. The RGB input size is Frames ? Height ? Width (3 channels are omitted). ? denotes the GFLOPs is estimated. ? denotes we only calculate the GFLOPs of our spatial encoder part, whose performance is not available.</figDesc><table><row><cell></cell><cell>? 512 ? 512</cell><cell>630.83</cell><cell>27.4</cell></row><row><cell cols="2">CMDy [39]  ? 16 ? 512 ? 512</cell><cell>&gt; 600</cell><cell>33.3</cell></row><row><cell>Ours</cell><cell>8 ? 320 ? 320</cell><cell>213.06</cell><cell>39.9</cell></row><row><cell>Ours-Spa  ?</cell><cell>8 ? 320 ? 320</cell><cell>19.54</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing natural language in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weakly-supervised spatio-temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02549</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarialnas: Adversarial neural architecture search for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jie Cao, Haoqian He, Ran He, and Shuicheng Yan. Interactgan: Learning to generate human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defa</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ordnet: Capturing omnirange dependencies for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic structure guided context modeling for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint learning of object and action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Manigan: Text-guided image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gps: Group people segmentation with detailed part inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefei</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A real-time cross-modality correlation filtering method for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual-textual capsule routing for text-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Polar relative positional encoding for video-language segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reverie: Remote embodied visual referring expression in real indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scene graph generation with hierarchical context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intelligent video surveillance: a review through deep learning techniques for crowd analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sreenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context modulated dynamic networks for actor and action video segmentation with language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Asymmetric cross-guided attention network for actor and action video segmentation from natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actor-action semantic segmentation with grouping process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Hang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatio-temporal person retrieval via natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weakly supervised actor-action segmentation via robust multi-task ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-modal omni interaction modeling for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
