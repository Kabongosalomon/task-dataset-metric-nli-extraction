<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 ROBUST AND GENERALIZABLE VISUAL REPRESENTA- TION LEARNING VIA RANDOM CONVOLUTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenlin</forename><surname>Xu</surname></persName>
							<email>1zhenlinx@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
							<email>2junlin.yang@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
							<email>craffel@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 ROBUST AND GENERALIZABLE VISUAL REPRESENTA- TION LEARNING VIA RANDOM CONVOLUTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generalizability and robustness to out-of-distribution samples have been major pain points when applying deep neural networks (DNNs) in real world applications <ref type="bibr" target="#b46">(Volpi et al., 2018)</ref>. Though DNNs are typically trained on datasets with millions of training samples, they still lack robustness to domain shift, small perturbations, and adversarial examples <ref type="bibr" target="#b27">(Luo et al., 2019)</ref>. Recent research has shown that neural networks tend to use superficial features rather than global shape information for prediction even when trained on large-scale datasets such as ImageNet <ref type="bibr" target="#b9">(Geirhos et al., 2019)</ref>. These superficial features can be local textures or even patterns imperceptible to humans but detectable to DNNs, as is the case for adversarial examples <ref type="bibr" target="#b18">(Ilyas et al., 2019)</ref>. In contrast, image semantics often depend more on object shapes rather than local textures. For image data, local texture differences are one of the main sources of domain shift, e.g., between synthetic virtual images and real data <ref type="bibr" target="#b42">(Sun &amp; Saenko, 2014)</ref>. Our goal is therefore to learn visual representations that are invariant to local texture and that generalize to unseen domains. While texture and color may be treated as different concepts, we follow the convention in <ref type="bibr" target="#b9">Geirhos et al. (2019)</ref> and include color when talking about texture.</p><p>We address the challenging setting of robust visual representation learning from single domain data. Limited work exists in this setting. Proposed methods include data augmentation <ref type="bibr" target="#b46">(Volpi et al., 2018;</ref><ref type="bibr" target="#b34">Qiao et al., 2020;</ref><ref type="bibr" target="#b9">Geirhos et al., 2019)</ref>, domain randomization <ref type="bibr" target="#b43">(Tobin et al., 2017;</ref><ref type="bibr" target="#b50">Yue et al., 2019)</ref>, self-supervised learning <ref type="bibr" target="#b3">(Carlucci et al., 2019)</ref>, and penalizing the predictive power of low-level network features . Following the spirit of adding inductive bias towards global shape information over local textures, we propose using random convolutions to improve the robustness to domain shifts and small perturbations. While recently <ref type="bibr">Lee et al. (2020)</ref> proposed a similar technique for improving the generalization of reinforcement learning agents in Input k = 1 k = 3 k = 5 k = 7 k = 11 k = 15</p><p>Input ? = 0.9 ? = 0.7 ? = 0.5 ? = 0.3 ? = 0.1 ? = 0 First column is the input image of size 224 2 ; following columns are convolutions results using random filters of different sizes k. Bottom: Mixing results between an image and one of its random convolution results with different mixing coefficients ?.</p><p>unseen environments, we focus on visual representation learning and examine our approach on visual domain generalization benchmarks. Our method also includes the multiscale design and a mixing variant. In addition, considering that many computer vision tasks rely on training deep networks based on ImageNet-pretrained weights (including some domain generalization benchmarks), we ask "Can a more robust pretrained model make the finetuned model more robust on downstream tasks?" Different from <ref type="bibr" target="#b20">(Kornblith et al., 2019;</ref><ref type="bibr" target="#b36">Salman et al., 2020)</ref> who studied the transferability of a pretrained ImageNet representation to new tasks while focusing on in-domain generalization, we explore generalization performance on unseen domains for new tasks.</p><p>We make the following contributions:</p><p>? We develop RandConv, a data augmentation technique using multi-scale random-convolutions to generate images with random texture while maintaining global shapes. We explore using the RandConv output as training images or mixing it with the original images. We show that a consistency loss can further enforce invariance under texture changes. ? We provide insights and justification on why RandConv augments images with different local texture but the same semantics with the shape-preserving property of random convolutions. ? We validate RandConv and its mixing variant in extensive experiments on synthetic and realworld benchmarks as well as on the large-scale ImageNet dataset. Our methods outperform single domain generalization approaches by a large margin on digit recognition datasets and for the challenging case of generalizing to the Sketch domain in PACS and to ImageNet-Sketch. ? We explore if the robustness/generalizability of a pretrained representation can transfer. We</p><p>show that transferring a model pretrained with RandConv on ImageNet can further improve domain generalization performance on new downstream tasks on the PACS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Domain Generalization (DG) aims at learning representations that perform well when transferred to unseen domains. Modern techniques range between feature fusion <ref type="bibr" target="#b39">(Shen et al., 2019)</ref>, metalearning <ref type="bibr" target="#b24">(Li et al., 2018a;</ref><ref type="bibr" target="#b0">Balaji et al., 2018)</ref>, and adversarial training <ref type="bibr" target="#b38">(Shao et al., 2019;</ref><ref type="bibr" target="#b25">Li et al., 2018b)</ref>. Note that most current DG work <ref type="bibr" target="#b10">(Ghifary et al., 2016;</ref><ref type="bibr" target="#b24">Li et al., 2018a;</ref> requires a multisource training setting to work well. However, in practice, it might be difficult and expensive to collect data from multiple sources, such as collecting data from multiple medical centers <ref type="bibr" target="#b35">(Raghupathi &amp; Raghupathi, 2014)</ref>. Instead, we consider the more strict single-domain generalization DG setting,</p><p>where we train the model on source data from a single domain and generalize it to new unseen domains <ref type="bibr" target="#b3">(Carlucci et al., 2019;</ref><ref type="bibr" target="#b48">Wang et al., 2019b)</ref>.</p><p>Domain Randomization (DR) was first introduced as a DG technique by <ref type="bibr" target="#b43">Tobin et al. (2017)</ref> to handle the domain gap between simulated and real data. As the training data in <ref type="bibr" target="#b43">(Tobin et al., 2017)</ref> is synthesized in a virtual environment, it is possible to generate diverse training samples by randomly selecting background images, colors, lighting, and textures of foreground objects. When a simulation environment is not accessible, image stylization can be used to generate new domains <ref type="bibr" target="#b50">(Yue et al., 2019;</ref><ref type="bibr" target="#b9">Geirhos et al., 2019)</ref>. However, this requires extra effort to collect data and to train an additional model; further, the number of randomized domains is limited by the number of predefined styles.</p><p>Data Augmentation has been widely used to improve the generalization of machine learning models <ref type="bibr" target="#b41">(Simard et al., 2003)</ref>. DR approaches can be considered a type of synthetic data augmentation.</p><p>To improve performance on unseen domains, <ref type="bibr" target="#b46">Volpi et al. (2018)</ref> generate adversarial examples to augment the training data; <ref type="bibr" target="#b34">Qiao et al. (2020)</ref> extend this approach via meta-learning. As with other adversarial training algorithms, significant extra computation is required to obtain adversarial examples.</p><p>Learning Representations Biased towards Global Shape <ref type="bibr" target="#b9">Geirhos et al. (2019)</ref> demonstrated that convolutional neural networks (CNNs) tend to use superficial local features even when trained on large datasets. To counteract this effect, they proposed to train on stylized ImageNet, thereby forcing a network to rely on object shape instead of textures. Wang et al. improved out-of-domain performance by penalizing the correlation between a learned representation and superficial features such as the gray-level co-occurrence matrix <ref type="bibr" target="#b48">(Wang et al., 2019b)</ref>, or by penalizing the predictive power of local, low-level layer features in a neural network via an adversarial classifier . Our approach shares the idea that learning representations invariant to local texture helps generalization to unseen domains. However, RandConv avoids searching over many hyper-parameters, collecting extra data, and training other networks. It also scales to large-scale datasets since it adds minimal computation overhead.</p><p>Random Mapping in Machine Learning Random projections have also been effective for dimensionality reduction based on the distance-preserving property of the Johnson-Lindenstrauss lemma <ref type="bibr" target="#b19">(Johnson &amp; Lindenstrauss, 1984)</ref>. <ref type="bibr" target="#b45">(Vinh et al., 2016)</ref> applied random projections on entire images as data augmentation to make neural networks robust to adversarial examples. <ref type="bibr">Lee et al. (2020)</ref> recently used random convolutions to help reinforcement learning (RL) agents generalize to new environments. Neural networks with fixed random weights can encode meaningful representations <ref type="bibr" target="#b37">(Saxe et al., 2011)</ref> and are therefore useful for neural architecture search <ref type="bibr" target="#b6">(Gaier &amp; Ha, 2019)</ref>, generative models <ref type="bibr" target="#b14">(He et al., 2016b)</ref>, natural language processing <ref type="bibr" target="#b49">(Wieting &amp; Kiela, 2019)</ref>, and RL <ref type="bibr" target="#b30">(Osband et al., 2018;</ref><ref type="bibr" target="#b2">Burda et al., 2019)</ref>. In contrast, RandConv uses non-fixed randomly-sampled weights to generate images with different local texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RANDCONV: RANDOMIZE LOCAL TEXTURE AT DIFFERENT SCALES</head><p>We propose using a convolution layer with non-fixed random weights as the first layer of a DNN during training. This strategy generates images with random local texture but consistent shapes, and is beneficial for robust visual representation learning. Sec. 3.1 justifies the shape-preserving property of a random convolution layer. Sec. 3.2 describes RandConv, our data augmentation algorithm using a multi-scale randomized convolution layer and input mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A RANDOM CONVOLUTION LAYER PRESERVES GLOBAL SHAPES</head><p>Convolution is the key building block for deep convolutional neural networks. Consider a convolution layer with filters ? ? R h?w?Cin?Cout with an input image I ? R H?W ?Cin , where H and W are the height and width of the input and C in and C out are the number of feature channels for the input and output, and h and w are the height and width of the layer's filter. The output (with appropriate input padding) will be g = I * ? with g ? R H?W ?Cout .</p><p>In images, nearby pixels with similar color or texture can be grouped into primitive shapes that represent parts of objects or the background. A convolution layer linearly projects local image patches to features at corresponding locations on the output map using shared parameters. While a convolution with random filters can project local patches to arbitrary output features, the output of a random linear projection approximately preserves relative similarity between input patches, proved in Appendix B. In other words, since any two locations within the same shape have similar local textures in the input image, they tend to be similar in the output feature map. Therefore, shapes that emerge in the output feature map are similar to shapes in the input image provided that the filter size is sufficiently small compared to the size of a typical shape.</p><p>In other words, the size of a convolution filter determines the smallest shape it can preserve. For example, 1x1 random convolutions preserve shapes at the single-pixel level and thus work as a random color mapping; large filters perturb shapes smaller than the filter size that are considered local texture of a shape at this larger scale. See <ref type="figure" target="#fig_0">Fig. 1</ref> for examples. More discussion and a formal proof are in Appendix A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MULTI-SCALE IMAGE AUGMENTATION WITH A RANDOMIZED CONVOLUTION LAYER</head><p>Algorithm 1 Learning with Data Augmentation by Random Convolutions 1: Input: Model ?, task loss L task , training images {Ii} N i=1 and their labels {yi} N i=1 , pool of filter sizes K = {1, ..., n}, fraction of original data p, whether to mix with original images, consistency loss weight ? 2: function RA N DCO N V(I, K, mix, p) 3:</p><p>Sample p0 ? U (0, 1) 4:</p><p>if p0 &lt; p and mix is False then 5:</p><p>return I When not in mix mode, use the original image with probability p 6: else 7:</p><p>Sample scale k ? K 8:</p><p>Sample convolution weights ? ? R k?k?3?3 ? N (0, 1 3k 2 ) 9: Irc = I * ? Apply convolution on I 10:</p><p>if mix is True then 11:</p><p>Sample ? ? U (0, 1) 12:</p><p>return ?I + (1 ? ?)Irc Mix with original images 13: else 14:</p><p>return Irc 15: Learning Objective: 16: for i = 1 ? N do 17:</p><p>for j = 1 ? 3 do 18:? j i = ?(RandConv(Ii)) Predict labels for three augmented variants of the same image 19:</p><formula xml:id="formula_0">Lcons = ? 3 j=1 KL(? j i ||?i) where?i = 3 j=1? j i /3</formula><p>Consistency Loss 20: L = L task (? 1 i , yi) + ?Lcons Learning with the task loss and the consistency loss Sec. 3.1 discussed how outputs of randomized convolution layers approximately maintain shape information at a scale larger than their filter sizes. Here, we develop our RandConv data augmentation technique using a randomized convolution layer with C out = C in to generate shape-consistent images with randomized texture (see Alg. 1). Our goal is not to use RandConv to parameterize or represent texture as in previous filter-bank based texture models <ref type="bibr" target="#b15">(Heeger &amp; Bergen, 1995;</ref><ref type="bibr" target="#b33">Portilla &amp; Simoncelli, 2000)</ref>. Instead, we only use the three-channel outputs of RandConv as new images with the same shape and different "style" (loosely referred to as "texture"). We also note that, a convolution layer is different from a convolution operation in image filtering. Standard image filtering applies the same 2D filter on three color channels separately. In contrast, our convolution layer applies three different 3D filters and each takes all color channels as input and generates one channel of the output.</p><p>Our proposed RandConv variants are as follows:</p><p>RC img : Augmenting Images with Random Texture A simple approach is to use the randomized convolution layer outputs, I * ?, as new images; where ? are the randomly sampled weights and I is a training image. If the original training data is in the domain D 0 , a sampled weight ? k generates images with consistent global shape but random texture forming the random domain D k . Thus, by random weight sampling, we obtain an infinite number of random domains D 1 , D 1 , . . . , D ? . Input image intensities are assumed to be a standard normal distribution N (0, 1) (which is often true in practice thanks to data whitening). As the outputs of RandConv should follow the same distribution, we sample the convolution weights from N (0, ? 2 ) where ? = 1/ ? C in ? h ? w, which is commonly applied for network initialization <ref type="bibr" target="#b12">(He et al., 2015)</ref>. We include the original images for training at a ratio p as a hyperparameter. RC mix : Mixing Variant As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, outputs from RC img can vary significantly from the appearance of the original images. Although generalizing to domains with significantly different local texture distributions is useful, we may not want to sacrifice much performance on domains similar to the training domain. Inspired by the AugMix <ref type="bibr" target="#b17">(Hendrycks et al., 2020b</ref>) strategy, we propose to blend the original image with the outputs of the RandConv layer via linear convex combinations ?I + (1 ? ?)(I * ?), where ? is the mixing weight uniformly sampled from [0, 1].In RC mix , the RandConv outputs provide shape-consistent perturbations of the original images. Varying ?, we continuously interpolate between the training domain and the randomly sampled domains of RC img .</p><p>Multi-scale Texture Corruption As discussed in Sec. 3.1" image shape information at a scale smaller than a filter's size will be corrupted by RandConv. Therefore, we can use filters of varying sizes to preserve shapes at various scales. We choose to uniformly randomly sample a filter size k from a pool K = 1, 3, ...n before sampling convolution weights ? ? R k?k?Cin?Cout from a Gaussian distribution N (0, 1 k 2 Cin ). <ref type="figure" target="#fig_0">Fig. 1</ref> shows examples of multi-scale RandConv outputs. Consistency Regularization To learn representations invariant to texture changes, we use a loss encouraging consistent network predictions for the same RandConv-augmented image for different random filter samples. Approaches for transform-invariant domain randomization <ref type="bibr" target="#b50">(Yue et al., 2019)</ref>, data augmentation <ref type="bibr" target="#b17">(Hendrycks et al., 2020b)</ref>, and semi-supervised learning <ref type="bibr" target="#b1">(Berthelot et al., 2019)</ref> use similar strategies. We use Kullback-Leibler (KL) divergence to measure consistency. However, enforcing prediction similarity of two augmented variants may be too strong. Instead, following <ref type="bibr" target="#b17">(Hendrycks et al., 2020b)</ref>, we use RandConv to obtain 3 augmentation samples of image I: G j = RandConv j (I) for j = 1, 2, 3 and obtain their predictions with a model ?: y j = ?(G j ). We then compute the relaxed loss as ? 3 j=1 KL(y j ||?), where? = 3 j=1 y j /3 is the sample average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Secs. 4.1 to 4.3 evaluate our methods on the following datasets: multiple digit recognition datasets, PACS, and ImageNet-sketch. Sec. 4.4 uses PACS to explore the out-of-domain generalization of a pretrained representation in transfer learning by checking if pretraining on ImageNet with our method improves the domain generalization performance in downstream tasks. All experiments are in the single-domain generalization setting where training and validation sets are drawn from one domain. Additional experiments with ResNet18 as the backbone are given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DIGIT RECOGNITION</head><p>The five digit recognition datasets <ref type="bibr">(MNIST (LeCun et al., 1998)</ref>, MNIST-M <ref type="bibr" target="#b8">(Ganin et al., 2016)</ref>, SVHN <ref type="bibr" target="#b29">(Netzer et al., 2011)</ref>, SYNTH <ref type="bibr" target="#b7">(Ganin &amp; Lempitsky, 2014)</ref> and <ref type="bibr">USPS (Denker et al., 1989)</ref>) have been widely used for domain adaptation and generalization research <ref type="bibr" target="#b31">(Peng et al., 2019a;</ref><ref type="bibr" target="#b34">Qiao et al., 2020)</ref>. Following the setups in <ref type="bibr" target="#b46">(Volpi et al., 2018)</ref> and <ref type="bibr" target="#b34">(Qiao et al., 2020)</ref>, we train a simple CNN with 10,000 MNIST samples and evaluate the accuracy on the test sets of the other four datasets. We also test on MNIST-C <ref type="bibr" target="#b28">(Mu &amp; Gilmer, 2019)</ref>, a robustness benchmark with 15 common corruptions of MNIST and report the average accuracy over all corruptions. (1-n refers to using scales 1,3,..,n) for RC img,p=0.5 (orange) and RC mix (blue); (c) consistency loss weight ? for RC img1?7,p=0.5 (orange) and RC mix1?7 (blue).</p><p>Selecting Hyperparameters and Ablation Study. <ref type="figure" target="#fig_1">Fig. 2(a)</ref> shows the effect of the hyperparameter p on RC img with filter size 1. We see that adding only 10% RandConv data (p = 0.9) immediately improves the average performance (DG-Avg) on MNIST-M, SVHN, SYNTH and USPS performance from 53.53 to 69.19, outperforming all other approaches (see Tab. 1) for every dataset. We choose p = 0.5, which obtains the best DG-Avg. <ref type="figure" target="#fig_1">Fig. 2(b)</ref> shows results for a multiscale ablation study. Increasing the pool of filter sizes up to 7 improves DG-Avg performance. Therefore we use multiscale 1-7 to study the consistency loss weight ?, shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. Adding the consistency loss improves both RandConv variants on DG-avg: RC mix1?7 favors ? = 10 while RC img1?7,p=0.5 performs similarly for ? = 5 and ? = 10. We choose ? = 10 for all subsequent experiments.</p><p>Results. Tab. 1 compares the performance of RC img1?7,p=0.5,?=10 and RC mix1?7,?=10 with other state-of-the-art approaches. We show results of the adversarial training based methods GUD <ref type="bibr" target="#b46">(Volpi et al., 2018)</ref>, M-ADA <ref type="bibr" target="#b34">(Qiao et al., 2020)</ref>, and PAR ). The baseline model is trained only on the standard classification loss. To show RandConv is more than a trivial color/contrast adjustment method, we also compare to ColorJitter 2 data augmentation (which randomly changes image brightness, contrast, and saturation) and GreyScale (where images are transformed to greyscale for training and testing). We also tested data augmentation with a fixed Laplacian of Gaussian filter (Band-Pass) of size=3 and ? = 1 and the data augmentation pipeline (Multi-Aug) that was used in a recently proposed large scale study on domain generalization algorithms and datasets <ref type="bibr" target="#b11">(Gulrajani &amp; Lopez-Paz, 2020)</ref>. RandConv and its mixing variant outperforms the best competing method (M-ADA) by 17% on DG-Avg and achieves the best 91.62% accuracy on MNIST-C. While the difference between the two variants of RandConv is marginal, RC mix1?7,?=10 performs better on both DG-Avg and MNIST-C. When combined with Multi-Aug, RandConv achieves improved performance except on MNIST-C. <ref type="figure">Fig 3 shows</ref> t-SNE image feature plots for unseen domains generated by the baseline approach and RC mix1?7,?=10 . The RandConv embeddings suggest better generalization to unseen domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PACS EXPERIMENTS</head><p>The PACS dataset <ref type="bibr" target="#b25">(Li et al., 2018b)</ref> considers 7-class classification on 4 domains: photo, art painting, cartoon, and sketch, with very different texture styles. Most recent domain generalization work studies the multi-source domain setting on PACS and uses domain labels of the training data. Although we follow the convention to train on 3 domains and to test on the fourth, we simply pool the data from the 3 training domains as in , without using domain labels during the training.</p><p>Baseline and State-of-the-Art. Following <ref type="bibr" target="#b23">(Li et al., 2017)</ref>, we use Deep-All as the baseline, which finetunes an ImageNet-pretrained AlexNet on 3 domains using only the classification loss and tests on the fourth domain. We test our RandConv variants RC img1-7,p=0.5 and RC mix1-7 with and without consistency loss, and ColorJitter/GreyScale/BandPass/MultiAug data augmentation as in the digit datasets. We also implemented PAR ) using our baseline model. RC <ref type="bibr">mix1-7</ref> MNIST-M SVHN USPS SYNTH <ref type="figure">Figure 3</ref>: t-SNE feature embedding visualization for digit datasets for models trained on MNIST without (top) and with our RC mix1-7,?=10 approach (bottom). Different colors denote different classes. Results. Tab. 2 shows significant improvements on Sketch for both RandConv variants. Sketch is the most challenging domain with no color and much less texture compared to the other 3 domains. The success on Sketch demonstrates that our methods can guide the DNN to learn global representations focusing on shapes that are robust to texture changes. Without using the consistency loss, RC mix1-7 achieves the best overall result improving over Deep-All by ?4% but adding MultiAug does not further improve the performance. Adding the consistency loss with ? = 10, RC mix1-7 and RC img1-7,p=0.5 performs better on Sketch but degrades performance on the other 3 domains, so do GreyScale and ColorJitter. This observation will be discussed in Sec 4.4. ImageNet-Sketch  is an out-of-domain test set for models trained on ImageNet. We trained AlexNet from scratch with RC img1-7,p=0.5,?=10 and RC mix1-7,?=10 . We evaluate their performance on ImageNet-Sketch. We use the AlexNet model trained without RandConv as our baseline. Tab. 3 compares PAR and its baseline model and AlexNet trained with Stylized ImageNet (SIN) <ref type="bibr" target="#b9">(Geirhos et al., 2019)</ref> on ImageNet-Sketch. Although PAR uses a stronger baseline, RandConv achieves significant improvements over our baseline and outperforms PAR by a large margin. Our methods achieve more than a 7% accuracy improvement over the baseline and surpass PAR by 5%. SIN as an image stylization approach that can modify image texture in a hierarchical and realistic way. However, albeit its complexity, it still performs on par with RandConv. Note that image stylization techniques require additional data and heavy precomputation. Further, the images for the style source also need to be chosen. In contrast, RandConv is much easier to use: it can be applied to any dataset via a simple convolution layer. We also measure the shape-bias metric proposed by <ref type="bibr" target="#b9">Geirhos et al. (2019)</ref> for RandConv trained AlexNet. RC img1-7,p=0.5,?=10 and RC mix1-7,?=10 improve the baseline from 25.36% to 48.24% and 54.85% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GENERALIZING AN IMAGENET MODEL TO IMAGENET-SKETCH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">REVISITING PACS WITH MORE ROBUST PRETRAINED REPRESENTATIONS</head><p>A common practice for many computer vision tasks (including the PACS benchmark) is transfer learning, i.e. finetuning a backbone model pretrained on ImageNet. Recently, how the accuracy on ImageNet <ref type="bibr" target="#b20">(Kornblith et al., 2019)</ref> and adversial robustness <ref type="bibr" target="#b36">(Salman et al., 2020</ref>) of the pretrained model affect transfer learning has been studied in the context of domain generalization. Instead, we study how out-of-domain generalizability transfers from pretraining to downstream tasks and shed light on how to better use pretrained models.  model. Cartoon and Art are also improved. The best average domain generalization accuracy is 73.03%, with a more than 6% improvement over our initial Deep-All baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of ImageNet Pretraining</head><p>This experiment confirms that generalizability may transfer: removing texture bias may not only make a pretrained model more generalizable, but it may help generalization on downstream tasks. For similar target and pretraining domains like Photo and ImageNet, where learning texture bias may actually be beneficial, performance may degrade slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND DISCUSSION</head><p>Randomized convolution (RandConv) is a simple but powerful data augmentation technique for randomizing local image texture. RandConv helps focus visual representations on global shape information rather than local texture. We theoretically justified the approximate shape-preserving property of RandConv and developed RandConv techniques using multi-scale and mixing designs. We also make use of a consistency loss to encourage texture invariance. RandConv outperforms state-of-the-art approaches on the digit recognition benchmark and on the sketch domain of PACS and on ImageNet-Sketch by a large margin. By finetuning a model pretrained with RandConv on PACS, we showed that the generalizability of a pretrained model may transfer to and benefit a new downstream task. This resulted in a new state-of-art performance on PACS in the Sketch domain.</p><p>RandConv can help computer vision tasks when a shape-biased model is helpful e.g. for object detection. RandConv can also provide a shape-biased pretrained model to improve performance on downstream tasks when generalizing to unseen domains. However, local texture features can be useful for many computer vision tasks, especially for fixed-domain fine-grained visual recognition. In such cases, visual representations that are invariant to local texture may hurt in-domain performance. Therefore, important future work includes learning representations that disentangle shape and texture features and building models to use such representations in an explainable way.</p><p>Adversarial robustness of deep neural networks has received significant recent attention. Interestingly, <ref type="bibr">Zhang &amp; Zhu (2019)</ref> find that adversarially-trained models are more shape biased; <ref type="bibr" target="#b40">Shi et al. (2020)</ref> show that their method for increasing shape bias also helps adversarial robustness, especially when combined with adversarial training. Therefore, exploring how RandConv affects the adversarial robustness of models could be interesting future work. Moreover, recent biologically inspired models for improving adversarial robustness <ref type="bibr" target="#b4">(Dapello et al., 2020)</ref> use Gabor filters with fixed random configurations followed by a stochastic layer to add Gaussian noise to the network input, which may explain the importance of randomness in RandConv. Exploring connections between RandConv and biological mechanisms in the human visual system would be interesting future work. This supplementary material provides additional details. Specifically, in Sec. A and B, we discuss definitions of shapes and textures in images and justify why random convolution preserves global shapes and disrupts local texture formally by proving Theorem 1. This theorem shows that random linear projections are approximately distance preserving. We also discuss our simulation-based bound based on 80% distance rescaling on real image data. Sec. C provides more experimental details for the different datasets. Sec. D shows experimental results with a stronger backbone architecture and on a new benchmark ImageNet-R <ref type="bibr" target="#b16">(Hendrycks et al., 2020a)</ref>. Sec. E provides more detailed results regarding hyperparameter selection and ablation studies. Lastly, Sec. F shows example visualizations of RandConv outputs and for its mixing variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SHAPES AND TEXTURE IN IMAGES</head><p>As discussed in the main text, we define shapes in images that are preserved by a random convolution layer as primitive shapes: spatial clusters of pixels with similar local texture. An object in a image can be a single primitive shape alone but in most cases it is the composition of multiple primitive shapes e.g. a car includes wheels, body frames, windshields. Note that the definition of texture is not necessarily opposite to shapes, since the texture of a larger shape can includes smaller shapes. For example, in <ref type="figure" target="#fig_2">Fig.4</ref>, the left occluded triangle shape has texture composed by shapes of cobble stones while cobble stones have their own texture. Random convolution can preserve those large shapes that usually define the image semantics while distorting the small shapes as local texture.</p><p>To formally define the shape-preserving property, we assume (x 1 , y 1 ), (x 2 , y 2 ) and (x 3 , y 3 ) are three locations on a image and (x 1 , y 1 ) has closer color and local texture with (x 2 , y 2 ) than (x 3 , y 3 ). For example, (x 1 , y 1 ) and (x 2 , y 2 ) are within the same shape while (x 3 , y 3 ) is located at a neighboring shape. Then we have p(x 1 , y 1 ) ? p(x 2 , y 2 ) &lt; p(x 1 , y 1 ) ? p(x 3 , y 3 ) , where p(x i , y i ) is the image patch at location (x i , y i ). A transformation f is shape-preserving if it maintains such relative distance relations for most location triplets, i.e.</p><formula xml:id="formula_1">f (p(x i , y i )) ? f (p(x j , y j )) / p(x i , y i ) ? p(x j , y j ) ? r<label>(1)</label></formula><p>for any two spatial location (x i , y i ) and (x j , y j ); r ? 0 is a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RANDOM CONVOLUTION IS SHAPE-PRESERVING AS RANDOM LINEAR PROJECTION IS DISTANCE PRESERVING</head><p>We can express a convolution layer as a local linear projection:</p><formula xml:id="formula_2">g(x, y) = Up(x, y) ,<label>(2)</label></formula><p>where p(x, y) ? R d (d = h ? w ? C in ) is the vectorized image patch centerized at location (x, y), g(x, y) ? R Cout is the output feature at location (x, y), and U ? R Cout?d is the matrix expressing the convolution layer filters ?. I.e., for each sliding window centered at (x, y), a convolution layer applies a linear transform f : R d ? R Cout projecting the d dimensional local image patch p(x, y) to its C out dimensional feature g(x, y). When ? is independently randomly sampled, e.g. from a Gaussian distribution, the convolution layer preserves global shapes since that a random linear projection is approximately distance-preserving by bounding the range of r in Eq. 1 in Theorem 1. Theorem 1. Suppose we have N data points z 1 , ? ? ? , z N ? R d . Let f (z) = Uz be a random linear projection f : R d ? R m such that U ? R m?d and U i,j ? N (0, ? 2 ). Then we have:</p><formula xml:id="formula_3">P sup i =j;i,j?[N ] r i,j := f (z i ) ? f (z j ) z i ? z j &gt; ? 1 ? , P inf i =j;i,j?[N ] r i,j := f (z i ) ? f (z j ) z i ? z j &lt; ? 2 ? ,<label>(3)</label></formula><p>where ? 1 := ? ? 2 2 N (N ?1) (m) and ? 2 := ? ? 2 1? 2 N (N ?1) (m). Here, ? 2 ? (m) denotes the ?-upper quantile of the ? 2 distribution with m degrees of freedom.</p><p>Thm. 1 tells us that for any data pair (z i , z j ) in a set of N points, the distance rescaling ratio r i,j after a random linear projection is bounded by ? 1 and ? 2 with probability 1 ? . A Smaller N and a larger output dimension m give better bounds. E.g., when m = 3, N = 1, 000, ? = 1 and = 0.1, ? 1 = 5.8 and ? 2 = 0.01. Thm. 1 gives a theoretical bound for all the N (N ? 1)/2 pairs. However, in practice, preserving distances for a majority of N (N ? 1)/2 pairs is sufficient. To empirically verify this, we test the range of central 80% of {r i,j } on real image data. Using the same (m, N, ?, ), 80% of the pairs lie in [0.56, 2.87], which is significantly better than the strict bound: [0.01, 5.8]. A proof of the theorem and simulation details are given in the following.</p><formula xml:id="formula_4">Proof. Let U k represent to the k-th row of U. It is easy to check that v k := U k , z i ?z j / z i ?z j ? N (0, ? 2 ). Therefore, f (z i ) ? f (z j ) 2 ? 2 z i ? z j 2 = 1 ? 2 (z i ? z j ) U U(z i ? z j ) z i ? z j 2 = m k=1 v 2 k ? 2 ? ? 2 (m).</formula><p>Therefore, for 0 &lt; &lt; 1, we have</p><formula xml:id="formula_5">P f (z i ) ? f (z j ) 2 ? 2 z i ? z j 2 &gt; ? 2 2 N (N ?1) (m) ? 2 N (N ? 1)</formula><p>.</p><p>From the above inequality, we have</p><formula xml:id="formula_6">P sup i =j;i,j?[N ] f (zi)?f (zj ) 2 zi?zj 2 &gt; ? 2 ? 2 2 N (N ?1) (m) = P sup i =j;i,j?[N ] f (zi)?f (zj ) 2 ? 2 zi?zj 2 &gt; ? 2 2 N (N ?1) (m) = P i =j;i,j?[N ] f (zi)?f (zj ) 2 ? 2 zi?zj 2 &gt; ? 2 2 N (N ?1) (m) ? i =j;i,j?[N ] P f (zi)?f (zj ) 2 ? 2 zi?zj 2 &gt; ? 2 2 N (N ?1) (m) ? , which is equivalent to P sup i =j;i,j?[N ] f (z i ) ? f (z j ) z i ? z j &gt; ? ? 2 2 N (N ?1) (m) ? .</formula><p>Similarly, we have</p><formula xml:id="formula_7">P inf i =j;i,j?[N ] f (z i ) ? f (z j ) z i ? z j &lt; ? ? 2 1? 2 N (N ?1) (m) ? .</formula><p>Simulation on Real Image Data To better understand the relative distance preservation property of random linear projections in practice, we use Algorithm 2 to empirically obtain a bound for real image data. We choose m = 3, N = 1, 000, ? = 1 and = 0.1 as in computing our theoretical bounds. We use M = 1, 000 real images from the PACS dataset for this simulation. Note that the image patch size or d does not affect the bound. We use a patch size of 3 ? 3 resulting in d = 27. This simulation tell us that applying linear projections with a randomly sampled U on N local images patches in every image, we have a 1 ? chance that 80% of r i,j is in the range [? 10% , ? 90% ].</p><p>Algorithm 2 Simulate the range of central 80% of r i,j on real image data 1: Input: M images {Ii} M i=1 , number of data points N , projection output dimension m, standard deviation ? of normal distribution, confidence level . 2: for m = 1 ? M do 3:</p><p>Sample images patches in Im at 1,000 locations and vectorize them as {z m l } N l=1 4: Sample a projection matrix U ? R m?d and Ui,j ? N (0, ? 2 ) 5:</p><p>for i = 1 ? N do 6:</p><p>for </p><formula xml:id="formula_8">j = i + 1 ? N do 7: Compute r m i,j = f (z m i )?f (z m j ) z m i ?z m j , where f (z) = Uz 8: q m 10% = 10% quantile of r m i,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS</head><p>Digits Recognition The network for our digits recognition experiments is composed of two Conv5?5-ReLU-MaxPool2?2 blocks with 64/128 output channels and three fully connected layer with 1024/1024/10 output channels. We train the network with batch size 32 for 10,000 iterations.</p><p>During training, the model is validated every 250 iterations and saved with the best validation score for testing. We apply the Adam optimizer with an initial learning rate of 0.0001.</p><p>PACS We use the official data splits for training/validation/testing; no extra data augmentation is applied. We use the official PyTorch implementation and the pretrained weights of AlexNet for our PACS experiments. AlextNet is finetuned for 50,000 iterations with a batch size 128. Samples are randomly selected from the training data mixed between the three domains. We use the validation data of source domains only at every 100 iterations. We use the SGD optimizer for training with an initial learning rate of 0.001, Nesterov momentum, and weight decay set to 0.0005. We let the learning rate decay by a factor of 0.1 after finishing 80% of the iterations.</p><p>ImageNet Following the PyTorch example 3 on training ImageNet models, we set the batch size to 256 and train AlexNet from scratch for 90 epochs. We apply the SGD optimizer with an initial learning rate of 0.01, momentum 0.9, and weight decay 0.0001. We reduce the learning rate via a factor of 0.1 every 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE EXPERIMENTS WITH RESNET-18</head><p>In this section, we demonstrate that RandConv also works on other stronger backbone architectures, e.g. for a Residual Network <ref type="bibr" target="#b13">He et al. (2016a)</ref>. Specifically, we run the PACS and ImageNet experiments with ResNet-18 as the baseline and RandConv. As <ref type="table" target="#tab_6">Table 5</ref> shows, RandConv improves the baseline using ResNet18 on ImageNet-sketch by 10.5% accuracy. When using a RandConv pretrained ResNet-18 on PACS, the performance of finetuning with DeepAll and RandConv are both improved shown in <ref type="table" target="#tab_8">Table 7</ref>. The best average domain generalization accuracy is 84.09%, with a more than 8% improvement over our initial Deep-All baseline. A model pretrained with RC mix1-7,?=10 generally performs better than when pretrained with RC img1-7,p=0.5,?=10 . We also provide the ResNet-18 performance of JiGen (Carlucci et al., 2019) on PACS as reference. Note that JiGen uses extra data augmentation and a different data split than our approach and it only improves over its own baseline by 1.5%. In addition, we test RandConv trained ResNet-18 on ImageNet-R <ref type="bibr" target="#b16">(Hendrycks et al., 2020a)</ref>, a domain generalization benchmark that contains images of artistic renditions of 200 object classes from the original ImageNet dataset. As <ref type="table" target="#tab_7">Table 6</ref> shows, RandConv also improve the generalization performance on ImageNet-R and reduce the gap between the in-domain (ImageNet-200) and out-of-domain (ImageNet-R) performance.   We provide detailed experimental results for the digits recognition datasets. <ref type="table" target="#tab_9">Table 8</ref> shows results for different hyperameters p for RC img1 . <ref type="table" target="#tab_10">Table 9</ref> shows results for an ablation study on the multi-scale design for RC mix and RC img,p=0.5 . <ref type="table" target="#tab_0">Table 10</ref> shows results for studying the consistency loss weight ? for RC mix1-7 and RC img1-7,p=0.5 . Tables 8, 9, and 10 correspond to <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>(b)(c) in the main text respectively.    <ref type="figure">Fig. 6</ref> and for its mixing variants at scale k = 7 with different mixing coefficients in <ref type="figure">Fig. 5</ref>. We observe that RandConv with different filter sizes retains shapes at different scales. The mixing strategy can continuously interpolate between the training domain and a randomly sampled domain.</p><p>Input ? = 0.9 ? = 0.7 ? = 0.5 ? = 0.3 ? = 0.1 ? = 0 following columns are convolution results using random filters of different sizes k. We can see that the smaller filter sizes help maintain the finer shapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Top: Illustration that RandConv randomize local texture but preserve shapes in the image. Middle:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Average accuracy and 5-run variance of MNIST model on MNIST-M, SVHN, SYNTH and USPS. Studies for: (a) original data fraction p for RC img ; (b) multiscale design</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Left: An image with texture and shapes at different scales; Middle: The output of RandConv with a small filter size which largely preserves the shapes of the stones. Right: The output of RandConv with a large filter size distorts the shape of the stones as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Examples of the RandConv mixing variant RCmix7 on images of size 224 2 with different mixing coefficients ?. When ? = 1, the output is just the original image input;when ? = 0, we use the output of the random convolution layer as the augmented image. RandConv data augmentation examples on images of size 224 2 . First column is the input image;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average accuracy and 5-run standard deviation (in parenthesis) of MNIST10K model on MNIST-M, SVHN, SYNTH, USPS and their average (DG-avg); and average accuracy of 15 types of corruptions in MNIST-C. Both RandConv variants significantly outperform all other methods. MultiAug 98.82(0.06) 87.89(0.29) 62.07(0.62) 84.39(1.02) 63.90(0.63) 74.56(0.46) 91.40(0.93)</figDesc><table><row><cell></cell><cell>MNIST</cell><cell cols="2">MNIST-M SVHN</cell><cell>USPS</cell><cell cols="3">SYNTH DG-Avg MNIST-C</cell></row><row><cell>Baseline</cell><cell cols="7">98.40(0.84) 58.87(3.73) 33.41(5.28) 79.27(2.70) 42.43(5.46) 53.50(4.23) 88.20(2.10)</cell></row><row><cell>GreyScale</cell><cell cols="7">98.82(0.02) 58.41(0.99) 36.06(1.48) 80.45(1.00) 45.00(0.80) 54.98(0.86) 89.15(0.44)</cell></row><row><cell>ColorJitter</cell><cell cols="7">98.72(0.05) 62.72(0.66) 39.61(0.88) 79.18(0.60) 46.40(0.34) 56.98(0.39) 89.48(0.18)</cell></row><row><cell>BandPass</cell><cell cols="7">98.65(0.11) 70.22(2.73) 48.34(2.56) 78.60(0.82) 57.17(2.01) 63.58(1.89) 87.89(0.68)</cell></row><row><cell>MultiAug</cell><cell cols="7">98.80(0.05) 62.32(0.66) 39.07(0.68) 79.31(1.02) 46.48(0.80) 56.79(0.34) 89.54(0.11)</cell></row><row><cell>PAR (our imp)</cell><cell cols="7">98.79(0.05) 61.16(0.21) 36.08(1.27) 79.95(1.18) 45.48(0.35) 55.67(0.33) 89.34(0.45)</cell></row><row><cell>GUD</cell><cell>-</cell><cell>60.41</cell><cell>35.51</cell><cell>77.26</cell><cell>45.32</cell><cell>54.62</cell><cell>-</cell></row><row><cell>M-ADA</cell><cell>-</cell><cell>67.94</cell><cell>42.55</cell><cell>78.53</cell><cell>48.95</cell><cell>59.49</cell><cell>-</cell></row><row><cell>RCimg1-7, p=0.5, ?=5</cell><cell cols="7">98.86(0.05) 87.67(0.37) 54.95(1.90) 82.08(1.46) 63.37(1.58) 72.02(1.15) 90.94(0.51)</cell></row><row><cell>RC mix1-7,?=10</cell><cell cols="7">98.85(0.04) 87.76(0.83) 57.52(2.09) 83.36(0.96) 62.88(0.78) 72.88(0.58) 91.62(0.77)</cell></row><row><cell>RC mix1-7,?=10 +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MultiAug is also tested. Further, we compare to the following state-of-the-art approaches: Jigen (Carlucci et al., 2019) using self-supervision, MLDG<ref type="bibr" target="#b24">(Li et al., 2018a)</ref> using meta-learning, and the conditional invariant deep domain generalization method CIDDG<ref type="bibr" target="#b26">(Li et al., 2018c)</ref>. Note that previous methods used different Deep-All baselines which make the final accuracy not directly comparable, and MLDG and CIDDG use domain labels for training.</figDesc><table><row><cell>Base</cell><cell>Method</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Average</cell></row><row><cell>Ours</cell><cell>Deep-All</cell><cell cols="5">86.7771(0.58)</cell></row><row><cell></cell><cell>RCimg1-7, p=0.5</cell><cell cols="5">86.50(0.72) 61.10(0.38) 64.24(0.62) 68.50(1.83) 70.09(0.43)</cell></row><row><cell></cell><cell>RCmix1-7</cell><cell cols="5">86.60(0.67) 61.74(0.90) 64.05(0.66) 69.74(0.66) 70.53(0.25)</cell></row><row><cell></cell><cell>RCmix1-7 + MultiAug</cell><cell cols="5">86.23(0.74) 61.91(0.76) 62.69(0.76) 67.74(1.21) 69.64(0.49)</cell></row><row><cell></cell><cell>RCimg1-7, p=0.5, ?=10</cell><cell cols="5">81.15(0.76) 59.56(0.79) 62.42(0.59) 71.74(0.43) 68.72(0.58)</cell></row><row><cell></cell><cell>RCmix1-7,?=10</cell><cell cols="5">81.78(1.11) 61.14(0.51) 63.57(0.29) 71.97(0.38) 69.62(0.24)</cell></row><row><cell cols="6">Results below are not directly comparable due to different Deep-All implementations.</cell><cell></cell></row><row><cell></cell><cell>Deep-All (our run)</cell><cell>88.40</cell><cell>66.26</cell><cell>66.58</cell><cell>59.40</cell><cell>70.16</cell></row><row><cell>Wang et al. (2019a)</cell><cell>PAR (our run)</cell><cell>88.40</cell><cell>65.19</cell><cell>68.58</cell><cell>61.86</cell><cell>71.10</cell></row><row><cell></cell><cell>PAR (reported)</cell><cell>89.6</cell><cell>66.3</cell><cell>68.3</cell><cell>64.1</cell><cell>72.08</cell></row><row><cell>Carlucci et al. (2019)</cell><cell>Deep-All Jigen</cell><cell>89.98 89.00</cell><cell>66.68 67.63</cell><cell>69.41 71.71</cell><cell>60.02 65.18</cell><cell>71.52 73.38</cell></row><row><cell>Li et al. (2018a)</cell><cell>Deep-All MLDG (use domain labels)</cell><cell>86.67 88.00</cell><cell>64.91 66.23</cell><cell>64.28 66.88</cell><cell>53.08 58.96</cell><cell>67.24 70.01</cell></row><row><cell>Li et al. (2018c)</cell><cell>Deep-All CIDDG (use domain labels)</cell><cell>77.98 78.65</cell><cell>57.55 62.70</cell><cell>67.04 69.73</cell><cell>58.52 64.45</cell><cell>65.27 68.88</cell></row><row><cell>combined with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Mean and 5-run standard deviation (in parenthesis) results for domain generalization on PACS. Best results with our Deep-All baseline are in bold. The domain name in each column represents the target domain. Base column indicates different baselines and results under different baselines are not directly comparable. MLDG and CIDDF used domain labels for training.(0.42) 60.11(1.33) 64.12(0.32) 55.28(4.71) 66.57(1.36) GreyScale 83.93(1.47) 61.60(1.18) 62.12(0.61) 60.07(2.47) 66.93(0.83) ColorJitter 84.61(0.83) 59.01(0.24) 61.43(0.68) 62.44(1.68) 66.88(0.33) BandPass 87.08(0.57) 59.46(0.27) 64.39(0.51) 55.39(2.95) 66.58(0.73) MultiAug 85.21(0.47) 59.51(0.38) 62.88(1.01) 61.67(0.76) 67.32(0.23) PAR (our imp.) 87.21(0.42) 60.17(0.95) 63.63(0.88) 55.83(2.57) 66.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of ImageNet-trained AlexNet on ImageNet-Sketch (IN-S) data. Our methods outperform PAR by 5% and are on par with a Stylized-ImageNet (SIN) trained model. Note that PAR was built on top of a stronger baseline than our model, and both PAR and SIN fine-tuned the baseline model which helped the performance, while we train RandConv model from scratch.</figDesc><table><row><cell></cell><cell>Baseline (Wang et al., 2019a)</cell><cell>PAR (Wang et al., 2019a)</cell><cell>Baseline</cell><cell>RCimg1-7, p=0.5,?=10</cell><cell>RCmix1-7, ?=10</cell><cell>SIN (Geirhos et al., 2019)</cell></row><row><cell>Top1</cell><cell>12.04</cell><cell>13.06</cell><cell>10.28</cell><cell>18.09</cell><cell>16.91</cell><cell>17.62</cell></row><row><cell>Top5</cell><cell>25.60</cell><cell>26.27</cell><cell>21.60</cell><cell>35.40</cell><cell>33.99</cell><cell>36.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>A model trained on ImageNet may be biased towards textures<ref type="bibr" target="#b9">(Geirhos et al., 2019)</ref>. Finetuning ImageNet pretrained models on PACS may inherit this texture bias, thereby benefitting generalization on the Photo domain (which is similar to ImageNet), but hurting performance on the Sketch domain. Therefore, as shown in Sec. 4.2, using RandConv to correct this texture bias improves results on Sketch, but degrades them on the Photo domain.Since pretraining has such a strong impact on transfer performance to new tasks, we ask: "Can the generalizability of a pretrained model transfer to downstream tasks? I.e., does a pretrained model with better generalizability improve performance on unseen domains on new tasks?" To answer this, we revisit the PACS tasks based on ImageNet-pretrained weights where our two RandConv variants of Sec. 4.3 are used during ImageNet training. We study if this results in performance changes for the Deep-All baseline and for finetuning with RandConv.Better Performance via RandConv pretrained model We start by testing the Deep-All baselines using the two RandConv-trained ImageNet models of Sec. 4.3 as initialization. Tab. 4 shows significant improvements on Sketch. Results are comparable to finetuning with RandConv on a</figDesc><table /><note>normal pretrained model. Art is also consistently improved. Performance drops slightly on Photo as expected, since we reduced the texture bias in the pretrained model, which is helpful for the Photo domain. A similar performance improvement is observed when using the SIN-trained AlexNet as initialization. Using RandConv for both ImageNet training and PACS finetuning, we achieve 76.11% accuracy on Sketch. As far as we know, this is the best performance using an AlexNet baseline. This approach even outperforms Jigen (Carlucci et al., 2019) (71.35%) with a stronger ResNet18 baseline</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Generalization results on PACS with RandConv and SIN pretrained AlexNet. ImageNet column shows how the pretrained model is trained on ImageNet (baseline represents training the ImageNet model using only the classification loss); PACS column indicates the methods used for finetuning on PACS. Best and second best accuracy for each target domain are highlighted in bold and underlined.</figDesc><table><row><cell>PACS</cell><cell>ImageNet</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Avg</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="5">86.77(0.42) 60.11(1.33) 64.12(0.32) 55.28(4.71) 66.57(1.36)</cell></row><row><cell>Deep-All</cell><cell cols="6">RC img1-7,p=0.5,?=10 84.48(0.52) 62.61(1.23) 66.13(0.80) 69.24(0.80) 70.61(0.53) RC mix1-7,?=10 85.59(0.40) 63.30(0.99) 63.83(0.85) 68.29(1.27) 70.25(0.45)</cell></row><row><cell></cell><cell>SIN</cell><cell cols="5">85.33(0.66) 65.85(0.87) 65.39(0.62) 65.75(0.59) 70.58(0.21)</cell></row><row><cell>RCimg1-7, p=0.5,?=10</cell><cell cols="6">Baseline RC img1-7,p=0.5,?=10 84.36(0.36) 81.15(0.76) 59.56(0.79) 62.42(0.59) 71.74(0.43) 68.72(0.58) 63.73(0.91) 68.07(0.55) 75.41(0.57) 72.89(0.33) RC mix1-7,?=10 84.63(0.97) 63.41(1.22) 66.36(0.43) 74.59(0.84) 72.25(0.54)</cell></row><row><cell>RCmix1-7 ?=10</cell><cell cols="6">Baseline RC img1-7,p=0.5,?=10 85.16(1.03) 63.17(0.38) 67.68(0.60) 76.11(0.43) 73.03(0.46) 81.78(1.11) 61.14(0.51) 63.57(0.29) 71.97(0.38) 69.62(0.24) RC mix1-7,?=10 86.17(0.56) 65.33(1.05) 65.52(1.13) 73.21(1.03) 72.56(0.50)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of ImageNet-trained ResNet-18 on ImageNet-Sketch data.</figDesc><table><row><cell></cell><cell cols="3">Baseline RCimg1-7, p=0.5, ?=10 RCmix1-7, ?=10</cell></row><row><cell>Top1</cell><cell>20.23</cell><cell>28.79</cell><cell>30.70</cell></row><row><cell>Top5</cell><cell>37.26</cell><cell>49.02</cell><cell>51.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Top 1 Accuracy of ImageNet-trained ResNet-18 on ImageNet-R data. ImageNet-200 are the original ImageNet data with the same 200 classes as ImageNet-R.</figDesc><table><row><cell></cell><cell cols="3">Baseline RCimg1-7, p=0.5, ?=10 RCmix1-7, ?=10</cell></row><row><cell>ImageNet-200 (%)</cell><cell>88.15</cell><cell>83.72</cell><cell>72.7</cell></row><row><cell>ImageNet-R (%)</cell><cell>33.06</cell><cell>37.38</cell><cell>35.75</cell></row><row><cell>Gap</cell><cell>55.09</cell><cell>46.34</cell><cell>36.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Generalization results on PACS with RandConv pretrained model using ResNet-18. Ima-geNet column shows how the pretrained model is trained on ImageNet (baseline represents training using only the classification loss); PACS column indicates the methods used for finetuning on PACS. Best and second best accuracy for each target domain are highlighted in bold and underlined. The performance of JiGen<ref type="bibr" target="#b3">(Carlucci et al., 2019)</ref> and its baseline using ResNet-18 is also given.</figDesc><table><row><cell>PACS</cell><cell>ImageNet</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Avg</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="5">95.45(0.43) 74.96(0.99) 71.48(1.22) 62.09(1.12) 76.00(0.37)</cell></row><row><cell>Deep-All</cell><cell cols="6">RC img1-7,p=0.5,?=10 94.65(0.16) 73.85(0.97) 74.78(0.58) 73.51(1.16) 79.20(0.40)</cell></row><row><cell></cell><cell>RC mix1-7,?=10</cell><cell cols="5">94.10(0.43) 76.72(1.43) 73.41(1.29) 77.60(0.55) 80.46(0.74)</cell></row><row><cell>RCimg1-7, p=0.5,?=10</cell><cell cols="6">Baseline RC img1-7,p=0.5,?=10 94.43(0.22) 79.80(1.03) 73.40(0.37) 81.51(0.85) 82.28(0.38) 92.37(0.54) 76.50(0.55) 71.33(0.29) 79.65(1.32) 79.96(0.53) RC mix1-7,?=10 94.57(0.45) 81.32(1.00) 76.28(0.82) 84.18(0.94) 84.09(0.61)</cell></row><row><cell>RCmix1-7 ?=10</cell><cell cols="6">Baseline RC img1-7,p=0.5,?=10 95.23(0.30) 80.56(0.82) 74.18(0.53) 80.70(1.43) 82.67(0.46) 93.57(0.40) 77.73(0.91) 71.24(0.91) 75.53(2.17) 79.52(0.61) RC mix1-7,?=10 95.01(0.32) 81.09(1.24) 76.04(0.92) 83.02(0.93) 83.79(0.60)</cell></row><row><cell>Deep-All JiGen</cell><cell>Baseline</cell><cell>95.73 96.03</cell><cell>77.85 79.42</cell><cell>74.86 75.25</cell><cell>67.74 71.35</cell><cell>79.05 80.51</cell></row><row><cell cols="7">E HYPERPARAMETER SELECTIONS AND ABLATION STUDIES ON DIGITS</cell></row><row><cell cols="3">RECOGNITION BENCHMARKS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of hyperparameter p for RC img1 on digits recognition benchmarks. DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results are bold.</figDesc><table><row><cell></cell><cell cols="2">MNIST-10k MNIST-M</cell><cell>SVHN</cell><cell>USPS</cell><cell>SYNTH</cell><cell>DG Avg</cell><cell>MNIST-C</cell></row><row><cell>Baseline</cell><cell>98.40(0.84)</cell><cell>58.87(3.73)</cell><cell cols="4">33.41(5.28) 79.27(2.70) 42.43(5.46) 53.50(4.23) 88.20(2.10)</cell></row><row><cell>RCimg1, p=0.9</cell><cell>98.68(0.06)</cell><cell>83.53(0.37)</cell><cell cols="4">53.67(1.54) 80.38(1.41) 59.19(0.85) 69.19(0.34) 89.79(0.44)</cell></row><row><cell>RCimg1, p=0.7</cell><cell>98.64(0.07)</cell><cell>84.17(0.61)</cell><cell cols="4">54.50(1.55) 80.85(0.91) 60.25(0.85) 69.94(0.50) 89.20(0.60)</cell></row><row><cell>RCimg1, p=0.5</cell><cell>98.72(0.08)</cell><cell>85.17(1.12)</cell><cell cols="4">55.97(0.54) 80.31(0.85) 61.07(0.47) 70.63(0.42) 88.66(0.62)</cell></row><row><cell>RCimg1, p=0.3</cell><cell>98.71(0.12)</cell><cell>85.45(0.87)</cell><cell cols="4">54.62(1.52) 79.78(1.40) 60.51(0.41) 70.09(0.60) 89.02(0.32)</cell></row><row><cell>RCimg1, p=0.1</cell><cell>98.66(0.06)</cell><cell>85.57(0.79)</cell><cell cols="4">54.34(1.52) 79.21(0.44) 60.18(0.63) 69.83(0.38) 88.53(0.38)</cell></row><row><cell>RCimg1, p=0</cell><cell>98.55(0.13)</cell><cell>86.27(0.42)</cell><cell cols="4">52.48(3.00) 79.01(1.11) 59.53(1.14) 69.32(1.19) 88.01(0.36)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of multi-scale RandConv on digits recognition benchmarks for RC mix and RC img,p=0.5 . Best entries for each variant are bold.</figDesc><table><row><cell>MNIST-10k MNIST-M SVHN</cell><cell>USPS</cell><cell>SYNTH DG Avg MNIST-C</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Ablation study of consistency loss weight ? on digits recognition benchmarks for RCmix1-7   and RCimg1-7,p=0.5  . DG-Avg is the average performance on MNIST-M, SVHN, SYNTH and USPS. Best results for each variant are bold. .05) 87.18 (0.81) 57.68 (1.64) 83.55 (0.83) 63.08 (0.50) 72.87 (0.47) 91.14 (0.53) 10 98.85 (0.04) 87.76 (0.83) 57.52 (2.09) 83.36 (0.96) 62.88 (0.78) 72.88 (0.58) 91.62 (0.77) 5 98.94 (0.09) 87.53 (0.51) 55.70 (2.22) 83.12 (1.08) 62.37 (0.98) 72.18 (1.04) 91.46 (0.50) 1 98.95 (0.05) 86.77 (0.79) 56.00 (2.39) 83.13 (0.71) 63.18 (0.97) 72.27 (0.82) 91.15 (0.42) 0.1 98.84 (0.07) 85.41 (1.02) 56.51 (1.58) 81.84 (1.14) 61.86 (1.44) 71.41 (0.98) 90.72 (0.60) 0 98.82 (0.06) 84.91 (0.68) 55.61 (2.63) 82.09 (1.00) 62.15 (1.30) 71.19 (1.21) 90.30 (0.44) RCimg1-7,p=0.5 20 98.79 (0.04) 87.53 (0.79) 53.92 (1.59) 81.83 (0.70) 62.16 (0.37) 71.36 (0.49) 91.20 (0.53) 10 98.86 (0.05) 87.67 (0.37) 54.95 (1.90) 82.08 (1.46) 63.37 (1.58) 72.02 (1.15) 90.94 (0.51) 5 98.90 (0.04) 87.77 (0.72) 55.00 (1.40) 82.10 (0.55) 63.58 (1.33) 72.11 (0.62) 90.83 (0.71) 1 98.86 (0.04) 86.74 (0.32) 53.26 (2.99) 81.51 (0.48) 62.00 (1.15) 70.88 (0.93) 91.11 (0.62) 0.1 98.85 (0.14) 86.85 (0.31) 53.55 (3.63) 81.23 (1.02) 62.77 (0.80) 71.10 (1.31) 91.13 (0.69) 0 98.83 (0.07) 86.08 (0.27) 54.93 (1.27) 81.58 (0.74) 62.78 (0.86) 71.34 (0.61) 91.18 (0.38) F MORE EXAMPLES OF RA N DCO N V DATA AUGMENTATION We provide additional examples of RandConv outputs for different convolution filter sizes in</figDesc><table><row><cell>? MNIST-10k MNIST-M SVHN</cell><cell>USPS</cell><cell>SYNTH DG Avg MNIST-C</cell></row><row><cell>20 98.90 (0</cell><cell></cell><cell></cell></row><row><cell>RCmix1-7</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/wildphoton/RandConv. 1 arXiv:2007.13003v3 [cs.CV] 3 May 2021Published as a conference paper at ICLR 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See PyTorch documentation for implementation details; all parameters are set to 0.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pytorch/examples/tree/master/imagenet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank Zhiding Yu for discussions on initial ideas and the experimental setup. We also thank Nathan Cahill for advice on proving the properties of random convolutions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploration by random network distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1lJJnR5Ym" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simulating a primary visual cortex at the front of cnns improves robustness to image perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural network recognizer for handwritten zip code digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="323" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weight agnostic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5364" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bygh9j09KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<title level="m">search of lost domain generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A powerful generative model using random weights for the deep image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James R Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 22nd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augmix: A simple method to improve robustness and uncertainty under data shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1gmrxHFvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Network randomization: A simple technique for generalization in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mnist-c: A robustness benchmark for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02337</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Randomized prior functions for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8617" to="8629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to learn single domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengchun</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13216</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Big data analytics in healthcare: promise and potential. Health information science and systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wullianallur</forename><surname>Raghupathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viju</forename><surname>Raghupathi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08489</idno>
		<title level="m">Do adversarially robust imagenet models transfer better? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bipin</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-adversarial discriminative deep domain generalization for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10023" to="10031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Situational fusion of visual representation for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>William B Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Informative dropout for robust representation learning: A shape-bias perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icdar</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From virtual to reality: Fast adaptation of virtual object detectors to real domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training robust models using random projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakrapee</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamohanarao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="531" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10506" to="10518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning robust representations by projecting superficial statistics out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJEjjoR9K7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">No training required: Exploring random encoders for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkgPajAcY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2100" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpreting adversarially trained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno>2019. RCmix1 98.62(0.06) 83.98(0.98) 53.26(2.59) 80.57(1.09) 59.25(1.38) 69.26(1.35) 88.59(0.38) RCmix1-3 98.76(0.02) 84.66(1.67) 55.89(0.83) 80.95(1.15) 60.07(1.05) 70.39(0.58) 89.80(0.94) RCmix1-5 98.76(0.06) 84.32(0.43) 56.50(2.68) 81.85(1.05) 60.76(1.02) 70.86(0.86) 90.06(0.80) RCmix1-7 98.82(0.06) 84.91(0.68) 55.61(2.63) 82.09(1.00) 62.15(1.30) 71.19(1.21) 90.30(0.44) RCmix1-9 98.81(0.12) 85.13(0.72) 54.18(3.36) 82.07(1.28) 61.85(1.41) 70.81(1.24) 90.83(0.52</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="7502" to="7511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rcimg1</surname></persName>
		</author>
		<idno>p=0.5 98.66(0.05) 85.12(0.96) 55.59(0.29) 80.65(0.71) 60.85(0.48) 70.55(0.15) 89.00(0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
