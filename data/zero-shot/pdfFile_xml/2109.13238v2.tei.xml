<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visually Grounded Reasoning across Languages and Cultures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Emanuele</forename><surname>Bugliarello</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edoardo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Ponti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy @</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge C University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Mila -Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit3">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visually Grounded Reasoning across Languages and Cultures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since its creation, ImageNet <ref type="bibr" target="#b17">(Deng et al., 2009</ref>) has charted the course for research in computer vision <ref type="bibr" target="#b43">(Russakovsky et al., 2014)</ref>. Its backbone consists of a hierarchy of concepts selected from English <ref type="bibr">Word-Net (Fellbaum, 2010)</ref>, a database for lexical semantics. Several other datasets, such as NLVR2 (Suhr * Equal contribution. <ref type="bibr">Correspondence: fl399@cam.ac.uk,</ref><ref type="bibr">emanuele@di.ku.dk. (a)</ref> ("In one of the two photos, more than two yellow-shirted players are seen engaged in bull taming."). Label: TRUE.</p><p>(b) Picha moja ina watu kadhaa waliovaa leso na picha nyingine ina leso bila watu. ("One picture contains several people wearing handkerchiefs and another picture has a handkerchief without people."). Label: FALSE.  <ref type="bibr" target="#b22">(Lin et al., 2014)</ref>, and Visual Genome <ref type="bibr">(Krishna et al., 2017)</ref> are built on top of this hierarchy, 1 and likewise many pre-trained encoders of visual data (e.g., ResNet), which are instrumental for transfer learning <ref type="bibr">(Huh et al., 2016)</ref>.</p><p>How suitable are the concepts and images found in ImageNet, beyond the English language and Northern American and European culture in which it was created? Their ideal distribution may be challenging to define and is-to some extent-specific to the intended application <ref type="bibr" target="#b62">(Yang et al., 2020)</ref>. However, if the goal is world-wide representation, evidence suggests that both the origin <ref type="bibr" target="#b45">(Shankar et al., 2017;</ref><ref type="bibr" target="#b16">de Vries et al., 2019)</ref> and content <ref type="bibr" target="#b48">(Stock and Cisse, 2018)</ref> of ImageNet data is skewed. To remedy this, <ref type="bibr" target="#b62">Yang et al. (2020)</ref> proposed to intervene on the data, filtering and re-balancing a subset of categories.</p><p>Nevertheless, this remains insufficient if the coverage of the original distribution does not encompass multiple languages and cultures in the first place. Hence, to extend the global outreach of multimodal technology, a more radical overhaul of its hierarchy is necessary. In fact, both the most salient concepts <ref type="bibr" target="#b27">(Malt, 1995;</ref><ref type="bibr" target="#b6">Berlin, 2014)</ref> and their prototypical members <ref type="bibr" target="#b26">(MacLaury, 1991;</ref><ref type="bibr">Lakoff and Johnson, 1980</ref>)-as well as their visual denotations-may vary across cultural or environmental lines. This variation can be obfuscated by common practices in dataset creation, such as (randomly) selecting concepts from language-specific resources, or automatically scraping images from web queries (cf. ?2).</p><p>In this work, we streamline existing protocols by mitigating the biases they introduce, in order to create multicultural and multilingual datasets. In particular, we let the selection of both concepts and images be driven by members of a community of native speakers. We focus on a diverse set of cultures and languages, including Indonesian, Swahili, Tamil, Turkish, and Mandarin Chinese. In addition, we elicit native-language descriptions by asking annotators to compare and contrast image pairs. The task is to determine whether these grounded descriptions are true or false. We choose this specific task, pioneered by <ref type="bibr" target="#b50">Suhr et al. (2017)</ref>, as it requires the integration of information across modalities <ref type="bibr">(Goyal et al., 2017)</ref> and deep linguistic understanding <ref type="bibr" target="#b51">(Suhr et al., 2019)</ref>, rather than just matching superficial features <ref type="bibr" target="#b0">(Agrawal et al., 2016)</ref>. Two examples from our dataset, Multicultural Reasoning over Vision and Language <ref type="bibr">(henceforth, MaRVL)</ref>, are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We benchmark a series of state-of-the-art visiolinguistic models <ref type="bibr" target="#b23">(Liu et al., 2019;</ref><ref type="bibr" target="#b11">Chen et al., 2020)</ref> on MaRVL through both zero-shot and translationbased cross-lingual transfer. Their performance deteriorates considerably compared to an English dataset for the same task (NLVR2; <ref type="bibr" target="#b51">Suhr et al., 2019)</ref>. Investigating the causes of this failure, we find that while not constructed adversarially, the combined domain shift in concepts, images, and language variety conspires to make MaRVL extremely challenging. Therefore, we conclude, it may provide more reliable estimates of the generalisation abilities of state-of-the-art models compared to existing benchmarks. The dataset, annotation guideline, code and models are available at marvl-challenge.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>The ImageNet Large-Scale Visual Recognition Challenge <ref type="bibr">(Russakovsky et al., 2014, ILSRVC1K</ref>) is a landmark evaluation benchmark for computer vision. It is based on a subset of 1,000 concepts from Ima-geNet <ref type="bibr" target="#b17">(Deng et al., 2009)</ref>, which consists of a collection of images associated to concepts extracted from the WordNet lexical database <ref type="bibr">(Fellbaum, 2010)</ref>. This subset is also used as the basis for other multimodal datasets, such as NLVR2 <ref type="bibr" target="#b51">(Suhr et al., 2019)</ref>. To what extent, however, are images and concepts in these datasets capable of representing multiple languages and cultures? To address this question, we first need to define concepts more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concepts: Basic Level and Prototypes</head><p>A concept is the mental representation of a category (e.g. BIRD), where instances of objects and events with similar properties are grouped together <ref type="bibr" target="#b30">(Murphy, 2021)</ref>. Category members, however, do not all have equal status: some are more prototypical than others (e.g. PENGUINS are more atypical BIRDS than ROBINS; <ref type="bibr" target="#b42">Rosch, 1973b;</ref><ref type="bibr" target="#b39">Rosch and Mervis, 1975)</ref> and boundaries of peripheral members are fuzzy <ref type="bibr">(Mc-Closkey and Glucksberg, 1978;</ref><ref type="bibr">Hampton, 1979)</ref>. Although prototypes (e.g. for hues and forms) are not entirely arbitrary <ref type="bibr" target="#b41">(Rosch, 1973a)</ref>, they display a degree of variation across cultures <ref type="bibr" target="#b26">(MacLaury, 1991;</ref><ref type="bibr">Lakoff and Johnson, 1980)</ref>. Categories form a hierarchy, from the general to the specific. Among them, basic-level categories are cognitively most salient <ref type="bibr" target="#b40">(Rosch et al., 1976)</ref>. These are used most often to name things by adults <ref type="bibr" target="#b2">(Anglin, 1977)</ref> and are the first to be learned by children <ref type="bibr" target="#b8">(Brown, 1958)</ref>. The basic level, however, is not universal, as different cultures may adopt different basic-level concepts <ref type="bibr" target="#b6">(Berlin, 2014)</ref>. Familiarity of individuals with the domain also plays a role (Wisniewski and <ref type="bibr" target="#b59">Murphy, 1989)</ref>. So, for instance, in the case of plants, Tzeltal Mayans will choose finergrained nouns than American undergraduates to indicate the same vine.</p><p>Hence, prototypes, basic-level categories, and number of categories for a domain are restricted both by perceptive, cognitive, and environmental factors on the one hand, and by cultural and individual preferences on the other <ref type="bibr" target="#b27">(Malt, 1995;</ref><ref type="bibr" target="#b58">Wierzbicka, 1996)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Limitations of ImageNet</head><p>The original annotation of ImageNet was not intended to ensure that its concepts are universal and Figure 2: Multilingual and multicultural pervasiveness of ImageNet 1k, NLVR2 and MaRVL synsets by language (left), language family (middle) and macro-area (right) coverage. The Eurasian macro-area is over-represented in ImageNet. Concepts in MaRVL cover more languages and are represented in more language families than the ones in ImageNet. lie at the basic level (i.e. are most salient for humans); however, these design choices may prove important limitations for the purpose of enabling multimodal systems to reason over everyday life scenarios in many languages and cultures.</p><p>ImageNet concepts are not universal. Since Ima-geNet is based on the English WordNet, its synsets comprise concepts that are familiar in the Anglosphere but might be exotic or even unknown in other cultures, such as TAILGATING <ref type="bibr" target="#b55">(van Miltenburg et al., 2017)</ref>. Conversely, it may fail to cover concepts from other cultures (cf. ?3.3). To quantify the relevance of the ImageNet concepts across languages, we map each synset manually onto its Wikipedia page 2 and use the Wikipedia API to extract the languages for which a given page is available. <ref type="figure">Fig. 2</ref> (left) illustrates that most of the synsets are only present in 30 or fewer languages, and that only a small number of "universal" concepts exist. Relying on the WALS database <ref type="bibr">(Dryer and Haspelmath, 2013)</ref>, we also show that the same argument applies to language families (middle) and that most of these languages are from the Eurasian macro-area (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet concepts are overly specific to English.</head><p>ImageNet contains overly specific concepts that belong to leaf nodes in WordNet, e.g. BLENHEIM SPANIEL, rather than basic-level concepts such as DOG. To demonstrate this, we calculate the (shortestpath) depth in WordNet of a subset of 447 ImageNet concepts from <ref type="bibr" target="#b34">Ordonez et al. (2013)</ref>, 3 and compare them with the depth of the labels that people used to refer to objects in corresponding images <ref type="bibr" target="#b34">(Ordonez et al., 2013)</ref>. Whereas humans tend to employ higherlevel synsets (depth ? = 8.92, ? 2 = 3.94), Im-ageNet systematically prefers finer-grained synsets (? = 10.61, ? 2 = 6.13). Therefore, not only are the concepts overly specific for English, but this mis-2 Automatic mapping is hard <ref type="bibr" target="#b32">(Nielsen, 2018)</ref>. 3 Please refer to <ref type="figure">Fig. 5</ref> in App. ?B for more details.</p><p>match may be aggravated in other cultures. Anecdotally, we found that KOTO, a Japanese instrument, was simply denoted as instrument by English speakers; while we expect Japanese annotators to prefer the more precise expression ? (koto).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sources of Bias</head><p>We now turn our attention to the potential sources of the biases emerging from ?2.2. In particular, we scrutinise each individual step that is part of the creation of datasets such as ImageNet, ILSVRC 1K, and NLVR2: 1) Concept selection, 2) Candidate image retrieval, and 3) Manual cleanup.</p><p>A first source of bias is the selection of concepts. From WordNet, ImageNet originally selected 12 subtrees for a total of 5,247 synsets. 4 Finer-grained synsets were preferred to obtain a "densely populated semantic hierarchy" <ref type="bibr">(Deng et al., 2009, p. 2)</ref>. Among these, 1K concepts were selected randomly for the ILSVRC 2012-2017 shared tasks. 5 Thus, the 1K concepts form a random sample that may be skewed towards non-basic levels (e.g. 147 synsets are species of dogs).</p><p>The second source of bias is candidate image retrieval. Image results from search engines (Flickr and other unspecified engines for ILSVRC 1K; Google Images for NLVR2) do not follow the real-world distribution, e.g. of gender <ref type="bibr">(Kay et al., 2015)</ref> and ethnicity <ref type="bibr" target="#b33">(Noble, 2018)</ref>. In fact, they tend to customise results according to the user's individual profile and localisation. Moreover, queries for ImageNet were again expressed in English, and to a minor (unspecified) extent in Spanish, Dutch, Italian, and Mandarin Chinese, all Western European except for the latter.</p><p>Thirdly, additional bias may lie also in image filtering, which is necessary as only an estimate of 10% results per query are of acceptable quality <ref type="bibr" target="#b54">(Torralba et al., 2008)</ref>. In ImageNet, cleanup was performed via crowdsourcing (in particular, Amazon Mechanical Turk). While no information about the language and culture of the annotators is available, there exists no reason to assume that they were representative of global diversity. Moreover, annotations without consensus were discarded, hence possibly eliminating cultural variations (where disagreement may just represent different basic levels or prototypes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MaRVL: Dataset Annotation</head><p>Given the biases in ImageNet-derived or inspired datasets, we define a protocol to collect data that is driven by native speakers of a language, consisting of concepts arising from their lived experiences. <ref type="bibr">6</ref> The dataset creation consists of five distinct phases: 1) selection of languages; 2) selection of universal concepts; 3) selection of language-specific concepts; 4) selection of images; 5) annotation of captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selection of Languages</head><p>We chose five languages, i.e. Indonesian (ID), Swahili (SW), Tamil (TA), Turkish (TR), and Mandarin Chinese (ZH), that are typologically, genealogically, and geographically diverse <ref type="bibr" target="#b12">Clark et al., 2020;</ref>. In addition, this sample covers different writing systems and includes lowresource languages (Tamil and Swahili). 7 This aims at demonstrating the universal applicability of our annotation protocol and reflecting the world's linguistic and cultural diversity during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Selection of Universal Concepts</head><p>Ideally, datasets for different languages and cultures should reflect the most salient concepts and their typical visual denotations, while retaining some thematic coherence for comparability. Hence, we start from a shared pool of universal semantic fields.</p><p>There are multiple lists of human universals from ethnographic studies <ref type="bibr" target="#b7">(Brown, 1991)</ref> and from comparative linguistics <ref type="bibr" target="#b52">(Swadesh, 1971;</ref><ref type="bibr">Haspelmath and Tadmor, 2009)</ref>. As a source for our pool of concepts, we opt for the Intercontinental Dictionary Series (Key and Comrie, 2015), because it is an opensource cooperative and evolving database, and it is cross-lingually comprehensive, as it collects lexical <ref type="bibr">6</ref> We discard the alternative of selecting concepts from Word-Nets in other languages <ref type="bibr">(Artale et al., 1997, inter alia)</ref> as they are often translations from English (thus, bare of culture-specific concepts) and do not specify the basic level for synsets. 7 All of these languages have large populations of speakers to ensure the availability of sufficient annotators. material about languages from all over the world. From its 22 chapters, we retain only the set of 18 semantic fields that cover concrete objects and events. The full list is available in Tab. 6 (App. ?A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Selection of Language-Specific Concepts</head><p>For each language, we hire 5 native speaker annotators 8 to provide Wikipedia page links for 5-10 specific concepts in their culture 9 of each semantic field. <ref type="bibr">10</ref> The two key requirements are for the concepts to be "commonly seen or representative in the speaking population of the language;" and "ideally, to be physical and concrete." For example, for the semantic field MUSIC INSTRUMENT, a Chinese annotator might supply "https://zh.wikipedia.org/wiki/??." We reiterated the requirement that the concepts need to be "common/popular" so unusual concepts related to heritage, traditions, folklore, etc. can be avoided. Then, we obtain a ranked list for the most popular concepts in each semantic field and keep only the top 5 concepts (more in case of ties) that have been selected by more than 1 annotator (&gt; 1 vote). As a result, we obtain 86-96 specific concepts for each language (see detailed statistics in ?4). Among the selected concepts, 72.4% are with ? 3 votes while the remaining 27.6% have 2 votes. The high consensus among annotators suggest that the chosen concepts are representative in the culture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Selection of Images</head><p>After obtaining the full list of concepts, we again hire native annotators to select images for these concepts by collecting image links from the web. We provided a detailed guideline to specify the desired images. In particular, we adopted the image selection requirements of NLVR2: finding images that (1) contain more than one instance of the concept; (2) show an instance of the concept interacting with other objects;</p><p>(3) show an instance of the concept performing an activity; and (4) display a diverse set of objects or features. These requirements help elicit complex images where the challenges are more likely to lie on compositional reasoning instead of object detection <ref type="bibr" target="#b51">(Suhr et al., 2019)</ref>. In addition to these, to ensure that the ?????????????????? (The man in the right image is serving a ball while the man in the left image is returning a ball.) ?????????????????? (The man in the right image is serving a ball while the man in the left image is returning a ball.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False True</head><p>Figure 3: Left: For each annotation instance, eight images are randomly picked from the image set of a concept and randomly paired into four pairs. Annotators then write a caption that is True for two pairs but False for the other two. Right: Labels are hidden and a different set of annotators will relabel them.</p><p>images authentically reflect the native annotators' everyday experience, annotators were also required to select images that are "commonly found or representative in the speaking population of the language." As a result, annotators from different cultures tend to pick visually diverse images even for the same concept, as shown in <ref type="figure" target="#fig_3">Fig. 7</ref> (App. ?C). Other requirements include images are CC-licensed, natural images, etc. (view App. ?D for details). The annotators are allowed to crowd-source the images by any means (local websites, search engines, Wikipedia, etc.) as long as all requirements are met. In general, we made a detailed guideline (available online) to guide the annotators through the whole process and encourage them to find qualified images. We hire two annotators per language. For each concept, we ask for 12 images. Concepts with &lt; 8 valid images are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Annotation of Captions</head><p>To generate an annotation instance for a given concept, we randomly draw 8 images from its image set, and randomly form 4 image pairs. An annotator is asked to write a caption that is true for two pairs but false for the other two pairs <ref type="figure">(Fig. 3, left)</ref>. The captions are required to centre around the "theme concept," which is given during annotation. We do so to prevent annotators from writing captions that rely on oversimplified cues and instead focus on the main objects in the images. Note that each annotation instance contributes 4 data points to our dataset (2 true pairs and 2 false pairs). This annotation scheme largely follows that of NLVR2's <ref type="bibr" target="#b51">(Suhr et al., 2019)</ref>. We generate 4 annotation instances from each concept. 11 During caption writing, annotators are obliged to report any error (e.g. duplicated images, wrong theme concept name, etc.), and can choose to skip an instance if they find it too hard. We hire native-speaking professional linguists (translators) from proz.com with at least a bachelor's degree to write the captions. Before caption writing, a training session is conducted. For each language, two to four annotators are hired (subject to the availability of annotators in the language).</p><p>After a batch of captions is written, we hide the True/False labels and hire a set of validators to relabel all photo-caption pairs <ref type="figure">(Fig. 3, right)</ref>. The validators are also required to flag any grammatical errors and typos. After finishing labelling, the "ground-truths" (i.e., labels by the original annotator) are revealed and all conflicted answers are highlighted. Validators then write down why they disagree with the label. The instances with different True/False assignments, along with grammatical errors or typos, are returned to the original annotator for revision. After revisions, the captions are considered finalised. Finally, a native speaker runs a final pass on the examples resolving minor typos and inconsistencies.</p><p>In the final dataset, a data point consists of two images, a caption, and a True/False label ( <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Analysis</head><p>Human validation. We run a final-round evaluation for reporting human accuracy and interannotator agreement (without changing the finalised captions). For each language, 200 examples are randomly sampled from our dataset. We mask the True/False labels and ask two new validators to relabel the examples (same as <ref type="figure">Fig. 3, right)</ref>. Across all languages, the Fleiss' kappas among three annotators (one caption writer and two final-round validators) are at least 0.887 (Tab. 1). According to <ref type="bibr">Landis and Koch (1977)</ref>, it indicates almost perfect interannotator agreement. Suppose the labels given by the caption writer are correct, the average human accu-     <ref type="bibr">, 2018)</ref>. <ref type="bibr">13</ref> As shown in <ref type="figure" target="#fig_1">Fig. 4 (top)</ref>, the Chinese images have very different distributions compared to the English <ref type="bibr">12</ref> We report human accuracy scores as the average correct prediction ratios of the two validators. <ref type="bibr">13</ref> We tried running UMAP for multiple times and found the embedding structure generally stable. ones (from NLVR2). Specifically, we note that a lot of the clusters of English NLVR2 images are different species of dogs. This is caused by the granularity problem induced by ImageNet. In <ref type="figure" target="#fig_1">Fig. 4</ref> (bottom), we compare the image distribution of two languages (Indonesian and Swahili) in MaRVL. Within MaRVL, we still find distributions of images across languages vary. This is largely caused by the distinct concept sets. As shown in the figure, the different clusters stem from the fact that the two regions have very different animal species. We note that, since ResNet50 is pre-trained on ImageNet, the formed clusters may be biased towards ImageNet concepts. As suggested in <ref type="figure" target="#fig_1">Fig. 4 (top)</ref>, the NLVR2 images are usually better clustered than the Chinese ones from MaRVL. UMAP visualisations for more languages are shown in App. ?F.</p><formula xml:id="formula_0">?? (CARRYING POLE) Various species of dogs? ?? (HOTPOT) Kakatua (COCKATOO) Kasuari (CASSOWARY) Chui (LEOPARD) Nyani (BABOON) Mbuni (OSTRICH)</formula><p>Multilingual and multicultural statistics. <ref type="figure">Fig. 2</ref> compares key multilingual and multicultural statistics of the concepts in MaRVL with the ones in Ima-geNet and NLVR2. We can clearly see that MaRVL concepts are present in more languages than the ones in ImageNet and NLVR2 despite being languagespecific. We hypothesise this is a result of the concepts in MaRVL being more prototypical and hence reflecting more neighbouring cultures. This is supported by the middle and right plots in <ref type="figure">Fig. 2</ref> that show how more concepts in MaRVL are found in more language families and macro-areas.</p><p>Limitations. While we selected the international annotation platforms that cover the most languages (proz.com and prolific.co), it remains challenging to recruit speakers of low-resource languages: we could find only 2-4 qualified annotators per language for caption writing. This might amplify the bias from individual annotators. The authors of the present work do not speak some of the languages considered natively. Moreover, all our concepts are mapped to Wikipedia pages. For low-resource languages, these might be missing for certain concepts. Finally, only ?5 concepts are selected for each semantic field. This could also inject some bias as frequent concepts are likely to distribute in different categories in an imbalanced way. In general, the protocol for MaRVL can still be improved, albeit our goal to minimise our biases as dataset creators was partly achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines</head><p>Several pre-trained Transformer models for visionand-language tasks have been proposed, inspired by the BERT architecture <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref>, and redesigned to handle multimodal inputs. They are pretrained on large-scale image-text corpora <ref type="bibr" target="#b46">(Sharma et al., 2018)</ref>, that are usually only available in English. The M 3 P model <ref type="bibr" target="#b31">(Ni et al., 2021)</ref> extends Unicoder-VL <ref type="bibr">(Li et al., 2020a)</ref> to encode multilingual inputs, resulting in the first multilingual multimodal BERT-like architecture. Pre-training alternates between modelling multimodal English data and text-only multilingual data. In this paper, we follow this approach and propose two multilingual variants of UNITER <ref type="bibr" target="#b11">(Chen et al., 2020)</ref>: mUNITER, obtained by initialising UNITER with mBERT <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref>, and xUNITER, obtained by initialising UNITER with XLM-R BASE <ref type="bibr" target="#b13">(Conneau et al., 2020)</ref>.</p><p>The UNITER architecture consists of a stack of Transformer layers similar to BERT BASE , whose input is the concatenation of language and vision embeddings. The language inputs are first split into sub-word units <ref type="bibr" target="#b60">(Wu et al., 2016;</ref><ref type="bibr" target="#b44">Sennrich et al., 2016)</ref> and surrounded by two special tokens,</p><formula xml:id="formula_1">{[CLS], w 1 , . . . , w T , [SEP]}.</formula><p>The language embeddings are then obtained as in the original BERT architecture. The vision input consists of a set of visual features given by a pre-trained object detector, to which we add a special feature [IMG] that encodes the entire image, {[IMG], v 1 , . . . , v K }. Each feature is embedded using a BERT-like embedding layer by using its bounding box coordinates as the input position. Finally, the global representation for an image-text pair is obtained via multiplicative pooling <ref type="bibr" target="#b24">(Lu et al., 2019)</ref> wherein the pooled representations for the text modality, extracted from the [CLS] token, and for the visual modality, extracted from the [IMG] feature, are element-wise multiplied to obtain a single vector for the image-text pair.</p><p>We code our models in VOLTA 14 and pre-train them using the same data and hyperparameters as in the controlled setup proposed by <ref type="bibr" target="#b9">Bugliarello et al. (2021)</ref>. <ref type="bibr">15</ref> This lets us fairly compare the performance of our multilingual versions with the corresponding monolingual ones. Afterwards, our models are fine-tuned on NLVR2 following the approach initially proposed by . For more details regarding our architecture, pre-training and finetuning, see App. ?G. After English fine-tuning, multilingual models are tested on MaRVL in a 'zero-shot' cross-lingual transfer setting.</p><p>In addition, we also benchmark the performance of five monolingual vision-and-language BERT models available in VOLTA: UNITER, VL-BERT <ref type="bibr" target="#b49">(Su et al., 2020)</ref>, VisualBERT <ref type="bibr" target="#b19">(Li et al., 2019a)</ref>, ViL-BERT <ref type="bibr" target="#b24">(Lu et al., 2019)</ref> and LXMERT <ref type="bibr" target="#b53">(Tan and Bansal, 2019)</ref>. These models are also pre-trained in the same controlled setup and fine-tuned on the English training set of NLVR2. Following the established 'translate test' approach to cross-lingual transfer <ref type="bibr" target="#b5">(Banea et al., 2008;</ref><ref type="bibr" target="#b15">Conneau et al., 2018;</ref><ref type="bibr" target="#b37">Ponti et al., 2021b)</ref>, they are evaluated on the test sets of MaRVL automatically translated into English. 16</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In Tab. 4, we show the performance of the baselines outlined in ?5 on MaRVL. We report two metrics: accuracy across all examples and consistency, the proportion of unique sentences for which predictions 14 https://github.com/e-bug/volta. <ref type="bibr">15</ref> As multilingual data, we use Wikipedia in 104 languages. <ref type="bibr">16</ref> We use neural machine translation in the Google Cloud API.   on all corresponding image pairs are correct. We note that the differences among all models for specific transfer methods are not statistically significant. This indicates that varying neural architectures does not have an appreciable impact on performance once they are pre-trained on the same amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLVR2</head><p>Zero-shot vs. translate test. We find that both multilingual and monolingual models perform comparably well in English (NLVR2). When evaluated on the languages in MaRVL, however, the performance of zero-shot multilingual baselines dramatically drops by 10 ? 20% points, floating just above chance level.</p><p>Remarkably, this is also the case for resource-rich languages like Mandarin Chinese (ZH), for which unlabelled text is abundant. Compared to zero-shot transfer, all translate-test baselines gain 4 ? 15% across the different languages, with Turkish improving the most. Yet, there persists a considerable gap of more than 10% compared to the performance on English in NLVR2. Arguably, this is caused by the out-of-distribution nature of the data in MaRVL.</p><p>Disentangling shifts in distribution. There are two sources of difficulty that make MaRVL challenging: 1) cross-lingual transfer and 2) out-of-distribution concepts, in both images and descriptions, with respect to English datasets. To assess the effect of each of these factors on model performance, we conduct a controlled study on Mandarin Chinese (MaRVL-ZH). First, we manually translate MaRVL-ZH into English, hence removing any possible confound due to machine translation in Tab. 4. As shown in Tab. 5 (left column), compared to the Translate test evaluation, each model improves its accuracy by only 1-2%, with the exception of mUNITER. Thus, the translation is fairly reliable. Moreover, out-of-distribution concepts are responsible for the largest share of errors (on average, a drop in accuracy of 10%).</p><p>Second, we sample 250 unique descriptions, corresponding to 1,000 data points, from the NLVR2 test set and manually translate them into Mandarin Chinese. The performance of mUNITER and xUNITER on this subset, which we denote as NLVR2 1k , is listed in Tab. 5 (right column). Although all data points are in-domain, both our multilingual models, mU-NITER and xUNITER, lose 16% in accuracy compared to the English NLVR2 1k test set (central column). Hence, this gap can be attributed to crosslingual transfer from English into Chinese.</p><p>Translate train. Finally, we establish a baseline for a third possible approach to cross-lingual transfer, 'translate train'. To this end, we machine translate the training set of NLVR2 into Mandarin Chinese and then evaluate on MaRVL-ZH. We find that the performance of mUNITER (62.5/18.7) and xUNITER (61.8/16.7) is close to their respective performance when machine translating MaRVL-ZH into English (translate test). Again, the lack of access to culturally relevant concepts hinders generalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Grounded language reasoning. Several datasets to assess reasoning over language and vision have been released in the recent years <ref type="bibr" target="#b3">(Antol et al., 2015;</ref><ref type="bibr">Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b61">Xie et al., 2019;</ref><ref type="bibr" target="#b65">Zellers et al., 2019)</ref>. Most notably, CLEVR and its extensions <ref type="bibr">(Johnson et al., 2017a,b;</ref><ref type="bibr" target="#b23">Liu et al., 2019;</ref><ref type="bibr">Kottur et al., 2019)</ref>, as well as GQA (Hudson and Manning, 2019), address the task of answering complex, compositional questions over images. NLVR <ref type="bibr" target="#b50">(Suhr et al., 2017)</ref>, on the other hand, compares and contrasts pairs of synthetic images via human-written descriptions. The reasoning capabilities of artificial models are evaluated through binary classification over these grounded descriptions. We closely follow the task formulation of NLVR2 <ref type="bibr" target="#b51">(Suhr et al., 2019)</ref>, which extends the NLVR collection to realworld photographs. Our dataset, MaRVL, is the first multilingual and multicultural dataset for grounded language reasoning.</p><p>Multilingual multimodal datasets. <ref type="bibr">Elliott et al. (2016)</ref> introduced Multi30k, one of the most widely used multilingual image-caption datasets. Multi30k enriches Flickr30K <ref type="bibr" target="#b64">(Young et al., 2014</ref>)-originally in English-with translated and newly written German captions. Others provided captions for subsets of MS-COCO images in German and French <ref type="bibr">(Rajendran et al., 2016)</ref>, Japanese <ref type="bibr" target="#b63">(Yoshikawa et al., 2017)</ref>, and Chinese <ref type="bibr" target="#b20">(Li et al., 2019b)</ref>. Chinese captions were made available also for videos by . All these datasets suffer from at least one of these two deficiencies: 1) they start from a sample of images crowd-sourced from North America and Western Europe; 2) they contain at most three (high-resource) languages, only a pale reflection of the world's crosslingual and cross-cultural variation. Finally, <ref type="bibr" target="#b47">Srinivasan et al. (2021)</ref> automatically scraped a multilingual text-image dataset from Wikipedia. While not suitable for evaluation, this is a promising training resource for multilingual multimodal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>Our empirical and theoretical analyses reveal that concepts and images documented in existing visiolinguistic datasets are likely neither salient nor prototypical in many languages different from English and in cultures outside Europe and North America. Therefore, to mitigate these biases, we devise a new annotation protocol where the selection of images and captions is entirely driven by native speakers. More-over, we elicit descriptions comparing and contrasting image pairs in 5 typologically diverse languages: Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. We publicly release the resulting multicultural and multilingual dataset for grounded language reasoning, MaRVL, and its annotation guidelines, with the hope that other members of the scientific community will be able to further expand it.</p><p>Moreover, we develop and benchmark a series of multilingual and multimodal baselines, including model-and translation-based transfer. We find that their performance is sometimes just above chance level and suffers considerably from the outof-distribution nature of concepts, images, and languages in MaRVL compared to English datasets. This gives us reason to believe that it offers a more faithful estimate of state-of-the-art models' suitability in real-world applications, outside a narrow linguistic and cultural domain.</p><p>Better methods for out-of-distribution crosslingual transfer <ref type="bibr" target="#b35">(Ponti et al., 2021a</ref>) and a deeper understanding of how visual stimuli affect language <ref type="bibr" target="#b21">(Li et al., 2020b;</ref><ref type="bibr">Rodr?guez Luna et al., 2020)</ref> might prove essential for future progress in the MaRVL challenge. In future work, we will also assess model performance on additional tasks already supported by our dataset, such as object recognition, and experiment with multilingual extensions of visiolinguistic models based on contrastive learning (Carlsson and Ekgren, 2021).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>We collected data from crowdworkers spread across the world in languages that are under-represented in current vision and language datasets. Our reasoning was two-fold: first, we believe that researchers should work with lower-resourced languages, and second, we believe that language data should be drawn from different language families. We hired workers from two crowdsourcing platforms: Prolific and Proz. The identity of the workers on Prolific were anonymous to us, while those on Proz were not anonymous. The use of Profilic was authorised by the relevant authority in our university.</p><p>Workers hired from Prolific were predominantly based in North American and Western European countries; those hired on Proz were nearly exclusively based in Indonesia, India, Turkey, China, Tanzania, Kenya, and Somalia. All workers were paid ?15-?20/hour, and were paid even if their data did not appear in the final datasets. (There were several rounds of filtering in the collection process.)  <ref type="bibr">Intelligence, EAAI 2020</ref><ref type="bibr">, New York, NY, USA, February 7-12, 2020</ref>. AAAI Press.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Chapter to Semantic Field Mapping</head><p>We list all chapters, semantic fields, and the mapping between them in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Depth of ImageNet Concepts in the WordNet Hierarchy</head><p>Fig <ref type="figure">. 5</ref> compares the distribution of minimum and maximum depth in WordNet for 447 synsets from <ref type="bibr" target="#b34">Ordonez et al. (2013)</ref> according to their labels in Ima-geNet and to the labels given by the annotators (i.e. basic-level categories). The mapping of human labels onto WordNet was done manually, resolving any disambiguation. We find that humans typically tend to prefer higher-level synsets than the ones present in ImageNet, showing that concepts in ImageNet are already over-specific for English. <ref type="figure" target="#fig_3">Fig. 7</ref> shows images for the concept BASKETBALL across our five languages in MaRVL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C BASKETBALL across Languages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Annotation Details</head><p>Selection of language-specific concepts. We pay 0.1 GBP for each Wikipedia link (maximum 10 links for each semantic field) such that annotators would have incentives to write down as many concepts as possible for each semantic field. Annotators hired in this phase are from prolific.co 17 and proz.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of images.</head><p>We have also included requirements such as avoid (1) synthetic images (2) collages (3) watermarks (4) low-resolution images, etc. to make sure that only natural images (photos) are included in our dataset. CC-license is also demanded for all collected images. For each valid image, we pay 0.12 GBP. After collecting enough images, we run a second round check by ourselves to delete all unqualified images. Annotators hired in this phase are from prolific.co and proz.com.</p><p>Annotation of descriptions. Before assigning the task, we conduct training sessions with the description writers. They are required to complete a sample. We then provide feedback and ask for revisions if the sample is not perfectly aligned with the guideline. Each valid description is paid with 0.6 GBP. During quality control, 0.1 GBP is paid for every reviewed example. Description writers are all hired from proz.com while validators are from both proz.com and prolific.co.</p><p>17 An extra 33.3% VAT is paid to the platform for all prolific. co payment. <ref type="figure">Figure 5</ref>: Distributions of minimum and maximum synset depth in the WordNet hierarchy for 447 ImageNet labels compared to basic-level categories provided by humans <ref type="bibr" target="#b34">(Ordonez et al., 2013)</ref>. The lower depth of the latter suggests that ImageNet categories may be over-specific. Final-round human validation. In this round, the re-labelling annotators are asked to focus on logical correctness instead of the grammaticality/fluency of the language. Again, 0.1 GBP is paid for every assigned label. Validators are from both proz.com and prolific.co.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Dataset Statistics</head><p>We plot the distribution of the description lengths in <ref type="figure" target="#fig_2">Fig. 6</ref>. Except for Turkish and Tamil, our collected descriptions are longer than the ones in NLVR2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Image Embedding Visualisations</head><p>We plotted image feature distributions of MaRVL-ZH, SW, ID in the main text. Here we also visualise the features of TA, TR <ref type="figure" target="#fig_4">(Fig. 8, left)</ref>, and provide a full graph for all languages <ref type="figure" target="#fig_4">(Fig. 8, right)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Baselines Details</head><p>In this section, we describe our multilingual visionand-language models, mUNITER and xUNITER, as well as their training procedures in detail. We use the same hyperparameters as in the controlled study   of <ref type="bibr" target="#b9">Bugliarello et al. (2021)</ref> and build our models on top of the VOLTA repository. 18</p><p>Architecture. Our models extend the UNITER <ref type="bibr" target="#b11">(Chen et al., 2020)</ref> architecture. This is a single-stream system consisting of a single stack of Transformer layers, similar to BERT <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref>. The input to the model is the concatenation of language and vision inputs. The text input consists of a sequence of sub-word units <ref type="bibr" target="#b60">(Wu et al., 2016;</ref><ref type="bibr" target="#b44">Sennrich et al., 2016)</ref>   <ref type="bibr" target="#b1">(Anderson et al., 2018)</ref>. Similar to BERT BASE , the body of our Transformers consists of 12 layers, each with 12 attention heads, and a hidden size with dimension 768. Finally, the global representation for an image-text pair is obtained via multiplicative pooling <ref type="bibr" target="#b24">(Lu et al., 2019)</ref> wherein the pooled representations for the text modality, extracted from the [CLS] token, and for the visual modality, extracted from the [IMG] feature, are element-wise multiplied to obtain a single vector for 18 https://github.com/e-bug/volta. the image-text pair.</p><p>Pre-training. <ref type="bibr">Following Ni et al. (2021)</ref>, we pretrain our models by alternating multilingual textonly batches and multimodal English-only batches. Masked language modelling (MLM; <ref type="bibr" target="#b18">Devlin et al. 2019</ref>) is the sole objective used in the multilingual steps, while the loss in multimodal batches is given by the sum of three objectives: MLM, masked region classification with KL-divergence (MRC-KL; <ref type="bibr" target="#b24">Lu et al. 2019</ref>) and image-text matching (ITM; <ref type="bibr" target="#b11">Chen et al. 2020)</ref>. During multimodal pre-training, some of the image regions are randomly masked 19 and the model is tasked to predict the distribution of the corresponding classes given by the Faster R-CNN model. ITM is the multimodal version of next sentence prediction in BERT. Here, the caption of an image is replaced with probability 0.5 with a random caption in the training corpus. The model is then trained to identify whether the given image-caption pairs are a match or not. Following , MLM and MRC-KL losses are not computed when image and captions are not matched to avoid sub-optimal mapping between visual and linguistic inputs. For multilingual pre-training, we use Wikipedia dumps <ref type="bibr">(Rosa,</ref>   2018) for 104 languages, 20 while we rely on Conceptual Captions for multimodal pre-training. <ref type="bibr">21</ref> In the multilingual steps, we first sample languages according to the following multinomial distribution that accounts for the size of each Wikipedia dump (Conneau and Lample, 2019), and then, for each sampled language, a sentence is uniformly sampled. We set the multinomial parameter ? as in the original papers: 0.7 for mBERT-based mUNITER and 0.3 for XLM-R-based xUNITER. Each model is pre-trained for 10 Conceptual Captions epochs as done in previous work. Following the implementation of <ref type="bibr" target="#b31">Ni et al. (2021)</ref>, the models' parameters are updated after each multilingual and multimodal batch, while the learning rate scheduler only after both of them.</p><p>where [ ] denotes the concatenation of the pooled representations.</p><p>Experimental setup. We train our models on a single NVIDIA Titan RTX. Pre-training each model takes 9 days, while NLVR2 fine-tuning for 20 epochs takes 1 day. The parameter sets giving the best validation performance in NLVR2 are used for zero-shot, cross-lingual evaluation on MaRVL. <ref type="figure" target="#fig_5">Fig. 9</ref> shows the performance of mUNITER and xU-NITER for each chapter in each language. Compared to the overall performance ("ALL"), we find that no chapter is easier or difficult in all languages for both models. Cross-lingual performance of the two models varies acriss chapters: While both models find smaller fluctuations on "Speech and language", mUNITER shows minimal deviation on "Agriculture and vegetation" but twice as much as xUNITER's on "Cognition." Interestingly, in Swahili, mUNITER and xUNITER show opposite behaviour for "Motion," with mUNITER largely outperforming the overall accuracy, while a similarly large drop is given by xUNITER. Overall, we find that both models find almost every chapter almost as difficult, resulting in per-chapter accuracy close to the overall one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Performance by Chapter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I More Examples from MaRVL</head><p>To demonstrate more details of our dataset, we pick two examples from each language (TA: <ref type="figure" target="#fig_0">Fig. 10</ref>, ZH: <ref type="figure" target="#fig_0">Fig. 11</ref>, SW: <ref type="figure" target="#fig_0">Fig. 12</ref>, ID: <ref type="figure" target="#fig_0">Fig. 13 TR: Fig. 14)</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two examples from MaRVL. The Tamil images (a) are from the concept (JALLIKATTU, part of an Indian festivity), while the Swahili images (b) are from the concept leso (HANDKERCHIEF). et al., 2019), MS-COCO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Image feature distributions of MaRVL-ZH and NLVR2 (top) and MaRVL-ID and MaRVL-SW (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Description length distribution by languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Five examples of "basketball" from our dataset (one per language). Though describing the same concept, the visual representations can vary drastically across languages/cultures, e.g. in the personal attributes of the players or the field background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Left: image feature distributions of MaRVL-TA, TR. Right: image feature distributions of all languages in MaRVL and NLVR2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Accuracy of mUNITER and xUNITER when grouping concepts by chapter. 'ALL' denotes the overall accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :Figure 13 :</head><label>1113</label><figDesc>images contain a lot of masala vadas.", concept: (VADA, a popular Indian food), label: FALSE) (b) ("In one of the two pictures, the finger shows the vote.", concept: (INK), label: TRUE) Figure 10: More examples from MaRVL-TA. (a) ??????????????????????? ???????????("In total, there are more than five people playing drums in the two images combined and people in the two images are playing different kinds of drums.", concept: ? (DRUM), label: TRUE) (b) ??????????????????("At least one of the two pictures shows a mandarin duck pot.", concept: ?? ? (MANDARIN DUCK POT, a specific type of pot), label: TRUE) More examples from MaRVL-ZH. (a) Picha moja ina mti wa maparachichi na picha nyingine ina maparachich yaliyokatwa vipande. ("One picture contains an avocado tree and another has avocados chopped.", concept: Parachichi (AVOCADO), label: FALSE) (b) Picha ya upande wa kushoto mtu mmoja tu anapiga zumari na picha ya upande wa kulia watu wawili wanapiga zumari. ("Picture on the left is just one person blowing the flute and in the picture on the right two people are blowing the flute.", concept: Zumari (FLUTE), label: TRUE) Figure 12: More examples from MaRVL-SW. (a) Salah satu gambar adalah gambar rendang yang disediakan di restoran Padang, dan gambar di sebelahnya adalah gambar rendang yang disajikan dengan lauk-pauk lain. ("One of the pictures contains rendang provided at Padang restaurant, and the picture next to it contains rendang served with other side dishes.", concept: rendang (RENDANG, a popular Indonesian dish), label: TRUE) (b) Salah satu gambar adalah topeng yang sedang dipakai seseorang, dan gambar di sebelahnya adalah gambar topeng yang dipajang. ("In one of the pictures, someone wears a mask, and the picture next to it is a mask display.", concept: Topeng (MASK), label: TRUE) More examples from MaRVL-ID. (a) Sa?daki foto?rafta kurban bayram? nedeniyle boynuzlar? s?slenmi? en az iki kurbanl?k hayvan bulunuyor. ("In the right, there are at least two sacrificial animals decorated by the horns due for the sacrifice feat.", concept: Kurban Bayram? (EID AL-ADHA, an Islamic holiday), label: FALSE) (b) G?rsellerden birinde dizlerinde kanun bulunan birden ?ok insan var. ("In one of the images, there are multiple people with qanuns on their knees.", concept: Kanun (?alg?) (QANUN, a popular instrument in Turkey), label: TRUE) Figure 14: More examples from MaRVL-TR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ID SW ZH TR TA avg.Human accuracy .963 .930 .955 .970 .980 .960  Fleiss' kappa  .913 .887 .933 .954 .966 .931    </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Human accuracy of validators on ground-truth labels and inter-annotator agreement (Fleiss' kappa).</figDesc><table><row><cell>ID SW ZH TR TA tot.</cell></row><row><cell>Concepts selected with &gt;8 images % not in WordNet 18.8% 8.0% 27.7% 21.1% 30.2% 21.1% 96 88 94 90 86 454 95 78 94 79 83 429 Total images 1153 1110 1271 972 946 5464 used for captions 1091 875 1107 917 924 4914 Total examples 1128 1108 1012 1180 1242 5670 Total unique captions 282 276 253 295 305 1411</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Key stats of concepts and images in MaRVL.</figDesc><table><row><cell>NLVR2</cell><cell>MaRVL</cell></row><row><cell>EN</cell><cell>ID SW ZH TR TA avg.</cell></row><row><cell cols="2">Avg. length 15.8 18.2 17.8 16.1 11.0 11.7 15.0 Word types 811 684 516 848 784 775 721 TTR 0.21 0.15 0.12 0.21 0.28 0.26 0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Key stats of MaRVL captions: average length, number of word types, and type-to-token ratio (TTR). racy scores 12 are also very high, mostly in the high 90%s except for Swahili (93.0%). For a detailed statistics about our dataset, see Tab. 2. After the image collection, on average 5 concepts are filtered out for each language. Among the final concepts, several are not found in the English WordNet, e.g. sports like ya?l? g?re? (OIL WRESTLING), architectures like ? ?? (SIHEYUAN), or food like (DOSA). We report the key statistics of MaRVL captions as well as 250 randomly sampled NLVR2 captions in Tab. 3 The length distributions of the captions are visualised inFig. 6(App. ?E).</figDesc><table><row><cell>Concept and image statistics. Caption statistics. Image distribution. To better understand the dis-</cell></row><row><cell>tribution of MaRVL images (and also how it differs</cell></row><row><cell>from NLVR2), we extract the features of (1) MaRVL</cell></row><row><cell>images and (2) 1K randomly sampled NLVR2 images</cell></row><row><cell>using an ImageNet pre-trained ResNet50 (He et al.,</cell></row><row><cell>2016), then visualise their embedding distribution us-</cell></row><row><cell>ing UMAP (McInnes et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance (accuracy/consistency) in MaRVL and NLVR2 (Test-P). Translate test denotes the setup of the multilingual MaRVL datasets translated into English. Best scores are put in bold, but do not imply statistical significance.</figDesc><table><row><cell>Model</cell><cell>MaRVL ZH?EN</cell><cell>NLVR21K EN</cell><cell>NLVR21K EN?ZH</cell></row><row><cell cols="3">LXMERT ViLBERT VisualBERT 59.8 / 13.1 61.7 / 14.3 62.5 / 13.5 VL-BERT 65.4 / 20.6 UNITER 63.8 / 19.4 mUNITER 61.3 / 17.1 72.2 / 33.2 -----xUNITER 64.4 / 20.6 73.0 / 31.6</cell><cell>-----56.2 / 6.8 57.1 / 9.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Performance (accuracy/consistency) in MaRVL- ZH when manually translated into English and NLVR2 1k when manually translated into Mandarin Chinese.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Chapter to semantic field mapping.</figDesc><table><row><cell>Bola basket (Indonesian)</cell><cell>Mpira wa kikapu (Swahili)</cell><cell>?? (Chinese)</cell><cell>Basketbol (Turkish)</cell><cell>(Tamil)</cell></row><row><cell>Bola basket (Indonesian)</cell><cell>Mpira wa kikapu (Swahili)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>surrounded by two special tokens, {[CLS], w 1 , . . . , w T , [SEP]}. The vision input consists of a set of visual features given by a pre-trained object detector and a special feature [IMG] that encodes the entire image, {[IMG], v 1 , . . . , v K }. Specifically, we use K = 36 visual features per image given by a Faster R-CNN architecture</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Often, only a subset of 1K concepts from the ILSVRC challenges of 2012-2017<ref type="bibr" target="#b43">(Russakovsky et al., 2014)</ref> is considered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Currently expanded to 21,841 synsets according to https: //www.image-net.org as of 9 May 2021. 5 411 were subsequently substituted to accommodate new challenges (object localisation and fine-grained classification).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We hired 5 speakers per language as we found this was generally enough to achieve a high agreement of selected concepts.9  Eliciting the selection from native speakers is ideal as salient concepts are generated first and prototypical members to represent them (such as images) are preferred (cf. ?2.1).10  In the rare cases where a Wikipedia page is unavailable, the annotators are asked to write the names of the concept directly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We aim to collect &gt;1K examples per language, which can be achieved by generating 4 annotation instances per concept.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, and Balaraman Ravindran. 2016. Bridge correlational neural networks for multilingual multimodal representation learning. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 171-181, San Diego, California. Association for Computational Linguistics. Diana Rodr?guez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, and Elia Bruni. 2020. Internal and external pressures on language emergence: least effort, object constancy and frequency. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4428-4437, Online. Association for Computational Linguistics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">Regions whose IoU is greater than 0.4 are also masked.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">https://github.com/google-research/bert/blob/ master/multilingual.md</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the anonymous reviewers, Rita Ramos, ?kos K?d?r, Stella Frank, and members of the CoAStaL NLP group for their constructive feedback. We also thank Alane Suhr for clarifications regarding the NLVR2 dataset collection protocol. This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk?odowska-Curie grant agreement No 801199, and Cambridge Digital Humanities Digitisation/Digital Resources Awards.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">21</ref> <p>We use the 2.77M data points available in VOLTA.</p><p>Fine-tuning. We fine-tune our models on the English NLVR2 dataset and then measure zero-shot performance on our MaRVL dataset. In NLVR2, given two images (I l and I r ) and a description D, the model is trained to assess the validity of the description given the images (true or false for both images). We follow  and cast this as a classification problem: Given embeddings that encode the two image-description pairs, (I l , D) and (I r , D), the probability that they are both valid is predicted by a 2-layer MLP with a GeLU (Hendrycks and Gimpel, 2016) activation in between, followed by a softmax over two classes (representing true and false labels):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1960" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Word, object, and conceptual development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">M</forename><surname>Anglin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>W. W. Norton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">WordNet for Italian and its use for lexical discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Artale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress of the Italian Association for Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="346" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity analysis using machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ethnobiological classification: Principles of categorization of plants and animals in traditional societies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Berlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">185</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Human Universals. McGraw Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How shall a thing be called? Psychological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language BERTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fredrik Carlsson and Ariel Ekgren. 2021. Multilingual-CLIP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised crosslingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating crosslingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-06-25" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coco-cn for cross-lingual image tagging, captioning, and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2347" to="2360" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emergent communication pretraining for few-shot machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyiran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4716" to="4731" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clevr-ref+: Diagnosing visual reasoning with referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="4185" to="4194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Social and cognitive motivations of change: Measuring variability in color semantics. Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="34" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Category coherence in crosscultural perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="148" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Natural categories: Well defined or fuzzy sets?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glucksberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="462" to="472" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gro?berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Categories and concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noba textbook series: Psychology. DEF publishers</title>
		<editor>Edward Diener and Robert Biswas-Diener</editor>
		<meeting><address><addrLine>Champaign, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">M3p: Learning universal representations via multitask multilingual multimodal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3977" to="3986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linking imagenet wordnet synsets with wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finn ?rup Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1809" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Algorithms of oppression: How search engines reinforce racism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noble</forename><surname>Safiya Umoja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NYU Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013-12-01" />
			<biblScope unit="page" from="2768" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Minimax and neyman-Pearson meta-learning for outlier languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Edoardo Maria Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Disha</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1245" to="1260" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">XCOPA: A multilingual dataset for causal commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Edoardo Maria Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianchu</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2362" to="2376" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Modelling latent translations for crosslingual transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Edoardo Maria Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
		<idno>abs/2107.11353</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</author>
		<title level="m">Plaintext wikipedia dump 2018. LIN-DAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (?FAL), Faculty of Mathematics and Physics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Family resemblances: Studies in the internal structure of categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="605" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Basic objects in natural categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penny</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boyes-Braem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="439" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Natural categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eleanor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="350" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the internal structure of perceptual and semantic categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eleanor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive development and acquisition of language</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1973" />
			<biblScope unit="page" from="111" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. ArXiv preprint, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">No classification without representation: Assessing geodiversity issues in open data sets for the developing world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimbo</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1711.08536</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SI-GIR &apos;21</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SI-GIR &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2443" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="504" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The origin and diversification of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morris</forename><surname>Swadesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<pubPlace>Aldine, Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5100" to="5111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cross-linguistic differences and similarities in image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-SimLex: A large-scale evaluation of multilingual and crosslingual lexical semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulla</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Petti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eden</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="847" to="897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Vatex: A largescale, high-quality multilingual dataset for video-andlanguage research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="4580" to="4590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Wierzbicka</surname></persName>
		</author>
		<title level="m">Semantics: Primes and universals: Primes and universals</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Superordinate and basic category names in discourse: A textual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory L</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="261" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1609.08144</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Visual entailment: A novel task for fine-grained image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1901.06706</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">STAIR captions: Constructing a large-scale Japanese image caption dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akikazu</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="417" to="421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
