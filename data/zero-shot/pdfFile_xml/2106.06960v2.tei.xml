<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation and Correlation Enhanced Encoder-Decoder Framework for Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Cui</surname></persName>
							<email>mengmeng.cui@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>wangwei@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Zhang</surname></persName>
							<email>jinjin.zhang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Representation and Correlation Enhanced Encoder-Decoder Framework for Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>STR ? Sequence-to-Sequence ? Multi-Head Attention ? Lay- ernorm &amp; Dropout</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention-based encoder-decoder framework is widely used in the scene text recognition task. However, for the current state-ofthe-art(SOTA) methods, there is room for improvement in terms of the efficient usage of local visual and global context information of the input text image, as well as the robust correlation between the scene processing module(encoder) and the text processing module(decoder). In this paper, we propose a Representation and Correlation Enhanced Encoder-Decoder Framework(RCEED) to address these deficiencies and break performance bottleneck. In the encoder module, local visual feature, global context feature, and position information are aligned and fused to generate a small-size comprehensive feature map. In the decoder module, two methods are utilized to enhance the correlation between scene and text feature space. 1) The decoder initialization is guided by the holistic feature and global glimpse vector exported from the encoder. 2) The feature enriched glimpse vector produced by the Multi-Head General Attention is used to assist the RNN iteration and the character prediction at each time step. Meanwhile, we also design a Layernorm-Dropout LSTM cell to improve model's generalization towards changeable texts. Extensive experiments on the benchmarks demonstrate the advantageous performance of RCEED in scene text recognition tasks, especially the irregular ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene Text Recognition(STR) refers to the text recognition of natural scene images captured by camera. Compared with traditional Optical Character Recognition(OCR) systems dedicated to high-quality document images, STR techniques are developed for the outdoor images and applied in a wider range of fields, such as street view positioning, image advertisement filtering, bill recognition, et al. Due to the randomness in the process of capturing text images in natural scenes, STR has many challenges in practical applications, including uneven lighting and focusing caused image quality degradation, complex image background, occluded and incomplete characters. Moreover, the characters themselves also have diverse font types, font sizes and colors; the text distribution is irregular, many of which are perspective, distorted or oriented. Therefore, as a complex problem, STR has been extensively studied in industry and academia.</p><p>STR is divided into regular and irregular text recognition tasks according to the text distribution. Modern technical solutions mainly include Connectionist Temporal Classification(CTC) based methods <ref type="bibr" target="#b0">[1]</ref>, attention-based encoderdecoder framework <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and the combination of both <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. These methods only need word-level annotations and robust to complicated scene text images. CTCbased methods solve the problem of misalignment between the input image and the target outputs but can not leverage the contextual dependency between characters, thus it is mainly used for horizontal text recognition. Comparatively, attention mechanism is a good way to strengthen the relevance between visual and semantic features and improve the interpretability of the model, making it a suitable choice for irregular STR scenarios. The major architectures of the attention-based encoder-decoder framework include the sequence-to-sequence models which adopt 1D attention mechanism <ref type="bibr" target="#b1">[2]</ref> or 2D attention mechanism <ref type="bibr" target="#b5">[6]</ref> in the decoder, and the transformer-based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>However, there are still shortcomings for the existing attention-based encoderdecoder framework. 1D attention methods generally use RNN layer(s) to model contextual dependencies but lose apparent information of the text <ref type="bibr" target="#b1">[2]</ref>. Although 2D attention is able to handle the irregular spatial distribution of the text, its performance is greatly restricted by the size of the encoded feature map <ref type="bibr" target="#b5">[6]</ref>. Since the inter-character dependence in STR is weaker than the inter-word dependence in machine translation, the self-attention design in transformer-based models which targets at building long-range dependencies may not achieve the expected performance <ref type="bibr" target="#b2">[3]</ref>, but increases the parameters due to its multiple fullyconnected layers and the multi-layer stacking structure of decoder. The common reasons for these problems lie in the information loss of the global and local feature, as well as the weak relevance between the encoder working on the visual space and the decoder working on the language space. Therefore, we propose the Representation and Correlation Enhanced Encoder-Decoder Framework(RCEED). The encoder generates a comprehensive representation of local visual feature and global context feature. The decoder utilizes the Multi-Head General Attention mechanism to capture an enriched glimpse of the encoded feature. The initialization manner and the efficient workflow of the decoder increase the correlation between the visual feature and the decoded characters. Our main contributions are summarized as follows: 1. In the encoder module, a representation enhanced feature map is obtained by combining the visual, context and position information. The encoded feature map has a small size corresponding to the spatial distribution of characters. 2. In the decoder module, a holistic feature and the a global glimpse vector are introduced from the encoder to guide the initialization of the decoder. The intuitive workflow enables the glimpse vector to participate in the update of the decoded hidden state and the character prediction at the same time. These integrated designs make the model achieve SOTA performance in public benchmarks, especially the irregular ones. 3. We devise a Mulit-Head General Attention mechanism to capture the main information and the supplementary information of the encoded feature with fewer operations and parameters. 4. We specially design the LD-LSTM cell as basic block to form the RNN layers of the encoder and the decoder. The LD-LSTM can balance independence and relevance between characters and improve model's generalization for irregular texts, which is very important for the STR applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early text image recognition is oriented to the document recognition scenario which has a clear picture and a fixed pattern. People use binarization method <ref type="bibr" target="#b8">[9]</ref> and sliding window method <ref type="bibr" target="#b9">[10]</ref> for individual character detection, and then integrate the characters into words by dynamic programming. These methods are vulnerable to the background noise, and unable to use the global context information. Later works tend to treat the text image recognition as a sequence recognition problem. These methods are more capable of the complex STR tasks and mainly divided into two categories, the CTC-based methods and the attention based methods. CRNN <ref type="bibr" target="#b0">[1]</ref> utilizes the CNN and RNN to generate feature sequence from the visual information, and CTC to align the characters predicted by the RNN decoder. Attention based encoder-decoder models like RARE <ref type="bibr" target="#b10">[11]</ref> and R 2 AM <ref type="bibr" target="#b11">[12]</ref> are developed to introduce the attention-mechanism from machine translation <ref type="bibr" target="#b12">[13]</ref> to solve the image-based sequence recognition problem. Focusing attention network <ref type="bibr" target="#b13">[14]</ref> is raised to fix the attention drifting caused by complex scenes or low image quality. With similar targets, Bai et al. introduce the edit probability <ref type="bibr" target="#b14">[15]</ref> method to alleviate character missing or superfluous in text recognition. In addition, model as a combination of CTC and attention based methods <ref type="bibr" target="#b4">[5]</ref> also performs well on regular scene text datasets.</p><p>In recent years, many approaches have been proposed regarding the more challenging irregular scene text recognition task. The first type is the rectification based methods. ASTER <ref type="bibr" target="#b1">[2]</ref> combines the Thin-Plate Spline (TPS) method <ref type="bibr" target="#b15">[16]</ref> and the Spatial Transformer Network(STN) <ref type="bibr" target="#b16">[17]</ref> to form the rectification network. The line-fitting transformation method is proposed in ESIR <ref type="bibr" target="#b17">[18]</ref> which employs iterative rectification to improve the performance. MORAN <ref type="bibr" target="#b18">[19]</ref> proposes a pixel transform method to make a smooth conversion to the text images. The other type is the character level methods. Models like Char-Net <ref type="bibr" target="#b19">[20]</ref> and Mask TextSpotter <ref type="bibr" target="#b20">[21]</ref> detect and rectify the individual characters, which requires additional character-level supervision. The last type is the attention based encoderdecoder frameworks. 2D feature map is used for both the sequence-to-sequence models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> and the transformer-based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. The expanded focus range contributes to the recognition of characters with arbitrary shape and position. RobustScanner <ref type="bibr" target="#b21">[22]</ref> introduces a positional enhancement branch to the 2D attentional encoder-decoder structure proposed in SAR <ref type="bibr" target="#b5">[6]</ref>, leveraging the positional information to make prediction during decoding process. MASTER <ref type="bibr" target="#b6">[7]</ref> employs the non-local network as the encoder of the transformer-based structure to capture longer contextual dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>As presented in <ref type="figure" target="#fig_0">Figure 1</ref>, there are three important components of RCEED: the Rectification Network which redistributes the characters, the Representation Enhanced Encoder which combines the local visual feature and global context feature, the Multi-Head General Attention Decoder which increases the correlation between the visual space and the language space. Two basic composition methods are utilized in the encoder and decoder, including the LD-LSTM cell which improves model's generalization and the Multi-Head General Attention mechanism which makes effective use of main and supplementary information. Architecture of the RCEED. The input image is transformed by the Rectification Network before fed into the encoder. The encoder combines the visual feature from CNN, the context feature from the LD-BiLSTM layer and the position information to generate the comprehensive feature map(v). The holistic feature(h f ) and the global glimpse vector(g f ) from encoder are used to guide decoder initialization. At each decoding step t, the glimpse vector gt(? g) is calculated by MHGAT based on the current hidden state ht(? h), and exported to the next LD-LSTM iteration to generate ht+1. Both gt and ht are used to make the current prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rectification Network</head><p>Thin-Plat Spline <ref type="bibr" target="#b15">[16]</ref> is a 2D interpolation method which makes conversion with minimal bending energy based on a set of corresponding control points of two pictures. Similar to the STN in <ref type="bibr" target="#b1">[2]</ref>, our rectification network utilizes a lightweight CNN to generate the control points of the source image, associating them with the pre-defined control points of the target image through TPS to accomplish the rectification process. We also find that the rectification network is able to adjust the width and spacing of characters adaptively, which reduces the divergence of input text images and improves the alignment between receptive fields and characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic Composition Methods for Encoder-Decoder Framework</head><p>In order to facilitate understanding, before the illustration of the Encoder and Decoder, we introduce two methods which are important components of them.</p><p>Layernorm-Dropout LSTM Cell(LD-LSTM) The Long Short-Term Memory(LSTM) <ref type="bibr" target="#b22">[23]</ref> is widely used in the machine translation models <ref type="bibr" target="#b12">[13]</ref>. Considering the difference between the text recognition task and the language processing task which has strong semantic dependencies between tokens, we specially design the Layernorm-Dropout LSTM Cell.</p><p>Firstly, we add layernorm <ref type="bibr" target="#b23">[24]</ref> to the current input and the previous hidden state of the LSTM cell to speed up the convergence of training process and regularize the network. Secondly, on account of the relatively weak dependencies between characters in the text image, we introduce the dropout function <ref type="bibr" target="#b24">[25]</ref> to reduce feature co-adaptation and improve the presentation of important information. Different from the conventional way of applying dropout in the feed-forward connections between RNN layers <ref type="bibr" target="#b25">[26]</ref>, we design a per-step dropout method in the recurrent connections of RNN cells. Both the hidden state and the cell state are sampled by the dropout masks with probability p to balance the relevance and independence between characters, and improve the generalization at the same time.</p><p>The layernorm operation is given by:</p><formula xml:id="formula_0">LN (x; ?, ?) = (x ? ?) ? ? + ? (1) ? = 1 D D i=1 x i , ? = 1 D D i=1 (x i ? ?) 2<label>(2)</label></formula><p>Where x i is the i th element of x with length D. ? and ? are defined as gain and bias parameters. In this paper, we set ? as zero. Then the LD-LSTM cell is defined as:</p><formula xml:id="formula_1">? ? ? ? f t i t o t c t ? ? ? ? = LN (W x x t ; ? 1 , ? 1 ) + LN (W h h t?1 ; ? 2 , ? 2 ) (3) c t = Dropout (sigm(f t ) c t?1 + sigm(i t ) tanh(? t ), p) (4) h t = Dropout (sigm(o t ) tanh(c t ), p)<label>(5)</label></formula><p>Where W x and W h are weight matrixes of the input x t and the hidden state h t?1 . denotes the element-wise product operation. p is the probability of each element of the vector being zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head General Attention(MHGAT)</head><p>The commonly used attention functions include general attention, additive attention, dot product attention et al. Unlike most sequence-to-sequence text recognition models which utilize additive attention mechanism to build connection between encoder and decoder <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>, we use general function <ref type="bibr" target="#b26">[27]</ref> to reduce the computational complexity. Compared with additive attention <ref type="bibr" target="#b12">[13]</ref> which needs two-step operations to obtain the attention weights(add first and then multiply by the transform matrix), general attention only needs one-step matrix multiplication operation(Equation <ref type="formula">(6)</ref>).</p><formula xml:id="formula_2">Given v =[v 1 , v 2 , ..., v N ]</formula><p>as a splitted part of the flattened comprehensive feature map v with length N, the formulation of the general attention mechanism for v can be expressed as follows:</p><formula xml:id="formula_3">score(h t , v ) = vW a h t d 2 v (6) a t = sof tmax(score(h t , v )) ? R N (7) GeneralAttention(h t , v ) = N i=1 a t,i v i (8)</formula><p>Where a t,i is the i th element of the attention weights a t . W a ? R dv?d v is a parameter matrix with d v and d v representing the dimensions of v and v . In order to build a harder attention module to suppress the background noise, we set the a relatively large scale factor, which is d 2 v under 8 parallel heads. Due to the diversity of character size and distribution, there is misalignment between the feature map obtained after CNN downsampling and the character visual information. Therefore, we adopt the multi-head attention(shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>) to increase the attention flexibility and reduce information loss. Given the hidden state h t at time step t as a query, the Multi-Head General Attention for v is generated by:</p><formula xml:id="formula_4">g t = M ultiHead(h t , v) = Concat(head 1 , ..., head m ) (9) where head j = GeneralAttention(h t W h,j , v j ), v j ? Split(v, m)</formula><p>The parameters are W h,j ? R d? dv m , where d denotes the dimension of h t , m is the number of parallel heads. v j is obtained by splitting m times along d v . g t as a concatenation of the attention heads refers to the "glimpse" of the encoded feature where the current step attends to. <ref type="figure" target="#fig_1">Figure 2</ref> visualizes the 8heads attention maps at each decoding step. Head 8 and head 4 are respectively in charge of the main and the supplementary attention tasks, and accurately aligned with the target characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Representation Enhanced Encoder</head><p>The encoder of RCEED consists of two parts: the Resnet <ref type="bibr" target="#b17">[18]</ref> based CNN backbone(presented in <ref type="table">Table 1</ref>) which extracts local visual feature, and the single-layer LD-BiLSTM which outputs global context feature. Limited by the receptive field, the CNN encoder can not well represent the contextual information. Therefore, BiLSTM <ref type="bibr" target="#b22">[23]</ref> layer with the LD-LSTM Cell as block(abbreviated as LD-BiLSTM) is utilized to generate the context feature sequence, which is based on the intermediate feature sequence produced by average pooling on the visual feature column vectors. The final hidden state is taken as the holistic feature.</p><p>However, due to the abstractness of the LD-BiLSTM operation, the complex shape and spatial information of the text is lost. Thus, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we add the context feature sequence and the corresponding visual feature column vectors along the row axis, getting the comprehensive feature representation. Since the character position distribution along the vertical direction is mainly divided into three parts: upper, middle, and lower. Meanwhile, for the sake of reducing computational consumption, we set the feature map size as 3?20, where 3 corresponds to the 3 types vertical position. Compared with <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>, our feature <ref type="table">Table 1</ref>. The configuration of the ResNet based CNN feature extractor. The stride and padding for the convolutional layers are all set to 1. The 'k' and 's' in the max-pooling layers refer to kernel size and stride. Height and width of the output of each layer are presented in the last column. map size is only a quarter of theirs. Similar to <ref type="bibr" target="#b27">[28]</ref>, we consider the influence of position information and use sinusoid function to encode absolute as well as relative position information to the comprehensive feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recurrent Multi-Head General Attention Decoder</head><p>As shown in <ref type="figure" target="#fig_3">Figure 4(b)</ref>, the decoder is built on a sequence-to-sequence architecture with the LD-LSTM cell and the Multi-Head General Attention as basic blocks. Before the decoding starts, given the flattened comprehensive feature map v and the holistic feature h f from the encoder as inputs, a global glimpse vector g f related to h f is derived from MHGAT. h f and g f represent the global information of the encoded feature map and serve as the guidance for the initialization of decoder, which helps to focus on the explicit part at the first decoding step and improve the accuracy. Then, the LD-LSTM operation is performed fed with g f , h f , and a start token y s . Note that the LD-LSTM is slightly changed in the decoder, the word embedding of the start token and sequential outputs y = [y s , y 1 , ..., y T ?1 ] is added to Equation <ref type="formula">(3)</ref>.</p><formula xml:id="formula_5">(h t , c t ) = LD ? LST M (g f , h f , y s ) t = 1 LD ? LST M (g t?1 , h t?1 , y t?1 ) t &gt; 1<label>(10)</label></formula><p>Next, the Multi-Head General Attention mechanism is performed to generate the glimpse vector related to the current hidden state h t .</p><formula xml:id="formula_6">g t = M ultiHead(h t , v)<label>(11)</label></formula><p>Finally, the predicted character is calculated by:</p><formula xml:id="formula_7">y t = ?(h t , g t ) = argmax(sof tmax(W o [h t ; g t ] + b o ))<label>(12)</label></formula><p>The hidden state h t and current glimpse vector g t are concatenated and passed through the linear transformation to make the final prediction to 63 classes, including 10 digits, 52 case sensitive letters and the 'EOS' token. Then the hidden state, cell state and glimpse vector are recovered and passed to the next step. The decoder works iteratively until the 'EOS' token is predicted or the maximum time step T is arrived.</p><p>Attention based sequence-to-sequence models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> generally use the decoding workflow based on <ref type="bibr" target="#b12">[13]</ref>, which is h t?1 ? a t ? g t ? h t , where the glimpse vector g t is related to the last time step and cannot be directly used for the current prediction. On the other hand, the workflow of RCEED is more intuitive, which is (h t?1 , g t?1 ) ? h t ? a t ? g t . The decoding workflow of SAR <ref type="bibr" target="#b5">[6]</ref> is comparable to us. Nevertheless, the first round RNN iteration of SAR does not make predictions, and its glimpse vector is not utilized in the RNN calculation. In our design, g as representation of a certain part of the encoded feature directly participates in the recurrent iteration of h and the character prediction. Therefore, a content enriched expression is obtained and the correlation between the visual feature and the character sequence is enhanced. When decoding starts, the holistic feature h f and global glimpse vector g f from encoder are input to the LD-LSTM cell together with the start token ys to get the hidden state h1. h1 and its related glimpse vector g1 are both used to make the first prediction. Then decoding iteration continues based on the previous outputs until the 'EOS' token is predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We train our RCEED on part of the two synthetic datasets: MJSynth and SynthText, and the training sets of the four scene text datasets, i.e., IIIT5K, ICDAR2013, SVT, ICDAR2015. Evaluation results are based on the six standard benchmarks, including three regular datasets and three irregular datasets.</p><p>MJSynth(MJ) <ref type="bibr" target="#b28">[29]</ref> is also named Synth90k. MJ has 9 million synthetic images of english words, each of them is annotated with a word-level ground truth. We use 4.8 million images from MJ for training.</p><p>SynthText(ST) <ref type="bibr" target="#b29">[30]</ref> is another synthetic dataset which is widely used in text detection and recognition. We crop the word patches from the background images and random select 4.8 million out of 7 million word images for training.</p><p>IIIT5K <ref type="bibr" target="#b30">[31]</ref> is a natural scene text dataset cropped from the Google search images. It is divided into 2000 training and 3000 testing images.</p><p>SVT <ref type="bibr" target="#b9">[10]</ref> consists of 257 training and 647 testing data cropped from the Google Street View images. SVT is challenging for the blur or noise interference.</p><p>ICDAR2013 <ref type="bibr" target="#b31">[32]</ref> contains 848 images for training and 1095 images for evaluation. We filter out the images that only contain the non-alphanumeric characters, resulting in a test set with 1078 images.</p><p>ICDAR2015 <ref type="bibr" target="#b32">[33]</ref> has 4468 word patches for training and 2077 for testing. IC15 is a benchmark dataset for irregular scene text recognition. The images are captured from the Google Glasses under arbitrary angles. Therefore, most words are irregular and have a changeable perspective.</p><p>SVT-Perspective(SVTP) <ref type="bibr" target="#b33">[34]</ref> has 645 cropped scent text images for testing. It is also from the Google Street View but has more perspective words than SVT.</p><p>CUTE80 (CUTE) <ref type="bibr" target="#b34">[35]</ref> consists of 288 word patches. CUTE is widely used for STR model evaluation for various curved text images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We build our model on the Tensorflow framework. All experiments are conducted on 2 NVIIDA Titan X GPUs with 12GB memory. The training dataset consists of 4.8 million from MJ, 4.8 million from ST, and 50k from the mixed training sets of IIIT5K, IC13, IC15 and SVT. Data augmentation methods such as perspective distortion and color transformation are applied to the mixed real datasets. The batch size is set to 42, including 20 from the MJ, 20 from the ST and 2 from the mixed dataset. The training process only needs 2.2 epochs in total with word-level annotation. Adam is utilized for optimization, with the addition of cross-entropy loss and l2 regularization loss(with the coefficient of 1e-4) as the objective function. Learning rate is set to 1e-4 per GPU, and reduced to 1e-5 in the last 0.2 epoch.</p><p>The input size of our model is 48 ? 160. Height is fixed, if the image width is shorter than target width after resize with the original scale, we apply the padding operation, else we change the scale and simply reshape the image to the target size. The LD-BiLSTM in the encoder and LD-LSTM in the decoder are both single-layer, with the dropout rate of 0.1 and 0.5 respectively. The dimensions of the visual feature/context feature/decoded hidden state/word embedding matrix are set to 512/512/512/256. The maximum time step T is 27.</p><p>At the inference stage, no lexicon is used. For the sake of efficiency, we simply rotate 90 degree clockwise for the images of which the height is larger than 2 times of the width, instead of applying two directions rotation as previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>. Besides, unlike <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, we do not use the beam search method to improve the performance. The decoder works in the forward direction, rather than the bidirectional strategy which is adopted in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. <ref type="table">Table 2</ref>. Recognition accuracy of our method and other sate-of-the-art methods on the six public datasets. All the results listed are lexicon free. The approaches which need character-level annotations are marked *. In each column, the best performance is shown in bold font, and the second best result is underlined. Our model achieves the best accuracy in regular dataset SVT and the most challenging irregular dataset IC15, the second highest accuracy in regular dataset IC13 and curve dataset CUTE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Regular Text Irregular Text IIIT5K SVT IC13 IC15 SVTP CUTE CRNN(2015) <ref type="bibr" target="#b0">[1]</ref> 81.2 82.7 89.6 ---R 2 AM (2016) <ref type="bibr" target="#b11">[12]</ref> 78.4 80.7 90.0 ---RARE(2016) <ref type="bibr" target="#b10">[11]</ref> 81.9 81.9 88.6 ---FAN(2017) <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>In this section, we test the model performance on regular and irregular STR public datasets, and make comparison with the SOTA methods. As shown in Table 2, RCEED achieves the best performance in regular dataset SVT and irregular dataset IC15, and the second highest accuracy in regular dataset IC13 and curve dataset CUTE. Compared with competitors SAR <ref type="bibr" target="#b5">[6]</ref> and RobustScanner <ref type="bibr" target="#b21">[22]</ref> which are baseline and up-to-date methods targeting at irregular scene text recognition and trained with synth&amp;real datasets, RCEED performs accuracy increases of 3pp to 3.4pp in the most challenging irregular dataset IC15 and outperforms them by 4 out of 6 benchmarks, while the size of the encoded feature map is only 1/4 of theirs. Compared with SAR modeled with the same-size encoded feature map(SAR 3?20), our model outperforms it in 5 benchmarks with 6pp ahead in maximum. Experimental results demonstrate that RCEED has got rid of the performance bottleneck caused by small-size feature map, and is competitive in both the regular and the irregular datasets. <ref type="figure" target="#fig_4">Figure 5</ref> shows the success and failure cases of RCEED. Success cases in the first row demonstrate that the proposed model has a robust ability to deal with blur, variation, occlusion, distortion, uneven light and other difficult situations. However, when the problems become serious, failure cases such as over-prediction and less-prediction appear. It can be seen that image quality still has a significant impact on the recognition results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this section, we conduct a series of comparison experiments to analyse the effectiveness of the key contributions in RCEED. Evaluation results of 8 conditions are listed in <ref type="table">Table 3</ref>. Impact of comprehensive feature We evaluate the performance when visual feature and context feature are used alone as the input of the decoder. As shown in Row 2 and 3, single visual and single context features both have lower accuracy of up to -1.6pp on the regular dataset and -2.8pp on the irregular dataset compared with the comprehensive feature, indicating that the feature fusion operation plays an important role in improving model performance. Impact of Layernorm-Dropout LSTM Cell Row 1 reveals the model performance when the LD-LSTM design in the encoder and decoder are both replaced with normal LSTM cells. Compared with Row 8, the model implemented with normal LSTM cells shows an accuracy drop of -1.3pp on the largest irregular dataset IC15. While in the largest regular dataset IIIT5K, the accuracy is 0.6pp higher and equals to the best performance <ref type="bibr" target="#b21">[22]</ref>. The dropout design in the LD-LSTM improves model's ability in dealing with changeable characters, but is not compatible with regular texts with stable features. Overall, LD-LSTM cell still outperforms the normal one in most benchmarks.</p><p>Impact of guided initialization of decoder Row 4 lists the results when the decoder is initialized with zero vectors instead of the holistic feature and global glimpse vector from the encoder. Accuracy shows a drop up to -1.6pp in the five out of six public datasets, which demonstrates that the global information from the encoder is helpful for improving the decoding accuracy. Impact of glimpse vector for prediction We build a comparison model which does not use glimpse vector for prediction. As shown in Row 5, the performance degradation is more serious in irregular datasets(up to -1.8pp), proving that visual information has a greater impact on recognizing complex texts. Impact of heads number We compare the evaluation results when attention heads number m is set to 1, 4 and 8 in Row 6, 7, 8. The 1 head condition is equivalent to not using the multi-head design. We set m=8 in our model for the superior performance on irregular datasets. <ref type="table">Table 3</ref>. Ablation studies by changing model structures and hyper-parameters. LD refers to models w/o the Layernorm-Dropout method in the RNN layers. VF/CF denote whether the Visual Feature and Context Feature are involved in the encoded feature map. GI/GP analyze the impact of the Guided Initialization of decoder and the Glimpse vector for Prediction. Heads column presents the conditions with different number of heads. All the comparison models are trained from scratch with word-level annotations.</p><p>Cond LD VF CF GI GP Heads IIIT5K SVT IC13 IC15 SVTP CUTE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a representation and correlation enhanced encoderdecoder framework for scene text recognition. The encoder enhances model's representation ability through the aligning and fusing operation between the local visual feature and the global context feature. The decoder strengthens the correlation between the encoded comprehensive feature and the decoded character sequence through the guided initialization and the efficient workflow. Essential components including the Multi-Head General Attention mechanism and the LD-LSTM cell are designed to reduce feature deficiency and improve the generalization towards changeable texts. The model breaks the constraint of feature map size and has a superior performance on public benchmarks. In future research, we will develop an end-to-end integrated detection and recognition model for the text spotting task and develop advanced applications of the visualsemantic interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work is supported by National Natural Science Foundation of China (61976214, 61721004, 61633021), and Science and Technology Project of SGCC Research on feature recognition and prediction of typical ice and wind disaster for transmission lines based on small sample machine learning method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the RCEED. The input image is transformed by the Rectification Network before fed into the encoder. The encoder combines the visual feature from CNN, the context feature from the LD-BiLSTM layer and the position information to generate the comprehensive feature map(v). The holistic feature(h f ) and the global glimpse vector(g f ) from encoder are used to guide decoder initialization. At each decoding step t, the glimpse vector gt(? g) is calculated by MHGAT based on the current hidden state ht(? h), and exported to the next LD-LSTM iteration to generate ht+1. Both gt and ht are used to make the current prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of the 8-heads attention map at each decoding step. Head 8 is in charge of the main information and completely aligned with the decoded characters. Head 4 pays attention to the supplementary information around the target area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Structure of the Representation Enhanced Encoder. The intermediate feature sequence is obtained by applying average pooling on the visual feature column vectors. After the column-aligned addition between the visual feature and the context feature, as well as the positional encoding operation, a comprehensive feature map is generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Structure of the Multi-Head General Attention. (b) Structure of the Recurrent Multi-Head General Attention Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Success and failure cases of RCEED. GT refers to the ground truth, Pred refers to the predicted results.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A holistic representation guided attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scatter: selective context attentional scene text recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11962" to="11972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural scene text recognition based on encoder-decoder framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Q</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="62616" to="62623" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Master: Multi-aspect nonlocal network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02562</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nrtr: A no-recurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of methods and strategies in character segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lecolinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A thin-plate spline and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fl Bookstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Methods in Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="14" to="28" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moran: A multi-object rectified attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask textspotter: An endto-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="532" to="548" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robustscanner: Dynamically enhancing positional clues for robust text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC-British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
		<title level="m">Icdar 2013 robust reading competition. In ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
