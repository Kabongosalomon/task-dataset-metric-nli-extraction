<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ESAD: End-to-end Semi-supervised Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
							<email>huangchaoqin@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
							<email>pszhao@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>ya_zhang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ESAD: End-to-end Semi-supervised Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>HUANG ET AL.: ESAD: END-TO-END SEMI-SUPERVISED ANOMALY DETECTION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores semi-supervised anomaly detection, a more practical setting for anomaly detection where a small additional set of labeled samples are provided. We propose a new KL-divergence based objective function for semi-supervised anomaly detection, and show that two factors: the mutual information between the data and latent representations, and the entropy of latent representations, constitute an integral objective function for anomaly detection. To resolve the contradiction in simultaneously optimizing the two factors, we propose a novel encoder-decoder-encoder structure, with the first encoder focusing on optimizing the mutual information and the second encoder focusing on optimizing the entropy. The two encoders are enforced to share similar encoding with a consistent constraint on their latent representations. Extensive experiments have revealed that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, including medical diagnosis and several classic anomaly detection benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anomaly detection (AD), with broad application in medical diagnosis <ref type="bibr" target="#b52">[53]</ref>, credit card fraud detection <ref type="bibr" target="#b38">[39]</ref>, and autonomous driving <ref type="bibr" target="#b16">[17]</ref>, has received significant attention among the machine learning community. The main challenge in AD is that, it is prohibitive, even if not impossible, to collect a representative set of anomalous samples due to its remarkable scarcity in the population. To bypass the challenge, many approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62]</ref> have resorted to unsupervised learning so that only normal samples are needed for model training. <ref type="figure">Figure 1</ref>: The training processes of ESAD for semi-supervised anomaly detection. (a) ESAD leverages an encoder-decoder-encoder structure, where the two encoders are enforced to share similar encoding with a consistent constraint on their latent representations, with the first encoder targeting to optimize the mutual information and the second encoder focusing on the entropy. (b) shows the T-SNE <ref type="bibr" target="#b29">[30]</ref> visualization results for the latent representations.</p><p>Semi-supervised anomaly detection, where a small set of labeled data are provided for training in addition to a large amount of unlabeled data, represents a more practical setting of anomaly detection. In the real-world scenario, it is feasible to obtain a small set of 'biased' anomalous data. Earlier semi-supervised AD methods follow the unsupervised learning paradigm and employ such a labeled anomalous set through a certain form of regularization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref>. More recently, Deep SAD <ref type="bibr" target="#b43">[44]</ref>, the first deep semi-supervised AD method, builds upon the Infomax principle <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28</ref>] that maximizes the mutual information between the data and the latent representations and enforces an additional regularization on the latent representations. Due to the contradiction between the mutual information-based objective and entropy-based regularization, named model collapse in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, Deep SAD adopts a two-stage process: (i) autoencoder pre-training for mutual information maximization; and (ii) encoder fine-tuning for entropy regularization. This sequential learning process cannot guarantee the two objectives are simultaneously optimized and cannot well resolve the contradiction between the mutual information and entropy during the optimization. The model tends to collapse when the entropy is minimized to zero at the second stage, and the model inevitably leads to low mutual information as all data are mapped into a constant <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>In this paper, we introduce ESAD, an end-to-end method for semi-supervised anomaly detection. We start with exploring an alternative optimization target for AD by maximizing the KL-divergence between the normal and the anomalous class. Considering the challenge in estimating the anomalous distribution which results in the infeasibility of direct optimization, the KL-divergence based objective function is relaxed and further decomposed into two factors: (i) mutual information between the data and the latent representations and (ii) entropy of latent representations. While the two factors in the final objective function seem to be the same as those of Deep SAD, the difference lies in that, here mutual information and entropy are considered an integral part of the single objective function and need to be optimized simultaneously in an end-to-end training fashion.</p><p>In addition, to resolve the contradiction between the mutual information and entropy during optimization, we extend the autoencoder structure widely adopted for deep anomaly detection into an encoder-decoder-encoder structure illustrated in <ref type="figure">Figure 1</ref> (a), where two separate but closely resembled encoders are employed to emphasize different factors in the optimization so that the model can be trained end-to-end. Specifically, although the two encoders are enforced to share similar encoding via a consistent constraint on the outputs, the first encoder focuses on mutual information through targeting on good representations only for the normal data but not for the labeled anomalous data, while the second encoder focuses on entropy by enforcing the compacted representations for the normal data and scattered representations for the anomalous data. With the encoder-decoder-encoder structure, we achieve end-to-end training for semi-supervised anomaly detection. <ref type="figure">Figure 1</ref> (b) shows that the two encoders actually result in quite different embeddings, confirming the difficulty in finding a common embedding that simultaneously optimizing both mutual information and entropy. However, embeddings from both encoders show a better separation between the normal and anomalous classes than that of Deep SAD.</p><p>To validate the effectiveness of ESAD, we experiment with two medical image datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56]</ref>, three natural image benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">58]</ref>, and several classic AD benchmarks <ref type="bibr" target="#b41">[42]</ref>. Extensive results and analysis have shown that ESAD outperforms state-of-the-art methods on almost all datasets. Ablation studies are conducted to show the effectiveness of the proposed objectives and the encoder-decoder-encoder architecture for ESAD.</p><p>Our main contribution is summarized as follows: ? We introduce a KL-divergence based objective for semi-supervised anomaly detection and show that it can be relaxed and decomposed into mutual information and entropy related objectives, which formulates the AD objective with information-theoretical terms. ? To achieve end-to-end training, we propose an encoder-decoder-encoder architecture to simultaneously optimize the two contradictory factors, mutual information and entropy. ? The proposed method outperforms state-of-the-arts on multiple AD benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Unsupervised Anomaly Detection. The vital challenge of unsupervised AD is that the training dataset contains only normal data. One-class classification based approaches tended to depict normal data directly with statistical approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59]</ref>. Self-supervised based approaches remedied the lack of supervision by introducing different self-supervisions, where the model was trained to optimize a self-supervised task, and then normal data can be separated with the assumption that anomalous data will perform differently. In this domain, reconstruction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b63">64]</ref> is the most popular self-supervision. Some approaches introduced other self-supervisions, e.g., <ref type="bibr" target="#b18">[19]</ref> applied dozens of image geometric transforms for transformation classification, and <ref type="bibr" target="#b59">[60]</ref> proposed a restoration framework to further improve the feature learning. Semi-supervised Anomaly Detection. Since classical semi-supervised approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref> are inappropriate and hardly detect new and unknown anomalies due to the cluster assumption <ref type="bibr" target="#b7">[8]</ref>, many semi-supervised approaches are still grounded on the unsupervised learning paradigm <ref type="bibr" target="#b21">[22]</ref>. Along this line, Deep SSAD <ref type="bibr" target="#b21">[22]</ref> has been studied recently in specific contexts such as videos <ref type="bibr" target="#b24">[25]</ref>, network intrusion detection <ref type="bibr" target="#b31">[32]</ref>, or specific neural network architectures <ref type="bibr" target="#b14">[15]</ref>. Deep SAD <ref type="bibr" target="#b43">[44]</ref>, a general method based on deep SVDD <ref type="bibr" target="#b42">[43]</ref>, built upon the Infomax principle, where the training processes are consist of two stages. TLSAD <ref type="bibr" target="#b17">[18]</ref> further consolidated the model's discriminative power with a transfer learning framework, which relied on an additional large-scale reference dataset for the model training. Anomaly Detection on Medical Images is an important application but rarely considered in deep anomaly detection literature. <ref type="bibr" target="#b62">[63]</ref> proposed P-Net for anomaly detection in retinal images by leveraging the specific relation between the image texture and the regular structure of retinal images, which is hard to generalize to other medical data. <ref type="bibr" target="#b52">[53]</ref> relied on the classical autoencoder approach with a re-designed training pipeline to handle high-resolution, complex images. <ref type="bibr" target="#b60">[61]</ref> proposed a confidence-aware anomaly detection model for detecting viral pneumonia with in-house data. In this paper, we conduct experiments on some wellorganized and open-source medical image datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>3 End-to-end Semi-supervised Anomaly Detection Given the input space X consisting of normal data X N and anomalous data X A , where X = X N ? X A . For semi-supervised anomaly detection (AD), we are given n unlabeled samples x u 1 , ? ? ? , x u n ? X and m labeled samples (x l 1 , y 1 ), ? ? ? , (x l m , y m ) ? X ?Y with Y = {?1, 1} where y = 1 denotes normal samples and y = ?1 denotes anomalous samples. We assume m n. Suppose the output space is Z, the goal of AD is to find f ? : X ? Z, parameterized by ? , that leads to the maximum distance between normal and anomalous data.</p><p>Targeting semi-supervised anomaly detection, we attempt to explore an objective function based on Kullback-Leibler (KL) divergence. Let X and Z be variables sampled from X and Z, respectively. Denote the joint distribution of data and latent representations for normal and anomalous data as p N (X, Z) and p A (X, Z), respectively, and the objective function for semi-supervised AD is then formulated as: max</p><formula xml:id="formula_0">? KL [p N (X, Z) p A (X, Z)] . Here p N (X, Z)</formula><p>can be approximately estimated using the labeled normal samples and the large numbers of unlabeled data, with the widely adopted assumption for AD that almost all unlabeled data are normal <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62]</ref>. On the contrary, it is impossible to estimate p A (X, Z) due to the extremely limited labeled instances. We here introduce another distribution, p?(X, Z), and reformulated the objective function as follows:</p><formula xml:id="formula_1">max ? KL [p N (X, Z) p A (X, Z)] ? KL p?(X, Z) p A (X, Z) ,<label>(1)</label></formula><p>where p?(X, Z) can be estimated by the limited labeled anomalous data. With this objective function, we attempt to simultaneously (i) maximize KL divergence between the normal class and the anomalous class and (ii) minimize the KL divergence between the labeled anomalous class and the real anomalous class. Considering that it is impossible to estimate p A , we decompose the KL term KL [p N (X, Z) p A (X, Z)] as follows:</p><formula xml:id="formula_2">KL [p N (X, Z)||p A (X, Z)] = E p N (X,Z) log p N (X, Z) p A (X, Z) = E p N (X,Z) log( p N (Z|X) p N (Z) ? p N (Z) ? 1 p A (Z|X) ? p N (X) p A (X) ) = I(X N , Z N ) ? H(Z N ) + E p N (X) [H(p N (Z|X), p A (Z|X))] + KL [p N (X)||p A (X)] ,<label>(2)</label></formula><p>where I(?, ?) is the mutual information, H(?) is the entropy, and H(?, ?) is the cross entropy.</p><p>With the non-negativity of the third and fourth terms (see the supplementary material for the proof), we get a lower bound to Eq.</p><formula xml:id="formula_3">(2): KL [p N (X, Z)||p A (X, Z)] ? I(X N , Z N ) ? H(Z N ). Sim- ilarly, KL [p?(X, Z) p A (X, Z)] is approximated with I(X?, Z?) ? H(Z?).</formula><p>The final objective function is thus formulated as:</p><formula xml:id="formula_4">max ? {[I(X N , Z N ) ? I(X?, Z?)] ? [H(Z N ) ? H(Z?)]}.<label>(3)</label></formula><p>Note that this objective function is coincidentally similar to that of Deep SAD <ref type="bibr" target="#b43">[44]</ref>, by optimizing on both the mutual information and entropy. However, the objective function here differs from <ref type="bibr" target="#b43">[44]</ref> in that: (i) we start with a KL based formulation and derive equal weights for the mutual information and entropy, while for Deep SAD, the entropy is introduced as regularization with a coefficient ? ; (2) the mutual information for our paper involves different directions of optimizations for normal and anomalous data, while Deep SAD treats the normal and anomalous data the same in maximizing the mutual information. In our formulation, the optimizations of mutual information and entropy are integral parts of the single anomaly detection objective function and hence need to be optimized simultaneously.</p><p>Architecture. We follow <ref type="bibr" target="#b43">[44]</ref> and employ an autoencoder to optimize the mutual information I(X, Z). To resolve the contradiction between mutual information and entropy and achieve end-to-end training, different from the straightforward solution by directly introducing two independent encoders <ref type="bibr" target="#b17">[18]</ref>, we propose to append an additional encoder to the autoencoder and introduce an encoder-decoder-encoder architecture, where the first encoder Enc 1 (?) emphasizes mutual information optimization and the second encoder Enc 2 (?) focuses on entropy optimization, and in the meanwhile, the two encoders are enforced to share similar encoding via a consistent constraint on their latent representations. The encoderdecoder-encoder architecture can be expressed as:</p><formula xml:id="formula_5">z = Enc 1 (x),x = Dec(z),? = Enc 2 (x),<label>(4)</label></formula><p>wherex is the output of the decoder, and z and? are the latent representations from the first and second encoders, respectively. The wights for the two encoders are are not shared. Losses. To optimize the two factors, i.e., mutual information and entropy, in Eq. (3), we propose the corresponding losses as follows.</p><p>The optimization of mutual information is achieved with reconstruction or restoration <ref type="bibr" target="#b54">[55]</ref>. With unlabeled samples x u 1 , ? ? ? , x u n and labeled samples x l 1 , ? ? ? , x l m , we want the autoencoder to well reconstruct the normal data but erroneously reconstruct the labeled anomalous data, thus the reconstruction likelihood is maximized for the normal data and minimized for the labeled anomalous data. A straight-forward loss definition for the anomalous data is the negative squared norm loss. However, due to its unbounded nature, it is expected to result in an ill-posed optimization problem and caused optimization to diverge <ref type="bibr" target="#b43">[44]</ref>. We therefore introduce a transformation function ? (?) on the input, forcing the network to reconstruct the anomalous data x to its transformation ? (x), where ? (x) = x, ?x ? X A . The transformation makes the network unable to correctly reconstruct the anomalous samples. The reconstruction loss is defined as follows:</p><formula xml:id="formula_6">L rec?semi = 1 n n ? i=1 x u i ? x u i 2 + 1 m m ? j=1 x l j ? ?(x l j ) 2 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">?(x l j ) = x l j , if y j = 1, ? (x l j ), if y j = ?1.</formula><p>For the data which is functioned as a vector, ? (x l j ) can be a version adding Gaussian noise or a random permutation between various dimensions; for the image data, it can be a noised and rotated version of the original images. Besides the proposed transformation function, we also try another strategy, which enforces the model to reconstruct the labeled anomalous data to the normal data <ref type="bibr" target="#b37">[38]</ref>. But this task is too strict and difficult, especially for two types of samples that are quite different, which makes the model hard to converge.</p><p>For the entropy H(Z), assuming Z follows an isotropic Gaussian <ref type="bibr" target="#b9">[10]</ref>, Z ? N(?, ? 2 I) with ? &gt; 0, the entropy of Z is proportional to its log-variance, i.e., H(Z) ? log ? 2 (see the supplementary material for the proof). In this case, for z ? p(Z), an L 2 norm can be used for the optimization of the entropy, since it minimizes the empirical variance and thus minimizes the entropy of a latent Gaussian.</p><formula xml:id="formula_8">L norm?semi = 1 n n ? i=1 ? u i 2 + 1 m m ? j=1 ( ? l j 2 ) y j ,<label>(6)</label></formula><p>where y j = ?1 for the labeled anomalous data while y j = 1 for the labeled normal data. This loss enforces the compacted representation for the normal data and scattered representation for the labeled anomalous data. Note that the inverse squared norm loss used for labeled anomalous data here is bounded from below and smooth, which are crucial properties for losses used in deep learning <ref type="bibr" target="#b20">[21]</ref>. Compared with the SVDD loss in <ref type="bibr" target="#b43">[44]</ref> where a pretraining process is necessary for initializing an additional hypersphere center, L norm?semi does not need the pre-training, which indicates that the end-to-end training can be achieved.</p><p>To define the consistency between the two encoders, similar to the assistant loss <ref type="bibr" target="#b0">[1]</ref>, we resort to a consistent constraint between their corresponding latent representations:</p><formula xml:id="formula_9">L ass = 1 n + m n+m ? i=1 ? i ? z i 2 .<label>(7)</label></formula><p>Finally, we define our training loss as follow:</p><formula xml:id="formula_10">L semi = L rec?semi + ? 1 L norm?semi + ? 2 L ass ,<label>(8)</label></formula><p>where ? 1 and ? 2 are two hyperparameters. We will further discuss the impacts of these two hyperparameters in the experiment section. To this end, we achieve end-to-end training for semi-supervised anomaly detection. Anomaly Score Measurement. We discuss how we calculate the anomaly score in the test phase. Since both the mutual information and the entropy are related to the performance of anomaly detection, we use both L rec?semi and L norm?semi to measure the anomaly score for the given samples, which are related to the mutual information and the entropy, respectively. We calculate the reconstruction error of each input sample x and the value of L 2 norm for its representation? for anomaly detection. The anomaly score is formulated as:</p><formula xml:id="formula_11">S test = x ? x 2 + ? 1 ? 2 ,<label>(9)</label></formula><p>where ? 1 is the same as the setting in the training process. We will further discuss the impact of ? 1 in Section 4.5. To the best of our knowledge, it is the first time considering both the terms of the mutual information and the entropy for the anomaly score measurement. On the contrary, most one-class classification based methods, e.g., OC-SVM <ref type="bibr" target="#b48">[49]</ref>, only consider the term of the entropy. Similarly, Deep SVDD <ref type="bibr" target="#b42">[43]</ref>, Deep SAD <ref type="bibr" target="#b43">[44]</ref> and TLSAD <ref type="bibr" target="#b17">[18]</ref> also consider only the term of the entropy, since they only use the SVDD loss as the final anomaly score. Most reconstruction based methods or restoration based methods, including the vanilla AE <ref type="bibr" target="#b30">[31]</ref> and ARNet <ref type="bibr" target="#b59">[60]</ref>, only consider the term of mutual information, since they only use the reconstruction or restoration loss as the anomaly score. Results show that considering both of the two terms significantly improves the performance of anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct substantial experiments to validate our method. The ESAD is first evaluated on multiple AD benchmark datasets, comparing with several state-of-the-arts. Then we present the respective effects of different designs through ablation study. Finally, we visualize the latent representations of ESAD through T-SNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Datasets. We conduct semi-supervised anomaly detection experiments on three popular natural image datasets MNIST <ref type="bibr" target="#b26">[27]</ref>, Fashion-MNIST <ref type="bibr" target="#b57">[58]</ref> and CIFAR-10 <ref type="bibr" target="#b25">[26]</ref>, together with six non-image classic AD datasets <ref type="bibr" target="#b41">[42]</ref>, all following the settings in <ref type="bibr" target="#b43">[44]</ref>. To validate our method on real-world AD scenarios, i.e., with higher resolution and with more complex anomalies, we additionally conduct experiments on two medical image datasets Camelyon16 <ref type="bibr" target="#b3">[4]</ref> and the NIH dataset <ref type="bibr" target="#b55">[56]</ref>. For all datasets, the training and testing partitions remain as default. More details are shown in the supplementary material. <ref type="table">Table 1</ref>: Results of anomaly detection on natural image datasets, where we increase the ratio of labeled anomalies ? l in the training set. We report the avg. AUC in % with st. dev. computed over 90 experiments at various ? l . Results of SSAD Hybrid, SS-DGM and Deep SAD are borrowed from <ref type="bibr" target="#b43">[44]</ref>. Results of TLSAD are borrowed from <ref type="bibr" target="#b17">[18]</ref>. Evaluation Protocol. We quantify the model performance using the area under the Receiver Operating Characteristic (ROC) curve metric (AUC). It is commonly adopted as performance measurement in anomaly detection (AD) tasks.</p><p>Model Configuration. For ESAD, the architecture of the autoencoder and the data preprocessing for the image dataset is aligned with <ref type="bibr" target="#b59">[60]</ref>. Different from Deep SAD, which uses different networks for each dataset, ESAD uses the same autoencoder network since it is robust enough. For non-image classic AD datasets, we use the autoencoder network aligned with <ref type="bibr" target="#b43">[44]</ref>. The hyperparameter ? 1 and ? 2 are set to 1 as default. We give the full details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Natural Images</head><p>Competing Methods. We consider several semi-supervised anomaly detection state-of-thearts, including SSAD <ref type="bibr" target="#b21">[22]</ref>, SS-DGM <ref type="bibr" target="#b23">[24]</ref>, Deep SAD <ref type="bibr" target="#b43">[44]</ref> and TLSAD <ref type="bibr" target="#b17">[18]</ref> as baselines.</p><p>Following <ref type="bibr" target="#b43">[44]</ref>, as <ref type="bibr" target="#b21">[22]</ref> is sensitive to hyperparameters, SSAD Hybrid here uses a subset (10%) of the test set for hyperparameter selection to establish a strong baseline. More details for these baseline methods are shown in the supplementary material. Experiment Settings. For a dataset with C classes, we conduct a batch of C experiments respectively with each of the C classes set as the normal class once. We then evaluate performance on an independent test set, which contains samples from all classes, including normal and anomalous data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Medical Images</head><p>Medical images, such as H&amp;E stained images, X-ray, etc., have extremely high resolution compared to natural images. In addition, the patient's lesions may only occupy a small part of the entire image, which brings great challenges to AD. To validate the AD performance of ESAD on real-world AD scenarios, we examined two challenging medical problems with different image characteristics and abnormality appearance, i.e., Camelyon16 <ref type="bibr" target="#b3">[4]</ref> and chest X-rays in NIH <ref type="bibr" target="#b55">[56]</ref>. We consider several state-of-the-arts, including DAOL <ref type="bibr" target="#b49">[50]</ref>, DGEO <ref type="bibr" target="#b18">[19]</ref>, PIAD <ref type="bibr" target="#b51">[52]</ref>, DIF <ref type="bibr" target="#b36">[37]</ref> Deep SAD <ref type="bibr" target="#b43">[44]</ref>, and DPA <ref type="bibr" target="#b52">[53]</ref>. Note that for <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52]</ref>, anomalous samples in the training set are used for the validation. We re-train Deep SAD <ref type="bibr" target="#b43">[44]</ref> with the same encoder and decoder network as ESAD to obtain a better baseline. Anomaly Detection on Chest X-Rays. NIH images without any disease marker were considered normal. Pulmonary and cardiac abnormalities in this dataset are all considered anomalous. Following <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b52">53]</ref>, we split the dataset into two sub-datasets having only posteroanterior (PA) or anteroposterior (AP) projections. The labeled anomalous samples contain only the most frequent disease ('Infiltration') out of fourteen possibilities and there are still thirteen unseen possibilities of anomalies in the test set. We also experiment on a subset containing clearer normal/anomalous cases <ref type="bibr" target="#b49">[50]</ref>. Default preprocessing of chest X-rays involved a 768 ? 768 central crop and resize to 64 ? 64. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the anomaly detection performance of ESAD outperforms all state-of-the-arts on the clearer subset <ref type="bibr" target="#b49">[50]</ref> and AP subset. DPA <ref type="bibr" target="#b52">[53]</ref>    <ref type="table">Table 5</ref>: Ablation study on shallow and deep networks, for both the encoder and the decoder. 'Shallow' is a LeNet-type network utilized in <ref type="bibr" target="#b43">[44]</ref>. 'Deep' is the network utilized in ESAD. We report the avg. AUC in % with st. dev. computed over 90 experiments at various ? l on F-MNIST. Results with * are lower than expected because of the model collapse problem for Deep SAD under the small labeled anomalies ratio. images for the test. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the anomaly detection performance of ESAD outperforms all state-of-the-art methods.</p><formula xml:id="formula_12">Network Method ? l = 0.0 ? l = 0.01 ? l = 0.05 ? l = 0.1 ? l = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on Classic Anomaly Detection Benchmark Datasets</head><p>We examine the performance of the various methods on well-established classic AD benchmark datasets <ref type="bibr" target="#b41">[42]</ref> with ? l = 0.01. Networks of both the encoder and the decoder are aligned with <ref type="bibr" target="#b43">[44]</ref>. The corresponding results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Comparing with other stateof-the-arts, ESAD shows the highest AUCs and stability. It shows that unlike other deep approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>, ESAD is not domain or data-type specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>The model architecture and different losses for ESAD are discussed in <ref type="table" target="#tab_5">Table 4</ref> through ablation studies. Experiments are conducted on two datasets, i.e., cardio and satellite. Firstly, for the model architecture, results show that without the encoder-decoder-encoder architecture, ESAD with vanilla autoencoder shows relatively low and unstable AUCs (the entropy loss is conducted on the first encoder in this case). Secondly, ablation studies on two proposed losses, i.e., L ass and L rec?semi , show impressive results. Comparing with vanilla reconstruction loss, L rec?semi utilizes two transformations for changing the supervisions of labeled anomalous data. Without these transformations, it degrades to the vanilla reconstruction loss where the original data are used as the reconstruction supervisions, leading to relatively lower AUCs. Note that the entropy loss should always be utilized in all experiments since it is highly relative to the anomaly score measurement, but its importance can be shown towards the following discussions for the hyperparameters. Then we analyze the influence of the network choices. For the natural image datasets, Deep SAD <ref type="bibr" target="#b43">[44]</ref> uses different LeNet-type networks for each dataset. ESAD does not fol- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>Normal Sample Labeled Anomalous Sample Anomalous Sample 1 presents the same pattern in all datasets, which means that when changing datasets, we may not need to spend too many resources on the adjustment of <ref type="bibr">1.</ref> We further explain that the major changes discussed in Section 3.5 are important. If we replace the Lnorm semi back to the SVDD loss, the architecture of ESAD is broken since it is hard to achieve end-to-end training with the SVDD loss. In this case, its performance collapses to Deep SAD directly. If we replace Lrec semi back to the original reconstruction loss in Deep SAD, the AUC on MNIST significantly reduces from 97.8% to 92.3% ( l = 0.1, p = 0.1), while the original AUC for Deep SAD is 91.2%. This shows the importance of the optimization for the term related to the mutual information using the labeled outlier samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a theoretic framework for semisupervised anomaly detection and a novel technique to achieve end-to-end training. We provide feasible optimization strategies for both normal and labeled outlier samples. We look forward to more optimization strategies under this framework, especially for the labeled outlier samples. In addition, the proposed theoretic framework can also be applied to more semi-supervised tasks, opening avenues for future research.  We further explain that the major changes discussed in Section 3.5 are important. If we replace the Lnorm semi back to the SVDD loss, the architecture of ESAD is broken since it is hard to achieve end-to-end training with the SVDD loss. In this case, its performance collapses to Deep SAD directly. If we replace Lrec semi back to the original reconstruction loss in Deep SAD, the AUC on MNIST significantly reduces from 97.8% to 92.3% ( l = 0.1, p = 0.1), while the original AUC for Deep SAD is 91.2%. This shows the importance of the optimization for the term related to the mutual information using the labeled outlier samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a theoretic framework for semisupervised anomaly detection and a novel technique to achieve end-to-end training. We provide feasible optimization strategies for both normal and labeled outlier samples. We look forward to more optimization strategies under this framework, especially for the labeled outlier samples. In addition, the proposed theoretic framework can also be applied to more semi-supervised tasks, opening avenues for future research.  Section 3.5 are important. If we replace the back to the SVDD loss, the architecture of ES since it is hard to achieve end-to-end training w loss. In this case, its performance collapses to D rectly. If we replace Lrec semi back to the o struction loss in Deep SAD, the AUC on M cantly reduces from 97.8% to 92.3% ( l = 0. while the original AUC for Deep SAD is 91.2% the importance of the optimization for the term mutual information using the labeled outlier sa</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a theoretic framew supervised anomaly detection and a novel achieve end-to-end training. We provide feasi tion strategies for both normal and labeled ou We look forward to more optimization strateg framework, especially for the labeled outlier sa dition, the proposed theoretic framework can a to more semi-supervised tasks, opening avenu research. We further explain that the major changes Section 3.5 are important. If we replace the back to the SVDD loss, the architecture of ES since it is hard to achieve end-to-end training w loss. In this case, its performance collapses to D rectly. If we replace Lrec semi back to the or struction loss in Deep SAD, the AUC on MN cantly reduces from 97.8% to 92.3% ( l = 0. while the original AUC for Deep SAD is 91.2% the importance of the optimization for the term mutual information using the labeled outlier sa</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a theoretic framewo supervised anomaly detection and a novel achieve end-to-end training. We provide feasi tion strategies for both normal and labeled ou We look forward to more optimization strateg framework, especially for the labeled outlier sa dition, the proposed theoretic framework can al to more semi-supervised tasks, opening avenu research.</p><p>( dinov, R. R. 2017. Good semi-supervised learning that requires a bad gan. In NeurIPS. class Novelty Detection Using GANs with Constrained Latent Representations. In CVPR. quires a bad gan. In NeurIPS.</p><p>tent Representations. In CVPR. quires a bad gan. In NeurIPS.</p><p>tent Representations. In CVPR.</p><p>We further show that the latent representations extracted by ESAD can better be used to distinguish samples of different categories through T-SNE (Maaten and Hinton 2008) analysis. We conduct experiments on MNIST and CIFAR-10. <ref type="figure" target="#fig_6">Figure 3</ref> (a) and (b) are two baselines, which show the results using latent representations extracted by Deep SAD and ESAD (without optimizing the labeled outlier samples with opposite directions), respectively. <ref type="figure" target="#fig_6">Figure 3</ref> (c) and (d) visualize different latent representations, i.e., z and?, extracted by ESAD, which are more discriminative than the baselines. In <ref type="figure" target="#fig_6">Figure 3 (d)</ref>,? shows a more specific structure. The labeled outlier samples are better and evenly distributed in the anomalous samples. It shows that ESAD can make good use of the labeled outlier samples to well represent the anomalous data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a theoretic framework for semisupervised anomaly detection and a novel technique to achieve end-to-end training. We provide feasible optimization strategies for both normal and labeled outlier samples.</p><p>We look forward to more optimization strategies under this framework, especially for the labeled outlier samples. In addition, the proposed theoretic framework can also be applied to more semi-supervised tasks, opening avenues for future research.   low <ref type="bibr" target="#b43">[44]</ref> and uses the same network for all datasets. To show the influence of network choices, as shown in <ref type="table">Table 5</ref>, we experiment with different networks and show that: i) ESAD is robust to different networks, with a performance gap of 0.3% -0.4% between using the shallow or deep network on F-MNIST, while Deep SAD encounters model collapse with certain networks. ii) ESAD outperforms Deep SAD for both shallow and deep networks. Results on more datasets are shown in the supplemental material. We further analyze the sensitivity of the hyperparameters of ESAD. According to Eq. (8), ? 1 has a certain impact on the performance of semi-supervised AD. The larger ? 1 means more attention is paid to the entropy, while the smaller ? 1 pays more attention to the mutual information. <ref type="figure">Figure 2</ref> shows the ESAD performance with different ? 1 . The results show that the best AUC can be obtained when ? 1 is set as 1 in all datasets. When ? 1 is relatively too small or too large, relatively poor AD performance will be achieved. Fortunately, the relationship between AUCs and the ? 1 presents the same pattern in all datasets, which means that when changing datasets, we may not need to spend too many resources on the adjustment of ? 1 . For ? 2 , we found through experiments that modifying ? 2 has a relatively small impact. We thus always set ? 2 as 1. More details are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization Analysis</head><p>We show that the latent representations extracted by ESAD can better be used to distinguish samples of different categories through T-SNE <ref type="bibr" target="#b29">[30]</ref> analysis. We conduct experiments on MNIST, CIFAR-10 and the medical image dataset NIH. <ref type="figure" target="#fig_6">Figure 3 (a)</ref> shows the results using latent representations extracted by Deep SAD. <ref type="figure" target="#fig_6">Figure 3</ref> (b) and (c) visualize different latent representations, i.e., z and?, extracted by ESAD, which are more discriminative than the baseline. In <ref type="figure" target="#fig_6">Figure 3</ref> (c),? shows a more specific structure. It shows that the two latent representations have learned different information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we show that factors of mutual information and entropy constitute an integral objective function for anomaly detection. We achieve end-to-end training by proposing a novel model architecture. The proposed information theoretic framework can also be applied to more semi-supervised tasks, opening avenues for future research.</p><p>Proof 1 The KL divergence for the joint distributions can be decomposed with the chain rule <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_13">KL [p N (X, Z)||p A (X, Z)] =E p N (X,Z) log p N (X, Z) p A (X, Z) =E p N (X,Z) log p N (X) p A (X) + log p N (Z|X) p A (Z|X) = KL [p N (X)||p A (X)] + E p N (X) [KL [p N (Z|X)||p A (Z|X)]] .</formula><p>To maximize the KL divergence for the joint distributions, it is equivalent that we maximize the KL divergence for both marginal and conditional distributions <ref type="bibr" target="#b13">[14]</ref>.</p><p>Proposition 2 Let I(X N , Z N ) denotes the mutual information between X N and Z N ; H(Z N ) denotes the entropy of Z N ; H(p N (Z|X), p A (Z|X)) denotes the cross-entropy between p N (Z|X) and p A (Z|X); KL [p N (X) || p A (X)] denotes the KL divergence between p N (X) and p A (X). Then:</p><formula xml:id="formula_14">KL [p N (X, Z)||p A (X, Z)] = I (X N , Z N ) ? H(Z N ) + E p N (X) [H(p N (Z|X), p A (Z|X))] + KL [p N (X)||p A (X)] .<label>(10)</label></formula><p>Proof 2 The KL divergence can be reformulated as:</p><formula xml:id="formula_15">KL [p N (X, Z)||p A (X, Z)] = E p N (X,Z) log p N (X, Z) p A (X, Z) = E p N (X,Z) log p N (Z|X) ? p N (X) p A (Z|X) ? p A (X) = E p N (X,Z) log p N (Z|X) ? p N (X) ? p N (Z) p A (Z|X) ? p A (X) ? p N (Z) = E p N (X,Z) log p N (Z|X) p N (Z) ? p N (Z) ? 1 p A (Z|X) ? p N (X) p A (X) .</formula><p>The above formula is decomposed into four components. The first term refers to the mutual information between the original data X N and its latent representation Z N :</p><formula xml:id="formula_16">E p N (X,Z) log p N (Z|X) p N (Z) =E p N (X,Z) log p N (Z|X) ? p N (X) p N (X) ? p N (Z) =E p N (X,Z) log p N (X, Z) p N (X) ? p N (Z) =I (X N , Z N ) .</formula><p>The second term refers to the negative entropy of Z N :</p><formula xml:id="formula_17">E p N (X,Z) [log p N (Z)] = ?E p N (Z) log 1 p N (Z) = ?H(Z N ).</formula><p>The third term refers to the expected value of the cross entropy between the conditional distributions p A (Z|X) and p N (Z|X):</p><formula xml:id="formula_18">E p N (X,Z) log 1 p A (Z|X) =E p N (X) E p N (Z|X) [? log p A (Z|X)] =E p N (X) [H(p N (Z|X), p A (Z|X))] .</formula><p>The fourth term is a constant, since p N (X) and p A (X) are fixed when the dataset is given:</p><formula xml:id="formula_19">E p N (X,Z) log p N (X) p A (X) = KL [p N (X)||p A (X)] = C.</formula><p>Thus the KL divergence can be reformulated as:</p><formula xml:id="formula_20">KL [p N (X, Z)||p A (X, Z)] = I (X N , Z N ) ? H(Z N ) + E p N (X) [H(p N (Z|X), p A (Z|X))] + KL [p N (X)||p A (X)] .</formula><p>Proposition 3 The third term in the objective Eq. (10), i.e., E p N (X) [H(p N (Z|X), p A (Z|X)], is nonnegative.</p><p>Proof 3 We assume that p N (Z|X) and p A (Z|X) are separable at the latent space, i.e., for X, Z ? p N (X, Z), the evaluated density log p A (Z|X) 0. This assumption is indeed consist with the fundamental assumption in <ref type="bibr" target="#b6">[7]</ref>: data can be embedded into a certain representation space where normal instances and anomalies appear significantly different. With the above assumption,</p><formula xml:id="formula_21">E p N (X) [H(p N (Z|X), p A (Z|X)</formula><p>] is shown to be non-negative:</p><formula xml:id="formula_22">inf E p N (X) [H(p N (Z|X), p A (Z|X)] = inf E p N (X,Z) [? log p A (Z|X)] E p N (X,Z) [inf (? log p A (Z|X))] 0.</formula><p>Proposition 4 Assuming Z follows an isotropic Gaussian, with mean ?, covariance ? and Z ? R d , the entropy of Z, i.e., H(Z), is proportional to its log-variance for a fixed dimensionality d, without dependence on its mean ?.</p><p>Proof 4 For Z with covariance ? and Z ? R d ,</p><formula xml:id="formula_23">H(Z) = E[? log p(Z)] = ? Z p(Z) log p(Z)dZ ? 1 2 log((2?e) d det ?),</formula><p>which holds with equality iff Z is jointly Gaussian <ref type="bibr" target="#b9">[10]</ref>. Assuming Z follows an isotropic Gaussian, Z ? N(?, ? 2 I) with ? &gt; 0, we get,</p><formula xml:id="formula_24">H(Z) = 1 2 log((2?e) d det ? 2 I) = d 2 (1 + log(2?? 2 )) ? log ? 2 ,</formula><p>which shows that the entropy of Z is proportional to its log-variance for a fixed dimensionality d. The above proof has no dependence on the mean ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Analysis of Deep SAD</head><p>Deep SAD builds upon Infomax principle, which maximizes the mutual information I(X, Z) between data and latent representations with regularization on the representations. The objective function for Deep SAD is formulated as: max</p><formula xml:id="formula_25">? I(X, Z) + ? (H(Z A ) ? H(Z N )),<label>(11)</label></formula><p>where regularization is enforced through entropy. For ?x ? X , Deep SAD adopts an autoencoder consisting of an encoder Enc(?) and a decoder Dec(?): z = Enc(x),x = Dec(z), wherex is the reconstructed sample and z is the corresponding latent representation, and takes the following two-step process to implement the above objective function.</p><p>(i) Autoencoder Pre-training: To maximize the mutual information between the data and the latent representations, a reconstruction loss is adopted to pre-train the autoencoder:</p><formula xml:id="formula_26">L rec = 1 n + m n+m ? i=1 x i ? x i 2 ,<label>(12)</label></formula><p>where x 1 , ? ? ? , x n+m ? X .</p><p>(ii) Encoder Fine-turning: To further regularize the entropy of the latent representations, the encoder is fine-turned with an SVDD loss,</p><formula xml:id="formula_27">L SV DD = 1 n n ? i=1 z u i ? c 2 + ? m m ? j=1 z l j ? c y j 2 ,<label>(13)</label></formula><p>where z u 1 , ? ? ? , z u n ? Z are the corresponding latent representations of unlabeled samples x u 1 , ? ? ? , x u n , z l 1 , ? ? ? , z l m ? Z are the corresponding latent representations of labeled samples x l 1 , ? ? ? , x l m , and ? is set as 1. The hypersphere center c is set as the mean of the outputs obtained from a forward pass of the encoder for all the data. In fact, Deep SAD does not use the coefficient ? in Eq. (11), because the two terms are separately optimized in two stages.</p><p>We now argue that the reason for the two-stage implementation of Deep SAD is the contradiction between the optimization of mutual information and entropy. For example, when the latent representations have extremely low entropy, especially zero in the extreme case, the model can be considered as mapping all data into a constant in which the mutual information is restricted to zero, which contradicts with the mutual information maximization. The two-stage implementation for Deep SAD avoids directly facing the above contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Architecture and Training Details</head><p>The model architecture for ESAD is shown in <ref type="table">Table 6</ref>. For the training, we use stochastic gradient descent (SGD) <ref type="bibr" target="#b5">[6]</ref> optimizer with default hyperparameters in Pytorch. ESAD is trained using a batch size of 32 for 200 epochs with NVIDIA GTX 2080Ti. The learning rate is initially set as 0.1, and is divided by 2 every 50 epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Datasets</head><p>Natural Image Datasets. MNIST <ref type="bibr" target="#b26">[27]</ref>, a dataset consists of 70,000 28 ? 28 handwritten grayscale digit images; Fashion-MNIST <ref type="bibr" target="#b57">[58]</ref>, a relatively new dataset comprising 28 ? 28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category; CIFAR-10 <ref type="bibr" target="#b25">[26]</ref>, a dataset consists of 60,000 32 ? 32 RGB images of 10 classes, with 6,000 images for per class. Medical Image Datasets. Following <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b52">53]</ref>, we examine the detection of metastases in H&amp;E stained images of lymph nodes in Camelyon16 <ref type="bibr" target="#b3">[4]</ref> and the recognition of fourteen diseases on the chest X-rays in the NIH dataset <ref type="bibr" target="#b55">[56]</ref>.</p><p>For the NIH dataset, images without any disease marker were considered normal. Pulmonary and cardiac abnormalities in this dataset include atelectasis, effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation, edema, emphysema, fibrosis, pleural thickening, hernia and cardiomegaly, which are all considered anomalous. Following <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b52">53]</ref>, we split the dataset into two sub-datasets having only posteroanterior (PA) or anteroposterior (AP) projections. Note that in the training set, the ratios of labeled anomalous samples are 3.9% for AP and 3.3% for PA. We also experiment on a subset containing clearer normal/anomalous cases <ref type="bibr" target="#b49">[50]</ref>. This subset consists of 5110 normal and 857 anomalous images for training, and 677 normal and 677 anomalous images for testing.</p><p>For the Camelyon16 dataset, we sample the Vahadane-normalized <ref type="bibr" target="#b53">[54]</ref> 64 ? 64 tiles from the fully normal slides with magnification of 10?, and treat these as normal. Tiles with metastases are treated as anomalous. It contains 7612 normal and 200 anomalous training images, and 4000 (normal) + 817 (anomalous) images for the test. Classic anomaly detection benchmark datasets. We use six non-image classic anomaly detection benchmark datasets <ref type="bibr" target="#b41">[42]</ref>. Following <ref type="bibr" target="#b43">[44]</ref>, for the evaluation, we consider random train-to-test set splits   <ref type="bibr" target="#b37">[38]</ref> 97.5 -65.6 GANomaly * <ref type="bibr" target="#b1">[2]</ref> 92.8 80.9 69.5 P-KDGAN ? <ref type="bibr" target="#b61">[62]</ref> 97.8 -73.8 DGEO ? <ref type="bibr" target="#b18">[19]</ref> 98.0 93.5 86.0 ESAD (unsupervised) 98.5 ? 1.3 94.0 ? 4.5 78.8 ? 6.5 ESAD 99.6 ? 0.3 95.9 ? 4.0 88.5 ? 6.9</p><p>loss that also serves as the anomaly score.</p><p>(5) Deep SVDD <ref type="bibr" target="#b42">[43]</ref>: Both variants, Soft-Boundary Deep SVDD and One-Class Deep SVDD are considered as unsupervised baselines and always report the better performance as the unsupervised result. For Soft-Boundary Deep SVDD, The radius R on every mini-batch is optimally solved. For Deep SVDD, all the bias terms from a network are removed to prevent a hypersphere collapse as recommended by the authors in the original work <ref type="bibr" target="#b42">[43]</ref>. (6) SS-DGM <ref type="bibr" target="#b23">[24]</ref>: We consider both the M2 and M1 + M2 model and always report the better performing result. Other settings are following the original work <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr" target="#b6">(7)</ref> Deep SAD <ref type="bibr" target="#b43">[44]</ref>: The results are borrow from <ref type="bibr" target="#b43">[44]</ref>. We set ? = 10 ?6 and equally weight the unlabeled and labeled examples by setting ? = 1 if not reported otherwise.</p><p>To establish hybrid methods, we apply the OC-SVM, IF, and SSAD to the resulting bottleneck representations given by the respective converged autoencoders. To complete the full learning spectrum, we also include a fully supervised deep classifier trained on the binary cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Supplementary Experimental Results</head><p>Besides the experiments in the main paper, we examine three scenarios <ref type="bibr" target="#b43">[44]</ref> in which we vary the following three experimental parameters: (i) ? l , the ratio of labeled samples in the training data; (ii) ? p , the ratio of pollution, i.e., unknown anomalies, in the unlabeled training data, and (iii) the number of anomaly classes k l included in the labeled training data.</p><p>Besides the baselines considering in the main paper, we further consider several shallow unsupervised methods and deep unsupervised anomaly detection competitors as baselines. For the shallow unsupervised methods, OC-SVM <ref type="bibr" target="#b48">[49]</ref> and Isolation Forest <ref type="bibr" target="#b28">[29]</ref> are considered. For the deep unsu- <ref type="table">Table 9</ref>: Complete results of experimental scenario (ii), where we pollute the unlabeled part of the training set with (unknown) anomalies. We report the avg. AUC in % with st. dev. computed over 90 experiments at various ratios ? p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>?p OC-SVM Hybrid <ref type="bibr" target="#b48">[49]</ref> IF Hybrid <ref type="bibr" target="#b28">[29]</ref> CAE <ref type="bibr" target="#b30">[31]</ref> Deep SVDD <ref type="bibr" target="#b42">[43]</ref> SSAD Hybrid <ref type="bibr" target="#b21">[22]</ref> SS-DGM <ref type="bibr" target="#b23">[24]</ref> Deep SAD <ref type="bibr" target="#b43">[44]</ref> TLSAD <ref type="bibr">[</ref>   <ref type="bibr" target="#b3">[4]</ref> (top) and chest X-rays of NIH dataset <ref type="bibr" target="#b55">[56]</ref> (bottom). We show the predicted anomaly score by the proposed method. The higher the score, the more likely to be an anomaly. Best viewed in color. Experimental Scenario (iii). For the experimental scenario (iii), we increase the number of anomaly classes k l included in the labeled part of the training set to increase the diversity of labeled anomalous data. As shown in <ref type="table">Table 10</ref>, ESAD shows better performance in this scenario. For example, the AUC of ESAD on CIFAR-10 increases from 81.9% to 86.7% (? l = 0.05, ? p = 0.1) when we change k l from 1 to 5. Examples Visualization. We illustrate the predictions of our model in <ref type="figure" target="#fig_7">Figure 4</ref>. Samples are randomly chosen from H&amp;E-stained lymph node of Camelyon16 challenge <ref type="bibr" target="#b3">[4]</ref> (top) and chest X-rays of NIH dataset <ref type="bibr" target="#b55">[56]</ref> (bottom). These samples and their corresponding scores show that the higher the score, the more likely to be an anomaly. Sensitivity Analysis on ? 2 . We analyze the sensitivity of ESAD over the hyperparameters ? 2 . <ref type="figure" target="#fig_8">Figure 5</ref> shows the performance with different ? 2 using ESAD on MNIST. We set ? l = 0.05, ? p = 0.1, k l = 1 in this experiment. Results show that without the assistant loss, i.e., ? 2 = 0, ESAD shows relatively low and unstable AUCs. ESAD shows best performance when ? 2 = 1. When ? 2 is too large, ESAD also shows unstable performance. This is because both two encoders will converge into the same constant function if the impact of the assistant loss is much greater than the other two mutual information and entropy based loss functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 : 1 .</head><label>21</label><figDesc>ESAD sensitivity analysis w.r.t. ? We report avg. AUC in % with st. dev. over 90 experiments. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) z in Deep SAD (b) z in ESAD without opposite optimization (c) z in ESAD (d)? in ESAD References Akcay, S.; Atapour-Abarghouei, A.; and Breckon, T. P. 2018. GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training. In ACCV. Ak?ay, S.; Atapour-Abarghouei, A.; and Breckon, T. P. 2019. Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection. IJCNN . An, J.; and Cho, S. 2015. Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) z in Deep SAD (b) z in ESAD without opposite optimization (c) z in ESAD (d)? in ESAD References Akcay, S.; Atapour-Abarghouei, A.; and Breckon, T. P. 2018. GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training. In ACCV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) z in Deep SAD (b) z in ESAD without opposite optimization (c) z in ESAD (d)? in ESAD References Akcay, S.; Atapour-Abarghouei, A.; and B 2018. GANomaly: Semi-Supervised Anomaly Adversarial Training. In ACCV. Ak?ay, S.; Atapour-Abarghouei, A.; and B 2019. Skip-GANomaly: Skip Connected and Trained Encoder-Decoder Anomaly Detection An, J.; and Cho, S. 2015. Variational autoe anomaly detection using reconstruction probab Lecture on IE . Chandola, V.; Banerjee, A.; and Kumar, V. 20 detection: A survey. ACM computing surveys ( Dai, Z.; Yang, Z.; Yang, F.; Cohen, W. W.; a dinov, R. R. 2017. Good semi-supervised lea quires a bad gan. In NeurIPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a) z in Deep SAD (b) z in ESAD without opposite optimization (c) z in ESAD (d)? in ESAD References Akcay, S.; Atapour-Abarghouei, A.; and Br 2018. GANomaly: Semi-Supervised Anomaly Adversarial Training. In ACCV. Ak?ay, S.; Atapour-Abarghouei, A.; and Br 2019. Skip-GANomaly: Skip Connected and A Trained Encoder-Decoder Anomaly Detection. An, J.; and Cho, S. 2015. Variational autoe anomaly detection using reconstruction probab Lecture on IE . Chandola, V.; Banerjee, A.; and Kumar, V. 200 detection: A survey. ACM computing surveys ( Dai, Z.; Yang, Z.; Yang, F.; Cohen, W. W.; a dinov, R. R. 2017. Good semi-supervised lea quires a bad gan. In NeurIPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(b) z in ESAD-w References Akcay, S.; Atapour-Abarghouei, A.; and Breckon, T. P. 2018. GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training. In ACCV. An, J.; and Cho, S. 2015. Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE . Chandola, V.; Banerjee, A.; and Kumar, V. 2009. Anomaly detection: A survey. ACM computing surveys (CSUR) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>T-SNE visualization of latent representations on MNIST (top), CIFAR-10 (middle) and NIH (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Examples of normal (left) and anomalous (right) samples of H&amp;E-stained lymph node of Camelyon16 challenge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>ESAD sensitivity analysis w.r.t. ? 1 on MNIST. We report avg. AUC with st. dev. over 90 experiments for various values of hyperparameter ? 2 . Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparison with State-of-the-art Methods. The effectiveness of adding labeled anomalies during training is investigated. By adding labeled anomalous samples x 1 , . . . , x m to the training set, we increase the ratio of labeled training data ?</figDesc><table /><note>l = m/(n + m). The number of anomaly classes included in the labeled training data is set as 1, i.e., there are still eight unseen classes at testing time. We iterate this training set generation process and report the average results over the ten kinds of normal classes ? nine labeled anomalous classes, i.e., over 90 experiments per labeled ratio ? l . The corresponding results are shown in Table 1. On all involved datasets, results present that the average AUCs of ESAD outperform all other methods, including TLSAD which utilizes a large-scale additional dataset (ImageNet [45]) as the reference data for the model training. Results when ? l &gt; 0 are much better than the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of anomaly detection methods on medical image datasets. We report the avg. AUC in % with st. dev. computed over 3 runs.</figDesc><table><row><cell>Method</cell><cell>Cam.16</cell><cell>NIH (a sub.)</cell><cell>NIH (PA)</cell><cell>NIH (AP)</cell></row><row><cell>DAOL [50] DGEO [19] PIAD [52] DIF [37] Deep SAD [44] DPA [53] ESAD (ours)</cell><cell>-45.9 ? 2.1 89.5 ? 0.6 90.6 ? 0.3 92.1 ? 0.4 93.4 ? 0.3 96.8 ? 0.4</cell><cell>80.5 ? 2.1 85.3 ? 1.0 87.3 ? 0.9 85.3 ? 0.4 90.9 ? 0.2 92.6 ? 0.2 94.6 ? 0.4</cell><cell>-63.6 ? 0.6 68.7 ? 0.5 47.2 ? 0.4 51.9 ? 0.8 70.8 ? 0.1 68.9 ? 0.2</cell><cell>-54.4 ? 0.6 58.6 ? 0.3 56.1 ? 0.2 59.8 ? 0.1 58.5 ? 0.0 60.1 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on classic anomaly detection benchmark datasets with a ratio of labeled anomalies of ? l = 0.01. We report the avg. AUC in % with st. dev. computed over 10 seeds.</figDesc><table><row><cell>Data</cell><cell>Deep SVDD [43]</cell><cell>SSAD Hybrid [22]</cell><cell>SS-DGM [24]</cell><cell>Deep SAD [44]</cell><cell>ESAD (ours)</cell></row><row><cell>arrhythmia cardio satellite satimage-2 shuttle thyroid</cell><cell>74.6 ? 9.0 84.8 ? 3.6 79.8 ? 4.1 98.3 ? 1.4 86.3 ? 7.5 72.0 ? 9.7</cell><cell>78.3 ? 5.1 86.3 ? 5.8 86.9 ? 2.8 96.8 ? 2.1 97.7 ? 1.0 95.3 ? 3.1</cell><cell>50.3 ? 9.8 66.2 ? 14.3 57.4 ? 6.4 99.2 ? 0.6 97.9 ? 0.3 72.7 ? 12.0</cell><cell>75.9 ? 8.7 95.0 ? 1.6 91.5 ? 1.1 99.9 ? 0.1 98.4 ? 0.9 98.6 ? 0.9</cell><cell>85.2 ? 2.9 98.8 ? 0.5 92.5 ? 0.7 99.9 ? 0.1 99.1 ? 1.1 99.6 ? 0.2</cell></row></table><note>results when ? l = 0, showing the effectiveness of the semi-supervised training scheme.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on different designs of architecture and loss functions for ESAD on two different datasets. We report the avg. AUC in % with st. dev. computed over 10 seeds.</figDesc><table><row><cell>Architecture</cell><cell></cell><cell>Loss Functions</cell><cell cols="2">Dataset</cell></row><row><cell>Encoder-decoder-encoder</cell><cell>L ass</cell><cell>L rec?semi Gaussian Permutation</cell><cell>satellite</cell><cell>cardio</cell></row><row><cell></cell><cell></cell><cell></cell><cell>87.9 ? 1.7 90.0 ? 1.2 90.4 ? 1.1 92.0 ? 1.1 92.5 ? 1.0 92.5 ? 0.7</cell><cell>96.5 ? 1.1 97.2 ? 0.9 97.9 ? 1.0 98.2 ? 0.6 98.6 ? 0.6 98.8 ? 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Classic anomaly detection benchmarks<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Numbers</cell><cell>Dimensions</cell><cell>#outliers (%)</cell></row><row><cell>arrhythmia cardio satellite satimage-2 shuttle thyroid</cell><cell>452 1,831 6,435 5,803 49,097 3,772</cell><cell>274 21 36 36 9 6</cell><cell>66 (14.6%) 176 (9.6%) 2,036 (31.6%) 71 (1.2%) 3,511 (7.2%) 93 (2.5%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Average area under the ROC curve (AUC) in % on natural image datasets, comparing with unsupervised anomaly detection methods. " ?" denotes the highest test AUC among multiple running for the strong baselines. " * " denotes the highest test AUC among all training epochs for the stronger baselines. We report the results of unsupervised ESAD, where we ignore the labeled data in the training set. Emphasizing that ESAD focuses on the semi-supervised setting but not the unsupervised setting.</figDesc><table><row><cell>Method</cell><cell>MNIST</cell><cell>F-MNIST</cell><cell>CIFAR-10</cell></row><row><cell>CAE [31] IF Hybrid [29] Deep SVDD [43] AnoGAN  ? [48] ALOCC  *  [46] ADGAN  *  [12] OC-SVM Hybrid [49] OCGAN  ?</cell><cell>92.9 ? 5.7 90.5 ? 5.3 92.8 ? 4.9 93.7 93.3 94.7 96.3 ? 2.5</cell><cell>90.2 ? 5.8 82.5 ? 8.1 89.2 ? 6.2 --88.4 91.2 ? 4.7</cell><cell>56.2 ? 13.2 59.9 ? 6.7 60.9 ? 9.4 61.2 62.2 62.4 63.8 ? 9.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2012.04905v3 [cs.LG] 21 Oct 2021</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of 60:40 while maintaining the original proportion of anomalies in each set. The supplementary details of the classic anomaly detection benchmarks <ref type="bibr" target="#b41">[42]</ref> are shown in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Competing Methods</head><p>We consider several shallow unsupervised methods, deep unsupervised anomaly detection competitors and semi-supervised anomaly detection approaches as baselines. Complete details are shown as follows:</p><p>(1) OC-SVM/SVDD <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51]</ref> (2) Isolation Forest <ref type="bibr" target="#b28">[29]</ref>: The number of trees is set as t = 100 and the sub-sampling size is set as ? = 256 as recommended in the original work.  pervised anomaly detection competitors, we consider CAE <ref type="bibr" target="#b30">[31]</ref>, Deep SVDD <ref type="bibr" target="#b42">[43]</ref> AnoGAN <ref type="bibr" target="#b47">[48]</ref>, ALOCC <ref type="bibr" target="#b45">[46]</ref>, ADGAN <ref type="bibr" target="#b11">[12]</ref>, OCGAN <ref type="bibr" target="#b37">[38]</ref>, GANomaly <ref type="bibr" target="#b1">[2]</ref>, P-KDGAN <ref type="bibr" target="#b61">[62]</ref> and DGEO <ref type="bibr" target="#b18">[19]</ref>. OC-SVM here have unfair advantages by selecting their hyperparameters to maximize AUC on a subset (10%) of the test set to establish strong baselines. Experimental Scenario (i). For the experimental scenario (i), where the effectiveness of adding labeled anomalies during training is investigated, i.e., increasing ? l , has been shown in the main paper. In this part, we further report the results comparing with several unsupervised methods under the unsupervised setting in <ref type="table">Table 8</ref>. We emphasize that our ESAD is not designed for the unsupervised setting.</p><p>In these experiments, the semi-supervised terms are not working and make ESAD incomplete, since it remains only the unsupervised terms. Thus, these results are somewhat unfair for ESAD. Note that this paper still focus on the semi-supervised setting but not the unsupervised setting. Experimental Scenario (ii). For the experimental scenario (ii), where the robustness is investigated in this scenario through adding polluted data. With an increasing pollution ratio ? p , we pollute the unlabeled training set with anomalies drawn from all nine anomaly classes. We fix ? l = 0.05 in this scenario. We report the average results over 90 experiments per pollution ratio ? p . The corresponding results are shown in <ref type="table">Table 9</ref>. Results show that ESAD is least affected by the pollution data and show the best robustness in all the polluted levels.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitko</forename><surname>Babak Ehteshami Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Johannes</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litjens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<title level="m">series in Telecommunications and Signal Processing</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Elements of information theory</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Good semisupervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04758</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised anomaly detection with lstm neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Ergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suleyman</forename><surname>Kozat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection over noisy data using learned probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning discriminative features for semisupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishun</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Brefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><forename type="middle">Mathew</forename><surname>B Ravi Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjith</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parakkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Isolation forest</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked convolutional autoencoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Su-ids: A semisupervised and unsupervised framework for network intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erxue</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cloud Computing and Security</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semisupervised one-class support vector machines for classification of remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>M?noz-Mar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>G?mez-Chova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Camp-Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hybrid autoencoder and density estimation model for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Loi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Problem Solving from Nature</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Data-Efficient Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards practical unsupervised anomaly detection on retinal images. In Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Ouardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balagopal</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manon</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Garcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houssam</forename><surname>Zenati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive survey of data miningbased fraud detection research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifton</forename><surname>Phua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Gayler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computation Technology and Automation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coherence pursuit: Fast, simple, and robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">K</forename><surname>Atia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Odds library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shebuti</forename><surname>Rayana</surname></persName>
		</author>
		<ptr target="http://odds.cs.stonybrook.edu" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takehisa</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mlsda Workshop on Machine Learning for Sensory Data Analysis</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep adversarial one-class learning for normal and abnormal chest radiograph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You-Bao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2019: Computer-Aided Diagnosis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Support vector data description. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Perceptual image anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Tuluptceva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Fedulova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Anomaly detection with deep perceptual autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Tuluptceva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Fedulova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13265</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Structurepreserving color normalization and sparse stain separation for histological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingying</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Baust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Steiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Melissa</forename><surname>Schlitter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust pca via outlier pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attribute restoration framework for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Viral pneumonia screening on chest x-ray images using confidence-aware anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongji</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua Shen Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">P-kdgan: Progressive knowledge distillation with gans for one-class novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Encoding structure-texture relation with p-net for anomaly detection in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lumezanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>If</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>p N (X, Z)||p A (X, Z)] is maximized, then it is equivalent that KL [p N (X)||p A (X)] and KL [p N (Z|X)||p A (Z|X</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
