<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REGTR: End-to-end Point Cloud Correspondences with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Jian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yew</forename><surname>Gim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">REGTR: End-to-end Point Cloud Correspondences with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent success in incorporating learning into point cloud registration, many works focus on learning feature descriptors and continue to rely on nearest-neighbor feature matching and outlier filtering through RANSAC to obtain the final set of correspondences for pose estimation. In this work, we conjecture that attention mechanisms can replace the role of explicit feature matching and RANSAC, and thus propose an end-to-end framework to directly predict the final set of correspondences. We use a network architecture consisting primarily of transformer layers containing self and cross attentions, and train it to predict the probability each point lies in the overlapping region and its corresponding position in the other point cloud. The required rigid transformation can then be estimated directly from the predicted correspondences without further post-processing. Despite its simplicity, our approach achieves state-of-the-art performance on 3DMatch and ModelNet benchmarks. Our source code can be found at https://github.com/yewzijian/RegTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross attention FFN Output Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Decoder</head><p>Source Target " ! "</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Rigid point cloud registration refers to the problem of finding the optimal rotation and translation parameters that align two point clouds. A common solution to point cloud registration follows the following pipeline: 1) detect salient keypoints, 2) compute feature descriptors for these keypoints, 3) obtain putative correspondences via nearest neighbor matching, and 4) estimate the rigid transformation, typically in a robust fashion using RANSAC. In recent years, researchers have applied learning to point cloud registration. Many of these works focus on learning the feature descriptors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref> and sometimes also the keypoint detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b55">56]</ref>. The final two steps generally remain unchanged and these approaches still require nearest neighbor matching and RANSAC to obtain the final transformation. These algorithms do not take the post-processing into account during training, and their performance can be sensitive to the post-processing choices to pick out the correct correspondences, e.g. number of sampled interest points or distance threshold in RANSAC.</p><p>Several works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57]</ref> avoid the non-differentiable nearest neighbor matching and RANSAC steps by estimating the alignment using soft correspondences computed from the local feature similarity scores. In this work, we take a slightly different approach. We observe that the learned local features in these works are mainly used to establish correspondences. Thus, we focus on having the network directly predict a set of clean correspondences instead of learning good features. We are motivated by the recent line of works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref> which make use of transformer attention <ref type="bibr" target="#b48">[49]</ref> layers to predict the final outputs for various tasks with minimal post-processing. Although attention mechanisms have previously been used in registration of both point clouds <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50]</ref> and images <ref type="bibr" target="#b38">[39]</ref>, these works utilize attention layers mainly to aggregate contextual information to learn more discriminative feature descriptors. A subsequent RANSAC or optimal transport step is still often used to obtain the final correspondences. In contrast, our Registration Transformer (REGTR) utilizes attention layers to directly output a consistent set of final point correspondences, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Since our network outputs clean correspondences, the required rigid transformation can be estimated directly without additional nearest neighbor matching and RANSAC steps.</p><p>Our REGTR first uses a point convolutional backbone <ref type="bibr" target="#b44">[45]</ref> to extract a set of features while downsampling the input pair of point clouds. The features of both point clouds are passed into several transformer <ref type="bibr" target="#b48">[49]</ref> layers consisting of multi-head self and cross attentions to allow for global information aggregation, while taking into account the point positions through positional encodings to allow the network to utilize rigidity constraints to correct bad correspondences. The resulting features are then used to predict the corresponding transformed locations of the downsampled points. We additionally predict overlap probability scores to weigh the predicted correspondences when computing the rigid transformation. Unlike the more common approach of computing correspondences via nearest neighbor feature matching, which requires interest points to be present at the same locations in both point clouds, our network is trained to directly predict corresponding point locations. As a result, we do not require sampling large number of interest points (e.g. in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b60">61]</ref>) or a keypoint detector (e.g. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62]</ref>) that produces repeatable points. Instead, we establish correspondences on simple grid subsampled points.</p><p>Although our REGTR is simple in design, it achieves state-of-the-art performance on the 3DMatch <ref type="bibr" target="#b60">[61]</ref> and Mod-elNet <ref type="bibr" target="#b51">[52]</ref> datasets. It also has fast run times since it does not require running RANSAC on a large number of putative correspondences. In summary, our contributions are:</p><p>? We directly predict a consistent set of final point correspondences via self and cross attention, without using the commonly used RANSAC nor optimal transport layers. ? We evaluate on several datasets and demonstrate stateof-the-art performance, achieving precise alignments despite using a small number of correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Correspondence-based registration. Correspondencebased approaches for point cloud registration first establish correspondences between salient keypoints, followed by robust estimation of the rigid transformation. To accomplish the first step, many keypoint detectors <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b61">62]</ref> and feature descriptors <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref> have been handcrafted. Pioneered by 3DMatch <ref type="bibr" target="#b60">[61]</ref>, many researchers propose to improve feature descriptors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b60">61]</ref> and also keypoint detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">56]</ref> by learning from data. Recently, Predator <ref type="bibr" target="#b22">[23]</ref> utilizes attention mechanisms to aggregate contextual information to learn more discriminative feature descriptors. Most of these works are trained by optimizing a variant of the contrastive loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref> between feature descriptors of matching and non-matching points, and rely on a subsequent nearest neighbor matching step and RANSAC to select the correct correspondences.</p><p>Learned direct registration methods. Instead of combining learned descriptors with robust pose estimation, some works incorporate the entire pose estimation into the training pipeline. Deep Closest Point (DCP) <ref type="bibr" target="#b49">[50]</ref> proposes a learned version of Iterative Closest Point (ICP) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, and utilizes soft correspondences on learned pointwise features to compute the rigid transform in a differentiable manner. However, DCP cannot handle partial overlapping point clouds and thus later works overcome the limitation by detecting keypoints <ref type="bibr" target="#b50">[51]</ref> or using optimal transport layers with an added slack row and column <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b56">57]</ref>. IDAM <ref type="bibr" target="#b30">[31]</ref> considers both feature and Euclidean space during its pairwise matching process. PCAM <ref type="bibr" target="#b5">[6]</ref> multiplies the cross-attention matrices at multiple levels to fuse low and high-level contextual information. DeepGMR <ref type="bibr" target="#b59">[60]</ref> learns to compute point-to-distribution correspondences. A separate group of works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref> rely on global feature descriptors and circumvent the local feature correspondence step. Point-NetLK <ref type="bibr" target="#b0">[1]</ref> aligns two point clouds by minimizing the distances between their global PointNet <ref type="bibr" target="#b36">[37]</ref> features, in a procedure similar to the Lucas-Kanade <ref type="bibr" target="#b2">[3]</ref> algorithm. Li et al. <ref type="bibr" target="#b31">[32]</ref> extends it to use analytical Jacobians to improve the generalization behavior. OMNet <ref type="bibr" target="#b54">[55]</ref> incorporates masking into the global feature to better handle partial overlapping point clouds. Our method utilizes local features and is similar to e.g. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>, but we focus on predicting accurate corresponding point locations via transformer attention layers. Learned correspondence filtering. The putative correspondences obtained from correspondence-based methods contain outliers, and thus RANSAC is typically used to filter out wrong matches when estimating the required transformation. However, RANSAC is non-differentiable and cannot be used within a training pipeline. Recent works alleviate this problem by modifying RANSAC to enforce differentiability <ref type="bibr" target="#b4">[5]</ref>, or by learning to identify which of the putative correspondences are inliers <ref type="bibr">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b57">58]</ref>. In addition to inlier classification, 3DRegNet <ref type="bibr" target="#b35">[36]</ref> also regresses the rigid transformation parameters using a deep network. Different from these works, we directly predict the clean correspondences without explicitly computing the noisy putative correspondences.</p><p>Transformers. Transformers <ref type="bibr" target="#b48">[49]</ref> propose a novel attention mechanism that makes use of multiple layers of self and cross multi-head attention to exchange information between the input and output. Although originally designed for NLP tasks, the attention mechanism has recently been shown to be useful for many computer vision tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b58">59]</ref>, and we utilize it in our work to predict point correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition</head><p>Consider two point clouds X ? R M ?3 and Y ? R N ?3 , which we denote as the source and target, respectively. The objective of point cloud registration is to recover the unknown rigid transformation consisting of a rotation R ? SO(3) and translation t ? R 3 that aligns X to Y. <ref type="figure">Figure 2</ref> illustrates our overall framework. We first convert the input point clouds into a smaller set of downsampled keypointsX ? R M ?3 and? ? R N ?3 with M &lt; M, N &lt; N , and their associated features FX ? R M ?D , F? ? R N ?D (Sec. 4.1). Our network then passes these keypoints and features into several transformer crossencoder layers (Sec. 4.2) before finally outputting the corresponding transformed locations? ? R M ?3 ,X ? R N ?3 of the keypoints in the other point cloud (Sec. 4.3). The correspondences can then be obtained from the rows of X and?, i.e. {x i ?? i } and similarly for the other direction. Concurrently, our network outputs overlap score? o X ? R M ?1 ,? Y ? R N ?1 that indicate the probability of each keypoint lying in the overlapping region. Finally, the required rigid transformation can be estimated directly from the correspondences within the overlap region (Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Downsampling and Feature Extraction</head><p>We follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> to adopt the Kernel Point Convolution (KPConv) <ref type="bibr" target="#b44">[45]</ref> backbone for feature extraction. The KPConv backbone uses a series of ResNet-like blocks and strided convolutions to transform each input point cloud into a reduced set of keypointsX ? R M ?3 ,? ? R N ?3 and their associated features FX ? R M ?D , F? ? R N ?D . In contrast to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> that subsequently perform upsampling to obtain feature descriptors of the original point cloud resolution, our approach directly predicts the transformed keypoint locations using the downsampled features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transformer Cross-Encoder</head><p>The KPConv features from the previous step are linearly projected into a lower dimension d = 256. These projected features are then fed into L = 6 transformer cross-encoder 1 layers. Each transformer cross-encoder layer has three sublayers: 1) a multi-head self-attention layer operating on the two point clouds separately, 2) a multi-head cross-attention layer which updates the features using information from the other point cloud, and 3) a position-wise feed-forward network. The cross-attention enables the network to compare points from the two different point clouds, and the selfattention allows points to interact with other points within the same point cloud when predicting its own transformed position, e.g. using rigidity constraints. Note that the network weights are shared among the two point clouds but not among the layers.</p><p>Attention sub-layers. The multi-head attention <ref type="bibr" target="#b48">[49]</ref> operation in each sub-layer is defined as follows:</p><formula xml:id="formula_0">MHAttn(Q, K, V) = (Head 1 ? ... ? Head H ) W O (1a) Head h = Attn QW Q h , KW K h , VW V h ,<label>(1b)</label></formula><p>where ? denotes concatenation over the channel dimension, W Q h , W K h , W V h ? R d?dhead and W O ? R Hdhead?d are learned projection matrices. We set the number of heads H to 8, and d head = d/H. Each attention head employs a single-head dot product attention:</p><formula xml:id="formula_1">Attn(Q, K, V) = softmax QK ? d head V.<label>(2)</label></formula><p>Residual connections and layer normalization are applied to each sub-layer, and we use the "pre-LN" <ref type="bibr" target="#b53">[54]</ref> ordering which we find to be easier to optimize. The query, key, values are set to the same point cloud in the self-attention layers, i.e. MHAttn(FX , FX , FX ) for the source point cloud (and likewise for the target point cloud). This allows points to attend to other parts within the same point cloud. For the cross-attention layers, the keys and values are set to be the features from the other point cloud, i.e. MHAttn(FX , F? , F? ) for the source point cloud (and likewise for the target point cloud) to allow each point to interact with points in the other point cloud.</p><p>Position-wise Feed-forward Network. This sub-layer operates on the features of each keypoint individually. Following its usual implementation <ref type="bibr" target="#b48">[49]</ref>, we use a two-layer feed-forward network with a ReLU activation function after the first layer. Similar to the attention sub-layers, residual connections and layer normalization are applied.</p><p>Positional encodings. Unlike previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50]</ref> that use attentions to learn discriminative features, our transformer layers replace the role of RANSAC and thus requires information of the point positions. Specifically, we incorporate positional information by adding sinusoidal positional encodings <ref type="bibr" target="#b48">[49]</ref> to the inputs at each transformer layer.</p><p>The outputs of the transformer cross-encoder layers are featuresFX ? R M ?d ,F? ? R N ?d which are conditioned on the other point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Output Decoding</head><p>The conditioned features can now be used to predict the coordinates of transformed keypoints. To this end, we use a two-layer MLP to regress the required coordinates. Particularly, the corresponding locations? ? R M ?3 of the source keypointsX in the target point cloud are given as:</p><formula xml:id="formula_2">Y = ReLU(FX W 1 + b 1 )W 2 + b 2 ,<label>(3)</label></formula><p>where W 1 , W 2 and b 1 , b 2 are learnable weights and biases, respectively. We use the hat accent(?) to indicate predicted quantities. A similar procedure is used to obtain the predicted transformed locationsX of the target keypoints.</p><p>Alternatively, we also explore the use of a single-head attention layer (cf . Tab. 4), where the predicted locations? are a weighted sum of the target keypoint coordinates?:</p><formula xml:id="formula_3">Y = Attn(F X W Q out ,F Y W K out ,?),<label>(4)</label></formula><p>where Attn(?) is defined previously in Eq. <ref type="formula" target="#formula_1">(2)</ref>, and W Q out , W K out ? R d?d are learned projection matrices. In parallel, we separately predict the overlap confidenc? o X ? R M ?1 ,? Y ? R N ?1 using a single fully connected layer with sigmoid activation. This is used to mask out the influence of correspondences outside the overlap region which are not predicted as accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Estimation of Rigid Transformation</head><p>The predicted transformed locations in both directions are first concatenated to obtain the final set of M + N correspondences:</p><formula xml:id="formula_4">X corr = X X ,? corr = ? Y ,? corr = ? X o Y .<label>(5)</label></formula><p>The required rigid transformation can be estimated from the estimated correspondences by solving the following:</p><formula xml:id="formula_5">R,t = arg min R,t M +N i? i Rx i + t ?? i 2 ,<label>(6)</label></formula><p>wherex i ,? i ,? i denote the i th row ofX corr ,? corr ,? corr , respectively. We follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b56">57]</ref> to solve Eq. (6) in closed form using a weighted variant of the Kabsch-Umeyama <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47]</ref> algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Loss Functions</head><p>We train our network end-to-end with the ground truth poses {R * , t * } for supervision using the following losses: Overlap loss. The predicted overlap scores are supervised using the binary cross entropy loss. The loss for the source point cloud X is given by:</p><formula xml:id="formula_6">L X o = ? 1 M M i o * xi ? log?x i + (1 ? o * xi ) ? log (1 ??x i ).<label>(7)</label></formula><p>To obtain the ground truth overlap labels o * xi , we first compute the dense ground truth labels for the original point cloud in a similar fashion as <ref type="bibr" target="#b22">[23]</ref>. Specifically, the ground truth label for point x i ? X is defined as:</p><formula xml:id="formula_7">o * xi = 1, T * (x i ) ? NN(T * (x i ), Y) &lt; r o 0, otherwise ,<label>(8)</label></formula><p>where T * (x i ) denotes the application of the ground truth rigid transform {R * , t * }, NN(?) denotes the spatial nearest neighbor and r o is a predefined overlap threshold. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source and target point clouds</head><p>Overlap labels (raw)</p><p>Overlap labels (downsampled)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downsample</head><p>Overlap label 0 1 <ref type="figure">Figure 3</ref>. Pair of point clouds (left) and their corresponding ground truth overlap labels for the dense points (middle) and downsampled keypoints (right). Note that the keypoints near the overlap boundaries have a ground truth label between 0 and 1.</p><p>then obtain the overlap labels o * xi for the downsampled keypoints through average pooling using the same pooling indices from the KPConv downsampling. See <ref type="figure">Fig. 3</ref> for an example of our overlap ground truth labels. The loss L Y o for the target point cloud Y is obtained in a similar fashion. We thus get a total overlap loss of:</p><formula xml:id="formula_8">L o = L X o + L Y o . Correspondence loss.</formula><p>We apply a 1 loss on the predicted transformed locations for keypoints in the overlapping region:</p><formula xml:id="formula_9">L X c = 1 i o * xi M i o * xi |T * (x i ) ?? i |,<label>(9)</label></formula><p>and similarly for the target point cloud. We thus get a total correspondence loss of:</p><formula xml:id="formula_10">L c = L X c + L Y c . Feature loss.</formula><p>To encourage the network to take into account geometric properties when computing the correspondences, we apply an InfoNCE <ref type="bibr" target="#b34">[35]</ref> loss on the conditioned features. Considering the set of points x ?X with a correspondence in?, the InfoNCE loss for the source point cloud is:</p><formula xml:id="formula_11">L X f = ?E x?X log f (x, p x ) f (x, p x ) + nx f (x, n x ) ,<label>(10)</label></formula><p>where we follow <ref type="bibr" target="#b34">[35]</ref> to use a log-bilinear model for f (?, ?):</p><formula xml:id="formula_12">f (x, c) = exp f T x W ffc .<label>(11)</label></formula><p>f x denotes the conditioned feature for point x. p x and n x denote keypoints in? which match and do not match x, respectively. They are determined using the positive and negative margins (r p , r n ) which are set as (m, 2m). m is the voxel distance used in the final downsampling layer in the KPConv backbone, and all negative points that falls outside the negative margin are utilized for n x . Since the two point clouds contain the same type of features, we enforce the learnable linear transformation W f to be symmetrical by parameterizing it as the sum of a upper triangular matrix U f and its transpose, i.e. W f = U f + U f . As explained in <ref type="bibr" target="#b34">[35]</ref>, Eq. (10) maximizes the mutual information between features for matching points. Unlike <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, we do not use the circle loss <ref type="bibr" target="#b43">[44]</ref> that requires the matching features to be similar (w.r.t. 2 or cosine distance). This is unsuitable since: 1) our conditioned features contains information about the transformed positions, and 2) our keypoints are sparse and are unlikely to be at the same location in the two point clouds, and therefore the geometric features are also different.</p><p>Our final loss is a weighted sum of the three components: </p><formula xml:id="formula_13">L = L c + ? o L o + ? f L f ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We train our network using AdamW <ref type="bibr" target="#b32">[33]</ref> optimizer with a initial learning rate of 0.0001 and weight decay of 0.0001. Gradients are clipped at 0.1. For the 3DMatch dataset, we train for 60 epochs with a batch size of 2, halving the learning rate every 20 epochs. We train on the ModelNet40 dataset for 400 epochs with a batch size of 4, and halving the learning rate every 100 epochs. Training requires around 2.5 and 2 days for 3DMatch and ModelNet40 on a single Nvidia Titan RTX, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets and Results</head><p>3DMatch. The 3DMatch dataset <ref type="bibr" target="#b60">[61]</ref> contains 46 train, 8 validation and 8 test scenes. We use the preprocessed data from <ref type="bibr" target="#b22">[23]</ref> containing voxel-grid downsampled point clouds, and follow them to evaluate on both pairs with &gt; 30% overlap (3DMatch) and 10 ? 30% overlap (3DLoMatch). Each input point cloud contains an average of about 20,000 points, which are downsampled to an average of 345 points by our KPConv backbone. We perform training data augmentation by applying small rigid perturbations, jittering of the point locations and shuffling of points.</p><p>Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>, we evaluate using Registration Recall (RR) which measures the fraction of successfully registered pairs, defined as having a correspondence RMSE below 0.2m. We also evaluate on the Relative Rotation Errors (RRE) and Relative Translation Errors (RTE) that measures the accuracy of successful registrations. We follow <ref type="bibr" target="#b22">[23]</ref> and compare against several recent learned correspondencebased algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> 2 . These algorithms tend to perform better with a larger number of sampled interest points, and therefore we only show the results for the maximum number (5000) of sampled points. Since Predator <ref type="bibr" target="#b22">[23]</ref>    <ref type="table">Table 1</ref> shows the quantitative results, and Figs. 1 and 8a to 8d show several examples of the qualitative results. We also show the results for the individual scenes in the supplementary. For both 3DMatch and 3DLoMatch benchmarks, our method achieves the highest average registration recall across scenes. Interestingly, our registration is also very precise and achieved the lowest RTE and RRE on both 3DMatch and 3DLoMatch benchmarks despite only using a small number of points for pose estimation. In addition, we also compare with Predator-NR, a RANSAC-free variant of Predator-1k that utilizes the product of the predicted overlap and matchability scores to weigh the correspondences during pose estimation. The underperformance of Predator-NR indicates that our proposed method is more suitable for replacing RANSAC. The results support our claim that our attention mechanism can replace the role of RANSAC since our REGTR uses largely the same KPConv backbone as Predator. Lastly, we note that OMNet does not perform well on the 3DMatch dataset. This is likely due to the difficulty in describing complex scenes with a single global feature vector. This behavior is also previously observed in [13] for another global feature-based algorithm, PointNetLK <ref type="bibr" target="#b0">[1]</ref>.  <ref type="table">Table 3</ref>. Run time in milliseconds on the 3DMatch benchmark test set. * Similar pre-processing is used for D3Feat, Predator and our algorithm, but our algorithm uses a faster GPU implementation.</p><p>ModelNet40. We also evaluate on the ModelNet40 <ref type="bibr" target="#b51">[52]</ref> dataset comprising synthetic CAD models. We follow the data setting in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b56">57]</ref>, where the point clouds are sampled randomly from mesh faces of the CAD models, cropped and subsampled. Following <ref type="bibr" target="#b22">[23]</ref>, we evaluate on two partial overlap settings: ModelNet which has 73.5% pairwise overlap on average, and ModelLoNet which contains a lower 53.6% average overlap. We train only on ModelNet, and perform direct generalization to ModelLoNet. We follow <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b56">57]</ref> and measure the performance using Relative Rotation Error (RRE) and Relative Translation Error (RTE) on all point clouds, as well as the Chamfer distance (CD) between the registered scans.</p><p>The results are shown in Tab. 2, with example qualitative results in Figs. 8e and 8f. We compare against recent correspondence-based <ref type="bibr" target="#b22">[23]</ref> and end-to-end registration methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57]</ref>. Predator <ref type="bibr" target="#b22">[23]</ref> samples 450 points in this experiment. OMNet <ref type="bibr" target="#b54">[55]</ref> was originally trained only on axis-asymmetrical categories, and we retrained it on all categories to obtain a slightly improved result. As noted in <ref type="bibr" target="#b22">[23]</ref>, many of the end-to-end registration methods are specifically tuned for ModelNet. RPM-Net <ref type="bibr" target="#b56">[57]</ref> additionally uses surface normal information. Despite this, our REGTR substantially outperforms all baseline methods in all metrics under both normal overlap (ModelNet) and low overlap (ModelLoNet) regimes. Our learned attention mechanism is able to outperform the optimal transport (in RPM-Net) and RANSAC step (in Predator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head><p>We perform further analysis in this section to better understand our algorithm behavior. All experiments in this section are performed on the larger 3DMatch dataset.</p><p>Runtime. We compare the runtime of REGTR against several algorithms in Tab. 3. We conducted the test on a single Nvidia Titan RTX with Intel Core i7-6950X @ 3.0GHz and 64GB RAM. Our entire pipeline runs under 100ms, and is feasible for many real-time applications. The time consuming step for correspondence-based algorithms is the pose estimation which includes feature matching and RANSAC. For example, excluding preprocessing required for the KPConv backbone, Predator <ref type="bibr" target="#b22">[23]</ref> takes 234ms for the registration when sampling just 1,000 points. DGR <ref type="bibr">[13]</ref> and PCAM <ref type="bibr" target="#b5">[6]</ref> also require long times for pose estimation due to their robust refinement and RANSAC safeguard. Although the global feature-based OMNet runs faster than our algorithm, we note that it is unable to obtain good accuracy on the 3DMatch dataset.</p><p>Accuracy of predicted correspondences. In <ref type="figure" target="#fig_2">Fig. 4</ref>, we plot the distribution of 2 -error of the predicted correspondences for keypoints within the overlap region (where o * xi , o * yi &gt; 0.5) for the 3DMatch test set. The median error of our predicted correspondences is 0.028m, which is significantly smaller than the median distance between keypoints (0.112m). For comparison, an oracle matcher that matches every keypoint to the closest keypoint using the ground truth pose obtains a median error of 0.071m. Our direct prediction of correspondences is able to overcome the resolution issues from the downsampling, and thus explains the precise registration obtained by REGTR.</p><p>We also visualize the predicted correspondences of a point cloud pair in <ref type="figure">Fig. 5</ref>. The short green error lines in <ref type="figure">Fig. 5b</ref> indicate our predicted transformed locations are highly accurate within the overlap region, even in noninformative regions (e.g. floor). Interestingly, correspondence for points outside the overlap region are projected to near the overlap boundaries. These observations suggest that REGTR is able to make use of rigidity constraints to guide the positions of the predicted correspondences.</p><p>Visualization of attention. In <ref type="figure">Fig. 6</ref>, we visualize the attention for a point on the ground for the same point cloud pair from the previous section. Since the point lies in a non-informative region, the point attends to multiple similar looking regions in the other point cloud in the first transformer layer <ref type="figure">(Fig. 6a)</ref>. At the sixth layer, the point is confident of its position and mostly focuses on its correct corresponding location <ref type="figure">(Fig. 6b)</ref>. The self-attention in <ref type="figure">Fig. 6c</ref>   shows that the point makes use of feature rich regions within the same point cloud to help localize its correct location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablations</head><p>We further perform ablation studies on the 3DMatch dataset to understand the role of various components.</p><p>Number of cross-encoder layers. We evaluate how the performance varies with the number of cross-encoder layers in <ref type="figure" target="#fig_4">Fig. 7</ref>. Our network cannot function without any crossencoder layers, and we show the registration recall for two to eight layers. Performance generally improves with more cross-encoder layers, but saturates around L = 6 encoder layers (which we use for all our experiments).</p><p>Comparison with RANSAC. We compare with a version of REGTR where we replace our output decoder with a two-layer MLP which outputs 256D feature descriptors and a parallel single-layer decoder that outputs the overlap score. The pose is subsequently estimated using RANSAC on nearest neighbor feature matches. The network is trained   <ref type="table">Table 4</ref>. Ablation of components and losses using only L o and L f (using Circle Loss <ref type="bibr" target="#b43">[44]</ref>). This results in a lower registration recall, and significantly higher rotation and translation errors as the downsampled keypoints do not provide enough resolution for accurate registration. We also try applying RANSAC to the predicted correspondences from REGTR to see if the performance can further improve. Row 7 of Tab. 4 shows marginally worse registration recall. This indicates that RANSAC is no longer beneficial on the predicted correspondences that are already consistent with a rigid transformation. Decoding scheme. We compare with decoding the coordinates as a weighted sum of coordinates (Eq. 4). Compared to our simpler approach of regressing the coordinates using a MLP, computing the coordinates as a weighted sum achieves a slightly better RTE and RRE, but lower registration recall. See rows 2 and 6 of Tab. 4. Loss ablations. Rows 3-6 of Tab. 4 shows the registration performance with different loss configurations. Without the feature loss to guide the network outputs, the network obtained a 1.6% and 2.9% lower registration recall for 3DMatch and 3DLoMatch, respectively. Using circle loss from <ref type="bibr" target="#b22">[23]</ref> also underperformed as the network cannot incorporate positional information into the feature as effectively. We also experimented with applying the losses on all L = 6 transformer layers (instead of just the final one) with the output decoder shared among all cross-encoder layers. This additional supervision led to a 8.1% (3DMatch) and 18.0% (3DLoMatch) lower registration recall. Consequently, we only apply the supervision for the output of the last crossencoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>Our use of transformers layers with quadratic complexity prevents its use on large number of points, and we can only apply them on downsampled point clouds. Although our direct correspondence prediction alleviates the resolution issue, it is possible that a finer resolution can result in even higher performance. We have tried transformer layers with linear complexity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>, but that obtained subpar performance. Alternate workarounds include using sparse attention <ref type="bibr" target="#b8">[9]</ref>, or performing a coarse-to-fine registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We propose the REGTR for rigid point cloud registration, which directly predicts clean point correspondences using multiple transformer layers. The rigid transformation can then be estimated from the correspondences without further nearest neighbor feature matching nor RANSAC steps. The direct prediction of correspondences overcomes the resolution issues from the use of downsampled features, and our method achieves state-of-the-art performance on both scene and object point cloud datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary, we first provide additional details on the datasets and their preprocessing (Sec. A). We then describe the procedure for recovering the rigid transformation from the correspondences (Sec. B), our sinusoidal position encodings (Sec. C) and additional information on the network architecture (Sec. D). Finally, we show detailed results for ScanNet (Sec. E) and additional qualitative results (Sec. F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Details</head><p>3DMatch. The 3DMatch <ref type="bibr" target="#b60">[61]</ref> dataset comprises RGB-D frames obtained from several sources (Tab. 5). The data is captured from diverse scenes (e.g. bedrooms, kitchens, offices) and different sensors (e.g. Microsoft Kinect, Intel Realsense), and each point cloud is generated by fusing 50 consecutive depth frames using TSDF volumetric fusion <ref type="bibr" target="#b14">[15]</ref>. We use the voxel-grid downsampled data from Predator <ref type="bibr" target="#b22">[23]</ref>, and the same point cloud pairs for training and evaluation. The dataset contains 46 train, 8 validation, and 8 test scenes. The training and validation scenes contains a total of 20,586 3 and 1,331 point cloud pairs respectively, and the test scenes contain 1,279 (3DMatch) and 1,726 (3DLoMatch) pairs. We apply training data augmentation by applying a small rigid perturbation with magnitudes sampled from a Gaussian distribution with ?r = 0.1? and ?t = 0.1 for the rotation and translation, respectively. We then apply a Gaussian noise (? = 0.05) on the individual point locations, and shuffling of point order.</p><p>ModelNet40. The ModelNet40 <ref type="bibr" target="#b51">[52]</ref> dataset provides 3D CAD models from 40 object categories for academic use. We follow previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b56">57]</ref> in using the preprocessed data from <ref type="bibr" target="#b36">[37]</ref>. These data are generated by sampling 2,048 points from the mesh faces and then scaling them to fit into a unit sphere. The partial scans are generated from the procedure in <ref type="bibr" target="#b56">[57]</ref>: A half-space with random direction is sampled, and shifted such that a proportion p of points lie within the half space. Subsequently, random rotation of up to 45 ? , translation up to 0.5 units, Gaussian noise (? = 0.05) on the individual point locations, and shuffling of point order are applied to the point clouds. The point clouds are finally resampled to 717 points. Following <ref type="bibr" target="#b22">[23]</ref>, p is set to 0.7 and 0.5 for ModelNet and ModelLoNet benchmarks, respectively. We use the first 20 categories for training and validation, and the other 20 categories for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>License SUN3D <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref> CC BY-NC-SA 4.0 7-Scenes <ref type="bibr" target="#b41">[42]</ref> Non-commercial use only RGB-D Scenes v2 <ref type="bibr" target="#b27">[28]</ref> (License not stated) BundleFusion <ref type="bibr" target="#b15">[16]</ref> CC BY-NC-SA 4.0 Analysis-by-Synthesis <ref type="bibr" target="#b47">[48]</ref> CC BY-NC-SA 4.0 <ref type="table">Table 5</ref>. Raw data used in the 3DMatch <ref type="bibr" target="#b60">[61]</ref> dataset and their licenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimation of Rigid Transformation</head><p>In this section, we describe the closed form solution for the rigid transformation {R, t}, given correspondences {xi ? yi} with their weights {oi}, as used in Sec. 4.4:</p><formula xml:id="formula_14">R,t = arg min R,t N i oi Rxi + t ? yi 2 .<label>(12)</label></formula><p>Step 1. Compute the weighted centroids of the 2 point sets:</p><formula xml:id="formula_15">x = N i=1 oixi N i=1 oi ,? = N i=1 oiyi N i=1 oi .<label>(13)</label></formula><p>Step 2. Center the point clouds by subtracting away the cen-</p><formula xml:id="formula_16">troid:x i = xi ?x,?i = yi ??, ?i = 1, . . . , N.<label>(14)</label></formula><p>Step 3. Recover the rotation R. For this, we can use the Kabsch algorithm <ref type="bibr" target="#b24">[25]</ref>. First construct the following 3 ? 3 weighted covariance matrix:</p><formula xml:id="formula_17">H = N i=1 oixi? i .<label>(15)</label></formula><p>Considering the singular value decomposition H = U?V , the desired rotationR is given by:</p><formula xml:id="formula_18">R = V ? ? 1 0 0 0 1 0 0 0 det VU ? ? U ,<label>(16)</label></formula><p>where det(?) denotes the matrix determinant.</p><p>Step 4. Lastly, the translation can be computed as:</p><formula xml:id="formula_19">t =? ?Rx.<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Position Encodings</head><p>We encode the point coordinates by generalizing the sinusoidal positional encodings in <ref type="bibr" target="#b48">[49]</ref> to 3D continuous coordinates. The position encodings have the same dimension d = 256 as the feature embeddings used in the attention layers.</p><p>For a point x = (x, y, z), we separately transform each coordinate to their embeddings p x x , p x y , p x z ? R 2 d/6 . The x-coordinate is transformed as:</p><formula xml:id="formula_20">p x x [2i] = sin x 10000 2i/ d/3 (18a) p x x [2i + 1] = cos x 10000 2i/ d/3 .<label>(18b)</label></formula><p>The y and z coordinates are transformed in a similar manner. We then concatenate the embeddings for all three dimensions. Since the embedding dimension d = 256 is not divisible by 6, we pad the remaining 4 elements with zeros to obtain the final embedding.  <ref type="table">Table 6</ref>. Effects of different position encodings. "Dec." denotes correspondence decoding scheme, which can be either weighted coordinates using Eq. 4 (Wt.) or regression using Eq. 3 (Reg.). "Pos." denotes position encoding type.</p><p>Importance of position encodings. <ref type="table">Table 6</ref> compares different choices of position encodings. The position encodings can only be removed when decoding the correspondences as a weighted sum (Eq. 4). Without position encodings, the network suffered a significant drop in performance both in terms of registration recall (RR) and accuracy (RTE and RRE), further supporting our hypothesis that our attention mechanism utilizes rigidity constraints to correct bad matches. We also compare with learned embeddings (using a 5-layer MLP with 32-64-128-256-256 channels), which has a slightly lower performance in most metrics. We therefore chose to use sinusoidal encodings, which also reduces the number of learnable weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Architecture</head><p>KPConv backbone. We show the detailed network architecture of our KPConv <ref type="bibr" target="#b44">[45]</ref> backbone in <ref type="figure" target="#fig_0">Fig. 10</ref>. We use the same KPConv backbone as Predator, but we modify it to apply instance normalization on each point cloud individually instead of over all point clouds to allow for correct behavior over batch sizes larger than one. We do not make any other changes to the backbone for the 3DMatch dataset. However, to maintain a reasonable resolution for the downsampled keypoints for ModelNet, we use a shallower backbone consisting of only a single downsampling, and the voxel size used in the first level is set to 0.03 instead of 0.06.</p><p>Transformer. <ref type="figure">Figure 9</ref> provides the detailed description of our transformer cross-encoder. Geometric features from the KPConv backbone are first projected to d = 256 dimensions, and then passed through L = 6 transformer cross-encoder layers to obtain the conditioned featuresFX ,F? , which can be used to predict the output correspondences and overlap scores via our output decoder. We use the pre-LN <ref type="bibr" target="#b53">[54]</ref> configuration for the self-attention, cross-attention, and position-wise feed-forward networks (FFN). Positional encodings (Sec. C) are added to the queries, keys and values before every self-and cross-attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Detailed Registration Results for 3DMatch</head><p>We report the breakdown of the Registration Recall, Relative Rotation Error, and Relative Translation Error for each individual scene in Tab. 7. REGTR obtains the highest registration recall for three (3DMatch) and four (3DLoMatch) of the scenes, and the lowest rotation/translation errors for majority of the scenes in both settings, despite using downsampled features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Decoder</head><p>Overlap $ !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformed coordinates &amp;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Decoder</head><p>Overlap $ "</p><p>Transformed coordinates &amp; <ref type="figure">Figure 9</ref>. REGTR's transformer cross-encoder layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Qualitative Results</head><p>We show additional qualitative results for both 3DMatch and ModelNet datasets in <ref type="figure" target="#fig_0">Fig. 11</ref>. The last two rows show example failure cases. During failures, usually both overlap and correspondences are predicted wrongly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our network directly outputs final correspondences and the overlap scores. The required rigid transformation can then be directly computed from these correspondences without RANSAC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where we set ? o = 1.0 and ? f = 0.1 for all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Histogram and CDF plot of 2 errors of predicted correspondences. 95.7% of predicted correspondences have errors below 0.112m, the median keypoint-to-keypoint distance (denoted by red dashed line). Errors are clipped at 0.2m for clarity, with only 3.0% of predicted correspondences exceeding this error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Visualization of predicted correspondences. Keypoints are colored based on their predicted overlap score, where high scores are denoted in red. (a) Source X and keypointsX, (b) Target Y and predicted correspondences?, with green lines showing the correspondence error. Best viewed in color. (a) Cross att. (layer 1) (b) Cross att. (layer 6) (c) Self att. (layer 6) Visualization of attention weights for the point indicated with a red dot. Brighter colors indicate higher attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Performance for various number of cross-encoder layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Example qualitative registration results for (a, b) 3DMatch, (c, d) 3DLoMatch (e) ModelNet40, and (f) ModelLoNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>obtains the highest registration recall when using 1000 interest points, we also reran their open-source</figDesc><table><row><cell></cell><cell></cell><cell>3DMatch</cell><cell></cell><cell></cell><cell>3DLoMatch</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">RR(%) RRE( ? ) RTE(m) RR(%) RRE( ? ) RTE(m)</cell></row><row><cell>3DSN [21]</cell><cell>78.4</cell><cell>2.199</cell><cell>0.071</cell><cell>33.0</cell><cell>3.528</cell><cell>0.103</cell></row><row><cell>FCGF [14]</cell><cell>85.1</cell><cell>1.949</cell><cell>0.066</cell><cell>40.1</cell><cell>3.147</cell><cell>0.100</cell></row><row><cell>D3Feat [2]</cell><cell>81.6</cell><cell>2.161</cell><cell>0.067</cell><cell>37.2</cell><cell>3.361</cell><cell>0.103</cell></row><row><cell>Predator-5k [23]</cell><cell>89.0</cell><cell>2.029</cell><cell>0.064</cell><cell>59.8</cell><cell>3.048</cell><cell>0.093</cell></row><row><cell>Predator-1k [23]</cell><cell>90.5</cell><cell>2.062</cell><cell>0.068</cell><cell>62.5</cell><cell>3.159</cell><cell>0.096</cell></row><row><cell cols="2">Predator-NR [23] 62.7</cell><cell>2.582</cell><cell>0.075</cell><cell>24.0</cell><cell>5.886</cell><cell>0.148</cell></row><row><cell>OMNet [55]</cell><cell>35.9</cell><cell>4.166</cell><cell>0.105</cell><cell>8.4</cell><cell>7.299</cell><cell>0.151</cell></row><row><cell>DGR [13]</cell><cell>85.3</cell><cell>2.103</cell><cell>0.067</cell><cell>48.7</cell><cell>3.954</cell><cell>0.113</cell></row><row><cell>PCAM [6]</cell><cell>85.5</cell><cell>1.808</cell><cell>0.059</cell><cell>54.9</cell><cell>3.529</cell><cell>0.099</cell></row><row><cell>Ours</cell><cell>92.0</cell><cell>1.567</cell><cell>0.049</cell><cell>64.8</cell><cell>2.827</cell><cell>0.077</cell></row><row><cell cols="7">Table 1. Performance on 3DMatch and 3DLoMatch datasets. Re-</cell></row><row><cell cols="7">sults for 3DSN, FCGF, D3Feat and Predator-5k are from [23].</cell></row><row><cell></cell><cell></cell><cell cols="2">ModelNet</cell><cell></cell><cell cols="2">ModelLoNet</cell></row><row><cell>Methods</cell><cell cols="2">RRE RTE</cell><cell>CD</cell><cell cols="2">RRE RTE</cell><cell>CD</cell></row><row><cell>PointNetLK [1]</cell><cell cols="6">29.725 0.297 0.0235 48.567 0.507 0.0367</cell></row><row><cell>OMNet [55]</cell><cell cols="6">2.947 0.032 0.0015 6.517 0.129 0.0074</cell></row><row><cell>DCP-v2 [50]</cell><cell cols="6">11.975 0.171 0.0117 16.501 0.300 0.0268</cell></row><row><cell>RPM-Net [57]</cell><cell cols="6">1.712 0.018 0.00085 7.342 0.124 0.0050</cell></row><row><cell>Predator [23]</cell><cell cols="6">1.739 0.019 0.00089 5.235 0.132 0.0083</cell></row><row><cell>Ours</cell><cell cols="6">1.473 0.014 0.00078 3.930 0.087 0.0037</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Evaluation results on ModelNet40 dataset. Results of DCP-v2, RPM-Net and Predator are taken are from [23].code with 1000 interest points and include the results un- der Predator-1k. Furthermore, we compare with several methods [6,13,55] designed to avoid RANSAC. We trained OMNet [55] on 3DMatch with a batch size of 32 for 2000 epochs using 1024 random points. For [6, 13], we use the authors' trained weights. We disabled ICP refinement in DGR [13] for a fair comparison.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We name the layers cross-encoder layers to differentiate them from the usual transformer encoder layers<ref type="bibr" target="#b48">[49]</ref> which only take in a single source.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The initial Predator code had a bug which decreased the performance, and we list the improved results using its corrected version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">one point cloud (7-scenes-fire/19) has a wrong groundtruth pose and we exclude training pairs containing this point cloud.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This research/project is supported in part by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG2-RP-2020-016), and the Tier 2 grant MOE-T2EP20120-0011 from the Singapore Ministry of Education.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointnetlk: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">D3Feat: Joint learning of dense detection and description of 3d local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DSAC -Differentiable RANSAC for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6684" to="6692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PCAM: Product of cross-attention matrices for rigid registration of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Quan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object modeling by registration of multiple range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2724" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5556" to="5565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface re-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PPFNet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stickypillars: Robust and efficient feature matching on point clouds using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Olsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning multiview 3d point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-to-coarse global registration of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predator: Registration of 3d point clouds with low overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Featuremetric registration: A fast semi-supervised approach for robust point cloud registration without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11366" to="11374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A solution for the best rotation to relate two sets of vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kabsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hough voting for robust global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15994" to="16003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">USIP: Unsupervised stable interest point detection from 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative distance-aware similarity matrix convolution with mutual-supervised point elimination for efficient point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangning</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="378" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnetlk revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jhony</forename><surname>Kaesemodel Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12763" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An endto-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3DRegNet: A deep neural network for 3D point registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Dias Pais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacinto</forename><forename type="middle">C</forename><surname>Venu Madhav Govindu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miraldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7193" to="7203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pcrnet: Point cloud registration network using pointnet encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinit</forename><surname>Sarode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howie</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07906</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">NARF: 3D range image features for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Steder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Defining and Solving Realistic Perception Problems in Personal Robotics at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unique shape context for 3d data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="376" to="380" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Prnet: Self-supervised learning for partial-to-partial registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">OMNet: Learning overlapping mask for partialto-partial point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangfu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3DFeat-Net: Weakly supervised local 3d features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="607" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">RPM-Net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to find good correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2666" to="2674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="12498" to="12507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">DeepGMR: Learning latent gaussian mixture models for registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="733" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning local geometric descriptors from RGB-D reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Intrinsic shape signatures: A shape descriptor for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Results of 3DSN, FCGF, D3Feat and Predator-5k are taken from</title>
	</analytic>
	<monogr>
		<title level="m">Table 7. Detailed results on the 3DMatch and 3DLoMatch datasets</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
