<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeying</forename><surname>Jin</surname></persName>
							<email>jinyeying@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Yale-NUS College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Night image enhancement</term>
					<term>low-light image</term>
					<term>light-effects sup- pression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000?0001?7818?9534] , Wenhan Yang 2[0000?0002?1692?0069] , and Robby T. Tan 1,3[0000?0001?7532?6919]</p><p>Abstract. Night images suffer not only from low light, but also from uneven distributions of light. Most existing night visibility enhancement methods focus mainly on enhancing low-light regions. This inevitably leads to over enhancement and saturation in bright regions, such as those regions affected by light effects (glare, floodlight, etc). To address this problem, we need to suppress the light effects in bright regions while, at the same time, boosting the intensity of dark regions. With this idea in mind, we introduce an unsupervised method that integrates a layer decomposition network and a light-effects suppression network. Given a single night image as input, our decomposition network learns to decompose shading, reflectance and light-effects layers, guided by unsupervised layer-specific prior losses. Our light-effects suppression network further suppresses the light effects and, at the same time, enhances the illumination in dark regions. This light-effects suppression network exploits the estimated light-effects layer as the guidance to focus on the light-effects regions. To recover the background details and reduce hallucination/artefacts, we propose structure and high-frequency consistency losses. Our quantitative and qualitative evaluations on real images show that our method outperforms state-of-the-art methods in suppressing night light effects and boosting the intensity of dark regions. 4</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Night images can contain uneven light distributions, as shown in <ref type="figure" target="#fig_3">Fig. 1</ref>, where some regions are dark and some are significantly brighter, due to the presence Input Our Method Sharma <ref type="bibr" target="#b31">[32]</ref> EnlightenGAN <ref type="bibr" target="#b14">[15]</ref> Fig. <ref type="bibr" target="#b0">1</ref>. An existing night light-effects suppression method <ref type="bibr" target="#b31">[32]</ref> suffers from hallucination/artefacts and generates improper light effects, while an image enhancement method <ref type="bibr" target="#b14">[15]</ref> is not designed to handle night light effects and incorrectly intensifies it. In contrast, our method jointly suppresses light effects and enhances dark regions.</p><p>of light effects <ref type="bibr" target="#b4">5</ref> . Most existing nighttime visibility enhancement methods focus mainly on boosting the intensity of low-light regions, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>. Hence, when these methods are applied to night images that contain light effects, they inevitably amplify the light effects, and impair the visibility of the images even further. Unlike these methods, our goal in this paper is to suppress the light effects while, at the same time, boosting the intensity of dark regions. Fully-supervised learning methods could be a possible solution to achieving our goal. However, these methods would require a diverse and large collection of paired night images taken with and without light effects, which is intractable to obtain. Another possible solution would be the use of synthetic night images with rendered light effects. However, the effectiveness of methods trained on synthetic night data depends on the quality of the light-effects rendering model. To our knowledge, rendering physically correct night light effects with various background scenes and lighting conditions is still challenging <ref type="bibr" target="#b35">[36]</ref>.</p><p>In this paper, we introduce an unsupervised learning approach that integrates a decomposition network and a light-effects suppression network in a single unified framework. Our decomposition network is derived from an imagelayer model and guided by our layer-specific prior losses to decompose the input image into shading, reflectance and light-effects layers ( <ref type="figure">Fig. 3</ref> shows the examples of these three layers). Subsequently, our light-effects suppression network, which is trained on unpaired images with and without light effects, provides additional unsupervised constraints. This network not only strengthens the light effects decomposition but also enhances the intensity in dark regions. The two networks, the decomposition and light-effect suppression networks, are connected.</p><p>To recover the background details behind light-effects regions, we introduce structure and high-frequency (HF) features consistency losses. We employ the structure consistency based on the VGG network and utilize the guided filter to obtain HF features. The structure and HF-features consistency losses can also reduce hallucination. In summary, our main contributions are as follows:</p><p>-To enhance the visibility of night images that suffer from low light and light effects simultaneously, we introduce a network architecture that integrates layer decomposition and light-effects suppression in one unified framework. -To distinguish light effects from background regions, particularly when the color of the light effects is white or achromatic, we propose utilizing the estimated light-effects layer as guidance for our unsupervised light-effects suppression network. -To restore the background details, we introduce novel unsupervised losses based on the structure and HF-features consistency. Our perceptual structure information and HF texture information are less affected by light effects. Thus, they can be employed to preserve background details, and, importantly, to suppress unwanted artefacts.</p><p>Our experiments and evaluations show that our method is effective in suppressing light-effects regions and enhancing dark regions, outperforming state-of-the-art methods both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sharma and Tan <ref type="bibr" target="#b31">[32]</ref> introduce a method based on camera response function (CRF) estimation and HDR imaging to suppress light effects. The method is the first method that can suppress light effects and improve the dynamic range for night images. However, it suffers from artefacts and missing details as shown in <ref type="figure" target="#fig_3">Fig. 1</ref>, particularly for white (or achromatic) lights.</p><p>In the field of night image dehazing, a few methods have been proposed to suppress glow due to haze/fog particles. Li et al. <ref type="bibr" target="#b22">[23]</ref> address glow removal on foggy nights using layer separation. Zhang et al. <ref type="bibr" target="#b43">[44]</ref> use maximum reflectance prior for haze and glow removal. Ancuti et al. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> use a fusion process and the Laplace operator to deglow and dehaze. Yan et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> propose a semisupervised method <ref type="bibr" target="#b36">[37]</ref> employing a grayscale guided network. However, all these methods are designed for glow suppression in haze or foggy night, and not for removing light effects in clear night images. Moreover, unlike our method, they are also not designed for enhancing dark regions.</p><p>A number of methods have been developed to boost the brightness of lowlight images without considering the presence of night light effects. A few methods are based on histogram equalization <ref type="bibr" target="#b27">[28]</ref>, inversion and dehazing <ref type="bibr" target="#b7">[8]</ref>, the retinex model (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>), while more recent methods are based on deep networks <ref type="bibr" target="#b19">[20]</ref>. Most deep-learning-based methods (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>) adopt supervised learning to train their model and thus require a large number of pairs of low/normallight images. A few unsupervised methods (e.g. <ref type="bibr" target="#b14">[15]</ref>) rely on adversarial training using unpaired low/normal-light images. Semi-supervised methods (e.g. <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>) recompose coarse-to-fine representations towards perceptually pleasing images with the help of unpaired high-quality images. Recently, zero-shot learning methods (e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>) have been proposed for low-light enhancement. Most of these night image enhancement methods, however, are not designed to suppress night  <ref type="figure">Fig. 2</ref>. The overall architecture of our proposed method. We integrate decomposition and light-effects suppression networks in one unified unsupervised framework. Given the input night image, we suppress light effects through the layer decomposition network, in which light-effects, shading, and reflectance layers are obtained (see <ref type="figure">Fig. 3</ref>). The light-effects suppression is guided by the decomposed light-effects layer G and based on unpaired learning (see <ref type="figure" target="#fig_2">Fig. 4</ref>) to further suppress light effects and boost dark regions.</p><p>light effects and enhance low light regions simultaneously; therein lies the main difference with our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>To suppress light effects and, at the same time, boost the intensity of dark regions, we propose an unsupervised framework by integrating a decomposition network and a light-effects suppression network. Our decomposition network is based on an image-layer model and produces three separate layers: shading, reflectance, and light-effects layers. We input these layers into our light-effects suppression network to obtain our final output, where light effects are suppressed and dark regions are boosted. This network learns from unpaired data and is guided by our estimated light-effects layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model-Based Layer Decomposition Network</head><p>Our decomposition is based on the following image-layer model:</p><formula xml:id="formula_0">I = R L + G,<label>(1)</label></formula><p>where I represents the input night image, G represents the light-effects layer, R and L are the reflectance and shading layers, respectively. The notation represents element-wise multiplication. In this equation, we assume a linear gamma function. However, we do not use this equation explicitly in our method. Instead, we use it only to guide the design of our network in used in training, the background scenes are approximations of the physically correct values. Our decomposition goal is to obtain a background scene that is free from light effects, i.e., we want to estimate the background scene, J init = R L. Hence, even when non-linear images are used in training, applications that are less concerned about physically correct intensity values but suffer from light effects can benefit from our method. Our model differs from the widely used intrinsic model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref>, as the latter does not incorporate the light-effects layer. <ref type="figure">Fig. 2</ref> shows our pipeline. The decomposition network is based on our imagelayer model in Eq. (1). Given the input image (I), we first perform image decomposition. We use three separate networks and our novel unsupervised losses to obtain the light effects (G), shading (L), and reflectance (R) layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Light Effects, Shading and Reflectance Layers</head><p>To obtain the light effects (G), shading (L), and reflectance (R) layers, we use three networks respectively: Light-Effects-Net (? G ), Shading-Net (? L ) and Reflectance-Net (? R ), where G = ? G (I), L = ? L (I), and R = ? R (I). The three networks are trained using unsupervised losses, which will be discussed in the subsequent paragraphs. <ref type="figure">Fig. 3</ref> shows examples of these three layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light-Effects and Shading Initialization</head><p>To resolve the decomposition ambiguity problem, it is important to provide proper initial estimates of the layers. For the shading layer, we employ a shading map L i obtained by taking the maximum value of the three color channels, for each pixel <ref type="bibr" target="#b13">[14]</ref>. For the light-effects layer, we use a light-effects map G i , computed using the relative smoothness technique <ref type="bibr" target="#b21">[22]</ref>. This is extracted using the second-order Laplacian filter from the input image, since light effects are smooth variations. We define the loss function for the initialization step as:</p><formula xml:id="formula_1">L init = |G ? G i | 1 + |L ? L i | 1 .<label>(2)</label></formula><p>Gradient Exclusion Loss The gradients of the light effects layer have a short tail distribution, similar to that of 'glow' <ref type="bibr" target="#b22">[23]</ref>. In contrast, the gradients of the background image have a long tail distribution <ref type="bibr" target="#b21">[22]</ref>. Hence, we employ a gradient exclusion loss to recover the uncorrelated layers {G, J init }, where the goal is to separate the two layers as far as possible in the gradient space. The definition of the loss follows <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46]</ref>:</p><formula xml:id="formula_2">L excl = 3 n=1 tanh(? G ?n |?G ?n |) ? tanh(? J ?n init |?J ?n init |) F ,<label>(3)</label></formula><p>where ? F is the Frobenius norm, G ?n and J ?n init represent G and J init downsampled using the bilinear interpolation, and the parameters ? G ?n and ? J ?n init are normalization factors.</p><p>Color Constancy Loss To minimize any color shift in our decomposition output, inspired by the Gray World assumption <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>, we use a color-constancy prior, which encourages the range of the intensity values of the three color channels in the background image J init to be balanced:</p><formula xml:id="formula_3">L cc = (c1,c2) |J c1 init ? J c2 init | 1 ,<label>(4)</label></formula><p>where (c1, c2) ? {(r, g), (r, b), (g, b)} denotes a combination of two color channels.</p><p>Reconstruction Loss For our decomposition task, recombining the estimated layers should give us back the original input image. Hence, we define our reconstruction loss as:</p><formula xml:id="formula_4">L recon = |I ? (R L + G | 1 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Fusion HF-Features Consistency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Consistency</head><p>GAP Feature Map ..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Light-Effects Suppression Layer Decomposition</head><p>: Weighted Sum <ref type="figure">Fig. 5</ref>. Overview of our structure and HF-features consistency losses. We first use our adaptive fusion scheme to obtain a fused grayscale image I gray . Then, from I gray , we compute VGG features ? VGG (I gray ) that are less affected by light effects, and HF-features ? HF (I gray ) that are more robust to light effects and contain background details.</p><p>We multiply each unsupervised loss with its respective weight, where we set ? init , ? excl all set to 1 since they are in the same scale. We empirically set ? recon = 0.1 and employ the weight ? cc = 0.5 from <ref type="bibr" target="#b12">[13]</ref> to balance the decomposition process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Light-Effects Suppression Network</head><p>To better suppress light effects, we integrate our decomposition network with an unpaired light-effects suppression network. We design this network to suppress light effects by using the guidance of our estimated light-effects layer, enforcing the network to focus on light-effects regions. As shown in <ref type="figure">Fig. 2</ref>, our network comprises a generator ? gen and a classifier ? gen . It refines the initially estimated background scene (J init ), and generates the final light-effects-free output (J refine ). The details are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light-Effects Layer Guidance</head><p>We employ the estimated light-effects layer G to guide our training process, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The light-effects layer is taken as part of the input of our encoder-decoder network, and is modulated with the feature maps of the network at different scales. Specifically, we concatenate J init with the light-effects layer G, and then we input them to our network ? gen .</p><p>By resizing the light-effects layer, G, to fit the size of each feature map, and multiplying it with all the intermediate feature maps, our light-effects layer can guide our network to focus more on light-effects regions. <ref type="figure">Fig. 3b</ref> and <ref type="figure" target="#fig_3">Fig. 12</ref> show some results of our light-effects layers, demonstrating that our method can successfully separate white and multi-color light effects.</p><p>Input I Igray ?VGG(Igray) ?HF(Igray) J refine <ref type="figure">Fig. 6</ref>. Examples of feature map from VGG for I gray , and a HF feature map for I gray . As one can observe, these features are less affected by light effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light-Effects Suppression</head><p>Besides the light-effects layer, our suppression network is also guided by an attention mechanism <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. The basic idea is that, we input the light-effects and light-effects-free unpaired images into our encoderdecoder network. We then, use a domain classifier to judge whether the encoded features come from a certain domain, i.e., to judge whether the input is lighteffects or light-effects-free. Using this domain classification, the activated feature regions can form an attention map <ref type="bibr" target="#b48">[49]</ref> that is useful when guiding our network in suppressing light effects. More specifically, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, our network ? gen contains an auxiliary classifier ? gen . One of the inputs of the network is the concatenation of J init and G. Another input is a light-effects-free reference image, J ef , concatenated with a dummy all zero map G 0 , which of course has no light effects. Our classifier, ? gen , then performs domain classification based on the encoded features from f e = (G, J init ) or f ef = (G 0 , J ef ). To train the auxiliary classifier ? gen , we use the following attention loss:</p><p>L atten = ? E log(? gen (f e )) + E log(1 ? ? gen (f ef ))) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure and HF-Features Consistency Losses</head><p>To address hallucination/artefacts <ref type="bibr" target="#b30">[31]</ref>, and also to preserve background details, we employ two constraints: structure consistency, based on features obtained from the VGG network <ref type="bibr" target="#b16">[17]</ref>; and HF-features consistency, based on the HF features obtained from the guided filter <ref type="bibr" target="#b34">[35]</ref>. As shown in <ref type="figure">Fig. 5</ref>, to obtain the structure information and HF-features that are more robust to light effects, we adaptively fuse the RGB color channels of the input night image by applying: I gray (x) = c . Note that the range of I c (x) is [0,1], thus 0.5 is the median of the intensity range. Our weight has a low value if a pixel in a color channel is either low (under-exposed) or high (e.g., a light-effects pixel). We define ? = 0.2, which measures how well-exposed a pixel is. This makes the resulting grayscale image I gray less affected by light effects, as can be observed in <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref>. <ref type="table">Table 1</ref>. User study evaluation on the real night data, our method obtained the highest mean and lowest standard deviation (the max score is 7), showing our method is realistic, light-effects (L.E.) suppressed, and has good visibility. Having obtained I gray , we define our loss as follows:</p><formula xml:id="formula_6">L gray-feat = ? HF (J refine ) ? ? HF (I gray ) 1 + ? l VGG (J refine ) ? ? l VGG (I gray ) 1 ,</formula><p>where I gray = {I gray , I gray , I gray }. ? l VGG (.) represents the feature maps extracted from the l th layer of the VGG16 network (we set l = 15 in our experiments). ? HF (.) represent the high-frequency feature maps obtained from the guided filter. We concatenate these HF layers to get ? HF (I gray ). We use these features to better preserve the HF information in the generated refined background image J refine . <ref type="figure">Fig. 5</ref> shows our adaptive fusion scheme to obtain I gray from which we compute HF-features and VGG-features. <ref type="figure">Fig. 6</ref> shows that with our loss in place, the VGG and HF features of I gray preserve the structural information.</p><p>Adversarial and Identity Losses Our adversarial loss for the generator and discriminator ? dis uses its standard definition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>:</p><formula xml:id="formula_7">L adv = E log ? dis (J ef ) + E log 1 ? ? dis (J refine ) .<label>(7)</label></formula><p>While our light-effects suppression network is designed to refine J init by suppressing any remaining light effects, we also encourage it to output the same light-effects-free image when the input has no light-effects J ef . We achieve this by using the following identity loss function <ref type="bibr" target="#b50">[51]</ref>:</p><formula xml:id="formula_8">L iden = E ? gen (J ef ) ? J ef 1 .<label>(8)</label></formula><p>We multiply each loss function with its respective weight, we adjust ? gray-feat = 1, ? atten = 0.5 with the same scale, and employ the weights of ? adv = 1 and ? iden = 5 from <ref type="bibr" target="#b50">[51]</ref>. The HF layers use smoothing kernels K, with size given by k = 2 i , i = 2, 3, 4, ..., the regularization = 0.04, 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light-Effects Suppression on Night Data</head><p>The real night images used in our experiment are downloaded from the Internet and collected by ourselves. We use these images for our unpaired training since collecting the corresponding light-effects-free ground truth images is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Ours</head><p>Sharma <ref type="bibr" target="#b31">[32]</ref> EG <ref type="bibr" target="#b14">[15]</ref> Afifi <ref type="bibr" target="#b0">[1]</ref>  <ref type="figure">Fig. 7</ref>. Comparing light-effects suppression and dark regions enhancement results on the real night images. For the user study, we randomly selected 210 outputs (30 per method, seven methods) and presented them to the 12 participants in random order. We asked them to rank these methods from unrealistic (1) to realistic <ref type="bibr" target="#b6">(7)</ref>; light effects still present (1) to suppressed <ref type="bibr" target="#b6">(7)</ref>; poor visibility (1) to good visibility <ref type="bibr" target="#b6">(7)</ref>. <ref type="table">Table 1</ref> shows the user study results. <ref type="table" target="#tab_1">Table 2</ref> shows the quantitative results on the night data, where our method has the highest PSNR and SSIM scores. <ref type="figure">Fig. 7</ref> shows the qualitative results on real night images, which demonstrate the superiority of our results compared to the baseline methods. <ref type="figure">Fig. 8</ref> shows the evaluation on the Dark Zurich <ref type="bibr" target="#b29">[30]</ref> dataset. As can be observed, the light-effects suppression baseline <ref type="bibr" target="#b31">[32]</ref> suffers from hallucination/artefacts and cannot handle white light effects. In the supplementary material, we show the results of night Input Ours Sharma <ref type="bibr" target="#b31">[32]</ref> EG <ref type="bibr" target="#b14">[15]</ref> Afifi <ref type="bibr" target="#b0">[1]</ref>  <ref type="figure">Fig. 8</ref>. Comparing light-effects suppression and dark regions enhancement results on the real night image from Dark Zurich <ref type="bibr" target="#b29">[30]</ref> dataset.  dehazing baselines <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23]</ref>, which are too dark since they are not designed to enhance dark regions; while low-light image enhancement baselines <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7]</ref> wrongly intensify light effects, and thus degrade the visibility of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Light Enhancement</head><p>Besides night light-effects suppression, our method can boost the brightness of low light images with no light effects, by simply setting the light-effects layer to G 0 , which has no light-effects. For a fair comparison, we compare low-light boosting with image enhancement methods without considering the presence of light effects. We adopt the LOL dataset <ref type="bibr" target="#b6">[7]</ref> 6 , 485 training and 15 testing images, respectively. <ref type="table" target="#tab_2">Table 3</ref> shows quantitative results, where our method achieves better performance compared with the baseline methods in terms of PSNR, SSIM,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth Ours Sharma <ref type="bibr" target="#b31">[32]</ref> EG <ref type="bibr" target="#b14">[15]</ref>  <ref type="figure">Fig. 9</ref>. Low-light enhancement results on the LOL-test <ref type="bibr" target="#b6">[7]</ref>, LOL-Real <ref type="bibr" target="#b41">[42]</ref> datasets.  <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b37">38]</ref> ? ? SL;ZSL;UL Low-light Enhancement <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref> ? Mean Square Error (MSE) and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b44">[45]</ref>. We evaluate on LOL-Real [42] 7 , 100 testing images with more diversified scenes. We train our method on the LOL dataset and test on the LOL-Real test-split. The results are shown in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure">Fig. 9</ref>, showing the generality of our method. Our method achieves better performance compared with the baseline methods in terms of PSNR, SSIM. Baselines As shown in <ref type="table" target="#tab_4">Table 5</ref>, there is only one algorithm, i.e., <ref type="bibr">Sharma</ref>    <ref type="figure" target="#fig_3">Fig. 11</ref>. Ablation studies on the framework. 'w/o Trans' denotes our method without light-effects suppression. 'w/o Decomp.' denotes our method without layer decomposition. We can observe that our framework is important for night image enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Light-Effects Suppression and Dark Region Boosting</head><p>As shown in <ref type="figure" target="#fig_3">Fig. 10</ref>, jointly suppressing light-effects and then boosting dark regions are more effective than any other possibilities (namely, (b) the light-effects suppression alone, (c) light-effects suppression followed by boosting without jointly training them, (d) boosting alone, and (e) boosting followed by light effects suppression without joint training). If we suppress light effects first, then boost the intensity without the joint training, as shown in <ref type="figure" target="#fig_3">Fig. 10c</ref>, artefacts and remaining light effects are also enhanced. If we boost the intensity first, then suppress light effects without joint training, as shown in <ref type="figure" target="#fig_3">Fig. 10e</ref>, light effects cannot be effectively suppressed since the amplified light effects cause information and detail loss.</p><p>Ablation Studies <ref type="figure" target="#fig_3">Fig. 11, Fig. 12</ref> and <ref type="figure" target="#fig_3">Fig. 13</ref> show the effectiveness of our framework, light-effects layer guidance and structure and HF-features consistency losses used in our method, which clearly show that all the components are important for better performance. Decomposition + Suppression To show the effectiveness of our model-based unsupervised decomposition, we train our network without the decomposition module. We directly input the night images to the light-effects suppression network, thus there is no light-effects layer guidance and initial background results. Similarly, to show the effectiveness of our unsupervised light-effects suppression, we assume the initial background image J init generated by the decomposition Input Light-Effects Layer w/o G guidance w/ G guidance <ref type="figure" target="#fig_3">Fig. 12</ref>. Ablation studies on light-effects layer guidance, with light-effects layer G guidance, our light-effects suppression network can focus on light-effects regions, separate light effects more properly.</p><p>Input w/o L gray-feat w/ L gray-feat Input w/o L gray-feat w/ L gray-feat <ref type="figure" target="#fig_3">Fig. 13</ref>. Ablation studies on the structure and HF-features consistency losses L gray-feat , with L gray-feat , our method suppresses artefacts, and preserves details.</p><p>part is the final result without any refinement. Our final results are more effective in suppressing light effects and more natural in recovering the background. Light-Effects Layer Guidance We compare the results by our method with and without light-effects layer guidance. Instead of input (G, J init ), we input (G 0 , J init ) to the light-effects suppression network. That means there is no lighteffects layer G, we concatenate the initially estimated background scene with all zero maps G 0 . <ref type="figure" target="#fig_2">Figs. 2-4</ref> show the results of the light-effects layer. <ref type="figure" target="#fig_3">Fig. 12</ref> shows with light-effects layer guidance, our method can distinguish light-effects regions from background regions, focus on light-effects regions and properly suppress light effects (including white and multi-color light effects). Structure and HF-features Consistency Structure and HF-features consistency losses can suppress artefacts and restore missing details. <ref type="figure" target="#fig_3">Fig. 13</ref> compares the results by our method with and without this loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a method to suppress light effects, and at the same time, boost the intensity of dark regions, from a single night image. To achieve our goal, we cast the problem of light-effects suppression as an unsupervised decomposition problem. We proposed an integrated network consisting of layer decomposition and light-effects suppression networks. Our experiments show that our method outperforms the state-of-the-art visibility enhancement and light effects suppression methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 (Fig. 3 .</head><label>23</label><figDesc>i.e., the layer decomposition network). When non-linear images with non-linear gamma functions are Results of our model-based layer decomposition. (a) Input. (b) Lighteffects layer G. (c) Shading layer L and (d) Reflectance layer R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Overview of our unsupervised light-effects suppression network. The network comprises a generator ? gen and a classifier ? gen . The encoder block of our generator extracts feature maps from the input image layers. Our classifier ? gen is trained to learn the weights<ref type="bibr" target="#b48">[49]</ref> of the feature maps. ? gen performs domain classification based on two domains, i.e., the light-effects domain f e = (G, J init ) and the unpaired light-effects-free domain f ef = (G 0 , J ef ). Averaging the weighted feature maps generates an attention map that shows the network is focusing on the light-effects regions. As a result, the light effects are significantly suppressed in our output J refine .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 3</head><label>1</label><figDesc>(w c (x)I c (x)) where c ? (r, g, b) is a color channel, x is a pixel location, and the input image I = {I r , I g , I b }. The weight map for each color channel of the night image I c (x) is computed by w c (x) = exp ?(Ic(x)?0.5) 2 2? 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Experiments on the effectiveness of joint light-effects suppression and dark regions boosting: (a) input, (b) light-effects suppression, (c) light-effects suppression followed by boosting without joint training, (d) boosting, (e) boosting followed by light-effects suppression without joint training, (f) our joint training light-effects suppression and boosting. Input w/o Trans. w/o Decomp. Output</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 1.5 5.5 ? 1.3 3.7 ? 2.0 3.5 ? 1.6 3.1 ? 1.8 2.8 ? 1.5 6.1 ? 0.82.L.E. Supp.? 1.7 ? 0.8 3.1 ? 1.3 4.6 ? 1.4 3.9 ? 1.1 5.2 ? 1.2 3.0 ? 1.5 6.6 ? 0.7 3.Visibility? 3.1 ? 1.6 4.2 ? 1.5 4.7 ? 1.5 3.7 ? 1.1 3.8 ? 1.5 3.0 ? 1.4 6.4 ? 0.7</figDesc><table><row><cell cols="2">Three Aspects EG [15] Afifi [1] Yan [38] Zhang [44] Li [23] Sharma [32] Ours</cell></row><row><cell>1.Realism?</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative light-effects suppression comparison on the night data. In the table, UL = unsupervised learning, SL = supervised learning, SSL = semisupervised learning, ZSL = zero-shot learning, Opti = optimization method.</figDesc><table><row><cell>Learning</cell><cell>-</cell><cell>UL</cell><cell>ZSL</cell><cell>SL</cell><cell>SL</cell><cell>SSL</cell><cell>Opti</cell><cell>Opti</cell><cell>SSL</cell><cell>UL</cell></row><row><cell>Datasets</cell><cell cols="10">Metrics EG [15] ZD+ [19] RN [7] Afifi [1] Yan [38] Zhang [44] Li [23] Sharma [32] Ours</cell></row><row><cell>GTA5 [38]</cell><cell cols="2">PSNR? 10.94 SSIM? 0.31</cell><cell>21.13 0.68</cell><cell cols="2">7.79 15.47 0.23 0.53</cell><cell>26.99 0.85</cell><cell>20.92 0.65</cell><cell>21.02 0.64</cell><cell>8.14 0.29</cell><cell>29.79 0.88</cell></row><row><cell>Syn-light-effects [27]</cell><cell cols="2">PSNR? 7.38 SSIM? 0.17</cell><cell>7.84 0.20</cell><cell cols="2">6.39 11.31 0.16 0.35</cell><cell>14.88 0.23</cell><cell>16.30 0.38</cell><cell>14.66 0.37</cell><cell>14.00 0.37</cell><cell>16.95 0.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons on the LOL-test dataset<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell cols="2">Learning Method</cell><cell cols="2">LOL-test MSE ?10 3 ? PSNR? SSIM? LPIPS?</cell></row><row><cell>Opti</cell><cell>LIME [14]</cell><cell>-</cell><cell>16.760 0.560 0.350</cell></row><row><cell></cell><cell>RetinexNet [7]</cell><cell>1.651</cell><cell>16.774 0.462 0.474</cell></row><row><cell></cell><cell>KinD++ [47]</cell><cell>1.298</cell><cell>17.752 0.760 0.198</cell></row><row><cell>SL</cell><cell>Afifi [1]</cell><cell>4.520</cell><cell>15.300 0.560 0.392</cell></row><row><cell></cell><cell>RUAS [24]</cell><cell>3.920</cell><cell>18.230 0.720 0.350</cell></row><row><cell>ZSL</cell><cell>ZeroDCE [13]</cell><cell>3.282</cell><cell>14.861 0.589 0.335</cell></row><row><cell>SSL</cell><cell>DRBN [40]</cell><cell>2.359</cell><cell>15.125 0.472 0.316</cell></row><row><cell>UL</cell><cell>EnlightenGAN [15]</cell><cell>1.998</cell><cell>17.483 0.677 0.322</cell></row><row><cell>SSL</cell><cell>Sharma [32]</cell><cell>3.350</cell><cell>16.880 0.670 0.315</cell></row><row><cell>UL</cell><cell>Ours</cell><cell>1.070</cell><cell>21.521 0.763 0.235</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparisons on the LOL-Real dataset<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell>Learning</cell><cell>NA</cell><cell>Opti</cell><cell>Opti</cell><cell>Opti</cell><cell>ZSL</cell><cell>ZSL</cell><cell>ZSL</cell><cell>ZSL</cell><cell>SL</cell></row><row><cell>Method</cell><cell cols="9">Input JED [29] RRM [21] SRIE [9] RDIP [48] MIRNet [43] RRDNet [50] ZD [13] RUAS [24]</cell></row><row><cell>PSNR?</cell><cell>9.72</cell><cell>17.33</cell><cell>17.34</cell><cell>17.34</cell><cell>11.43</cell><cell>12.67</cell><cell>14.85</cell><cell>20.54</cell><cell>15.33</cell></row><row><cell>SSIM?</cell><cell>0.18</cell><cell>0.66</cell><cell>0.68</cell><cell>0.68</cell><cell>0.36</cell><cell>0.41</cell><cell>0.56</cell><cell>0.78</cell><cell>0.52</cell></row><row><cell>Learning</cell><cell>SL</cell><cell>SL</cell><cell>SL</cell><cell>SL</cell><cell>SL</cell><cell>SSL</cell><cell>UL</cell><cell>SSL</cell><cell>UL</cell></row><row><cell cols="7">Method LLNet [25] RN [7] DUPE [34] SICE [6] Afifi [1] DRBN [41]</cell><cell>EG [15]</cell><cell>Sharma [32]</cell><cell>Ours</cell></row><row><cell>PSNR?</cell><cell>17.56</cell><cell>15.47</cell><cell>13.27</cell><cell>19.40</cell><cell>16.38</cell><cell>19.66</cell><cell>18.23</cell><cell>18.34</cell><cell>25.51</cell></row><row><cell>SSIM?</cell><cell>0.54</cell><cell>0.56</cell><cell>0.45</cell><cell>0.69</cell><cell>0.53</cell><cell>0.76</cell><cell>0.61</cell><cell>0.64</cell><cell>0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Summary of comparisons between our method and existing night image enhancement methods. Our method can suppress light effects (including white light effects), preserve light source (L.S.) details, and boost dark regions simultaneously.</figDesc><table><row><cell cols="2">Learning Methods</cell><cell cols="2">Light Effects (L.E.) Suppression</cell><cell>Dark Regions Boosting</cell></row><row><cell></cell><cell></cell><cell cols="2">Normal L.E. White L.E. Details in L.S.</cell></row><row><cell>UL</cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell>SSL</cell><cell>Sharma and Tan [32]</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">Opti;SSL Night Dehazing</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and Tan<ref type="bibr" target="#b31">[32]</ref> that suppresses night light effects and boosts the dark regions simultaneously. Yet, the method cannot handle white light effects and suffers from hallucination/artefacts. Night dehazing methods can suppress glow, but are suboptimal to enhancing low-light regions. Low-light image enhancement methods are not designed to suppress night light effects and enhance low light regions simultaneously.Nevertheless for comprehensive comparisons, besides comparing with<ref type="bibr" target="#b31">[32]</ref>, we also compare our method with the state-of-the-art single-image low-light image enhancement methods: EnlightenGAN<ref type="bibr" target="#b14">[15]</ref>, Afifi et al.<ref type="bibr" target="#b0">[1]</ref>, etc. and the night dehazing methods: Yan et al.<ref type="bibr" target="#b37">[38]</ref>, Zhang et al.<ref type="bibr" target="#b43">[44]</ref>, Li et al.<ref type="bibr" target="#b22">[23]</ref>, etc. The codes of all the baseline methods are obtained from the authors. More baseline results are provided in the supplementary material.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our data and code is available at: https://github.com/jinyeying/ night-enhancement</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Following<ref type="bibr" target="#b31">[32]</ref>, light effects in this paper refer to glare, floodlight, etc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The LOL dataset link: https://daooshee.github.io/BMVC2018website/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">LOL-Real dataset link: https://github.com/flyywh/CVPR-2020-Semi-Low-Light/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD/2022-01-037[T]), and partially supported by MOE2019-T2-1-130. Wenhan Yang's research is supported by Wallenberg-NTU Presidential Postdoctoral Fellowship. Robby T. Tan's work is supported by MOE2019-T2-1-130.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning multi-scale photo exposure correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9157" to="9167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Night-time dehazing by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2256" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Day and night-time dehazing by local airlight estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6264" to="6275" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intrinsic images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A spatial processor model for object colour perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Buchsbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin institute</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2049" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y J L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast efficient algorithm for enhancement of low lighting video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2782" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">double-dip&quot;: Unsupervised image decomposition via coupled deep-image-priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11026" to="11035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recovering intrinsic images with a global sparsity prior on reflectance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="765" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1780" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enlightengan: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2340" to="2349" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dc-shadownet: Single-image hard and soft shadow removal using unsupervised domain-classifier guided network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5027" to="5036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">U-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10830</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to enhance low-light image via zero-reference deep curve estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-light image and video enhancement using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-revealing low-light image enhancement via robust retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2828" to="2841" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nighttime haze removal with glow and multiple light colors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10561" to="10570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new convolution kernel for atmospheric point spread function applied to computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Metari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deschenes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive histogram equalization and its variations. Computer vision, graphics, and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint enhancement and denoising method via sequential decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7374" to="7383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nighttime stereo depth estimation using joint translation-stereo learning: Light effects and uninformative regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="23" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nighttime visibility enhancement by increasing the dynamic range and suppression of light effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6849" to="6857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1838" to="1847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How to train neural networks for flare removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2239" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optical flow in dense foggy scenes using semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13259" to="13268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nighttime defogging using high-low frequency decomposition and grayscale-color networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="473" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-aligned video deraining with transmission-depth consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11966" to="11976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3063" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Band representation-based semisupervised low-light image enhancement: Bridging the gap between signal fidelity and perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3461" to="3473" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse gradient regularized deep retinex network for robust low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2072" to="2086" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning enriched features for real image restoration and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast haze removal for nighttime image using maximum reflectance prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7418" to="7426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4786" to="4794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Beyond brightening low-light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1013" to="1037" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Retinexdip: A unified deep framework for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-shot restoration of underexposed images via robust retinex decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
