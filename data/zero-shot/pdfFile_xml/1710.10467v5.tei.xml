<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENERALIZED END-TO-END LOSS FOR SPEAKER VERIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><forename type="middle">Quan</forename><surname>Li</surname></persName>
							<email>liwan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Papir</surname></persName>
							<email>papir@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GENERALIZED END-TO-END LOSS FOR SPEAKER VERIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speaker verification</term>
					<term>end-to-end loss</term>
					<term>Multi- Reader</term>
					<term>keyword detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based endto-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptationtraining a more accurate model that supports multiple keywords (i.e., "OK Google" and "Hey Google") as well as multiple dialects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Background</head><p>Speaker verification (SV) is the process of verifying whether an utterance belongs to a specific speaker, based on that speaker's known utterances (i.e., enrollment utterances), with applications such as Voice Match <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>.</p><p>Depending on the restrictions of the utterances used for enrollment and verification, speaker verification models usually fall into one of two categories: text-dependent speaker verification (TD-SV) and text-independent speaker verification (TI-SV). In TD-SV, the transcript of both enrollment and verification utterances is phonetially constrained, while in TI-SV, there are no lexicon constraints on the transcript of the enrollment or verification utterances, exposing a larger variability of phonemes and utterance durations <ref type="bibr" target="#b0">[3,</ref><ref type="bibr" target="#b1">4]</ref>. In this work, we focus on TI-SV and a particular subtask of TD-SV known as global password TD-SV, where the verification is based on a detected keyword, e.g. "OK Google" <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b4">6]</ref> In previous studies, i-vector based systems have been the dominating approach for both TD-SV and TI-SV applications <ref type="bibr" target="#b5">[7]</ref>. In recent years, more efforts have been focusing on using neural networks for speaker verification, while the most successful systems use end-to-end training <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. In such systems, the neural network output vectors are usually referred to as embedding vectors (also known as d-vectors). Similarly to as in the case of i-vectors, such embedding can then be used to represent utterances in a fix dimensional space, in which other, typically simpler, methods can be used to disambiguate among speakers.</p><p>More information of this work can be found at: https://google. github.io/speaker-id/publications/GE2E</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Tuple-Based End-to-End Loss</head><p>In our previous work <ref type="bibr" target="#b12">[13]</ref>, we proposed the tuple-based end-toend (TE2E) model, which simulates the two-stage process of runtime enrollment and verification during training. In our experiments, the TE2E model combined with LSTM <ref type="bibr" target="#b14">[14]</ref> achieved the best performance at the time. For each training step, a tuple of one evaluation utterance xj? and M enrollment utterances x km (for m = 1, . . . , M ) is fed into our LSTM network: {xj?, (x k1 , . . . , x kM )}, where x represents the features (log-melfilterbank energies) from a fixed-length segment, j and k represent the speakers of the utterances, and j may or may not equal k. The tuple includes a single utterance from speaker j and M different utterance from speaker k. We call a tuple positive if xj? and the M enrollment utterances are from the same speaker, i.e., j = k, and negative otherwise. We generate positive and negative tuples alternatively.</p><p>For each input tuple, we compute the L2 normalized response of the LSTM: {ej?, (e k1 , . . . , e kM )}. Here each e is an embedding vector of fixed dimension that results from the sequenceto-vector mapping defined by the LSTM. The centroid of tuple (e k1 , . . . , e kM ) represents the voiceprint built from M utterances, and is defined as follows:</p><formula xml:id="formula_0">c k = Em[e km ] = 1 M M m=1 e km .<label>(1)</label></formula><p>The similarity is defined using the cosine similarity function:</p><formula xml:id="formula_1">s = w ? cos(ej?, c k ) + b,<label>(2)</label></formula><p>with learnable w and b. The TE2E loss is finally defined as:</p><formula xml:id="formula_2">LT(ej?, c k ) = ?(j, k) 1 ? ?(s) + 1 ? ?(j, k) ?(s). (3)</formula><p>Here ?(x) = 1/(1 + e ?x ) is the standard sigmoid function and ?(j, k) equals 1 if j = k, otherwise equals to 0. The TE2E loss function encourages a larger value of s when k = j, and a smaller value of s when k = j. Consider the update for both positive and negative tuples -this loss function is very similar to the triplet loss in FaceNet <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Overview</head><p>In this paper, we introduce a generalization of our TE2E architecture. This new architecture constructs tuples from input sequences of various lengths in a more efficient way, leading to a significant boost of performance and training speed for both TD-SV and TI-SV. This paper is organized as follows: In Sec. 2.1 we give the definition of the GE2E loss; Sec. 2.2 is the theoretical justification for why GE2E updates the model parameters more effectively; Sec. 2.3 introduces a technique called "MultiReader", which enables us to train a single model that supports multiple keywords and languages; Finally, we present our experimental results in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GENERALIZED END-TO-END MODEL</head><p>Generalized end-to-end (GE2E) training is based on processing a large number of utterances at once, in the form of a batch that contains N speakers, and M utterances from each speaker in average, as is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training Method</head><p>We fetch N ? M utterances to build a batch. These utterances are from N different speakers, and each speaker has M utterances. Each feature vector xji (1 ? j ? N and 1 ? i ? M ) represents the features extracted from speaker j utterance i. Similar to our previous work <ref type="bibr" target="#b12">[13]</ref>, we feed the features extracted from each utterance xji into an LSTM network. A linear layer is connected to the last LSTM layer as an additional transformation of the last frame response of the network. We denote the output of the entire neural network as f (xji; w) where w represents all parameters of the neural network (including both, LSTM layers and the linear layer). The embedding vector (d-vector) is defined as the L2 normalization of the network output:</p><formula xml:id="formula_3">eji = f (xji; w) ||f (xji; w)||2 .<label>(4)</label></formula><p>Here eji represents the embedding vector of the jth speaker's ith utterance. The centroid of the embedding vectors from the jth speaker [ej1, . . . , ejM ] is defined as cj via Equation 1. The similarity matrix S ji,k is defined as the scaled cosine similarities between each embedding vector eji to all centroids c k (1 ? j, k ? N , and 1 ? i ? M ):</p><formula xml:id="formula_4">S ji,k = w ? cos(eji, c k ) + b,<label>(5)</label></formula><p>where w and b are learnable parameters. We constrain the weight to be positive w &gt; 0, because we want the similarity to be larger when cosine similarity is larger. The major difference between TE2E and GE2E is as follows:</p><p>? TE2E's similarity (Equation 2) is a scalar value that defines the similarity between embedding vector ej? and a single tuple centroid c k .</p><p>? GE2E builds a similarity matrix (Equation 5) that defines the similarities between each eji and all centroids c k . <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the whole process with features, embedding vectors, and similarity scores from different speakers, represented by different colors.</p><p>During the training, we want the embedding of each utterance to be similar to the centroid of all that speaker's embeddings, while at the same time, far from other speakers' centroids. As shown in the similarity matrix in <ref type="figure" target="#fig_0">Figure 1</ref>, we want the similarity values of colored areas to be large, and the values of gray areas to be small. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the same concept in a different way: we want the blue embedding vector to be close to its own speaker's centroid (blue triangle), and far from the others centroids (red and purple triangles), especially the closest one (red triangle). Given an embedding vector eji, all centroids c k , and the corresponding similarity matrix S ji,k , there are two ways to implement this concept:</p><p>Softmax We put a softmax on S ji,k for k = 1, . . . , N that makes the output equal to 1 iff k = j, otherwise makes the output equal to 0. Thus, the loss on each embedding vector eji could be defined as:</p><formula xml:id="formula_5">L(eji) = ?Sji,j + log N k=1 exp(S ji,k ).<label>(6)</label></formula><p>This loss function means that we push each embedding vector close to its centroid and pull it away from all other centroids.</p><p>Contrast The contrast loss is defined on positive pairs and most aggressive negative pairs, as:</p><formula xml:id="formula_6">L(eji) = 1 ? ?(Sji,j) + max 1?k?N k =j ?(S ji,k ),<label>(7)</label></formula><p>where ?(x) = 1/(1 + e ?x ) is the sigmoid function. For every utterance, exactly two components are added to the loss: (1) A positive component, which is associated with a positive match between the embedding vector and its true speaker's voiceprint (centroid).</p><p>(2) A hard negative component, which is associated with a negative match between the embedding vector and the voiceprint (centroid) with the highest similarity among all false speakers.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, the positive term corresponds to pushing eji (blue circle) towards cj (blue triangle). The negative term corresponds to pulling eji (blue circle) away from c k (red triangle), because c k is more similar to eji compared with c k . Thus, contrast loss allows us to focus on difficult pairs of embedding vector and negative centroid.</p><p>In our experiments, we find both implementations of GE2E loss are useful: contrast loss performs better for TD-SV, while softmax loss performs slightly better for TI-SV.</p><p>In addition, we observed that removing eji when computing the centroid of the true speaker makes training stable and helps avoid trivial solutions. So, while we still use Equation 1 when calculating negative similarity (i.e., k = j), we instead use Equation 8 when k = j:</p><formula xml:id="formula_7">c (?i) j = 1 M ? 1 M m=1 m =i ejm,<label>(8)</label></formula><formula xml:id="formula_8">S ji,k = w ? cos(eji, c (?i) j ) + b if k = j; w ? cos(eji, c k ) + b otherwise.<label>(9)</label></formula><p>Combining Equations 4, 6, 7 and 9, the final GE2E loss LG is the sum of all losses over the similarity matrix (1 ? j ? N , and 1 ? i ? M ):</p><p>LG(x; w) = LG(S) = j,i L(eji).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Comparison between TE2E and GE2E</head><p>Consider a single batch in GE2E loss update: we have N speakers, each with M utterances. Each single step update will push all N ?M embedding vectors toward their own centroids, and pull them away the other centroids. This mirrors what happens with all possible tuples in the TE2E loss function <ref type="bibr" target="#b12">[13]</ref> for each xji. Assume we randomly choose P utterances from speaker j when comparing speakers:</p><p>1. Positive tuples: {xji, (xj,i 1 , . . . , xj,i P )} for 1 ? ip ? M and p = 1, . . . , P . There are M P such positive tuples.  2. Negative tuples: {xji, (x k,i 1 , . . . , x k,i P )} for k = j and 1 ? ip ? M for p = 1, . . . , P . For each xji, we have to compare with all other N ? 1 centroids, where each set of those N ? 1 comparisons contains M P tuples. Each positive tuple is balanced with a negative tuple, thus the total number is the maximum number of positive and negative tuples times 2. So, the total number of tuples in TE2E loss is:</p><formula xml:id="formula_10">2 ? max M P , (N ? 1) M P ? 2(N ? 1).<label>(11)</label></formula><p>The lower bound of Equation 11 occurs when P = M . Thus, each update for xji in our GE2E loss is identical to at least 2(N ? 1) steps in our TE2E loss. The above analysis shows why GE2E updates models more efficiently than TE2E, which is consistent with our empirical observations: GE2E converges to a better model in shorter time (See Sec. 3 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training with MultiReader</head><p>Consider the following case: we care about the model application in a domain with a small dataset D1. At the same time, we have a larger dataset D2 in a similar, but not identical domain. We want to train a single model that performs well on dataset D1, with the help from D2:</p><formula xml:id="formula_11">L(D1, D2; w) = Ex?D 1 [L(x; w)] + ?Ex?D 2 [L(x; w)].<label>(12)</label></formula><p>This is similar to the regularization technique: in normal regularization, we use ?||w|| 2 2 to regularize the model. But here, we use Ex?D 2 [L(x; w)] for regularization. When dataset D1 does not have sufficient data, training the network on D1 can lead to overfitting. Requiring the network to also perform reasonably well on D2 helps to regularize the network.</p><p>This can be generalized to combine K different, possibly extremely unbalanced, data sources: D1, . . . , DK . We assign a weight ? k to each data source, indicating the importance of that data source. During training, in each step we fetch one batch/tuple of utterances from each data source, and compute the combined loss as:</p><formula xml:id="formula_12">L(D1, ? ? ? , DK ) = K k=1 ? k Ex k ?D k [L(x k ; w)], where each L(x k ; w)</formula><p>is the loss defined in Equation 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>In our experiments, the feature extraction process is the same as <ref type="bibr" target="#b4">[6]</ref>. The audio signals are first transformed into frames of width 25ms and step 10ms. Then we extract 40-dimension log-mel-filterbank energies as the features for each frame. For TD-SV applications, the same features are used for both keyword detection and speaker verification. The keyword detection system will only pass the frames containing the keyword into the speaker verification system. These frames form a fixed-length (usually 800ms) segment. For TI-SV applications, we usually extract random fixed-length segments after Voice Activity Detection (VAD), and use a sliding window approach for inference (discussed in Sec. 3.2) .</p><p>Our production system uses a 3-layer LSTM with projection <ref type="bibr" target="#b16">[16]</ref>. The embedding vector (d-vector) size is the same as the LSTM projection size. For TD-SV, we use 128 hidden nodes and the projection size is 64. For TI-SV, we use 768 hidden nodes with projection size 256. When training the GE2E model, each batch contains N = 64 speakers and M = 10 utterances per speaker. We train the network with SGD using initial learning rate 0.01, and decrease it by half every 30M steps. The L2-norm of gradient is clipped at 3 <ref type="bibr" target="#b17">[17]</ref>, and the gradient scale for projection node in LSTM is set to 0.5. Regarding the scaling factor (w, b) in loss function, we also observed that a good initial value is (w, b) = (10, ?5), and the smaller gradient scale of 0.01 on them helped to smooth convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text-Dependent Speaker Verification</head><p>Though existing voice assistants usually only support a single keyword, studies show that users prefer that multiple keywords are supported at the same time. For multi-user on Google Home, two keywords are supported simultaneously: "OK Google" and "Hey Google".</p><p>Enabling speaker verification on multiple keywords falls between TD-SV and TI-SV, since the transcript is neither constrained MultiReader has a great advantage compared to simpler approaches, e.g. directly mixing multiple data sources together: It handles the case when different data sources are unbalanced in size. In our case, we have two data sources for training: 1) An "OK Google" training set from anonymized user queries with ? 150 M utterances and ? 630 K speakers; 2) A mixed "OK/Hey Google" training set that is manually collected with ? 1.2 M utterances and ? 18 K speakers. The first dataset is larger than the second by a factor of 125 in the number of utterances and 35 in the number of speakers. For evaluation, we report the Equal Error Rate (EER) on four cases: enroll with either keyword, and verify on either keyword. All evaluation datasets are manually collected from 665 speakers with an average of 4.5 enrollment utterances and 10 evaluation utterances per speaker. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. As we can see, Mul-tiReader brings around 30% relative improvement on all four cases.</p><p>We also performed more comprehensive evaluations in a larger dataset collected from ? 83 K different speakers and environmental conditions, from both anonymized logs and manual collections. We use an average of 7.3 enrollment utterances and 5 evaluation utterances per speaker. <ref type="table">Table 2</ref> summarizes average EER for different loss functions trained with and without MultiReader setup. The baseline model is a single layer LSTM with 512 nodes and an embedding vector size of 128 <ref type="bibr" target="#b12">[13]</ref>. The second and third rows' model architecture is 3-layer LSTM. Comparing the 2nd and 3rd rows, we see that GE2E is about 10% better than TE2E. Similar to <ref type="table" target="#tab_0">Table 1</ref>, here we also see that the model performs significantly better with MultiReader. While not shown in the table, it is also worth noting that the GE2E model took about 60% less training time than TE2E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Text-Independent Speaker Verification</head><p>For TI-SV training, we divide training utterances into smaller segments, which we refer to as partial utterances. While we don't require all partial utterances to be of the same length, all partial utterances in the same batch must be of the same length. Thus, for each batch of data, we randomly choose a time length t within [lb, ub] = [140, 180] frames, and enforce that all partial utterances in that batch are of length t (as shown in <ref type="figure">Figure 3</ref>). During inference time, for every utterance we apply a sliding window of fixed size (lb + ub)/2 = 160 frames with 50% overlap. We compute the d-vector for each window. The final utterance-wise d-vector is generated by L2 normalizing the window-wise d-vectors, then taking the element-wise averge (as shown in <ref type="figure">Figure 4</ref>).</p><p>Our TI-SV models are trained on around 36M utterances from 18K speakers, which are extracted from anonymized logs. For evaluation, we use an additional 1000 speakers with in average 6.3 enrollment utterances and 7.2 evaluation utterances per speaker. <ref type="table" target="#tab_1">Table 3</ref> shows the performance comparison between different training loss functions. The first column is a softmax that predicts the speaker label for all speakers in the training data. The second column is a model trained with TE2E loss. The third column is a model trained with GE2E loss. As shown in the table, GE2E performs better than both softmax and TE2E. The EER performance improvement is larger than 10%. In addition, we also observed that GE2E training was about 3? faster than the other loss functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we proposed the generalized end-to-end (GE2E) loss function to train speaker verification models more efficiently. Both theoretical and experimental results verified the advantage of this novel loss function. We also introduced the MultiReader technique to combine different data sources, enabling our models to support multiple keywords and multiple languages. By combining these two techniques, we produced more accurate speaker verification models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>System overview. Different colors indicate utterances/embeddings from different speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>GE2E loss pushes the embedding towards the centroid of the true speaker, and away from the centroid of the most similar different speaker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Batch construction process for training TI-SV models. Sliding window used for TI-SV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>MultiReader vs. directly mixing multiple data sources.</figDesc><table><row><cell>Test data</cell><cell></cell><cell cols="3">Mixed data MultiReader</cell></row><row><cell>(Enroll ? Verify)</cell><cell></cell><cell cols="2">EER (%)</cell><cell>EER (%)</cell></row><row><cell cols="2">OK Google ? OK Google</cell><cell></cell><cell>1.16</cell><cell>0.82</cell></row><row><cell cols="2">OK Google ? Hey Google</cell><cell></cell><cell>4.47</cell><cell>2.99</cell></row><row><cell cols="2">Hey Google ? OK Google</cell><cell></cell><cell>3.30</cell><cell>2.30</cell></row><row><cell cols="2">Hey Google ? Hey Google</cell><cell></cell><cell>1.69</cell><cell>1.15</cell></row><row><cell cols="5">Table 2. Text-dependent speaker verification EER.</cell></row><row><cell>Model</cell><cell cols="2">Embed Loss</cell><cell>Multi</cell><cell>Average</cell></row><row><cell>Architecture</cell><cell>Size</cell><cell></cell><cell cols="2">Reader EER (%)</cell></row><row><cell>(512, ) [13]</cell><cell>128</cell><cell>TE2E</cell><cell>No</cell><cell>3.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Yes</cell><cell>2.78</cell></row><row><cell cols="2">(128, 64) ? 3 64</cell><cell>TE2E</cell><cell>No</cell><cell>3.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Yes</cell><cell>2.67</cell></row><row><cell cols="2">(128, 64) ? 3 64</cell><cell>GE2E</cell><cell>No</cell><cell>3.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Yes</cell><cell>2.38</cell></row><row><cell cols="5">to a single phrase, nor completely unconstrained. We solve this</cell></row><row><cell cols="5">problem using the MultiReader technique (Sec. 2.3).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Text-independent speaker verification EER (%).</figDesc><table><row><cell cols="3">Softmax TE2E [13] GE2E</cell></row><row><cell>4.06</cell><cell>4.13</cell><cell>3.55</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">REFERENCES</head><p>[1] Yury Pinsky, "Tomato, tomahto. Google Home now supports multiple users," https://www.blog.google/products/assistant/tomato-tomahtogoogle-home-now-supports-multiple-users, 2017.</p><p>[2] Mihai Matei, "Voice match will allow Google Home to recognize your voice," https://www.androidheadlines.com/2017/10/voice-matchwill-allow-google-home-to-recognize-your-voice.html, 2017.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An overview of textindependent speaker recognition: From features to supervectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomi</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="40" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tutorial on text-independent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinne</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Magrin-Chagnolleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Meignier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teva</forename><surname>Merlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Ortega-Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dijana</forename><surname>Petrovska-Delacr?taz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP journal on applied signal processing</title>
		<imprint>
			<biblScope unit="page" from="430" to="451" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smallfootprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="4087" to="4091" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4704" to="4708" />
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?da</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for small footprint text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="4052" to="4056" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Locallyconnected and convolutional neural networks for small footprint speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Yu-Hsin Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirk?</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep speaker: an end-to-end neural speaker embedding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1705.02304</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end attention based text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Shi-Xiong Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
		<idno>abs/1701.00562</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The IBM 2016 speaker recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Seyed Omid Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pelecanos</surname></persName>
		</author>
		<idno>abs/1602.07291</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5115" to="5119" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha?im</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?oise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1211.5063</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
