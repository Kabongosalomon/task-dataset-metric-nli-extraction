<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling Knowledge via Knowledge Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SmartMore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SmartMore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling Knowledge via Knowledge Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation transfers knowledge from the teacher network to the student one, with the goal of greatly improving the performance of the student network. Previous methods mostly focus on proposing feature transformation and loss functions between the same level's features to improve the effectiveness. We differently study the factor of connection path cross levels between teacher and student networks, and reveal its great importance. For the first time in knowledge distillation, cross-stage connection paths are proposed. Our new review mechanism is effective and structurally simple. Our finally designed nested and compact framework requires negligible computation overhead, and outperforms other methods on a variety of tasks. We apply our method to classification, object detection, and instance segmentation tasks. All of them witness significant student network performance improvement. Code is available at https://github.com/Jia-Research-Lab/ReviewKD</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolution neural networks (CNNs) have achieved remarkable success in a variety of computer vision tasks. However, the success of CNN is often accompanied with considerable computation and memory consumption, making it a challenging topic to apply to devices with limited resource. There have been techniques for training fast and compact neural networks, including designing new architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>, network pruning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>, quantization <ref type="bibr" target="#b12">[13]</ref> , and knowledge distillation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>We focus on knowledge distillation in this paper considering its practicality, efficiency, and most importantly the potential to be useful. It forms a very general line, applicable to almost all network architectures and can combine with many other strategies, such as network pruning and quantization <ref type="bibr" target="#b31">[32]</ref>, to further improve network design.</p><p>Knowledge distillation is first proposed in <ref type="bibr" target="#b8">[9]</ref>. The process is to train a small network (also known as the stu-dent) under the supervision of a larger network (a.k.a. the teacher). In <ref type="bibr" target="#b8">[9]</ref>, knowledge is distilled though the teacher's logit, which means the student is supervised by both ground truth labels and teacher's logits. Recently, effort has been made to improve distillation effectiveness. FitNet <ref type="bibr" target="#b24">[25]</ref> distilled knowledge though intermediate features. AT <ref type="bibr" target="#b37">[38]</ref> further optimized FitNet and used the attention map of features to deliver knowledge. PKT <ref type="bibr" target="#b22">[23]</ref> modeled knowledge of the teacher as a probability distribution while CRD <ref type="bibr" target="#b27">[28]</ref> used a contrastive objective to transfer knowledge. All these solutions focused on transformation and loss functions.</p><p>Our New Finding We in this paper tackle this challenging problem from a new perspective regarding the connection path between the teacher and student. To briefly understand our idea, we first show how previous work deals with these paths. As shown in <ref type="figure">Figure 1</ref>(a)-(c), all previous methods only use the-same-level information to guide the student. For example, when supervising the student's fourth-stage output, always the teacher's fourth-stage information is utilized. This procedure looks intuitive and easy to construct. But we intriguingly reveal that it is in fact a bottleneck in the whole knowledge distillation framework quick update of the structure surprisingly improves the whole-system performance consistently for many tasks.</p><p>We investigate the previously neglected importance of designing connection paths in knowledge distillation and propose a new effective framework accordingly. The key modification is to use low-level features in the teacher network to supervise deeper features for the student, which results in much improved overall performance.</p><p>We further analyze the network structure and discover the fact that the student high-level stage has the great capacity to learn useful information from the teacher's low-level features. More analysis is provided in Section 4.4. This process is analogous to human learning curve <ref type="bibr" target="#b34">[35]</ref> where a young kid can only comprehend a small portion of knowledge that is taught. During the course of grow-up, more and more knowledge from past years may be gradually understood and remembered as experience. <ref type="table">Teacher  Student  Teacher  Student  Teacher  Student   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1   stage4   stage3   stage2   stage1</ref> Cross levels information transfer Same level information transfer <ref type="figure">Figure 1</ref>. (a)-(c) Previous knowledge distillation frameworks. They only transfer knowledge within the same levels. (d) Our proposed "knowledge review" mechanism. We use multiple layers of the teacher to supervise one layer in the student. Thus, knowledge passing arises among different levels.</p><formula xml:id="formula_0">(a) (b) (c) (d) Teacher Student</formula><p>Our Knowledge Review Framework Based on these discoveries, we propose to use multi-level information of the teacher to guide one-level learning of the student network. Our novel pipeline is shown in <ref type="figure">Figure 1</ref>(d), which we call "knowledge review". The review mechanism is to use previous (shallower) features to guide the current feature. It means a student has to always check what has been studied before for refreshing understanding and context of "old knowledge". It is a common practice for our human study to connect knowledge taught at different stages during a period of time of study.</p><p>However, how to extract useful information from multilevel information from the teacher and how to transfer them to the student are open and challenge problems. To tackle them, we propose a residual learning framework to make the learning process stable and efficient. Further, a novel attention based fusion (ABF) module and a hierarchical context loss (HCL) function are designed to boost performance. Our proposed framework makes the student network much improve the effectiveness of learning.</p><p>By applying this idea, we achieve better performance in many computer vision tasks. Extensive experiments in Sec. 4 manifest the vast advantage of our proposed knowledge review strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Contributions</head><p>? We propose a new review mechanism in knowledge distillation, utlizing multi-level information of the teacher to guide one-level learning of the student net.</p><p>? We propose a residual learning framework to better realize the learning process of the review mechanism.</p><p>? To further improve the knowledge review mechanism, we propose an attentation based fusion (ABF) module and a hierarchical context loss (HCL) function.</p><p>? We achieve state-of-the-art performance of many compact models in multiple computer vision tasks by applying our distillation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Knowledge distillation concept was proposed in <ref type="bibr" target="#b8">[9]</ref>, where the student network learns from both the groundtruth labels and the soft-labels provided by the teacher. Fit-Net <ref type="bibr" target="#b24">[25]</ref> distilled knowledge through one stage intermediate feature. The idea in FitNet is simple, where the student network feature is transferred to the same shape of the teacher though convolution layers. L 2 distance is used to measure the distance between them.</p><p>Many methods follow FitNet and use one-stage feature to distill knowledge. PKT <ref type="bibr" target="#b22">[23]</ref> modeled knowledge of the teacher as a probability distribution and used KL divergence to measure the distance. RKD <ref type="bibr" target="#b21">[22]</ref> used multiple example relation to guide learning of the student. CRD <ref type="bibr" target="#b27">[28]</ref> combined contrastive learning and knowledge distillation, and used a contrastive objective to transfer knowledge.</p><p>There are also methods using multi-stage information to transfer knowledge. AT <ref type="bibr" target="#b37">[38]</ref> used multiple layer attention maps to transfer knowledge. FSP <ref type="bibr" target="#b35">[36]</ref> generated FSP matrix from layer feature and used the matrix to guide the student. SP <ref type="bibr" target="#b28">[29]</ref> further improved AT. Instead of single input information, SP uses the similarity between examples to guide the student. OFD <ref type="bibr" target="#b7">[8]</ref> contained a new distance function to distill major information between the teacher and student using marginal ReLU.</p><p>All previous methods do not discuss the possibility to "review knowledge", which, however, is found in our work very effective to quickly improve system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>We first formalize the knowledge distillation process and the review mechanism. Then we propose a novel framework and introduce attention based fusion module and hierarchical context loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review Mechanism</head><p>Given an input image X and student network S, we let Y s = S(X) represent the output logit of the student. S can be separated into different parts (S 1 , S 2 , ? ? ? , S n , S c ), where S c is the classifier and S 1 , ? ? ? , S n are different stages separated by downsample layers. Thus, the process of generating output Y s can be denoted as</p><formula xml:id="formula_1">Y s = S c ? S n ? ? ? ? ? S 1 (X).<label>(1)</label></formula><p>We refer to "?" as nesting of functions where g ? f (x) = g(f (x)). Y s is the output of student, and intermidate features are (F 1 s , ? ? ? , F n s ). The ith feature is calculated as</p><formula xml:id="formula_2">F i s = S i ? ? ? ? ? S 1 (X).<label>(2)</label></formula><p>For the teacher network T , the process is almost the same and we omit the details. Following previous notations, transform between features transform for loss fusion module loss function single-layer knowledge distillation can be represented as</p><formula xml:id="formula_3">L2 Distance L2 Distance L2 Distance L2 Distance Student Teacher (a) L Student Teacher (b) L2 Distance L2 Distance L2 Distance</formula><formula xml:id="formula_4">L SKD = D M i s (F i s ), M i t (F i t ) ,<label>(3)</label></formula><p>where M is transformation that transfers the feature to target representation of attention maps <ref type="bibr" target="#b37">[38]</ref> or factors <ref type="bibr" target="#b13">[14]</ref>. D is the distance function that measures the gap between the student teacher. Similarly, multiple-layers knowledge distillation is written as</p><formula xml:id="formula_5">L M KD = i?I D M i s (F i s ), M i t (F i t ) ,<label>(4)</label></formula><p>where I stores the layers of features to transfer knowledge. Our review mechanism is to use previous features to guide the current feature. The single-layer knowledge dis-tillation with the review mechanism is formalized as</p><formula xml:id="formula_6">L SKD R = i j=1 D M i,j s (F i s ), M j,i t (F j t ) .<label>(5)</label></formula><p>Although at the first glance it shares some similarity with multiple-layers knowledge distillation, it is in fact fundamentally different. Here feature of the student is fixed to F i s , and we use the teacher's first i levels of features to guide F i s . The review mechanism and multiple-layers distillation are complementary concepts. When combining the review mechanism with multiple-layers knowledge distillation, the loss function becomes In our experiments, the L M KD R loss is simply added alone with original losses during the training process, and the inference is exactly the same as the original model. So our method is totally cost-free at test time. We use factor ? to balance the distillation loss and original losses. Taking the classification task as an example, the whole loss function is defined as</p><formula xml:id="formula_7">L M KD R = i?I ? ? i j=1 D M i,j s (F i s ), M j,i t (F j t ) ? ? . (6) L2 Distance L2 Distance L2 Distance L2 Distance ABF 1 ? 1 ? ? + ABF (a) L2 Distance L2 Distance L2 Distance L2 Distance ? 2 ? 2 ? 2 ? 2</formula><formula xml:id="formula_8">L = L CE + ?L M KD R .<label>(7)</label></formula><p>In our proposed review mechanism, we only use shallower features of the teacher to supervise deeper features of the student. We found that the opposite brings marginal benefit and wastes many resources instead. The intuitive explanation is that deeper and more abstracted features are too complicated for early-stage learning. More analysis is in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Learning Framework</head><p>Following previous work, we first design a straightforward framework, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a). The transformation M i,j s is simply composed of convolution layers and nearest interpolation layers to transfer the ith feature of the student to match the size of teacher's jth feature. We do not transform teacher features F t . The student feature is transformed into the same size as the teacher features. <ref type="figure" target="#fig_0">Figure 2</ref>(b) shows directly applying the idea to multiplelayer distillation with all-stage features distilled. However, this strategy is not optimal because of the huge information difference between stages. Also, it yields a complicated process where all features are used. For instance, a network with n stages needs to calculate n(n + 1)/2 pairs of features regarding the loss functions, which makes the learning process cumbersome and costs many resources.</p><p>To make the procedure more feasible and elegant, we reformulate Eq. (6) for <ref type="figure" target="#fig_0">Figure 2</ref></p><formula xml:id="formula_9">(b) as L M KD R = n i=1 ? ? i j=1 D F i s , F j t ? ? .<label>(8)</label></formula><p>where the transform of features is omited for simplicity. We now switch the order of two summations of i and j as</p><formula xml:id="formula_10">L M KD R = n j=1 ? ? n i=j D F i s , F j t ? ? .<label>(9)</label></formula><p>When j is fixed, Eq. (9) accumulates the distance between the teacher feature F j t and student features F j s -F n s . With fusion of features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b15">16]</ref>, we approximate the summation of distance as the distance of fused features. It leads to</p><formula xml:id="formula_11">n i=j D F i s , F j t ? D U(F j s , ? ? ? , F n s ), F j t ,<label>(10)</label></formula><p>where U is a module to fuse features. This approximation is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(c) where the structure is more effective now. But the calculation of fusion can be further optimized in a progressively manner as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(d) for higher efficiency. Fusion of F j s , ? ? ? , F n s is calculated by combination of F j s and U(F j+1 s , ? ? ? , F n s ), where the fusion operation is recursively defined as U(?, ?), applied to consecutive feature maps. Denoting F j+1,n s as fusion of features from F j+1 s to F n s , the loss is written as</p><formula xml:id="formula_12">L M KD R = D(F n s , F n t ) + 1 j=n?1 D U(F j s , F j+1,n s ), F j t ,<label>(11)</label></formula><p>Here we loop from n ? 1 down to 1 to make use of F j+1,n s . F n,n s = M n,n s (F n s ). The detailed structure is shown in <ref type="figure" target="#fig_0">Figure 2(d)</ref>, where ABF and HCL are fusion module and loss function designed for this structure, respectively. Their details are discussed in Section 3.3.</p><p>The structure in <ref type="figure" target="#fig_0">Figure 2(d)</ref> is elegant and eases the distillation process with utilizing the concept of residual learning. For instance, the stage-4's feature of the student is aggregated with stage-3's feature of the student to mimic the stage-3's feature of the teacher. Therefore, stage-4's feature of the student learns the residual of stage-3's feature between the student and teacher. The residual information is very likely to be the key factor that the teacher yields higher-quality results.</p><p>This residual learning process is more stable and effective than directly letting high-level features of the student learned from low-level features of the teacher. With the residual learning framework, the high-level features of the student can better extract useful information progressively. Further, using Eq. (11), we eliminate the summation and reduce the total complexity to n pairs of distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ABF and HCL</head><p>There are two key components in <ref type="figure" target="#fig_0">Figure 2(d)</ref>. They are attention based fusion (ABF) and hierarchical context loss (HCL). We explain them here.</p><p>ABF module utilizes the insight of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a). The higher level features are first resized to the same shape as the lower level features. Then two features from different levels are concatenated together to generate two H ? W attention maps. These maps are multiplied with two features, respectively. Finally, the two features are added to generate the final output.</p><p>The ABF module can generate different attention maps according to input features. So the two feature maps can be dynamically aggregated. The adaptive sum is better than direct sum because the two feature maps are from different stages of the network and their information is diverse. The low-and high-level features may focus on different partitions. The attention maps can aggregate them more reasonably. More experimental results are included in Section 4.4.</p><p>The detail of HCL is shown in <ref type="figure" target="#fig_1">Figure 3(b)</ref>. Usually, we use L 2 distance as the loss function between the two feature maps. The L 2 distance is effective to transfer information between features from the same level. But in our framework, different levels' information is aggregated together to learn from the teacher. The trivial global L 2 distance is not powerful enough to transfer compound levels' information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>Teacher Student KD <ref type="bibr" target="#b8">[9]</ref> AT <ref type="bibr" target="#b37">[38]</ref> OFD <ref type="bibr" target="#b7">[8]</ref>  Inspired by <ref type="bibr" target="#b40">[41]</ref>, we propose HCL, utilizing spatial pyramid pooling, to separate the transfer of knowledge into different levels' context information. In this way, the information is better distilled in different abstract levels. The structure is very simple: we first extract different levels' knowledge from the feature using spatial pyramid pooling, and then use L 2 distance to distill between them respectively. Despite the simple structure, HCL is suitable for our framework. More experimental results are shown in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on various tasks. First, we compare our method with other knowledge distillation ones regarding classification. We experiment with different settings varying architecture and datasets. Also, we apply our method to the object detection and instance segmentation tasks. Our method also improves the baseline model by large margins consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification</head><p>Datasets (1) CIFAR-100 contains 50K training images with 0.5K images per class and 10K test images. (2) Im-ageNet <ref type="bibr" target="#b2">[3]</ref> is the most challenging dataset for classification, which provides 1.2 million images for training and 50K images for validation over 1,000 classes. Implementation Details On CIFAR-100 dataset, we experiment with different representative network architectures, including VGG <ref type="bibr" target="#b26">[27]</ref>, ResNet <ref type="bibr" target="#b6">[7]</ref>, WideResNet <ref type="bibr" target="#b36">[37]</ref>, MobileNet <ref type="bibr" target="#b25">[26]</ref>, and ShuffleNet <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b20">21]</ref>. We use the same training setting of <ref type="bibr" target="#b27">[28]</ref>, except for linearly scaling up the initial learning rate and setting batch size following <ref type="bibr" target="#b4">[5]</ref>.</p><p>Specifically, we train all models for 240 epochs with learning rate decayed by 0.1 for every 30 epochs after the first 150 epochs. We initialize the learning rate to 0.02 for MobileNet and ShuffleNet, and 0.1 for other models. The batch size is 128 for all models. We train all models for three times and report the mean accuracy. For fairness, previous method results are either reported in previous papers (when the training setting is the same as ours) or obtained using author released codes with our training setting.</p><p>On ImageNet, we use the standard training process that trains the model for 100 epochs and decays the learning rate for every 30 epochs. We initialize learning rate to 0.1 and set batch size to 256. <ref type="table">Table 1</ref> summarizes results on CIFAR-100 with the teacher and student having architectures of the same style. We separate previous methods in different groups according to the features they use. KD is the only method that uses logits. Methods in FitNet group use single-layer information, and methods in AT group use multiple-layer information. Our method employs multilayer feature with the review mechanism. It outperforms all previous methods on all architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on CIFAR-100</head><p>We also experiment with the setting that the student and teacher have different architectural styles, and show results in <ref type="table">Table 2</ref>. Method of OFD <ref type="bibr" target="#b7">[8]</ref> and ours use multiple layers for distillation. They outperform those with distillation from the last layer, manifesting that our knowledge review mechanism successfully relaxes previously emphasized intermediate-or last-layer distillation condition <ref type="bibr" target="#b27">[28]</ref>.</p><p>Results on ImageNet The number of images in CIFAR-100 is small. So we also conduct experiments on ImageNet to verify the scalability of our method. We experiment with two settings of distillation from ResNet50 to MobileNet <ref type="bibr" target="#b10">[11]</ref>, and from ResNet34 to ResNet18 respectively. Our method, again, outperforms all other methods, as reported in <ref type="table">Table 3</ref>. Setting (a) is challenging due to architecture difference. But the advantage of our method is consistently prominent. On setting (b), gap between the student and teacher is already reduced to a very small value 2.14 by previous best method. We further reduce it to 1.70, achieving 20% relative performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection</head><p>We also apply our method to other computer vision tasks. On object detection, like the procedure for the classification task, we distill between the student and teacher's backbone output features. More details are presented in the supplementary file. We use the representative COCO2017 dataset <ref type="bibr" target="#b17">[18]</ref> to evaluate our method and take the most popular opensource report Detectron2 <ref type="bibr" target="#b32">[33]</ref> as our strong baseline. We use the best pre-trained model provided by Detrctron2 as teacher. Student models are trained using the standard training policy following tradition <ref type="bibr" target="#b30">[31]</ref>. All performance is eval- uated on COCO2017 validation set. We conduct experiments on both two-and one-stage methods.</p><p>Since only a few methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref> are claimed workable for detection, we reproduce the popular ones <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> and the latest one <ref type="bibr" target="#b30">[31]</ref>. The comparison is presented in <ref type="table">Table 4</ref>. We note that knowledge distillation methods, such as KD and FitNet, also improve the performance of detection. But the gain is limited. FGFI <ref type="bibr" target="#b30">[31]</ref> is directly designed for detection, and works better than other methods on this task. Still, our method outperforms it by a large margin.</p><p>We also vary experimental setting to check the generality. On the two-stage method FasterRCNN <ref type="bibr">[</ref>  <ref type="table">Table 6</ref>. Results of knowledge distillation between different stages of the teacher and student. The student's baseline result is 69.1. We use red color to mark numbers lower than baseline and blue for those higher than baseline. It is clear that using the lower level information of the teacher to supervise the deeper stage of the student is helpful.</p><p>ResNet50 and MobileNetV2 still promotes the baseline from 29.47 to 33.71. On one-stage detector RetinaNet <ref type="bibr" target="#b16">[17]</ref>, the gap between student and teacher is small, our method also improves the mAP by 2.33. The success on challenging object detection tasks demonstrates the generality and effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Instance Segmentation</head><p>In this section, we apply our method to the even more challenging instance segmentation task. As far as we know, this is the first time for the knowledge distillation methods to apply to instance segmentation. We also use the strong baseline provided by Detectron2 <ref type="bibr" target="#b32">[33]</ref>. We take Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> as our models and distill between different backbone architectures. The models are trained on the COCO2017 training set and are evaluated on the validation set. The results are shown in <ref type="table">Table 5</ref>.</p><p>Our method also improves the performance of instance segmentation tasks notably. For distillation between architectures of the same style, we boost the performance of ResNet18 and ResNet50 by 2.37 and 1.74, and reduce the gap between the teacher and student by 32% and 51% relatively. Even for the distillation on architectures of different styles, we better MobileNetV2 by 3. <ref type="bibr" target="#b18">19</ref>.</p><p>The fact that our method performs decently on all image classification, object detection, and instance segmentation tasks and accomplishes all SOTA results, manifest the remarkable efficacy and applicability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">More Analysis</head><p>Knowledge Distillation across Stages We analyze the effectiveness of knowledge transfer across stages. We use ResNet20 as the student and ResNet56 as the teacher on CIFAR-100 dataset. There are four stages in ResNet20 and ResNet56. We choose the different stages in the student and vary stages in the teacher to supervise them. The results are summarized in <ref type="table">Table 6</ref> . RM RLF ABF HCL Accuracy (Variance) 74.3 (5e-2) 75.2 (6e-2) 75.6 (6e-2) 76.0 (6e-2) 75.8 (5e-2) 76.2 (4e-2) <ref type="table">Table 7</ref>. RM: The proposed review mechanism (Section 3.1). RLF: Residual learning frame work (Section 3.2). ABF: Attentation based fusion module (Section 3.3). HCL: Hierarchical context loss function (Section 3.3).</p><p>These results conclude that distilling student with the same stage information from the teacher is the best solution. This is in accordance with our intuition. Further, it is intriguing to observe that information from lower layers is also helpful. But distilling from teacher's higher levels adversely affects training of the student.</p><p>It indicates that deeper stages of the student are capable of learning useful information from lower stages of the teacher. In the other way around, deeper and more abstracted features from teacher are too complicated for earlystage of the student. This is consistent with our understanding and our proposed review mechanism, which uses shallow stages of the teacher to supervise deeper stages of the student.</p><p>Ablation Study Ablation experiments are conducted, in which the ablation components are added one-by-one to measure their effect. The results are summarized in <ref type="table">Table 7</ref> with accuracy and variance. We use WRN16-2 as the student and WRN40-2 as the teacher on CIFAR100 dataset. The baseline is trained with L 2 distance between the same level's features of the student and the teacher.</p><p>With our proposed review mechanism, the result is improved over the baseline, as shown in the second line, which uses the trival structure as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b). When we further refine the structure with the residual learning framework, the student yields larger gains. The attention based fusion module and hierarchical context loss function also provide great improvement when utilized separately. And when we aggregate them together, the best results are obtained. It is surprising that they are even better than the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have studied knowledge distillation from a new perspective and accordingly proposed the review mechanism, which uses multiple layers in the teacher to supervise one layer in the student. Our method achieves significant improvement consistently on all classification, object detection and instance segmentation tasks, compared with all previous SOTA. We only use output of stages, and already accomplish decent results in general.</p><p>For future work, we will also employ features inside a stage. Also, other loss functions will be investigated in our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Architecture for supervising one layer of the student according to the review mechanism. (b) Direct generalization from one layer to muliple ones. The process is straightforward but costly. (c) The architecture in (b) is optimized with fusion modules to obtain a compact framework. (d) We further improved the procudure in a progressive manner and utlize redisual learning as our final architecture. Structures of ABF and HCL are inFigure 3. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) Architecture of ABF. Different levels' features of the student are aggregated together with attention maps. (b) Architecture of HCL. The student and teacher's features are pyramid pooled to extract different context information to distill.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Results on object detection. We use AP on different settings to evaluate results. R101 represents using ResNet101 as backbone, and MV2 stands for MobileNetV2. Instance segmentation results. R101 and MV2 stand for ResNet101 and MobileNetV2.</figDesc><table><row><cell></cell><cell>Method</cell><cell>mAP</cell><cell>AP50</cell><cell>AP75</cell><cell>APl</cell><cell>APm</cell><cell>APs</cell></row><row><cell cols="3">Teacher Faster R-CNN w/ R101-FPN 42.04</cell><cell>62.48</cell><cell>45.88</cell><cell>54.60</cell><cell>45.55</cell><cell>25.22</cell></row><row><cell cols="2">Student Faster R-CNN w/ R18-FPN</cell><cell>33.26</cell><cell>53.61</cell><cell>35.26</cell><cell>43.16</cell><cell>35.68</cell><cell>18.96</cell></row><row><cell></cell><cell>w/ KD [9]</cell><cell>33.97 (+0.61)</cell><cell>54.66</cell><cell>36.62</cell><cell>44.14</cell><cell>36.67</cell><cell>18.71</cell></row><row><cell></cell><cell>w/ FitNet [25]</cell><cell>34.13 (+0.87)</cell><cell>54.16</cell><cell>36.71</cell><cell>44.69</cell><cell>36.50</cell><cell>18.88</cell></row><row><cell></cell><cell>w/ FGFI [31]</cell><cell>35.44 (+2.18)</cell><cell>55.51</cell><cell>38.17</cell><cell>47.34</cell><cell>38.29</cell><cell>19.04</cell></row><row><cell></cell><cell>w/ Our Method</cell><cell>36.75 (+3.49)</cell><cell>56.72</cell><cell>34.00</cell><cell>49.58</cell><cell>39.51</cell><cell>19.42</cell></row><row><cell cols="3">Teacher Faster R-CNN w/ R101-FPN 42.04</cell><cell>62.48</cell><cell>45.88</cell><cell>54.60</cell><cell>45.55</cell><cell>25.22</cell></row><row><cell cols="2">Student Faster R-CNN w/ R50-FPN</cell><cell>37.93</cell><cell>58.84</cell><cell>41.05</cell><cell>49.10</cell><cell>41.14</cell><cell>22.44</cell></row><row><cell></cell><cell>w/ KD [9]</cell><cell>38.35 (+0.42)</cell><cell>59.41</cell><cell>41.71</cell><cell>49.48</cell><cell>41.80</cell><cell>22.73</cell></row><row><cell></cell><cell>w/ FitNet [25]</cell><cell>38.76 (+0.83)</cell><cell>59.62</cell><cell>41.80</cell><cell>50.70</cell><cell>42.20</cell><cell>22.32</cell></row><row><cell></cell><cell>w/ FGFI [31]</cell><cell>39.44 (+1.51)</cell><cell>60.27</cell><cell>43.04</cell><cell>51.97</cell><cell>42.51</cell><cell>22.89</cell></row><row><cell></cell><cell>w/ Our Method</cell><cell>40.36 (+2.43)</cell><cell>60.97</cell><cell>44.08</cell><cell>52.87</cell><cell>43.81</cell><cell>23.60</cell></row><row><cell cols="2">Teacher Faster R-CNN w/ R50-FPN</cell><cell>40.22</cell><cell>61.02</cell><cell>43.81</cell><cell>51.98</cell><cell>43.53</cell><cell>24.16</cell></row><row><cell cols="3">Student Faster R-CNN w/ MV2-FPN 29.47</cell><cell>48.87</cell><cell>30.90</cell><cell>38.86</cell><cell>30.77</cell><cell>16.33</cell></row><row><cell></cell><cell>w/ KD [9]</cell><cell>30.13 (+0.66)</cell><cell>50.28</cell><cell>31.35</cell><cell>39.56</cell><cell>31.91</cell><cell>16.69</cell></row><row><cell></cell><cell>w/ FitNet [25]</cell><cell>30.20 (+0.73)</cell><cell>49.80</cell><cell>31.69</cell><cell>39.69</cell><cell>31.64</cell><cell>16.39</cell></row><row><cell></cell><cell>w/ FGFI [31]</cell><cell>31.16 (+1.69)</cell><cell>50.68</cell><cell>32.92</cell><cell>42.12</cell><cell>32.63</cell><cell>16.73</cell></row><row><cell></cell><cell>w/ Our Method</cell><cell>33.71 (+4.24)</cell><cell>53.15</cell><cell>36.13</cell><cell>46.47</cell><cell>35.81</cell><cell>16.77</cell></row><row><cell cols="2">Teacher RetinaNet101</cell><cell>40.40</cell><cell>60.25</cell><cell>43.19</cell><cell>52.18</cell><cell>44.34</cell><cell>24.03</cell></row><row><cell cols="2">Student RetinaNet50</cell><cell>36.15</cell><cell>56.03</cell><cell>38.73</cell><cell>46.95</cell><cell>40.25</cell><cell>21.37</cell></row><row><cell></cell><cell>w/ KD [9]</cell><cell>36.76 (+0.61)</cell><cell>56.60</cell><cell>39.40</cell><cell>48.17</cell><cell>40.56</cell><cell>21.87</cell></row><row><cell></cell><cell>w/ FitNet [25]</cell><cell>36.30 (+0.15)</cell><cell>55.95</cell><cell>38.95</cell><cell>47.14</cell><cell>40.32</cell><cell>20.10</cell></row><row><cell></cell><cell>w/ FGFI [31]</cell><cell>37.29 (+1.14)</cell><cell>57.13</cell><cell>40.04</cell><cell>49.71</cell><cell>41.47</cell><cell>21.01</cell></row><row><cell></cell><cell>w/ Our Method</cell><cell>38.48 (+2.33)</cell><cell>58.22</cell><cell>41.46</cell><cell>51.15</cell><cell>42.72</cell><cell>22.67</cell></row><row><cell></cell><cell>Method</cell><cell>mAP</cell><cell>AP50</cell><cell>AP75</cell><cell>APl</cell><cell>APm</cell><cell>APs</cell></row><row><cell cols="3">Teacher Mask R-CNN w/ R101-FPN 38.63</cell><cell>60.45</cell><cell>41.28</cell><cell>55.29</cell><cell>41.33</cell><cell>19.48</cell></row><row><cell>Student</cell><cell>Mask R-CNN w/ R18-FPN + Our Method</cell><cell>31.25 33.62 (+2.37)</cell><cell>51.07 53.91</cell><cell>33.10 35.96</cell><cell>45.53 50.30</cell><cell>32.80 35.31</cell><cell>14.18 15.03</cell></row><row><cell cols="3">Teacher Mask R-CNN w/ R101-FPN 38.63</cell><cell>60.45</cell><cell>41.28</cell><cell>55.29</cell><cell>41.33</cell><cell>19.48</cell></row><row><cell>Student</cell><cell>Mask R-CNN w/ R50-FPN + Our Method</cell><cell>35.24 36.98 (+1.74)</cell><cell>56.32 58.13</cell><cell>37.49 39.60</cell><cell>50.34 53.19</cell><cell>37.71 39.57</cell><cell>17.16 17.54</cell></row><row><cell cols="2">Teacher Mask R-CNN w/ R50-FPN</cell><cell>37.17</cell><cell>58.60</cell><cell>39.88</cell><cell>53.30</cell><cell>39.49</cell><cell>18.63</cell></row><row><cell>Student</cell><cell cols="2">Mask R-CNN w/ MV2-FPN 28.37 + Our Method 31.56 (+3.19)</cell><cell>47.19 50.70</cell><cell>29.95 33.44</cell><cell>41.70 47.39</cell><cell>29.01 32.44</cell><cell>12.09 12.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b23">24]</ref>, we change backbone architectures. The knowledge distillation between architectures of the same style boosts mAP of ResNet18 and ResNet50 by 3.49 and 2.43 respectively. They are significant numbers. The distillation between</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Teacher Stage</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>Student Stage</cell><cell cols="4">1 69.5 69.0 68.2 66.3 2 69.6 69.6 61.4 61.1 3 69.2 69.8 71.0 50.4 4 69.2 69.3 70.3 70.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and practical neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<editor>Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol? Cesa-Bianchi, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>NIPS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Shufflenet V2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Probabilistic knowledge transfer for deep representation learning. CoRR, abs/1803.10837</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tefas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Quantization mimic: Towards very tiny CNN for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Autoprune: Automatic network pruning by regularizing auxiliary parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zigeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanguthevar</forename><surname>Rajasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The learning curve: Historical review and comprehensive survey. Decision sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louis E Yelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Hoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
