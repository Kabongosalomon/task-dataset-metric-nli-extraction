<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Zheng</surname></persName>
							<email>chaodazheng@link.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
							<email>xuyan1@link.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Zhang</surname></persName>
							<email>haimingzhang@link.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghui</forename><surname>Cheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The future network of intelligence institute (FNII)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Research Institute of Big Data 4 Xiaobing.AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D single object tracking (3D SOT) in LiDAR point clouds plays a crucial role in autonomous driving. Current approaches all follow the Siamese paradigm based on appearance matching. However, LiDAR point clouds are usually textureless and incomplete, which hinders effective appearance matching. Besides, previous methods greatly overlook the critical motion clues among targets. In this work, beyond 3D Siamese tracking, we introduce a motion-centric paradigm to handle 3D SOT from a new perspective. Following this paradigm, we propose a matching-free two-stage tracker M 2 -Track. At the 1 st -stage, M 2 -Track localizes the target within successive frames via motion transformation. Then it refines the target box through motion-assisted shape completion at the 2 nd -stage. Extensive experiments confirm that M 2 -Track significantly outperforms previous state-of-the-arts on three large-scale datasets while running at 57FPS (? 8%, ? 17% and ? 22% precision gains on KITTI, NuScenes, and Waymo Open Dataset respectively). Further analysis verifies each component's effectiveness and shows the motioncentric paradigm's promising potential when combined with appearance matching. Code will be made available at https://github.com/Ghostish/Open3DSOT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single Object Tracking (SOT) is a basic computer vision problem with various applications, such as autonomous driving <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref> and surveillance system <ref type="bibr" target="#b31">[32]</ref>. Its goal is to keep track of a specific target across a video sequence, given only its initial state (appearance and location).</p><p>Existing LiDAR-based SOT methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> all follow the Siamese paradigm, which has been widely <ref type="bibr">Figure 1</ref>. Top. Previous Siamese approaches obtain a canonical target template using the previous target box and search for the target in the current frame according to the matching similarity, which is sensitive to distractors. Bottom. Our motion-centric paradigm learns the relative target motion from two consecutive frames and then robustly localizes the target in the current frame via motion transformation. adopted in 2D SOT since it strikes a balance between performance and speed. During the tracking, a Siamese model searches for the target in the candidate region with an appearance matching technique, which relies on the features of the target template and the search area extracted by a shared backbone (see <ref type="figure">Fig.1(a)</ref>).</p><p>Though the appearance matching for 3D SOT shows satisfactory results on KITTI dataset <ref type="bibr" target="#b8">[9]</ref>, we observe that KITTI has the following proprieties: i) the target's motion between two consecutive frames is minor, which ensures no drastic appearance change; ii) there are few/no distractors in the surrounding of the target. However, the above characteristics do not hold in natural scenes. Due to self-occlusion, significant appearance changes may occur in consecutive LiDAR views when objects move fast, or the hardware only supports a low frame sampling rate. Besides, the negative samples grow significantly in dense traffic scenes. In these scenarios, it is not easy to locate a target based on its appearance alone (even for human beings).</p><p>Is the appearance matching the only solution for LiDAR SOT? Actually, Motion Matters. Since the task deals with a dynamic scene across a video sequence, the target's movements among successive frames are critical for effective tracking. Knowing this, researchers have proposed various 2D Trackers to temporally aggregate information from previous frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>. However, the motion information is rarely explicitly modeled since it is hard to be estimated under the perspective distortion. Fortunately, 3D scenes keep intact information about the object motion, which can be easily inferred from the relationships among annotated 3D bounding boxes (BBoxes) <ref type="bibr" target="#b0">1</ref> . Although 3D motion matters for tracking, previous approaches have greatly overlooked it. Due to the Siamese paradigm, previous methods have to transform the target template (initialized by the object point cloud in the first target 3D BBox and updated with the last prediction) from the world coordinate system to its own object coordinate system. This transformation ensures that the shared backbone extracts a canonical target feature, but it adversely breaks the motion connection between consecutive frames.</p><p>Based on the above observations, we propose to tackle 3D SOT from a different perspective instead of sticking to the Siamese paradigm. For the first time, we introduce a new motion-centric paradigm that localizes the target in sequential frames without appearance matching by explicitly modeling the target motion between successive frames ( <ref type="figure">Fig. 1(b)</ref>). Following this paradigm, we design a novel two-stage tracker M 2 -Track ( <ref type="figure">Fig. 2</ref>). During the tracking, the 1 st -stage aims at generating the target BBox by predicting the inter-frame relative target motion. Utilizing all the information from the 1 st -stage, the 2 nd -stage refines the BBox using a denser target point cloud, which is aggregated from two partial target views using their relative motion. We evaluate our model on KITTI <ref type="bibr" target="#b8">[9]</ref>, NuScenes <ref type="bibr" target="#b2">[3]</ref> and Waymo Open Dataset (WOD) <ref type="bibr" target="#b28">[29]</ref>, where NuScenes and WOD cover a wide variety of real-world environments and are challenging for their dense traffics. The experiment results demonstrate that our model outperforms the existing methods by a large margin while running faster than the previous top-performer <ref type="bibr" target="#b42">[43]</ref>. Besides, the performance gap becomes even more significant when more distractors exist in the scenes. Furthermore, we demonstrate that our method can directly benefit from appearance matching when integrated with existing methods.</p><p>In summary, our main contributions are as follows:</p><p>? A novel motion-centric paradigm for real-time LiDAR SOT, which is free of appearance matching. ? A specific second-stage pipeline named M 2 -Track that leverages the motion-modeling and motion-assisted shape completion. ? State-of-the-art online tracking performance with significant improvement on three widely adopted datasets (i.e. KITTI, NuScenes and Waymo Open Dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single Object Tracking. A majority of approaches are built for camera systems and take 2D RGB images as input <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">44]</ref>. Although achieving promising results, they face great challenges when dealing with low light conditions or textureless objects. In contrast, LiDARs are insensitive to texture and robust to light variations, making them a suitable complement to cameras. This inspires a new trend of SOT approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> which operate on 3D LiDAR point clouds. These 3D methods all inherit the Siamese paradigm based on appearance matching. As a pioneer, <ref type="bibr" target="#b9">[10]</ref> uses the Kalman filter to heuristically sample a bunch of target proposals, which are then compared with the target template based on their feature similarities. The proposal which has the highest similarity with the target template is selected as the tracking result.</p><p>Since heuristic sampling is time-consuming and inhibits end-to-end training, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref> propose to use a Region Proposal Network (RPN) to generate high-quality target proposals efficiently. Unlike <ref type="bibr" target="#b41">[42]</ref> which uses an off-the-shelf 2D RPN operating on bird's eye view (BEV), <ref type="bibr" target="#b22">[23]</ref> adapts SiamRPN <ref type="bibr" target="#b14">[15]</ref> to 3D point clouds by integrating a pointwise correlation operator with a point-based RPN <ref type="bibr" target="#b18">[19]</ref>. The promising improvement brought by <ref type="bibr" target="#b22">[23]</ref> inspires a series of follow-up works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref>. They focus on either improving the point-wise correlation operator <ref type="bibr" target="#b42">[43]</ref> by feature enhancement, or refining the point-based RPN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> with more sophisticated structures. The appearance matching achieves excellent success in 2D SOT because images provide rich texture, which helps the model distinguish the target from its surrounding. However, LiDAR point clouds only contain geometric appearances that lack texture information. Besides, objects in LiDAR sweeps are usually sparse and incomplete. These bring considerable ambiguities which hinder effective appearance matching. Unlike existing 3D approaches, our work no more uses any appearance matching. Instead, we examine a new motion-centric paradigm and show its great potential for 3D SOT. </p><formula xml:id="formula_0">3D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targetness Prediction Stage II Stage I Target Points</head><p>(T-1)-th point cloud</p><formula xml:id="formula_1">(T-1)-th 3D BBox T-th point cloud</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse 3D BBox</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous Frame Current Frame Motion Estimation</head><p>Merged Points <ref type="figure">Figure 2</ref>. The overall architecture of M 2 -Track. Given two consecutive point clouds and the possible target BBox at the previous frame, M 2 -Track first segments the target points from their surroundings via joint spatial-temporal learning. At the 1 st stage, the model takes in the target points and obtains a coarse BBox at the current frame via motion prediction and transformation. The coarse BBox is further refined at the 2 nd stage using motion-assisted shape completion. A detailed illustration with data flows is presented in the supplementary.</p><p>an independent detector <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> to extract potential targets, which obstructs its application for unfamiliar objects (categories unknown by the detector). Current 3D MOT methods predominantly follow the "tracking-by-detection" paradigm, which first detects objects at each frame and then heuristically associates detected BBoxes based on objects' motion or appearance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>. Recently, <ref type="bibr" target="#b16">[17]</ref> proposes to jointly perform detection and tracking by combining object detection and motion association into a unified pipeline. Our motion-centric tracker draws inspiration from the motion-based association in MOT. But unlike MOT, which applies motion estimation on detection results, our approach does not depend on any detector and can leverage the motion prediction to refine the target BBox further.</p><p>Spatial-temporal Learning on Point Clouds. Our method utilizes spatial-temporal learning to infer relative motion from multiple frames. Inspired by recent advances in natural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>, there emerges methods that adapt LSTM <ref type="bibr" target="#b11">[12]</ref>, GRU <ref type="bibr" target="#b38">[39]</ref>, or Transformer <ref type="bibr" target="#b6">[7]</ref> to model point cloud videos. However, their heavy structures make them impractical to be integrated with other downstream tasks, especially for real-time applications. Another trend forms a spatial-temporal (ST) point cloud by merging multiple point clouds with a temporal channel added to each point <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. Treating the temporal channel as an additional feature (like RGB or reflectance), one can process such an ST point cloud using any 3D backbones <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> without structural modifications. We adopt this strategy to process successive frames for simplicity and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>Given the initial state of a target, our goal is to localize the target in each frame of an input sequence of a dynamic 3D scene. The frame at timestamp t is a LiDAR point clouds P t ? R Nt?3 with N t points and 3 channels, where the point channels encode the xyz global coordinate. The initial state of the target is given as its 3D BBox at the first frame P 1 . A 3D BBox B t ? R 7 is parameterized by its center (xyz coordinate), orientation (heading angle ? around the up-axis), and size (width, length, and height). For the tracking task, we further assume that the size of a target remains unchanged across frames even for non-rigid objects (for a non-rigid object, its BBox size is defined by its maximum extent in the scene). For each frame P t , the tracker outputs the amodal 3D BBox of the target with access to only history frames {P i } t i=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion-centric Paradigm</head><p>Given an input LiDAR sequence and the 3D BBox of the target in the first frame, the motion-centric tracker aims to localize the target frame by frame using explicit motion modeling. At timestamp t (t &gt; 1), the target BBox B t?1 at frame t ? 1 is known (either given as the initial state or predicted by the tracker). Having two consecutive frame P t and P t?1 as well as the target BBox B t?1 in P t?1 , the tracker predicts the relative target motion (RTM) between the successive two frames. We only consider 4DOF instead of 6DOF RTM since the targets are always aligned with the ground plane (no roll and pitch). Specifically, a 4DOF RTM M t?1,t ? R 4 is defined between two target BBoxes in frame t and t ? 1, and contains the translation offset (?x, ?y, ?z) and the yaw offset ??. We can formu-late this process as a function F:</p><formula xml:id="formula_2">F : R Nt?C ? R Nt?1?C ? R 7 ? R 4 , F(P t , P t?1 , B t?1 ) ? (?x, ?y, ?z, ??);<label>(1)</label></formula><p>Having the predicted RTM M t?1,t , one can easily obtain the target BBox in P t using rigid body transformation:</p><formula xml:id="formula_3">B t = Transform(B t?1 , M t?1,t ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">M 2 -Track: Motion-Centric Tracking Pipeline</head><p>Following the motion-centric paradigm, we design a two-stage motion-centric tracking pipeline M 2 -Track (illustrated in <ref type="figure">Fig.2</ref>). M 2 -Track first coarsely localizes the target through target segmentation and motion transformation at the 1 st stage, and then refines the BBox at the 2 nd stage using motion-assisted shape completion. More details of each module are given as follows.</p><p>Target segmentation with spatial-temporal learning To learn the relative target motion, we first need to segment the target points from their surrounding. Taking as inputs two consecutive frames P t and P t?1 together with the target BBox B t?1 , we do this by exploiting the spatialtemporal relation between the two frames (illustrated in the first part of <ref type="figure">Fig.2</ref>). Similar to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, we construct a spatial-temporal point cloud</p><formula xml:id="formula_4">P t?1,t ? R (Nt?1+Nt)?4 = {p i = (x i , y i , z i , t i )} Nt?1+Nt i=1</formula><p>from P t?1 and P t by adding a temporal channel to each point and then merging them together. Since there are multiple objects in a scene, we have to specify the target of interest according to B t?1 . To this end, we create a prior-targetness map S t?1,t ? R Nt?1+Nt to indicate target location in P t?1,t , where s i ? S t?1,t is defined as:</p><formula xml:id="formula_5">s i = ? ? ? 0 if p i is in P t?1 and p i is not in B t?1 1 if p i is in P t?1 and p i is in B t?1 0.5 if p i is in P t<label>(3)</label></formula><p>Intuitively, one can regard s i as the prior-confidence of p i being a target point. For a point in P t?1 , we set its confidence according to its location with respect to B t?1 . Since the target state in P t is unknown, we set a median score 0.5 for each point in P t . Note that S t?1,t is not 100% correct for points in P t?1 since B t?1 could be the previous output by the tracker. After that, we form a 5D point cloud by concatenating P t?1,t and S t?1,t along the channel axis, and use a PointNet <ref type="bibr" target="#b19">[20]</ref> segmentation network to obtain the target mask, which is finally used to extract a spatial-temporal target point cloud P t?1,t ? R (Mt?1+Mt)?4 , where M t?1 and M t are the numbers of target points in frame (t ? 1) and t respectively.</p><p>Stage I: Motion-Centric BBox prediction As shown in <ref type="figure">Fig. 3</ref>, we encode the spatial-temporal target point clouds P t?1,t into an embedding using another Point-Net encoder. A multi-layer perceptron (MLP) is applied on  <ref type="figure">Figure 3</ref>. Stage I. Taking in the segmented target points Pt?1,t and the target BBox Bt?1 at the previous frame, the model outputs a relative target motion state ( including a RTM Mt?1,t and 2D binary motion state logits), a refined target BBox Bt?1 at the previous frame, and a coarse target BBox Bt at the current frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PointNet Backbone</head><p>Refined BBox Merged Points (box coordinate) <ref type="figure">Figure 4</ref>. Stage II. Taking the segmented target points Pt?1,t and the coarse target BBox Bt as inputs, the model regresses a refined target BBox Bt on a denser point cloud, which is merged from two partial target point clouds according to their relative motion state.</p><p>top of the embedding to obtain the motion state of the target, which includes a 4D RTM M t?1,t and 2D binary classification logits indicating whether the target is dynamic. To reduce accumulation errors while performing frame-by-frame tracking, we generate a refined previous target BBox B t?1 by predicting its RTM with respect to B t?1 through another MLP (More details are presented in the supplementary). Finally, we can get the current target BBox B t by applying Eqn. 2 on M t?1,t and B t?1 if the target is classified as dynamic. Otherwise, we simply set B t as B t?1 . Stage II: BBox refinement with shape completion Inspired by two-stage detection networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, we improve the quality of the 1 st -stage BBox B t by additionally regressing a relative offset, which can be regarded as a RTM between B t and the refined BBox B t . Unlike detection networks, we refine the BBox via a novel motionassisted shape completion strategy. Due to self-occlusion and sensor movements, LiDAR point clouds suffer from great incompleteness, which hinders precise BBox regression. To mitigate this, we form a denser target point cloud by using the predicted motion state to aggregate the targets from two successive frames. According to the temporal channel, two target point clouds P t?1 ? R Mt?1?3 and P t ? R Mt?3 from different timestamps are extracted from P t?1,t . Based on the motion state, we transform P t?1 to the current timestamp using M t?1,t if the target is dynamic. The transformed point cloud (identical as P t?1 if the target is static) is merged with P t to form a denser point cloudP t ? R (Mt?1+Mt)?3 . Similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, we trans-formP t from the world coordinate system to the canonical coordinate system defined by B t . We apply a PointNet on the canonicalP t to regress another RTM with respect to B t . Finally, the refined target BBox B t is obtained by applying Eqn. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Box-aware Feature Enhancement</head><p>As shown in <ref type="bibr" target="#b42">[43]</ref>, LiDAR SOT directly benefits from the part-aware and size-aware information, which can be depicted by point-to-box relation. To achieve better target segmentation, we construct a distance map C t?1 ? R Nt?1?9 by computing the pairwise Euclidean distance between P t?1 and 9 key points of B t?1 (eight corners and one center arranged in a predefined order with respect to the canonical box coordinate system). After that, we extend C t?1 to size (N t?1 +N t )?9 with zero-padding (for points in P t ) and additionally concatenate it with P t?1,t and S t?1,t . The overall box-aware features are then sent to the PointNet segmentation network to obtain better target segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>Loss Functions. The loss function contains classification losses and regression losses, which is defined as L = ? 1 L cls target + ? 2 L cls motion + ? 3 (L reg motion + L reg refine prev + L reg 1st + L reg 2nd ). L cls target and L cls motion are standard cross-entropy losses for target segmentation and motion state classification at the 1 st -stage (Points are considered as the target if they are inside the target BBoxes; A target is regarded as dynamic if its center moves more than 0.15 meter between two frames). All regression losses are defined as the Huber loss <ref type="bibr" target="#b24">[25]</ref> between the predicted and ground-truth RTMs (inferred from ground-truth target BBoxes), where L reg motion is for the RTM between targets in the two frames; L reg refine prev is for the RTM between the predicted and the ground-truth BBoxes at timestamp (t ? 1); L reg 1st / L reg 2nd is for the RTM between the 1 st / 2 nd -stage and ground-truth BBoxes. We empirically set ? 1 = ? 2 = 0.1 and ? 3 = 1.</p><p>Input &amp; Motion Augmentation. Since SOT only takes care of one target in a scene, we only need to consider a subregion where the target may appear. For two consecutive frames at (t ? 1) and t timestamp, we choose the sub- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setups</head><p>Datasets. We extensively evaluate our approach on three large-scale datasets: KITTI <ref type="bibr" target="#b8">[9]</ref>, NuScenes <ref type="bibr" target="#b2">[3]</ref> and Waymo Open Dataset (WOD) <ref type="bibr" target="#b28">[29]</ref>. We follow <ref type="bibr" target="#b9">[10]</ref> to adapt these datasets for 3D SOT by extracting the tracklets of anno-  <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref> to split the training set into train/val/test splits due to the inaccessibility of the test labels. NuScenes contains 1000 scenes, which are divided into 700/150/150 scenes for train/val/test. Officially, the train set is further evenly split into "train track" and "train detect" to remedy overfitting. Following <ref type="bibr" target="#b42">[43]</ref>, we train our model with "train track" split and test it on the val set. WOD includes 1150 scenes with 798 for training, 202 for validation, and 150 for testing. We do training and testing respectively on the training and validation set. Note that NuScenes and WOD are much more challenging than KITTI due to larger data volumes and complexities. The LiDAR sequences are sampled at 10Hz for both KITTI and WOD. Though NuScenes samples at 20Hz, it only provides the annotations at 2Hz. Since only annotated keyframes are considered, such a lower frequency for keyframes introduces additional difficulties for NuScenes. Evaluation Metrics. We evaluate the models using the One Pass Evaluation (OPE) <ref type="bibr" target="#b35">[36]</ref>. It defines overlap as the Intersection Over Union (IOU) between the predicted and ground-truth BBox, and defines error as the distance between two BBox centers. We report the Success and Precision of each model in the following experiments. Success is the Area Under the Curve (AUC) with the overlap threshold varying from 0 to 1. Precision is the AUC with the error threshold from 0 to 2 meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-arts</head><p>Results on KITTI. We compare M 2 -Track with seven topperformance approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, which have published results on KITTI. As shown in Tab. 1, our method benefits both rigid and non-rigid object tracking, outperforming current approaches under all categories except Car, where PTT <ref type="bibr" target="#b25">[26]</ref> and V2B <ref type="bibr" target="#b12">[13]</ref> surpass us by minor margins. The lack of car distractors in the scenes makes our improvement over previous appearancematching-based methods minor for cars. But our improvement for pedestrians is significant (13.2%/13.7% in terms of success/precision) because pedestrian distractors are widespread in the scenes (see the supplementary for more details). Besides, methods using point-based RPN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref> all perform badly on cyclists, which are relative small in size but usually move fast across time. The second row in <ref type="figure">Fig. 5</ref> shows the case in which a cyclist moves rapidly across frames. Our method perfectly keeps track of the target while BAT almost fails. To handle such fastmoving objects, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref> leverage BEV-based RPN to generate high-recall proposals from a larger search region. In contrast, we handle this simply by motion modeling without sophisticated architectures. Results on NuScenes &amp; WOD. We select three representative open-source works: SC3D <ref type="bibr" target="#b9">[10]</ref>, P2B <ref type="bibr" target="#b22">[23]</ref> and BAT <ref type="bibr" target="#b42">[43]</ref> as our competitors on NuScenes and WOD. The results on NuScenes except for the Pedestrian class are provided by <ref type="bibr" target="#b42">[43]</ref>. We use the published codes of the competitors to obtain other results absent in <ref type="bibr" target="#b42">[43]</ref>. SC3D <ref type="bibr" target="#b9">[10]</ref> is omitted for WOD comparison due to its costly training time. As </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of distractors</head><p>Ours BAT P2B <ref type="figure">Figure 6</ref>. Robustness analysis with variant numbers of distractors.</p><p>shown in Tab. 2, M 2 -Track exceeds all the competitors under all categories, mostly by a large margin. On such two challenging datasets with pervasive distractors and drastic appearance changes, the performance gap between previous approaches and M 2 -Track becomes even larger (e.g. more than 30% precision gain on Waymo Pedestrian). Note that for large objects (i.e. Truck, Trailer, and Bus), even if the predicted centers are far from the target (reflected from lower precision), the output BBoxes of the previous model may still overlap with the ground truth (results in higher success). In contrast, the motion modeling helps to improve not only the success but also the precision by a large margin (e.g.+23.43% gain on Bus) for large objects. Visualization results are provided in <ref type="figure">Fig. 5</ref> and the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis Experiments</head><p>In this section, we extensively analyze M 2 -Track with a series of experiments. Firstly, we compare the behaviors of M 2 -Track and previous appearance-matching-based methods in different setups. Afterward, we equip M 2 -Track with the previous appearance matching approaches to show its potential. Finally, we study the effectiveness of each component in M 2 -Track. All the experiments are conducted on the Car category of KITTI unless otherwise stated. Robustness to Distractors. Though achieving promising improvement on NuScenes and WOD, M 2 -Track brings little improvement on the Car of KITTI. To explain this, we look at the scenes of three datasets and find that the surroundings of most cars in KITTI are free of distrac- tors, which are pervasive in NuScenes and WOD. Although appearance-matching-based methods are sensitive to distractors, they provide more precise results than our motionbased approach in distractor-free scenarios. But as the number of distractors increases, these methods suffer from noticeable performance degradation due to ambiguities from the distractors. To verify this hypothesis, we randomly add K car instances to each scene of KITTI, and then re-train and evaluate different models using this synthesis dataset. As shown in <ref type="figure">Fig. 6</ref>, M 2 -Track consistently outperforms the other two matching-based methods in scenes with more distractors, and the performance gap grows as K increases.</p><p>Thanks to the box-awareness, BAT <ref type="bibr" target="#b42">[43]</ref> can aid such ambiguities to some extent. But our performance is more stable than BAT's when more distractors are added. Besides, the first row in <ref type="figure">Fig. 5</ref> shows that, when the number of points decreases due to occlusion, BAT is misled by a distractor and then tracks off course, while M 2 -Track keeps holding tight to the ground truth. All these observations demonstrate the robustness of our approach. Influence of Motion Augmentation. We improve the performance of M 2 -Track using the motion augmentation in training, which is not adopted in previous approaches. For a fair comparison, we re-train BAT <ref type="bibr" target="#b42">[43]</ref> and P2B <ref type="bibr" target="#b22">[23]</ref> using the same configurations in their open-source projects except additionally adding motion augmentation. Tab. 3 shows that motion augmentation instead has an adverse effect on both BAT and P2B. Our model benefits from motion augmentation since it explicitly models target motion and is robust to distractors. In contrast, motion augmentation may move a target closer to its potential distractors and thus harm those appearance-matching-based approaches.</p><p>Combine with Appearance Matching. Although our motion-centric model outperforms previous methods from various aspects, appearance-matching-based approaches still show their advantage when dealing with distractorfree scenarios. To combine the advantages of both motionbased and matching-based methods, we apply BAT/P2B as a "re-tracker" to fine-tune the results of M 2 -Track. Specifically, we directly utilize BAT/P2B to search for the target in a small neighborhood of the M 2 -Track's output. Tab. 4 confirms that M 2 -Track can further benefit from appearance matching, even under this naive combination. On KITTI Car, both combined models outperform the top-ranking PTT <ref type="bibr" target="#b25">[26]</ref> by noticeable margins. We believe that one can further boost 3D SOT by combining motion-based and matching-based paradigms with a more delicate design.</p><p>Ablations. In Tab. 5, we conduct an exhaustive ablation study on both KITTI and NuScenes to understand the components of M 2 -Track. Specifically, we respectively ablate the box-aware feature enhancement, previous BBox refinement, binary motion classification and 2 nd stage from M 2 -Track. In general, the effectiveness of the components varies across the datasets, but removing any one of them causes performance degradation. The only exception is the binary motion classification used in the 1 st stage, which causes a slight drop on KITTI in terms of success. We suppose this is due to the lack of static objects for KITTI's cars, which results in a biased classifier. Besides, Tab. 5 shows that M 2 -Track keeps performing competitively even with module ablated, especially on NuScenes. This reflects that the main improvement of M 2 -Track is from the motioncentric paradigm instead of the specific pipeline design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">More Discussion</head><p>Running Overheads. M 2 -Track achieves exciting performance with just a simple PointNet <ref type="bibr" target="#b19">[20]</ref>. Compared with other hierarchical backbones (e.g. <ref type="bibr" target="#b20">[21]</ref>) used in previous works, PointNet saves more computational overheads since it does not perform any sampling or grouping operations, which are not only time-consuming but also memoryintensive. Therefore, M 2 -Track runs 1.67? faster as the previous top-performer BAT <ref type="bibr" target="#b42">[43]</ref> (only consider model forwarding time) but saves 31.1% memory footprint. Using a more advanced backbone (e.g. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>) may further boost the performance but inevitably slows down the running speed. Since we focus on online tracking, we prefer a simpler backbone to balance performance and efficiency.</p><p>Limitations. Unlike appearance matching, our motioncentric model requires a good variety of motion in the training data to ensure its generalization on data sampled with different frequencies. For instance, our model suffers from considerable performance degradation if trained with 2Hz data but tested with 10Hz data because the motion distribution of the 2Hz and 10Hz data differs significantly. But fortunately, we can aid this using a well-design motion augmentation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we revisit 3D SOT in LiDAR point clouds and propose to handle it with a new motion-centric paradigm, which is proven to be an excellent complement to the matching-based Siamese paradigm. In addition to the new paradigm, we propose a specific motion-centric tracking pipeline M 2 -Track, which significantly outperforms the state-of-the-arts from various aspects. Extensive analysis confirms that the motion-centric model is robust to distractors and appearance changes and can directly benefit from previous matching-based trackers. We believe that the motion-centric paradigm can serve as a primary principle to guide future architecture designs. In the future, we may try to improve M 2 -Track by considering more frames and integrating it with the appearance matching under a more delicate design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Target Segmentation Two-frame Points Target Points Motion State Refined 3D BBox Motion Centric Tracking Box Refinement</head><label></label><figDesc></figDesc><table><row><cell>Motion</cell><cell></cell></row><row><cell>State</cell><cell>Refined</cell></row><row><cell></cell><cell>3D BBox</cell></row><row><cell>Coarse</cell><cell></cell></row><row><cell>3D BBox</cell><cell></cell></row><row><cell cols="2">Multi-object Tracking / Detection. In parallel with</cell></row><row><cell cols="2">3D SOT, 3D multi-object tracking (MOT) focuses on track-</cell></row><row><cell cols="2">ing multiple objects simultaneously. Unlike SOT where</cell></row><row><cell cols="2">the user can specify a target of interest, MOT relies on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparison among our M 2 -Track and the state-of-theart methods on the KITTI datasets. Mean shows the average result weighed by frame numbers. Bold and underline denote the best performance and the second-best performance, respectively. Improvements over previous state-of-the-arts are shown in Italic. We then sample 1024 points from the subregion respectively at (t ? 1) and t timestamp to form P t?1 and P t . To simulate testing errors during the training, we feed the model a perturbed BBox by adding a slight random shift to the ground-truth target BBox at (t ? 1) timestamp. To encourage the model to learn various motions during the training, we randomly flip both targets' points and BBoxes in their horizontal axes and rotate them around their up-axes by Uniform[?10 ? ,10 ? ]. We also randomly translate the targets by offsets drawn from Uniform [-0.3, 0.3] meter. Training &amp; Inference. We train our models using the Adam optimizer with batch size 256 and an initial learning rate 0.001, which is decayed by 10 times every 20 epochs. The training takes ? 4 hours to converge on a V100 GPU for the KITTI Cars. During the inference, the model tracks a target frame-by-frame in a point cloud sequence given the target BBox at the first frame.</figDesc><table><row><cell></cell><cell>Category</cell><cell cols="5">Car Pedestrian Van Cyclist Mean</cell></row><row><cell></cell><cell>Frame Number</cell><cell>6424</cell><cell>6088</cell><cell>1248</cell><cell>308</cell><cell>14068</cell></row><row><cell></cell><cell>SC3D [10]</cell><cell>41.3</cell><cell>18.2</cell><cell>40.4</cell><cell>41.5</cell><cell>31.2</cell></row><row><cell></cell><cell cols="2">SC3D-RPN [42] 36.3</cell><cell>17.9</cell><cell>-</cell><cell>43.2</cell><cell>-</cell></row><row><cell></cell><cell>P2B [23]</cell><cell>56.2</cell><cell>28.7</cell><cell>40.8</cell><cell>32.1</cell><cell>42.4</cell></row><row><cell>Success</cell><cell cols="2">3DSiamRPN [8] 58.2 LTTR [6] 65.0 PTT [26] 67.8</cell><cell>35.2 33.2 44.9</cell><cell>45.6 35.8 43.6</cell><cell>36.1 66.2 37.2</cell><cell>46.6 48.7 55.1</cell></row><row><cell></cell><cell>V2B [13]</cell><cell>70.5</cell><cell>48.3</cell><cell>50.1</cell><cell>40.8</cell><cell>58.4</cell></row><row><cell></cell><cell>BAT [43]</cell><cell>65.4</cell><cell>45.7</cell><cell>52.4</cell><cell>33.7</cell><cell>55.0</cell></row><row><cell></cell><cell cols="2">M 2 -Track (Ours) 65.5</cell><cell>61.5</cell><cell>53.8</cell><cell>73.2</cell><cell>62.9</cell></row><row><cell></cell><cell>Improvement</cell><cell>?5.0</cell><cell>?13.2</cell><cell>?1.4</cell><cell>?7.0</cell><cell>?4.5</cell></row><row><cell></cell><cell>SC3D [10]</cell><cell>57.9</cell><cell>37.8</cell><cell>47.0</cell><cell>70.4</cell><cell>48.5</cell></row><row><cell></cell><cell cols="2">SC3D-RPN [42] 51.0</cell><cell>47.8</cell><cell>-</cell><cell>81.2</cell><cell>-</cell></row><row><cell>Precision</cell><cell cols="2">P2B [23] 3DSiamRPN [8] 76.2 72.8 LTTR [6] 77.1 PTT [26] 81.8</cell><cell>49.6 56.2 56.8 72.0</cell><cell>48.4 52.8 45.6 52.5</cell><cell>44.7 49.0 89.9 47.3</cell><cell>60.0 64.9 65.8 74.2</cell></row><row><cell></cell><cell>V2B [13]</cell><cell>81.3</cell><cell>73.5</cell><cell>58.0</cell><cell>49.7</cell><cell>75.2</cell></row><row><cell></cell><cell>BAT [43]</cell><cell>78.9</cell><cell>74.5</cell><cell>67.0</cell><cell>45.4</cell><cell>75.2</cell></row><row><cell></cell><cell cols="2">M 2 -Track (Ours) 80.8</cell><cell>88.2</cell><cell>70.7</cell><cell>93.5</cell><cell>83.4</cell></row><row><cell></cell><cell>Improvement</cell><cell>? 1.0</cell><cell>?13.7</cell><cell>?3.7</cell><cell>?3.6</cell><cell>?8.2</cell></row><row><cell cols="7">region by enlarging the target BBox at (t ? 1) timestamp</cell></row><row><cell cols="2">by 2 meters.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison of M 2 -Track against state-of-the-arts on the NuScenes and Waymo Open Dataset using the same annotations in Tab. 1. Visualization results. Top: Distractor case in KITTI. Middle: Large motion case in KITTI. Bottom: Case in NuScenes. tated tracked instances from each of the scenes. KITTI contains 21 training sequences and 29 test sequences. We follow previous works</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">NuScenes</cell><cell></cell><cell></cell><cell cols="3">Waymo Open Dataset</cell></row><row><cell></cell><cell>Category</cell><cell>Car</cell><cell>Pedestrian</cell><cell>Truck</cell><cell>Trailer</cell><cell>Bus</cell><cell>Mean</cell><cell>Vehicle</cell><cell>Pedestrian</cell><cell>Mean</cell></row><row><cell></cell><cell>Frame Number</cell><cell>64,159</cell><cell>33,227</cell><cell>13,587</cell><cell>3,352</cell><cell>2,953</cell><cell>117,278</cell><cell>1,057,651</cell><cell>510,533</cell><cell>1,568,184</cell></row><row><cell></cell><cell>SC3D [10]</cell><cell>22.31</cell><cell>11.29</cell><cell>30.67</cell><cell>35.28</cell><cell>29.35</cell><cell>20.70</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Success</cell><cell>P2B [23] BAT [43] M 2 -Track (Ours)</cell><cell>38.81 40.73 55.85</cell><cell>28.39 28.83 32.10</cell><cell>42.95 45.34 57.36</cell><cell>48.96 52.59 57.61</cell><cell>32.95 35.44 51.39</cell><cell>36.48 38.10 49.23</cell><cell>28.32 35.62 43.62</cell><cell>15.60 22.05 42.10</cell><cell>24.18 31.20 43.13</cell></row><row><cell></cell><cell>Improvement</cell><cell>?15.12</cell><cell>?3.27</cell><cell>?12.02</cell><cell>?5.02</cell><cell>?15.95</cell><cell>?11.14</cell><cell>?8.00</cell><cell>?20.05</cell><cell>?11.92</cell></row><row><cell>Precision</cell><cell>SC3D [10] P2B [23] BAT [43] M 2 -Track (Ours)</cell><cell>21.93 43.18 43.29 65.09</cell><cell>12.65 52.24 53.32 60.92</cell><cell>27.73 41.59 42.58 59.54</cell><cell>28.12 40.05 44.89 58.26</cell><cell>24.08 27.41 28.01 51.44</cell><cell>20.20 45.08 45.71 62.73</cell><cell>-35.41 44.15 61.64</cell><cell>-29.56 36.79 67.31</cell><cell>-33.51 41.75 63.48</cell></row><row><cell></cell><cell>Improvement</cell><cell>?21.80</cell><cell>?7.60</cell><cell>?16.96</cell><cell>?13.37</cell><cell>?23.43</cell><cell>?17.02</cell><cell>?17.49</cell><cell>?30.52</cell><cell>?21.73</cell></row><row><cell>Pedestrian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cyclist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Ground Truth</cell><cell></cell><cell>BAT</cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Influence of Motion Augmentation. "aug" stands for motion augmentation.</figDesc><table><row><cell>Method</cell><cell>Success</cell><cell>Precision</cell></row><row><cell>BAT [43] w/o aug</cell><cell>65.37</cell><cell>78.88</cell></row><row><cell>BAT [43] w/ aug</cell><cell cols="2">63.59 ? 1.78 76.99 ? 1.89</cell></row><row><cell>P2B [23] w/o aug</cell><cell>56.20</cell><cell>72.80</cell></row><row><cell>P2B [23] w/ aug</cell><cell cols="2">55.21 ? 0.99 71.51 ? 1.29</cell></row><row><cell>M 2 -Track w/o aug</cell><cell>65.29</cell><cell>77.12</cell></row><row><cell>M 2 -Track w/ aug</cell><cell cols="2">65.49 ? 0.20 80.81 ? 3.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Integration with Appearance Matching.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Success</cell><cell cols="2">Precision</cell></row><row><cell>PTT [26]</cell><cell></cell><cell>67.80</cell><cell>81.80</cell><cell></cell></row><row><cell>V2B [13]</cell><cell></cell><cell>70.50</cell><cell>81.30</cell><cell></cell></row><row><cell>M 2 -Track</cell><cell></cell><cell>65.49</cell><cell>80.81</cell><cell></cell></row><row><cell cols="2">M 2 -Track + BAT [43]</cell><cell cols="3">69.22 ? 3.73 81.09 ? 0.28</cell></row><row><cell cols="2">M 2 -Track + P2B [23]</cell><cell cols="3">70.21 ? 4.72 81.80 ? 0.99</cell></row><row><cell>61</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>59</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>57</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>53</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>51</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>49</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Results of M 2 -Track when different modules are ablated. The last row denotes the full model. Bold denotes the largest change. ? 3.49 76.15 ? 4.66 53.68 ? 2.17 62.47 ? 2.62 64.23 ? 1.26 78.12 ? 2.69 54.70 ? 1.15 61.94 ? 3.15 65.74 ? 0.25 80.29 ? 0.52 54.88 ? 0.97 64.40 ? 0.69 61.29 ? 4.20 77.31 ? 3.50 54.66 ? 1.99 64.15 ? 0.94</figDesc><table><row><cell>Box Aware Enhancement</cell><cell>Prev Box Refinement</cell><cell>Motion Classification</cell><cell>Stage-II</cell><cell>Success</cell><cell>Kitti</cell><cell>Precision</cell><cell cols="2">NuScenes Success Precision</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.00 65.49</cell><cell></cell><cell>80.81</cell><cell>55.85</cell><cell>65.09</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is greatly held for rigid objects (e.g. cars), and it is approximately true for non-rigid objects (e.g. pedestrian).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="205" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Probabilistic 3d multi-object tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu-Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Prioletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bohg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05673,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayao</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifan</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14921</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>3d object tracking with transformer</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Point 4d transformer networks for spatio-temporal modeling in point cloud videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14204" to="14213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4995" to="5011" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging shape completion for 3d siamese tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Zarzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1359" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What you see is what you get: Exploiting visibility for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ziglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11001" to="11009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An lstm approach to temporal 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d siamese voxel-to-bev tracker for sparse point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pnpnet: End-to-end perception and prediction with tracking in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11553" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring simple 3d multi-object tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="10488" to="10497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">4d-net for learned multi-modal alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="15435" to="15445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Offboard 3d object detection from point cloud sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">P2b: Point-to-box network for 3d object tracking in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6329" to="6338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caspr: Learning canonical spatiotemporal point cloud representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ptt: Point-track-transformer module for 3d single object tracking in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayao</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06455</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fooling automated surveillance cameras: adversarial patches to attack person detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simen</forename><surname>Thys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiebe</forename><surname>Van Ranst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toon</forename><surname>Goedem?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1571" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12549" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11495" to="11504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11275</idno>
		<title level="m">Centerbased 3d object detection and tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Centerbased 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient bird eye view proposals for 3d siamese tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Zarzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10168</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Box-aware feature enhancement for single object tracking on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Int</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="13199" to="13208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
