<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computational Visual Media Full-Duplex Strategy for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">IIAI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computational Visual Media Full-Duplex Strategy for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">x,No.x</biblScope>
							<biblScope unit="page" from="xx" to="xx"/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s41095-xxx-xxxx-x</idno>
					<note type="submission">Manuscript received: 2021-08-31; accepted: 20xx-xx-xx.</note>
					<note>Research Article</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Object Segmentation</term>
					<term>Salient Object Detection</term>
					<term>Visual Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous video object segmentation approaches mainly focus on using simplex solutions between appearance and motion, limiting feature collaboration efficiency among and across these two cues. In this work, we study a novel and efficient full-duplex strategy network (FSNet) to address this issue, by considering a better mutual restraint scheme between motion and appearance in exploiting the crossmodal features from the fusion and decoding stage. Specifically, we introduce the relational cross-attention module (RCAM) to achieve bidirectional message propagation across embedding sub-spaces. To improve the model's robustness and update the inconsistent features from the spatial-temporal embeddings, we adopt the bidirectional purification module (BPM) after the RCAM. Extensive experiments on five popular benchmarks show that our FSNet is robust to various challenging scenarios (e.g., motion blur, occlusion) and achieves favourable performance against existing cutting-edges both in the video object segmentation and video salient object detection tasks. The project is publicly available at: https://dpfan.net/FSNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref> <p>Comparison between three strategies for embedding appearance and motion patterns before the fusion and decoding stage.</p><p>(a) Direction-independent strategy <ref type="bibr" target="#b44">[44]</ref> without information transmission, (b) Simplex strategy <ref type="bibr" target="#b141">[141]</ref> with only unidirectional information transmission, e.g., using motion guides appearance or vice versa, and (c) our full-duplex strategy with simultaneously bidirectional information transmission. This paper mainly focuses on discussing directional modelling (b &amp; c) in the deep learning era.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past three years, social platforms have accumulated a large number of short videos. Analyzing these videos efficiently and intelligently has become a challenging issue today. Video object segmentation (VOS) <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b115">115,</ref><ref type="bibr" target="#b118">118]</ref> is a fundamental technique to address this issue, whose purpose is to delineate pixellevel moving object 1 masks in each frame. Besides video analysis, many other applications have also benefited from VOS, such as robotic manipulation <ref type="bibr" target="#b0">[1]</ref>, autonomous cars <ref type="bibr" target="#b70">[70]</ref>, video editing <ref type="bibr" target="#b43">[43]</ref>, action segmentation <ref type="bibr" target="#b103">[103]</ref>, optical flow estimation <ref type="bibr" target="#b24">[24]</ref>, medical diagnosis <ref type="bibr" target="#b45">[45]</ref>, interactive segmentation <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b131">131]</ref>, URVOS <ref type="bibr" target="#b87">[87]</ref>, and video captioning <ref type="bibr" target="#b77">[77]</ref>.</p><p>Recently, we have witnessed rapid development in addressing video object understanding by exploiting the relationships between the frames' appearance-aware <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b84">84,</ref><ref type="bibr" target="#b101">101,</ref><ref type="bibr" target="#b132">132]</ref> and motionaware <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b95">95]</ref>.</p><p>Unfortunately, short-term dependency prediction <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b95">95]</ref> generates unreliable estimations and suffers the common ordeals <ref type="bibr" target="#b39">[39]</ref> (e.g., noise, deformation, and diffusion). In addition, the capability of appearance-based modelling like recurrent neural network <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b97">97]</ref> (RNN) is severely hindered by blurred foregrounds or cluttered backgrounds <ref type="bibr" target="#b18">[18]</ref>.</p><p>Those conflicts are prone to accumulating inaccuracies and the propagation of spatial-temporal embeddings, which cause the shortterm feature drifting problem <ref type="bibr" target="#b129">[129]</ref>.</p><p>As shown in <ref type="figure">Fig. 1 (a)</ref>, the direction-independent strategy <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b122">122]</ref> is the earliest solution by encoding the appearance and motion features individually and fuse them directly. However, this intuitive way will implicitly cause feature conflicts since the motion-and appearance-aware features are derived from two distinctive modalities, which is extracted from separate branches. An alternative way is to integrate them in a guided manner. As illustrated in <ref type="figure">Fig. 1 (b)</ref>, several recent methods opt for the simplex strategy <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b100">100,</ref><ref type="bibr" target="#b141">141]</ref>, which is either appearance-based or motion-guided. Although these two strategies have achieved promising results, they both fail to consider the mutual restraint between the appearance and motion features that both guide human visual attention allocation during dynamic observation, according to previous studies in cognitive psychology <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b99">99,</ref><ref type="bibr" target="#b119">119]</ref> and computer vision <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b107">107]</ref>.</p><p>Intuitively, appearance and motion characteristics should be homogeneous to a certain degree for the same object within a short time. As seen in <ref type="figure">Fig. 2</ref>, the foreground region of appearance and motion intrinsically share the correlative patterns about perceptions, including semantic structure, movement trends. Nevertheless, misguided knowledge in the individual modality, e.g., static shadow under the chassis and small car in the background, will produce inaccuracies during the feature propagation. Thus, it easily stains the result (see blue boxes).</p><p>To address these challenges, we introduce a novel modality transmission strategy (full-duplex <ref type="bibr" target="#b4">[5]</ref>) between spatial-and temporal-aware, instead of embedding them individually. The proposed strategy is the bidirectional attention scheme across motion and appearance cues, which explicitly incorporates the appearance and motion patterns in a unified framework as depicted in <ref type="figure">Fig. 1 (c)</ref>. As seen in <ref type="figure">Fig. 2</ref>, our method visually performs better than the one with a simplex strategy (a &amp; b). Visual comparison between the simplex (i.e., (a) appearance-refined motion and (b) motion-refined appearance) and our full-duplex strategy under our framework. In contrast, our FSNet offers a collaborative way to leverage the appearance and motion cues under the mutual restraint of full-duplex strategy, thus providing more accurate structure details and alleviating the short-term feature drifting issue <ref type="bibr" target="#b129">[129]</ref>.</p><p>To fully investigate the simplex and full-duplex strategies of our framework, we present the following contributions:</p><p>? We propose a unified framework Full-duplex Strategy Network (FSNet) for robust video object segmentation, which makes full use of spatialtemporal representations. ? We adopt a bidirectional interaction module, dubbed the relational cross-attention module (RCAM), to extract discriminative features from the appearance and motion branches, which ensures mutual restraint between each other. To improve the model robustness, we introduce a bidirectional purification module (BPM), which is equipped with an interlaced decremental connection to update inconsistent features between the spatial-temporal embeddings automatically. ? We demonstrate that our FSNet achieves favourable performance on five mainstream benchmarks, especially for our FSNet (N =4, CRF) outperforms the SOTA U-VOS model (i.e., MAT <ref type="bibr" target="#b141">[141]</ref>) on the DAVIS 16 <ref type="bibr" target="#b82">[82]</ref> leaderboard by a margin of 2.4% in terms of Mean-F score, with less training data (i.e., Ours-13K vs. MAT-16K). As an extension of our ICCV-2021 version <ref type="bibr" target="#b46">[46]</ref>, we incorporate more details to provide a better understanding of our novel framework as follow:</p><p>? To provide our community with a comprehensive study, we have made a lot of efforts to improve the presentations (e.g., <ref type="figure">Fig. 1, Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Depending on whether or not the first frame of ground truth is given, the VOS task can be divided into two scenarios, i.e., semi-supervised <ref type="bibr" target="#b109">[109]</ref> and unsupervised/zero-shot <ref type="bibr" target="#b71">[71]</ref>. Some typical semisupervised VOS models can be referred to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b102">102,</ref><ref type="bibr" target="#b121">121,</ref><ref type="bibr" target="#b126">126,</ref><ref type="bibr" target="#b128">128,</ref><ref type="bibr" target="#b133">133,</ref><ref type="bibr" target="#b134">134]</ref>. This paper studies the unsupervised setting <ref type="bibr" target="#b140">[140,</ref><ref type="bibr" target="#b141">141]</ref>, leaving the semi-supervised setting as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised VOS</head><p>Although there are many works addressing the VOS task in a semi-supervised manner, i.e., by supposing an object mask annotation is given in the first frame, other researchers have attempted to address the more challenging unsupervised VOS (U-VOS) problem. Early U-VOS models resort to low-level handcrafted features for heuristic segmentation inference, such as long sparse point trajectories <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b111">111]</ref>, object proposals <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b83">83]</ref>, saliency priors <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b106">106,</ref><ref type="bibr" target="#b108">108]</ref>, optical flow <ref type="bibr" target="#b100">[100]</ref>, or superpixels <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b123">123]</ref>. These traditional models have limited generalizability and thus low accuracy in highly dynamic and complex scenarios due to their lack of semantic information and high-level content understanding. Recently, RNNbased models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b93">93,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b113">113,</ref><ref type="bibr" target="#b126">126,</ref><ref type="bibr" target="#b138">138]</ref> have become popular due to their better capability of capturing longterm dependencies and their use of deep learning. In this case, U-VOS is formulated as a recurrent modelling issue over time, where spatial features are jointly exploited with long-term temporal context.</p><p>How to combine motion cues with appearance features is a long-standing problem in this field. To this end, Tokmakov et al. <ref type="bibr" target="#b96">[96]</ref> proposed to use the motion patterns required from the video simply. However, their method cannot accurately segment objects between two similar consecutive frames since it relies heavily on the guidance of optical flow. To resolve this, several works <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b97">97]</ref> have integrated the spatial and temporal features from the parallel network, which can be viewed as plain feature fusion from the independent spatial and temporal branch with an implicit modelling strategy. Li et al. <ref type="bibr" target="#b62">[62]</ref> proposed a multi-stage processing method to tackle U-VOS, which first utilizes a fixed appearance-based network to generate objectness and then feeds this into the motionbased bilateral estimator to segment the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention-based VOS</head><p>The attention-based VOS task is closely related to U-VOS since it extracts attention-aware object(s) from a video clip. Traditional methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b112">112,</ref><ref type="bibr" target="#b125">125,</ref><ref type="bibr" target="#b142">142]</ref> first compute the single-frame saliency based on various hand-crafted static and motion features and then conduct spatial-temporal optimization to preserve coherency across consecutive frames. Recent works <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b110">110]</ref> aim to learn a highly semantic representation and usually perform spatial-temporal detection end-toend. Many schemes have been proposed to employ deep networks that consider temporal information, such as ConvLSTM <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b93">93]</ref>, take optical-flows/adjacentframes as input <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b110">110]</ref>, 3D convolutional <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b73">73]</ref>, or directly exploit temporally concatenated deep features <ref type="bibr" target="#b56">[56]</ref>. Besides, long-term influences are often taken into account and combined with deep learning. Li et al. <ref type="bibr" target="#b63">[63]</ref> proposed a key-frame strategy to locate representative high-quality video frames of salient objects <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b139">139]</ref> and diffused their saliency to illdetected non-key frames. Chen et al. <ref type="bibr" target="#b15">[15]</ref> improved saliency detection by leveraging long-term spatialtemporal information, where high-quality "beyond-thescope frames" are aligned with the current frames. Both types of information are fed to deep neural networks for classification. Besides considering how to better leverage temporal information, other researchers have attempted to address different problems in video salient object detection (V-SOD), such as reducing the data labelling requirements <ref type="bibr" target="#b127">[127]</ref>, developing semisupervised approaches <ref type="bibr" target="#b94">[94]</ref>, or investigating relative saliency <ref type="bibr" target="#b116">[116]</ref>. Fan et al. <ref type="bibr" target="#b30">[30]</ref> recently introduced a V-SOD model equipped with a saliency shift-aware ConvLSTM, together with an attention-consistent V-SOD dataset with high-quality annotations. Zhao et al. <ref type="bibr" target="#b137">[137]</ref> build a large-scale with scribble annotation for weakly supervised video salient object detection. They propose an appearance-motion fusion module to +2 +2 <ref type="figure">Fig. 3</ref> The architecture of our FSNet for video object segmentation. The Relational Cross-Attention Module (RCAM) abstracts more discriminative representations between the motion and appearance cues using the full-duplex strategy. Then four Bidirectional Purification Modules (BPM) are stacked to further re-calibrate inconsistencies between the motion and appearance features. Finally, we utilize the decoder to generate our prediction.</p><p>aggregate the spatial-temporal features attentively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Suppose that a video clip contains T consecutive frames {A t } T t=1 . We first utilize optical flow field generator H, i.e., FlowNet 2.0 <ref type="bibr" target="#b42">[42]</ref>, to generate T ? 1 optical flow maps {M t } T ?1 t=1 , which are all computed by two adjacent frames (M t = H[A t , A t+1 ]). To ensure the inputs match, we discard the last frame in the pipeline. Thus, the proposed pipeline takes both the appearance image {A t } T ?1 t=1 and its paired motion map {M t } T ?1 t=1 as the input. First, M t &amp; A t pairs at frame t 2 are fed to two independent ResNet-50 <ref type="bibr" target="#b36">[36]</ref> branches (i.e., motion and appearance blocks in <ref type="figure">Fig. 3</ref>). The appearance features {X k } K k=1 and motion features {Y k } K k=1 extracted from K layers are then sent to the Relational Cross-Attention Modules (RCAMs), which allows the network to embed spatialtemporal cross-modal features. Next, we employ the Bidirectional Purification Modules (BPMs) with N cascaded units. BPMs focus on distilling representative carriers from fused features {F n k } N n=1 and motion-based features {G n k } N n=1 . Finally, the predictions (i.e., S t M and S t A ) at frame t are generated from two decoder blocks. <ref type="bibr" target="#b1">2</ref> Here, we omit the superscript "t" for the convenient expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Cross-Attention Module</head><p>As discussed in Sec. 1, a single-modality (i.e., motion or appearance) guided stimulation may cause the model to make incorrect decisions. To alleviate this, we design a cross-attention module (RCAM) via the channel-wise attention mechanism, which focuses on distilling out effectively squeezed cues from two modalities and then modulating each other. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (c), the two inputs of RCAM are appearance features {X k } K k=1 and motion features {Y k } K k=1 , which are obtained from the two different branches of the standard ResNet-50 <ref type="bibr" target="#b36">[36]</ref>. Specifically, for each k -level, we first perform global average pooling (GAP) to generate channelwise vectors V X k and V Y k from each X k and Y k . Next, two 1?1 conv, i.e., ?(x; W ? ) and ?(x; W ? ), with learnable parameters W ? and W ? , generate two discriminated global descriptors. The sigmoid function ?[x] = e x /(e x + 1), x ? R is then applied to convert the final descriptors into the interval [0, 1], i.e., into the valid attention vector for channel weighting. Then, we perform outer product ? between X k and ? ?(V Y k ; W ? ) to generate a candidate feature Q X k , and vice versa, as follows:</p><formula xml:id="formula_0">Q X k = X k ? ? ?(V Y k ; W ? ) ,<label>(1)</label></formula><formula xml:id="formula_1">Q Y k = Y k ? ? ?(V X k ; W ? ) .<label>(2)</label></formula><p>Then, we combine Q X k , Q Y k , and lower-level fused feature Z k?1 for in-depth feature extraction. With an element-wise addition operation ?, conducted in the corresponding k -th level block B k [x] in the ResNet-50, we finally obtain the fused features Z k that contain comprehensive spatial-temporal correlations:</p><formula xml:id="formula_2">(c) full-duplex mode of RCAM ( . ? .) C ? 1 ? 1 C ? 1 ? 1 (a) simplex mode of RCAM ( . ? .) C ? 1 ? 1 (b) simplex mode of RCAM ( . ? .) C ? 1 ? 1 Multiplication Element-wise Addition</formula><formula xml:id="formula_3">Z k = B k Q X k ? Q Y k ? Z k?1 ,<label>(3)</label></formula><p>where k ? {1 : K} denotes different feature hierarchies in the backbone. Note that Z 0 denotes the zero tensor.</p><p>In our implementation, we use the top four feature pyramids, i.e., K = 4, suggested by <ref type="bibr" target="#b117">[117,</ref><ref type="bibr" target="#b135">135]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bidirectional Purification Module</head><p>In addition to the RCAM described above, which integrates common cross-modality features, we further introduce the bidirectional purification module (BPM) to improve the model robustness.</p><p>Following the standard in action recognition <ref type="bibr" target="#b89">[89]</ref> and saliency detection <ref type="bibr" target="#b120">[120]</ref>, our bidirectional purification phase comprises N BPMs connected in a cascaded manner. As shown in <ref type="figure">Fig. 3</ref>, we first employ the feature allocator ? {F,G} (x; W {F,G} ? ) to unify the feature representations from the previous stage:</p><formula xml:id="formula_4">F n k = ? F (Z k ; W F ? ), G n k = ? G (Y k ; W G ? ),<label>(4)</label></formula><p>where k ? {1 : K} and n ? {1 : N } denote different feature hierarchies and number of BPM, respectively. To be specific,</p><formula xml:id="formula_5">? {F,G} (x; W {F,G} ?</formula><p>) is composed of two 3?3 conv, each with 32 filters to reduce the feature channels. Note that the allocator is conducive to reduce the computational burden as well as facilitate various element-wise operations.</p><p>Here, we consider a bidirectional attention scheme (see <ref type="figure" target="#fig_3">Fig. 5</ref> (c)) that contains two simplex strategies (see <ref type="figure" target="#fig_3">Fig. 5</ref> (a &amp; b)) in the BPM. On the one hand, the motion features G n k contain temporal cues and can be used to enrich the fused features F n k by the concatenation operation.</p><p>On the other, the distractors in the motion feature G n k can be suppressed by multiplicating the fused features F n k . Besides acquiring robust feature representation, we introduce an efficient cross-modal fusion strategy in this scheme, which broadcasts high-level, semantically strong features to low-level, semantically weak features via interlaced decremental connection (IDC) with a topdown pathway <ref type="bibr" target="#b66">[66]</ref>. Specifically, as the first part, the spatial-temporal feature combination branch (see <ref type="figure" target="#fig_3">Fig. 5</ref> (b)) is formulated as:</p><formula xml:id="formula_6">F n+1 k = F n k ? K i=k [F n k , P(G n i )] ,<label>(5)</label></formula><p>where P is an up-sampling operation followed by a 1?1 convolutional layer (conv) to reshape the candidate guidance to a consistent size with F n k . Symbols ? and respectively denote element-wise addition and concatenation operations with an IDC strategy 3 , followed by a 1?1 conv with 32 filters. For the other part, we formulate the temporal feature re-calibration branch (see <ref type="figure" target="#fig_3">Fig. 5</ref> (a)) as:</p><formula xml:id="formula_7">G n+1 k = G n k ? K j=k [G n k , P(F n j )],<label>(6)</label></formula><p>where denotes element-wise multiplication with an IDC strategy, followed by a 1?1 conv with 32 filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder</head><p>After feature aggregation and re-calibration with multi-pyramidal interaction, the last BPM unit produces two groups of discriminative features (i.e., F N k &amp; G N k ) with a consistent channel number of 32. We integrate pyramid pooling module (PPM) <ref type="bibr" target="#b136">[136]</ref> into each skip connection of the U-Net <ref type="bibr" target="#b86">[86]</ref> as our decoder, and only adopt the top four layers in our implementation (K = 4). Since the features are fused from high to low level, global information is well retained at different scales of the designed decoder:</p><formula xml:id="formula_8">3 For instance,? n 2 = K=4 i=2 [F n 2 , P(G n i )] = F n 2 P(G n 2 ) P(G n 3 ) P(G<label>n</label></formula><formula xml:id="formula_9">F N k = C[F N k UP(F N k+1 )],<label>(7)</label></formula><formula xml:id="formula_10">G N k = C[G N k UP(? N k+1 )].<label>(8)</label></formula><p>Here, UP indicates the upsampling operation after the pyramid pooling layer, while is a concatenation operation between two features. Then, a conv C is used for reducing the channels from 64 to 32. Lastly, we use a 1?1 conv with a single filter after the upstream output (i.e.,F N 1 &amp;? N 1 ), followed by a sigmoid activation function to generate the predictions (i.e., S t A &amp; S t M ) at frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning Objective</head><p>Given a group of predictions S t ? {S t A , S t M } and the corresponding ground-truths G t at frame t, we employ the standard binary cross-entropy loss L bce to measure the dissimilarity between output and target, which computes:</p><formula xml:id="formula_11">L bce (S t , G t ) = ? (x,y) [G t (x, y) log(S t (x, y)) + (1 ? G t (x, y)) log(1 ? S t (x, y))],<label>(9)</label></formula><p>where (x, y) indicates a coordinate in the frame. The overall loss function is then formulated as:</p><formula xml:id="formula_12">L total = L bce (S t A , G t ) + L bce (S t M , G t ).<label>(10)</label></formula><p>For final prediction, we use S t A since our experiments show that it performs better when combining appearance and motion cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Backbone Details</head><p>Without any modification, three standard ResNet-50 <ref type="bibr" target="#b36">[36]</ref> (removing the top-three layers: average pooling, fully-connected, and softmax layers) backbones are adopted for the appearance branch, the motion branch and the merging branch. Each ResNet-50 backbone results in K = 4 hierarchies inspired by previous work <ref type="bibr" target="#b117">[117]</ref>. After removing the top fully connected layers, the feature hierarchies ({X k , Y k , Z k }, k ? {2 : 5}) from shallow to deep are extracted from the conv2 3 (k = 2), conv3 4 (k = 3), conv4 6 (k = 4), and conv5 3 (k = 5) layers of the ResNet-50, respectively. Note that we have also tried a two-branches setting, namely removing the merging branch and letting</p><formula xml:id="formula_13">Z k = Q X k ? Q Y k ? Z k?1 instead of Z k = B k [Q X k ? Q Y x ? Z k?1 ] in Eq. (3)</formula><p>. Unfortunately, this leads to a 2.5% performance drop in performance concerning S ? on the DAVIS <ref type="bibr" target="#b16">16</ref>  <ref type="bibr" target="#b82">[82]</ref> dataset. This is because the third merging branch can sequentially enhance and promote the spatial-temporal features from RCAMs, leading to better segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Training Settings</head><p>We implement our model in PyTorch <ref type="bibr" target="#b79">[79]</ref>, accelerated by an NVIDIA RTX TITAN GPU. All the inputs are uniformly resized to 352?352. To enhance the stability and generalizability of our learning algorithm, we employ the multi-scale (i.e., {0.75, 1, 1.25}) training strategy <ref type="bibr" target="#b35">[35]</ref> in the training phase. As can be seen from the experimental results in Tab. 5, the variant with N =4 (the number of BPM) achieves the best performance. We utilize the stochastic gradient descent (SGD) algorithm to optimize the entire network, with a momentum of 0.9, a learning rate of 2e ?3 , and a weight decay of 5e ?4 . The learning rate decreased by 10% per 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Testing Settings and Runtime</head><p>Given a frame along with its motion map, we resize them to 352?352 and feed them into the corresponding branch. Similar to <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b113">113,</ref><ref type="bibr" target="#b141">141]</ref>, We employ the conditional random field (CRF) <ref type="bibr" target="#b52">[52]</ref> post-processing technique. The inference time of our method is 0.08s per frame, regardless of flow generation and CRF postprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Protocols</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We evaluate the proposed model on four widely used VOS datasets. DAVIS <ref type="bibr" target="#b16">16</ref>  <ref type="bibr" target="#b82">[82]</ref> is the most popular of these, and consists of 50 (30 training and 20 validation) high-quality and densely annotated video sequences. MCL <ref type="bibr" target="#b49">[49]</ref> contains 9 videos and is mainly used as testing data. FBMS <ref type="bibr" target="#b76">[76]</ref> includes 59 natural videos, in which 29 sequences are used as the training set and 30 are for testing. SegTrack-V2 <ref type="bibr" target="#b59">[59]</ref> is one of the earliest VOS datasets and consists of 13 clips. In addition, DAVSOD 19 <ref type="bibr" target="#b30">[30]</ref> was specifically designed for the V-SOD task. It is the most challenging visual attention consistent V-SOD dataset with high-quality annotations and diverse attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training</head><p>Following a similar multi-task training setup as <ref type="bibr" target="#b61">[61]</ref>, we divide our training procedure into three steps:</p><p>? We first adopt a well-known static saliency dataset DUTS <ref type="bibr" target="#b104">[104]</ref> to train the spatial branch to avoid over-fitting, like in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b93">93,</ref><ref type="bibr" target="#b110">110]</ref>. This step lasts for 50 epoches with a batch size of 8 under the same training settings mentioned in Sec. 3.6.2. ? We then train the temporal branch on the generated optical flow maps. This step lasts for 50 epoches with a batch size of 8 under the same training settings mentioned in Sec. 3.6.2. ? We finally load the weights pre-trained on two subtasks into the spatial and temporal branches, and thus, the whole network is end-to-end trained on the training set of DAVIS 16 (30 clips) and FBMS (29 clips). The last step takes about 4 hours and converges after 20 epochs with a batch size of 8 under same the training settings mentioned in Sec. 3.6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Testing</head><p>We follow the standard benchmarks <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b82">82]</ref> to test our model on the validation set (20 clips) of DAVIS <ref type="bibr" target="#b16">16</ref> , the test set of FBMS (30 clips), the test set (Easy35 split) of DAVSOD 19 (35 clips), the whole of MCL (9 clips), and the whole of SegTrack-V2 (13 clips).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We define a prediction map at frame t as S t A and its corresponding ground-truth mask as G t . The formulations of the metrics are given as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Metrics for U-VOS task</head><p>Following <ref type="bibr" target="#b129">[129]</ref>, we utilize two standard metrics to evaluate the performance of U-VOS models. Note that all prediction maps are ensured to be binary in the U-VOS task.</p><p>1. Mean Region Similarity: This metric, also called jaccard similarity coefficient, is defined as the intersection-over-union of the prediction map and the ground-truth mask. The formulation is defined as:</p><formula xml:id="formula_14">J = |S t A ? G t | |S t A ? G t | ,<label>(11)</label></formula><p>where | ? | is the number of pixels in the area. In all of our experiments, we also report the mean value of Mean-J , similar to <ref type="bibr" target="#b129">[129]</ref>.</p><p>2. Mean Contour Accuracy: Here, the contour accuracy metric we used is also called the contour F-measure. We compute the contour-based precision and recall between the contour points of c(S t A ) and c(G t ), where c(?) is the extraction of contour points of a mask. The formulation is defined as:</p><formula xml:id="formula_15">F = 2 ? Precision c ? Recall c Precision c + Recall c ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_16">Precision c = |c(S t A ) ? c(G t )|/|c(S t A )|, Recall c = |c(S t A ) ? c(G t ))|/|c(G t )|.</formula><p>Similar to <ref type="bibr" target="#b129">[129]</ref>, we also report the mean value of Mean-F in all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Metrics for V-SOD task</head><p>Unlike the U-VOS task, the prediction map can be non-binary in the V-SOD benchmarking. More details refer to Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Mean Absolute Error (MAE):</head><p>It is a typical pixel-wise measure, which is defined as:</p><formula xml:id="formula_17">M = 1 W ? H W x H y |S t A (x, y) ? G t (x, y)|,<label>(13)</label></formula><p>where W and H are the width and height of ground-truth G t , and (x, y) are the coordinates of a pixel in G t .</p><p>2. Precision-Recall (PR) Curve: Precision and recall <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">22]</ref> are defined as:</p><formula xml:id="formula_18">Precision = |S t A (T ) ? G t | |S t A (T )| ,<label>(14)</label></formula><formula xml:id="formula_19">Recall = |S t A (T ) ? G t | |G t | ,<label>(15)</label></formula><p>where S t A (T ) is the binary mask obtained by directly thresholding the prediction map S t A with the threshold T ? [0, 255], and | ? | is the total area of the mask inside the map. By varying T , a precision-recall curve can be obtained.</p><p>3. Maximum F-measure: This is defined as:</p><formula xml:id="formula_20">F ? = (1 + ? 2 )Precision ? Recall ? 2 ? Precision + Recall ,<label>(16)</label></formula><p>where ? 2 is set to 0.3 to focus more on the precision value than the recall value, as recommended in <ref type="bibr" target="#b8">[9]</ref>. We convert the non-binary prediction map into binary masks with threshold values from 0 to 255. In this paper, we report the maximum (i.e., F max ? ) of a series of F-measure values calculated from the precision-recall curve by iterating over all the thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Maximum Enhanced-Alignment Measure:</head><p>As a recently proposed metric, E ? [3] is used to evaluate both the local and global similarity between two binary maps. The formulation is as follows:</p><formula xml:id="formula_21">E ? = 1 W ? H W x H y ? S t A (x, y), G t (x, y) ,<label>(17)</label></formula><p>where ? is the enhanced-alignment matrix. Similar to F max ? , we report the maximum E ? value computed from all the thresholds in all of our comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Structure Measure: Fan et al. <ref type="bibr" target="#b28">[28]</ref> proposed a metric to measure the structural similarity between a non-binary saliency map and a groundtruth mask: <ref type="bibr" target="#b18">(18)</ref> where ? balances the object-aware similarity S o and region-aware similarity S r . We use the default setting (? = 0.5) suggested in <ref type="bibr" target="#b28">[28]</ref>.</p><formula xml:id="formula_22">S ? = (1 ? ?) ? S o (S t A , G t ) + ? ? S r (S t A , G t ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">U-VOS and V-SOD tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation on DAVIS 16 dataset</head><p>As shown in Tab. 1, we compare our FSNet with 14 SOTA U-VOS models on the DAVIS 16 public leaderboard. We also compare it with seven recent semi-supervised approaches as reference. We use a threshold of 0.5 to generate the final binary maps for a fair comparison, as recommended by <ref type="bibr" target="#b129">[129]</ref>. Our FSNet outperforms the best model (AAAI'20-MAT <ref type="bibr" target="#b141">[141]</ref>) by a margin of 2.4% in Mean-F and 1.0% in Mean-J , achieving the new SOTA performance. Notably, the proposed U-VOS model also outperforms the semi-supervised model (e.g., AGA <ref type="bibr" target="#b47">[47]</ref>), even though it utilizes the first ground-truth mask to reference object location.</p><p>We also compare FSNet against 13 SOTA V-SOD models. The non-binary saliency maps 4 are obtained from the standard benchmark <ref type="bibr" target="#b30">[30]</ref>. This can be seen from Tab. 2, our method consistently outperforms all other models since 2018 on all metrics. In particular, for the S ? and F max ? metrics, our method improves the performance by ?2.0% compared with the best AAAI'20-PCAS [34] model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation on MCL dataset</head><p>This dataset has fuzzy object boundaries in the low-resolution frames due to fast object movements. Therefore, the overall performance is lower than on DAVIS <ref type="bibr" target="#b16">16</ref> . As shown in Tab. 2, our method still stands out in these extreme circumstances, with a 3.0?8.0% increase in all metrics compared with ICCV'19-RCR <ref type="bibr" target="#b127">[127]</ref> and CVPR'19-SSAV <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Evaluation on FBMS dataset</head><p>This is one of the most popular VOS datasets with diverse attributes, such as interacting objects, dynamic backgrounds, and no per-frame annotation. As shown in Tab. 2, our model achieves competitive performance in terms of M. Further, compared to the previous bestperforming SSAV <ref type="bibr" target="#b30">[30]</ref>, it obtains improvements in other metrics, including S ? (0.890 vs. SSAV=0.879) and E max ? (0.935 vs. SSAV=0.926), making it more suitable to the human visual system (HVS) as mentioned in <ref type="bibr">[28,</ref> Tab. 2 Video salient object detection (V-SOD) performance of our FSNet, compared with 13 SOTA models on three popular V-SOD datasets, including DAVIS 16 <ref type="bibr" target="#b82">[82]</ref>, MCL <ref type="bibr" target="#b49">[49]</ref>, and FBMS <ref type="bibr" target="#b76">[76]</ref>. ' ?' denotes that we generate non-binary saliency maps without CRF <ref type="bibr" target="#b52">[52]</ref> for a fair comparison. 'N/A' means the results are not available.</p><p>DAVIS <ref type="bibr" target="#b16">16</ref>  <ref type="bibr" target="#b82">[82]</ref> MCL <ref type="bibr" target="#b49">[49]</ref> FBMS <ref type="bibr" target="#b76">[76]</ref>   <ref type="bibr" target="#b30">[30]</ref>. ' ?' denotes that we generate non-binary saliency maps without CRF <ref type="bibr" target="#b52">[52]</ref> for a fair comparison. 'N/A' means the results are not available. </p><formula xml:id="formula_23">Model S ? ? E max ? ? F max ? ? M ? S ? ? E max ? ? F max ? ? M ? S ? ? E max ? ? F max ? ? M</formula><formula xml:id="formula_24">DAVSOD 19 -Easy35 DAVSOD 19 -Normal25 DAVSOD 19 -Difficult20 Model S ? ? E max ? ? F max ? ? M ? S ? ? E max ? ? F max ? ? M ? S ? ? E max ? ? F max ? ? M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Evaluation on SegTrack-V2 dataset</head><p>This is the earliest VOS dataset from the traditional era. Thus, only a limited number of deep U-VOS models have been tested on it. We only compare our FSNet against the top-3 models: AAAI'20-PCAS <ref type="bibr" target="#b34">[34]</ref> (S ? =0.866), ICCV'19-RCR <ref type="bibr" target="#b127">[127]</ref> (S ? =0.842), and CVPR'19-SSAV <ref type="bibr" target="#b30">[30]</ref> (S ? =0.850). Our method achieves the best performance (S ? =0.870).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Evaluation on DAVSOD 19 dataset</head><p>Recently published, DAVSOD 19 <ref type="bibr" target="#b30">[30]</ref> is the most challenging visual attention consistent V-SOD dataset with high-quality annotations and diverse attributes. It contains diversified challenging scenarios due to the video sequences containing shifts in attention. DAVSOD 19 is divided into three subsets, according to difficulty: DAVSOD 19 -Easy-35 (35 clips), DAVSOD 19 -Normal25 (25 clips), and DAVSOD 19 -Difficult20 (20 clips). Note that, in the saliency field, non-binary maps are required for evaluation; thus, we only report the results of FSNet without CRF post-processing in benchmarking the V-SOD task. In this document, we adopt the four metrics mentioned in Sec. 4.2.2, including S ? , E max ? , F max ? , and M. For showing the robustness of FSNet, in Tab. 3, we also make the first effort to benchmark all 11 SOTA models since 2018, in terms of the three difficulty levels:</p><p>? Easy35 subset: Most of the video sequences are similar to those in the DAVIS <ref type="bibr" target="#b16">16</ref> dataset, which also consists of a large number of single video objects. We see that FSNet outperforms all the reported algorithms across all metrics. As shown in Tab. 3, compared with the recent method (PCSA), our model achieves large improvements of 3.2% in terms of S ? . ? Normal25 subset: Different from previous subsets, this one includes multiple moving salient objects. Thus, it is more difficult than traditional V-SOD datasets due to the attention shift phenomena <ref type="bibr" target="#b30">[30]</ref>. As expected, FSNet still obtains the best performance, with significant improvement, e.g., 6.4% for F max ? metric.</p><p>? Difficult20 subset: This is the most challenging subset in existing V-SOD datasets since it contains a large number of attention shift sequences under cluttered scenarios. Therefore, from the results shown in Tab. 3, the performances of all the compared models decrease dramatically (e.g., F max ? ? 0.5). Even though our framework is not specifically designed for the V-SOD task, we still easily obtain the best performance in two metrics (e.g., S ? and F max ? ). Different from the best two models, which utilize additional training data (i.e., RCR leverages pseudo-labels, SSAV utilizes the validation set), our model does not use any additional training data and still outperforms the SSAV model by 8.8% (F max ? ), and achieves comparable performance to the second-best RCR (ICCV'19) model. These results are also supported by recent conclusions that "human visual attention should be an underlying mechanism that drives U-VOS and V-SOD" (TPAMI'20 <ref type="bibr" target="#b107">[107]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">PR Curve</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we further investigate the precision-recall curves of different models on six V-SOD datasets, including DAVIS 16 <ref type="bibr" target="#b82">[82]</ref>, MCL <ref type="bibr" target="#b49">[49]</ref>, FBMS <ref type="bibr" target="#b76">[76]</ref>, and DAVSOD 19 <ref type="bibr" target="#b30">[30]</ref> (i.e., Easy35, Normal25, and Difficult20). Note that the higher and more to the right in the PR curve, the more accurate performance. Even though existing SOTA methods have achieved significant progress in the V-SOD task on three typical benchmark datasets, we still obtain the best performance under all thresholds. Besides, as a recent and challenging dataset, the overall performances on the three subsets of DAVSOD 19 <ref type="bibr" target="#b30">[30]</ref> are relatively poor. However, our FSNet again achieves more satisfactory performance by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.7">Qualitative Results</head><p>Some qualitative results on the five datasets are shown in <ref type="figure">Fig. 7</ref>, validating that our method achieves high-quality U-VOS and V-SOD results. As can be seen in the 1 st row, the behind camel did not move, so it does not get noticed. Interestingly, as our fullduplex strategy model considers both appearance and motion bidirectionally, it can automatically predict the dominated camel in the centre of the video instead of the camel behind. A similar phenomenon is also presented in the 5 th row, our method successfully detects dynamic skiers with the video clip rather than the static man in the background. Overall, for these challenging situations, e.g., dynamic background <ref type="bibr">(</ref>  <ref type="bibr" target="#b82">[82]</ref>, MCL <ref type="bibr" target="#b49">[49]</ref>, FBMS <ref type="bibr" target="#b76">[76]</ref>, SegTrack-V2 <ref type="bibr" target="#b59">[59]</ref>, and DAVSOD 19 <ref type="bibr" target="#b30">[30]</ref>.</p><p>&amp; 5 th rows), fast-motion (4 rd row), out-of-view (6 rd &amp; 7 nd row), occlusion (7 nd row), and deformation (8 th row), our model is able to infer the real target object(s) with fine-grained details. From this point of view, we demonstrate that FSNet is a general framework for both U-VOS and V-SOD tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we conduct ablation studies to analyse our FSNet, including stimulus selection (Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Stimulus Selection</head><p>We explore the influence of different stimuli (appearance only vs. motion only) in our framework. We use only video frames or motion maps (using <ref type="bibr" target="#b42">[42]</ref>) to train the ResNet-50 <ref type="bibr" target="#b36">[36]</ref> backbone together with the proposed decoder block (see Sec. <ref type="bibr">3.4)</ref>. As shown in Tab. 4, M o. performs slightly better than App. in terms of S ? on DAVIS <ref type="bibr" target="#b16">16</ref> , which suggests that the "optical flow" setting can learn more visual cues than "video frames". Nevertheless, App. outperforms M o. in M metric on MCL. This motivates us to explore how to use appearance and motion cues simultaneously effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effectiveness of RCAM</head><p>To validate our RCAM (Rel.) effectiveness, we replace our fusion strategy with the vanilla fusion (Vanilla) using a concatenate operation followed by a convolutional layer to fuse two modalities. As expected (Tab. 4), the proposed Rel. performs consistently better than the vanilla fusion strategy on both DAVIS <ref type="bibr" target="#b16">16</ref> and MCL datasets. We would like to point out that our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tab. 6</head><p>Ablation study for the simplex and full-duplex strategies on DAVIS 16 <ref type="bibr" target="#b82">[82]</ref> and MCL <ref type="bibr" target="#b49">[49]</ref>. We set N = 4 for BPM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCAM has two important properties:</head><p>? It enables mutual correction and attention.</p><p>? It can alleviate error propagation within a network to an extent due to the mutual correction and bidirectional interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Effectiveness of BPM</head><p>To illustrate the effectiveness of the BPM (with N = 4), we derive two different models: Rel. and FSNet, referring to the framework without or with BPM. We observe that the model with BPM gains 2.0?3.0% than the one without BPM, according to the statistics in Tab. 4. We attribute this improvement to BPM's introduction of an interlaced decremental connection, enabling it to fuse the different signals effectively. Similarly, we remove the RCAM and derive another pair of settings (Vanilla &amp; Bi-Purf.) to test the robustness of our BPM. The results show that even using the bidirectional vanilla fusion strategy (Bi-Purf.) can still enhance the stability and generalization of the model. This benefits from the purification forward process and re-calibration backward process in the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Number of Cascaded BPMs</head><p>Naturally, more cascaded BPMs should lead to better boost performance. This is investigated and the evaluation results are shown in Tab. 5, where N = {0, 2, 4, 6, 8}. Note that N = 0 means that NO BPM is used. Clearly, as can be seen from <ref type="figure" target="#fig_8">Fig. 8 (red star)</ref>, we compare four variants of our FSNet, including N =0 <ref type="bibr" target="#b2">3)</ref>, and N =4, CRF (Mean-J =83.4, Mean-F=83.1). It demonstrates that more BPMs leads to better results, but the performance reaches saturation after N = 4. Further, too many BPMs (i.e., N &gt; 4) will cause high model-complexity and increase the over-fitting risk. As a trade-off, we use N = 4 throughout our experiments.</p><formula xml:id="formula_25">(Mean-J =76.4, Mean-F=76.8), N =2 (Mean-J =80.4, Mean-F=81.4), N =4 (Mean-J =82.1, Mean-F=83.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Effectiveness of Full-Duplex Strategy</head><p>To investigate the effectiveness of the RCAM and BPM modules with the full-duplex strategy, we study two unidirectional (i.e., simplex strategy in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Prediction Selection</head><p>Which is the final prediction, S t A or S t M ? As mentioned in Sec. 3.5, we choose S t A as our final segmentation result instead of S t M . The major reasons for doing so can be summarized as follows:</p><p>? We employ the auxiliary supervision for the motion-based branch to learn more motion patterns inspired by <ref type="bibr" target="#b96">[96]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effectiveness of CRF</head><p>From <ref type="figure" target="#fig_8">Fig. 8</ref> we can see that our FSNet without CRF post-processing technique, i.e., FSNet (N =4), still outperforms the best model AAAI'20-MAT in terms of Mean-F metric. This means that our initial method (i.e., FSNet without CRF) can distinguish hard samples around the object boundaries without postprocessing techniques. When equipped with the CRF post-processing technique <ref type="bibr" target="#b52">[52]</ref>, our FSNet (N =4, CRF) achieves the best performance in terms of both Mean-J and Mean-F metrics. <ref type="bibr" target="#b76">76</ref> 77 78 79 80 81 82 <ref type="bibr">83 84</ref> Mean region similarity, (%) Four variants are shown in bold-italic, in which 'N ' indicates the number of bidirectional purification modules (BPM) and 'CRF' means that using CRF <ref type="bibr" target="#b52">[52]</ref> postprocessing technique. Compared with the best unsupervised VOS model (i.e., MAT <ref type="bibr" target="#b141">[141]</ref> also with CRF), the proposed method FSNet (N =4, CRF) achieves the new SOTA by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Training Effectiveness with Less Data</head><p>As shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, the proposed method, i.e., FSNet (N =4, CRF), surpasses the best U-VOS model MAT <ref type="bibr" target="#b141">[141]</ref> (also with CRF), while our FSNet with less labelled data in the training phase (i.e., Ours-13K vs. MAT-16K). Besides, we also observe that the recently proposed method 3DC-Seg <ref type="bibr" target="#b71">[71]</ref>, based on a 3D convolutional network, can achieve the new stateof-the-art (Mean-J =84.3, Mean-F=84.7), while relies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interlaced Decremental</head><p>Connection (IDC) (a) self-purification mode of BPM . + . ? . <ref type="figure">Fig. 9</ref> Illustration of self-purification strategy (a) and the proposed bidirectional purification strategy (b).</p><p>Note that sub-figure (b) is same as the <ref type="figure" target="#fig_3">Fig. 5 (c)</ref> for convenient comparison. Note that ?, ?, and denote element-wise addition, multiplication, and concatenation, respectively. on a massive amount of labelled training samples as expert knowledge in the fine-tuning phase, including 158K images (i.e., COCO [67] + YouTube-VOS <ref type="bibr" target="#b126">[126]</ref> + DAVIS 16 <ref type="bibr" target="#b82">[82]</ref>). It requires about ten times more training data than the best model MAT <ref type="bibr" target="#b141">[141]</ref> (16K images) in the fine-tuning phase. Thus, it demonstrates the efficient training process in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Self-Purification Strategy in BPM</head><p>We provide more details on the different variants mentioned in Sec Here, note that all of these variants indicate unidirectional refinement, in contrast to the proposed bi-directional schemes.</p><p>Last but not least, to validate that the gains of bidirectional schemes in practice DO COME FROM the bi-directional procedure and not more complex model structures, we implement another variant using the same complex structures but without any branch interactions before the decoding stage. This is done by exchanging the places of G n k and F n k as illustrated in <ref type="figure">Fig. 9 (b)</ref>, leading to a kind of "selfpurification" strategy. Symbol " " in <ref type="figure">Fig. 9</ref> (a) means that there is NO interaction between the two branches, i.e., there is only interaction within itself. Comparisons of the uni-/bidirectional strategies are shown in Tab. 6.</p><p>The comparison results show that our elaborately designed modules (i.e., RCAM and BPM) jointly cooperate in a bidirectional manner and outperform all unidirectional settings. Besides, our bidirectional purification scheme (i.e., 'fulldup.' in Tab. 6) also achieves very notable improvement (2.1% and 1.0% gains in S ? on DAVIS 16 <ref type="bibr" target="#b82">[82]</ref> and MCL <ref type="bibr" target="#b49">[49]</ref>, respectively) against the "self-purification" variant (i.e., 'self-purf.' in Tab. 6), which has a similar complex structure, further validating the benefit of the bidirectional behavior claimed in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Relation Between RCAM and BPM</head><p>The two introduced modules, i.e., RCAM and BPM, focus on using appearance and motion features while ensuring the information flow between them. They can work collaboratively under the mutual restraint of our full-duplex strategy, but they cannot be substituted for one another. This is due to the RCAM transmits the features at each level in a point-to-point manner (e.g., X 1 ? Y 1 ), and thus, it fits with the progressive feature extraction in the encoder. The BPM, on the other hand, broadcasts high-level features to low-level features via an interlaced decremental connection in a set-to-point manner (e.g., {F n 2 , F n 3 , F n 4 } ? G n 2 ), which is more suitable for the multi-level feature interaction in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a simple yet efficient framework, termed full-duplex strategy network (FSNet), that fully leverages the mutual constraints of appearance and motion cues to address the video object segmentation problem. It consists of two core modules: the relational cross-attention module (RCAM) in the encoding stage and the efficient bidirectional purification module (BPM) in the decoding stage. The former one is used to abstract features from a dualmodality, while the latter is utilized to re-calibrate inconsistant features step-by-step. We thoroughly validate functional modules of our architecture via extensive experiments, leading to several interesting findings. Finally, FSNet acts as a unified solution that significantly advances SOTA models for both U-VOS and V-SOD tasks. In the future, we may extend our scheme to learn short-term and long-term information in an efficient Transformer-like framework <ref type="bibr" target="#b114">[114,</ref><ref type="bibr" target="#b143">143]</ref> to further boost the accurarcy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2 Visual comparison between the simplex (i.e., (a) appearance-refined motion and (b) motion-refined appearance) and our full-duplex strategy under our framework. In contrast, our FSNet offers a collaborative way to leverage the appearance and motion cues under the mutual restraint of full-duplex strategy, thus providing more accurate structure details and alleviating the short-term feature drifting issue [129].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Illustration of our Relational Cross-Attention Module (RCAM) with a simplex (a &amp; b) and full-duplex (c) strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>4 ) when k = 2 and K = 4.Interlaced Decremental Connection (IDC) (b) simplex mode of BPM . + . ? . (a) simplex mode of BPM . + . ? . (c) full-duplex mode of BPM . + . ? .MultiplicationElement-wise AdditionConcatenation Illustration of our Bidirectional Purification Module (BPM) with a simplex and full-duplex strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>DAVSOD-Difficult-20 2018-CVPR-FGRNE 2018-ECCV-MBN 2018-TCSVT-SCNN 2018-TIP-DLVSD 2019-CVPR-SSAV 2019-ICCV-RCRNet 2019-TCSVT-RSE 2019-TIP-LSTI 2019-TIP-SRP 2019-TMM-MESO 2019-TMM-SPD FSNet Precision-recall curves of SOTA V-SOD methods and the proposed FSNet across six datasets. Zoom in for details and the best view in color for friendlier observation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. 4.4.1), effectiveness of RCAM (Sec. 4.4.2) and BPM (Sec. 4.4.3), number of cascaded BPMs (Sec. 4.4.4), and effectiveness of full-duplex strategy (Sec. 4.4.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>M ? S ? ? M ? simplex App. ? M o. (App. + M o.) ? M o. 0.896 0.026 0.816 0.038 App. ? M o. (App. + M o.) ? M o. 0.902 0.025 0.832 0.031 App. ? M o. (App. + M o.) ? M o. 0.891 0.029 0.806 0.039 App. ? M o. (App. + M o.) ? M o. 0.897 0.028 0.840 0.028 self-purf. App. ? M o. (App. + M o.) M o. 0.899 0.026 0.854 0.023 full-dup. App. ? M o. (App. + M o.) ? M o. 0.920 0.020 0.864 0.023</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>. 4 &amp;</head><label>4</label><figDesc>Fig. 5) variants of our model. In Tab. 6, the symbols ?, ?, and ? indicate the feature transmission directions in the designed RCAM or BPM. Specifically, App. ? M o. indicates that the attention vector in the optical flow branch weights the features in the appearance branch and vice versa. (App.+M o.) ? M o. indicates that motion cues are used to guide the fused features extracted from both appearance and motion. The comparison results show that our elaborately designed modules (RCAM and BPM) jointly cooperate in a full-duplex fashion and outperform all simplex (unidirectional ) settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8</head><label>8</label><figDesc>Mean contour accuracy (F ) vs. mean region similarity (J ) scores on DAVIS 16 dataset<ref type="bibr" target="#b82">[82]</ref>.Circles indicate U-VOS methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. 4.4.5 including (App. + M o.) ? M o., (App.+M o.) ? M o. and (App.+M o.) ? M o. in BPM. The implementation of App. ? M o. and App. ? M o. in RCAM are illustrated in Fig. 4 (a) &amp; (b), whereas the structure implementation of (App. + M o.) ? M o. &amp; (App. + M o.) ? M o. in BPM are illustrated in Fig. 5 (a) &amp; (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The results further demonstrate the validity and rationality of our current design under various conditions. ? We provide more details to the conference version, such as backbone details (see Sec. 3.6.1), evaluation metrics (see Sec. 4.2), prediction selection (see Sec. 4.5.1), and post-processing techniques (see Sec. 4.5.2).</figDesc><table><row><cell>? We further present more performances under</cell></row><row><cell>different thresholds used in the final results (i.e.,</cell></row><row><cell>PR curve in Fig. 6). Additional test results (i.e.,</cell></row><row><cell>DAVSOD 19 -Normal25, DAVSOD 19 -Difficult20) on</cell></row><row><cell>a recent challenging dataset still show that our</cell></row><row><cell>framework is superior to existing SOTA models</cell></row><row><cell>(see Tab. 3).</cell></row><row><cell>, and Fig. 7) and</cell></row><row><cell>discussions (see Sec. 4.5) of our manuscript.</cell></row></table><note>? We investigate the self-purification mode of BPM under our FSNet (see Fig. 9 and Sec. 4.5.4), the relation between RCAM and BPM (see Sec. 4.5.5), and the training effectiveness with less data (see Sec. 4.5.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Tab. 1 Video object segmentation (VOS) performance of our FSNet, compared with 14 SOTA unsupervised models and seven semi-supervised models on DAVIS<ref type="bibr" target="#b16">16</ref> [82] validation set. 'w/ Flow': the optical flow algorithm is used. 'w/ CRF': conditional random field<ref type="bibr" target="#b52">[52]</ref> is used for post-processing. The best scores are marked in bold.FSNet MAT AGNN AnDiff COS AGS EpO+ MOA LSMO ARP LVO LMP SFL ELM FST CFBI AGA RGM FEEL FA OS MSK ? 83.4 82.1 82.4 80.7 81.7 80.5 79.7 80.6 77.2 78.2 76.2 75.9 70.0 67.4 61.8 55.8 85.3 81.5 81.5 81.1 82.4 79.8 79.7 Mean-F ? 83.1 83.3 80.7 79.1 80.5 79.5 77.4 75.5 77.4 75.9 70.6 72.1 65.9 66.7 61.2 51.1 86.9 82.2 82.0 82.2 79.5 80.6 75.4</figDesc><table><row><cell></cell><cell>Unsupervised</cell><cell>Semi-supervised</cell></row><row><cell>Metrics</cell><cell>(Ours) [141] [105] [129] [68] [113] [26] [92]</cell><cell>[98] [51] [97] [96] [21] [54] [78] [130] [47] [121] [102] [20] [11] [81]</cell></row><row><cell>w/ Flow</cell><cell></cell><cell></cell></row><row><cell>w/ CRF</cell><cell></cell><cell></cell></row><row><cell>Mean-J</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Qualitative results on five datasets, including DAVIS 16</figDesc><table><row><cell></cell><cell>time</cell></row><row><cell>DAVIS'16</cell><cell>camel</cell></row><row><cell>DAVIS'16</cell><cell>car-shadow</cell></row><row><cell>DAVSOD'19</cell><cell>select_0620</cell></row><row><cell>DAVSOD'19</cell><cell>select_0624</cell></row><row><cell>DAVSOD'19</cell><cell>select_0647</cell></row><row><cell>FBMS</cell><cell>cats01</cell></row><row><cell>MCL</cell><cell>Road</cell></row><row><cell>SegTrackV2</cell><cell>bird_of_para.</cell></row><row><cell cols="2">Fig. 7</cell></row><row><cell></cell><cell>1 st</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Tab. 4 Ablation studies (Sec. 4.4.1, Sec. 4.4.2, &amp; Sec. 4.4.3) for our components on DAVIS<ref type="bibr" target="#b16">16</ref> and MCL. We set N = 4 for BPM.</figDesc><table><row><cell></cell><cell cols="3">Component Settings</cell><cell></cell><cell>DAVIS 16</cell><cell cols="2">MCL</cell></row><row><cell></cell><cell cols="7">Appearance Motion RCAM BPM S ? ? M ? S ? ? M ?</cell></row><row><cell>App.</cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.834 0.047 0.754 0.038</cell></row><row><cell>M o.</cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.858 0.039 0.763 0.053</cell></row><row><cell>Vanilla</cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.871 0.035 0.776 0.046</cell></row><row><cell>Rel.</cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.900 0.025 0.833 0.031</cell></row><row><cell>Bi-Purf.</cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.904 0.026 0.855 0.023</cell></row><row><cell>FSNet  ?</cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.920 0.020 0.864 0.023</cell></row><row><cell>Tab. 5</cell><cell cols="7">Ablation study for the number (N ) of BPMs on</cell></row><row><cell cols="8">DAVIS 16 [82] and MCL [49], focusing on parameter and FLOPs</cell></row><row><cell cols="4">of BPMs, and runtime of FSNet.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Param.</cell><cell>FLOPs</cell><cell>Runtime</cell><cell cols="2">DAVIS 16</cell><cell cols="2">MCL</cell></row><row><cell></cell><cell>(M)</cell><cell>(G)</cell><cell>(s/frame)</cell><cell>S ? ?</cell><cell>M ?</cell><cell>S ? ?</cell><cell>M ?</cell></row><row><cell>N = 0</cell><cell>0.000</cell><cell>0.000</cell><cell>0.03</cell><cell>0.900</cell><cell>0.025</cell><cell>0.833</cell><cell>0.031</cell></row><row><cell>N = 2</cell><cell>0.507</cell><cell>1.582</cell><cell>0.05</cell><cell>0.911</cell><cell>0.026</cell><cell>0.843</cell><cell>0.028</cell></row><row><cell>N = 4</cell><cell>1.015</cell><cell>3.163</cell><cell>0.08</cell><cell cols="2">0.920 0.020</cell><cell cols="2">0.864 0.023</cell></row><row><cell>N = 6</cell><cell>1.522</cell><cell>4.745</cell><cell>0.10</cell><cell>0.918</cell><cell>0.023</cell><cell>0.863</cell><cell>0.023</cell></row><row><cell>N = 8</cell><cell>2.030</cell><cell>6.327</cell><cell>0.13</cell><cell>0.920</cell><cell>0.023</cell><cell>0.864</cell><cell>0.023</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ablation study (Sec. 4.5.1) for the choice of final segmentation result on DAVIS<ref type="bibr" target="#b16">16</ref> <ref type="bibr" target="#b82">[82]</ref> and MCL<ref type="bibr" target="#b49">[49]</ref> dataset.</figDesc><table><row><cell>Tab. 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DAVIS 16</cell><cell>MCL</cell><cell></cell></row><row><cell></cell><cell>S ? ?</cell><cell>M ?</cell><cell>S ? ?</cell><cell>M ?</cell></row><row><cell>(a) S t M as result</cell><cell>0.920</cell><cell>0.022</cell><cell>0.862</cell><cell>0.024</cell></row><row><cell>(b) (S t A + S t M )/2 as result</cell><cell>0.920</cell><cell>0.022</cell><cell>0.863</cell><cell>0.023</cell></row><row><cell>(c) S t A as result (Ours)</cell><cell>0.920</cell><cell>0.022</cell><cell>0.864</cell><cell>0.023</cell></row><row><cell cols="5">computational cost, we choose S t A as our final result for</cell></row><row><cell cols="2">comparison with other methods.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>? More informative appearance and motion cues</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>are contained in another branch at the phase of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>bidirectional purification.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>As shown in Tab. 7, three experiments are conducted</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>to verify our assumption: (a) choosing S t M as the final</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>result, (b) choosing (S t A + S t M )/2 as the final result,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and (c) choosing S t A as the final result (Ours). As can</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>be seen in Tab. 7, all three choices achieve very similar</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>results, while S t A performs slightly better than the other</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>two. Besides, considering the reduction of unnecessary</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that all compared maps in the V-SOD task, including ours, are non-binary.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth-supported real-time video segmentation with the kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>W?rg?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Full duplex radios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcmilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="777" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVMJ</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 davis challenge on vos: Unsupervised multi-object segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2051" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scribblebox: Interactive annotation framework for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="293" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved robust video saliency detection based on long-term spatial-temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1090" to="1100" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">State-aware tracker for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9384" to="9393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scom: Spatiotemporal constrained optimization for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3345" to="3357" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motionappearance interactive encoding for object segmentation in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1613" to="1624" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modular interactive video object segmentation: Interaction-tomask, propagation and difference-aware fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5559" to="5568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video saliency detection via sparsitybased reconstruction and propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4819" to="4831" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10713" to="10720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5912" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Epo-net: Exploiting geometric constraints on dense trajectories for motion saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1873" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video segmentation by nonlocal consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cognitive vision inspired object segmentation metric and loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCIENTIA SINICA Informationis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1846" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video segmentation with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="760" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid constrained self-attention network for fast video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10869" to="10876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interactive video object segmentation using global and local transfer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning position and target consistency for memorybased video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4144" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motion-guided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1400" to="1409" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation using motion saliency-guided spatio-temporal propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast video object segmentation with temporal aggregation network and dynamic template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8879" to="8889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Click carving: Segmenting objects in video with point clicks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (Human Computation and Crowdsourcing)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Progressively normalized selfattention network for video polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Full-duplex strategy for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8953" to="8962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency detection for video sequences based on random walk with restart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2552" to="2564" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Matters of intelligence</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="115" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7417" to="7425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">E3SN: Efficient End-to-End Siamese Network for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="701" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Extending layered models to 3d motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deeply supervised 3d recurrent fcn for salient object detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Video salient object detection using spatiotemporal deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5002" to="5015" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contourconstrained superpixels for image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Flow guided recurrent neural encoder for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3243" to="3252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation with motion-based bilateral networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="207" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Accurate and robust video saliency detection via self-paced diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1167" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Flow adaptive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">103864</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3623" to="3632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">1 year, 1000 km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">IJRR</biblScope>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Making a case for 3d convolutions for object segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hennen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Memory aggregation networks for efficient interactive video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10366" to="10375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Tased-net: Temporallyaggregating spatial encoder-decoder network for video saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2394" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6819" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Video captioning with transferred semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6504" to="6512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Automatic video object segmentation based on visual and motion saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3083" to="3094" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3227" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Temporally consistent superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jachalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning fast and robust target models for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="208" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12367</biblScope>
			<biblScope unit="page" from="629" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Video object segmentation using teacher-student adaptation in a human robot interaction (hri) setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICRA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Weakly supervised salient object detection with spatiotemporal cascade neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1973" to="1984" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3386" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4481" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="282" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Online video seeds for temporal window objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Context modulated dynamic networks for actor and action video segmentation with language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12152" to="12159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Robust video object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3137" to="3148" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Paying attention to video object pattern understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2413" to="2428" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Semisupervised video object segmentation with supertrajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Supertrajectory for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1671" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Saliencyaware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3064" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Ranking video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">F3net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12321" to="12328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">DMVOS: Discriminative matching for realtime video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Guided search: an alternative to the feature integration model for visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J EXP PSYCHOL HUMAN</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">419</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Online meta adaptation for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1205" to="1217" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="626" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Video saliency detection via graph clustering with motion energy and spatiotemporal objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2790" to="2805" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Video salient object detection via robust seeds extraction and multi-graphs manifold propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2191" to="2206" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Semi-supervised video salient object detection using pseudo-labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7284" to="7293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Anchor diffusion for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page" from="332" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Learning to recommend frame for interactive video object segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15445" to="15454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Event-based analysis of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Dual temporal memory network for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1515" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">A transductive approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6949" to="6958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Weakly supervised video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16826" to="16835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Cascaded convlstms using semantically-coherent data synthesis for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Piao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="132120" to="132129" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVMJ</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Matnet: Motion-attentive transition network for zeroshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8326" to="8338" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Motion-attentive transition for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13066" to="13073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Improving video saliency detection via localized estimation and spatiotemporal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2993" to="3007" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Kaleido-bert: Visionlanguage pre-training on fashion domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12647" to="12657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
