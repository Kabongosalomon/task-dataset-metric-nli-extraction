<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCROLLS: Standardized CompaRison Over Long Language Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Segal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avia</forename><surname>Efrat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Yoran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Haviv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI ? IBM Research ? Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCROLLS: Standardized CompaRison Over Long Language Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Standard benchmarks ? la GLUE <ref type="bibr" target="#b29">(Wang et al., 2018</ref><ref type="bibr" target="#b28">(Wang et al., , 2019</ref>, WMT <ref type="bibr" target="#b1">(Barrault et al., 2019</ref><ref type="bibr">(Barrault et al., , 2020</ref>, and SQuAD <ref type="bibr" target="#b21">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b20">(Rajpurkar et al., , 2018</ref>, have driven progress in natural language processing of short utterances. However, a large portion of natural language is produced in the context of longer discourses, such as books, articles, meeting transcripts, etc. To tackle the computational challenges associated with processing such long sequences, a plethora of new model architectures have recently emerged <ref type="bibr">(Tay et al., 2020b;</ref><ref type="bibr">Fournier et al., 2021)</ref>, without establishing a standard scheme for evaluating them on long natural language problems. Some long-context models are evaluated via language modeling perplexity, but this metric mostly captures model sensitivity to local, shortrange patterns <ref type="bibr">(Khandelwal et al., 2018;</ref><ref type="bibr">Sun et al., 1</ref> https://www.scrolls-benchmark.com Popular Datasets SCROLLS <ref type="figure">Figure 1</ref>: The distribution of words per input in SCROLLS datasets (blue), alongside frequently-used NLP datasets (pink). Dashed vertical lines indicate the maximal sequence length (in tokens) of BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and GPT3 <ref type="bibr">(Brown et al., 2020).</ref> 2021). Other studies rely on Long Range Arena <ref type="bibr" target="#b25">(Tay et al., 2021)</ref>, which is limited from a naturallanguage perspective, since only two of its datasets involve natural language, and those are artificiallyelongated through byte tokenization. To enable the research community to go beyond sentences and paragraphs, we present a new benchmark, SCROLLS: Standardized CompaRison Over Long Language Sequences.</p><p>SCROLLS incorporates multiple tasks (summarization, question answering, and natural language inference) over various domains (literature, meeting transcripts, TV shows, scientific articles, and more), where each example's input typically contains thousands of words. We review the existing literature on long-text tasks and manually curate a subset of 7 datasets, prioritizing those that require contextualizing and abstracting information across multiple parts of the text. We then clean and convert the data to a unified text-to-text format to enable the evaluation of a single model over all datasets. <ref type="figure">Figure 1</ref> shows that the texts in SCROLLS datasets are substantially longer than commonlyused NLP benchmarks. Moreover, our analysis reveals that, in SCROLLS, critical information is spread out across longer distances within the input documents.</p><p>SCROLLS is available via the Datasets library <ref type="bibr" target="#b12">(Lhoest et al., 2021)</ref> or direct download on its website, which hosts a live leaderboard that accepts submissions and automatically evaluates them against private test sets. By producing a single aggregate score, in addition to individual dataset scores, SCROLLS can serve as an evaluation platform for future approaches to processing long text, whether by new pretraining schemes, novel transformer architectures and alternatives, or even retrieval-based methods. We provide initial baselines for SCROLLS using two transformer models, <ref type="bibr">BART (Lewis et al., 2020)</ref>, and its length-efficient variant, Longformer Encoder-Decoder <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>. Our experiments indicate that SCROLLS poses a formidable challenge for these models, leaving much room for the research community to improve upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Contemporary</head><p>Evaluation of Long-Text Models While transformers <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> are the current go-to architecture for building state-of-theart models in NLP, they present a computational challenge when it comes to long sequences due to the O(n 2 ) complexity of self-attention, where n is the sequence's length. To address this problem, a wide variety of efficient alternatives and approximations have been proposed over the past couple of years <ref type="bibr">(Tay et al., 2020b;</ref><ref type="bibr">Fournier et al., 2021)</ref>. Much of these novel architectures were developed concurrently, leading to somewhat of a "Wild West" when it comes to model evaluation, making crossmodel comparison challenging. Roughly speaking, we can cluster the more prominent evaluation methodologies into three categories: language modeling, Long-Range Arena, and summarization.</p><p>The language modeling community typically uses perplexity to measure how well models predict the next token, a practice that has been adopted by several works on efficient transformer architectures <ref type="bibr" target="#b6">Choromanski et al., 2020;</ref><ref type="bibr" target="#b24">Tay et al., 2020a;</ref><ref type="bibr" target="#b17">Peng et al., 2021)</ref>. However, using perplexity to evaluate a model's long-range abilities is currently under scrutiny. A growing amount of literature shows that predicting the next token is mostly a local task that does not require modeling long-range dependencies <ref type="bibr">(Khandelwal et al., 2018;</ref><ref type="bibr" target="#b23">Sun et al., 2021)</ref>, and that masking or down-weighting distant tokens can actually improve perplexity <ref type="bibr">(Press et al., 2021a,b)</ref>.</p><p>A more recent approach to standardizing longsequence model evaluation is the Long Range Arena (LRA) <ref type="bibr" target="#b25">(Tay et al., 2021)</ref>. It incorporates 5 classification datasets: byte-level sentiment analysis (IMDB) and document relatedness (ACL Anthology); path-finding (Pathfinder) and image classification (CIFAR-10) over 1-dimensional pixel sequences; and executing a list of mathematical operations (ListOps). Of those, two involve visual reasoning, and one is a synthetic mathematical language (ListOps), leaving only two natural language datasets (sentiment analysis and document relatedness). The multi-modal nature of LRA makes it inappropriate as a testbed for pretrained language models, limiting its relevance for NLP. Moreover, LRA artificially inflates natural language sequences via byte tokenization, and truncates each example at 4,000 bytes, which is equivalent to less than 1,000 words. This exempts models from coping with the complex long-range dependencies that exist in naturally long texts.</p><p>The third practice uses summarization tasks to evaluate long-sequence models. The most popular datasets use abstracts of academic papers on arXiv and PubMed <ref type="bibr" target="#b7">(Cohan et al., 2018)</ref> as summaries. Other summarization datasets, however, are less frequently used, biasing the evaluation towards academic domains. SCROLLS includes summarization as one of its main tasks, selecting datasets from several different domains to increase diversity.</p><p>3 The SCROLLS Benchmark SCROLLS aims to challenge a model's ability to process long texts in the wild, and therefore focuses on discourses that are naturally long, encompassing domains such as literature, TV show scripts, scientific articles, and more. We review the datasets in existing literature, seeking ones that challenge models not only by the length of each input, but also by the need to process long-range dependencies across different sections. At the same time, we  Through this curation process, we handpick 7 datasets, and process them into a uniform textto-text format. <ref type="table" target="#tab_1">Table 1</ref> provides an overview of the datasets included in SCROLLS. <ref type="figure">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref> show two examples from SCROLLS datasets SummScreenFD and QuALITY, demonstrating how contextualizing and synthesizing information over long ranges of text is paramount to addressing the challenges in the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We survey the 7 datasets in SCROLLS, and elaborate how the original data was collected.</p><p>GovReport <ref type="bibr">(Huang et al., 2021)</ref>: A summarization dataset of reports addressing various national policy issues published by the Congressional Research Service 2 and the U.S. Government Accountability Office, 3 where each document is paired with an expert-written executive summary. The reports and their summaries are longer than their equivalents in other popular long-document summarization datasets; for example, GovReport's documents are approximately 1.5 and 2.5 times longer than the documents in arXiv and PubMed <ref type="bibr" target="#b7">(Cohan et al., 2018)</ref>, respectively.</p><p>SummScreenFD <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>: A summarization dataset in the domain of TV shows (e.g. Friends, Game of Thrones). Given a transcript of a specific episode, the goal is to produce the episode's recap. The original dataset is divided into two complementary subsets, based on the source of its community contributed transcripts. For SCROLLS, we use the ForeverDreaming (FD) subset, 4 as it incorporates 88 different shows, making it a more diverse alternative to the TV MegaSite (TMS) subset, 5 which has only 10 shows. Community-authored recaps for the ForeverDreaming transcripts were collected from English Wikipedia and TVMaze. 6</p><p>QMSum <ref type="bibr" target="#b31">(Zhong et al., 2021)</ref>: A query-based summarization dataset, consisting of 232 meetings transcripts from multiple domains and their corresponding summaries. The corpus covers academic group meetings at the International Computer Science Institute <ref type="bibr">(Janin et al., 2003)</ref>, 7 industrial product meetings for designing a remote control <ref type="bibr" target="#b4">(Carletta et al., 2005)</ref>, and committee meetings of the Welsh 8 and Canadian 9 Parliaments, dealing with a variety of public policy issues. Annotators were tasked with writing queries about the broad contents of the meetings, as well as specific questions about certain topics or decisions, while ensuring that the relevant text for answering each query spans at least 200 words or 10 turns.</p><p>Qasper <ref type="bibr" target="#b8">(Dasigi et al., 2021)</ref>: A question answering dataset over NLP papers filtered from the Semantic Scholar Open Research Corpus (S2ORC) <ref type="bibr" target="#b14">(Lo et al., 2020)</ref>. Questions were written by NLP practitioners after reading only the title and abstract of the papers, while another set of NLP practitioners annotated the answers given the entire document. Qasper contains abstractive, extractive, and Penny returns from visiting family in Nebraska, but mentions while picking up mail from Leonard that most of her relatives became sick. Sheldon, a germophobe according to Leonard, freaks out and becomes sick, becoming demanding on top of his already obnoxious personality. Familiar with Sheldon being sick, Leonard and the guys hide from him at a Planet of the Apes series marathon, leaving Penny to care for Sheldon. However, Leonard breaks his glasses in the cinema and has to retrieve his spare pair from the apartment, piloted by Howard and Raj using a laptop, an endoscope, and a Bluetooth helmet camera worn by the short-sighted Leonard. Penny intercepts him and abandons him to his fate with Sheldon. Leonard tries to escape, but runs into a wall and nearly knocks himself out. In the end, injured Leonard and sick Sheldon sit miserably on the couch.  <ref type="figure">Figure 2</ref>: An example from the SummScreenFD summarization dataset, where the task is to generate the recap (top paragraph) given the episode's script. In this example, the information required to compose the third sentence in the recap (highlighted) is scattered across several snippets throughout the transcript.</p><formula xml:id="formula_0">--Transcript -- ...[1,</formula><p>yes/no questions, as well as unanswerable ones.</p><p>NarrativeQA <ref type="bibr">(Ko?isk? et al., 2018)</ref>: An established question answering dataset over entire books from Project Gutenberg 10 and movie scripts from different websites. 11 Annotators were given summaries of the books and scripts obtained from Wikipedia, and asked to generate question-answer pairs, resulting in about 30 questions and answers for each of the 1,567 books and scripts. They were encouraged to use their own words rather then copying, and avoid asking yes/no questions or ones  about the cast. Each question was then answered by an additional annotator, providing each question with two reference answers (that may be identical).</p><p>QuALITY (Pang et al., 2021): A multiplechoice question answering dataset over stories and articles sourced from Project Gutenberg, 10 the Open American National Corpus <ref type="bibr" target="#b10">(Fillmore et al., 1998;</ref><ref type="bibr">Ide and Suderman, 2004)</ref>, and more. Experienced writers wrote questions and distractors, and were incentivized to write answerable, unambiguous questions such that in order to correctly answer them, human annotators must read large portions of the given document. To measure the difficulty of their questions, Pang et al. conducted a speed validation process, where another set of annotators were asked to answer questions given only a short period of time to skim through the document. As a result, 50% of the questions in QuALITY are labeled as hard, i.e. the majority of the annotators in the speed validation setting chose the wrong answer.</p><p>Contract NLI (Koreeda and Manning, 2021): A natural language inference dataset in the legal domain. Given a non-disclosure agreement (NDA, the premise), the task is to predict whether a particular legal statement (the hypothesis) is entailed, not entailed (neutral), or cannot be entailed (contradiction) from the contract. The NDAs were manually picked after simple filtering from the Electronic Data Gathering, Analysis, and Retrieval system (EDGAR) 12 and Google. The dataset contains a total of 607 contracts and 17 unique hypotheses, which were combined to produce the dataset's 10,319 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing</head><p>Data Cleansing As part of the curation process, we examine each dataset and clean or filter examples to ensure high quality data. In GovReport, we discard all examples where the report's length (in words) is less than twice the summary, or more than 1,000 times the summary, as well as examples where the summary exists verbatim in the report. This process removes 64 examples from the original dataset. In Qasper, we discard all papers that have less than 8,192 characters, removing a total of 176 questions over 63 papers, which appear to be of lower quality. In NarrativeQA, we locate markers indicating the start and end of the actual story, and use them to remove excess metadata such as licenses, HTML headers, etc.</p><p>Unified Format We reformulate every dataset in SCROLLS as a sequence-to-sequence task to allow for a simple unified input-output format. When a query is given in addition to the raw text (as in QMSum, Qasper, NarrativeQA, QuALITY, and ContractNLI), we prepend it to the text, using two newlines as a natural separator. For the multiplechoice dataset QuALITY, we also provide all four answer candidates as part of the query. For the summarization datasets, GovReport and Summ-ScreenFD, we use only the original documents as input. Some datasets (Qasper and NarrativeQA) contain multiple target outputs for each input; we split them into separate instances for training and development. For test, we score each prediction with every valid answer independently, and then merge the scores of identical inputs by taking the maximum of those scores. <ref type="table" target="#tab_6">Table 5</ref> in Appendix A provides an example from each SCROLLS dataset. 12 https://www.sec.gov/Archives/edgar/Oldloads</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Each dataset is split into training, validation, and test sets based on the original dataset splits. In SCROLLS, test set outputs are kept private, and only the inputs are publicly available. When evaluating a model, users must submit their model's outputs for all test sets via the SCROLLS website. Once a model is submitted, we compute the average performance metric across all datasets to provide the submission with a single aggregate SCROLLS score. We employ three different evaluation metrics across SCROLLS datasets: ROUGE for summarization tasks (GovReport, SummScreenFD, and QM-Sum), unigram overlap (F1) for question answering (Qasper and NarrativeQA), and exact match (EM) for multiple-choice (QuALITY) and classification (ContractNLI) tasks. The official evaluation script is available online. 13 ROUGE We use three flavors of ROUGE <ref type="bibr" target="#b13">(Lin, 2004)</ref> to measure the overlap between the systemgenerated output and the reference: unigram overlap (ROUGE-1), bigram overlap (ROUGE-2), and the longest overlapping subsequence (ROUGE-L). Both system output and reference are normalized by lowercasing and converting all nonalphanumeric characters to whitespaces, followed by whitespace tokenization. We compute the geometric mean of the three scores (ROUGE-1/2/L) to produce a single score per dataset, which is used to calculate the final SCROLLS score. 14 F1 Similar to ROUGE-1, the F1 metric calculates unigram overlap. The key difference is that both reference and system output strings are normalized slightly differently; in addition to lowercasing and punctuation removal, stopwords are also discarded, following the practice of SQuAD <ref type="bibr" target="#b21">(Rajpurkar et al., 2016)</ref> and other question-answering datasets <ref type="bibr" target="#b11">(Fisch et al., 2019)</ref>. Both Qasper and NarrativeQA contain questions with more than one reference answer; for each such example, we take the maximal F1 score over all of its reference answers.</p><p>EM Exact match normalizes the output strings using the same procedure as F1 (lowercasing, removing punctuation and stopwords, and normalizing whitespaces), and then compares whether the two normalized strings are identical. For QuAL-ITY, we calculate EM over the entire test set (EM-T), and also EM over its subset of hard questions (EM-H), as defined in the original dataset. For computing the final SCROLLS score, however, we only use the EM value calculated over the full test set (EM-T).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Quantitative Analysis</head><p>Length alone is not enough to make SCROLLS a challenging benchmark. Here, we provide a quantitative analysis that suggests that producing the correct output for a SCROLLS task typically requires fusing different parts of the input that are often hundreds and even thousands of words apart. This analysis complements the qualitative inspection of examples from SCROLLS, as shown in <ref type="figure">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>, and further discussed in Appendix E.</p><p>Methodology Each example in SCROLLS consists of a textual input and output. 15 Given a specific input-output pair, we measure the example's spread by computing the standard deviation between the locations of output bigrams in the input. <ref type="bibr">16</ref> Specifically, we represent the output string as a set of bigrams, and locate the first occurrence of each bigram in the input (if exists); we then compute the standard deviation between these locations (where a bigram is represented by the position of its first word in the input). Now that we have an example-level measure of spread, we can plot an entire dataset's spread on a histogram, and compare different datasets. <ref type="figure" target="#fig_3">Figure 4a</ref> compares the three summarization datasets in SCROLLS to the canonical CNN/DM summarization dataset <ref type="bibr">(Hermann et al., 2015)</ref>, as well as arXiv <ref type="bibr" target="#b7">(Cohan et al., 2018)</ref>, which has been used to evaluate longsequence models. We observe that the reference bigrams are spread out across much larger distances in SCROLLS than in CNN/DM, and by a factor of 1.5 to 2 times more than arXiv on average. <ref type="figure" target="#fig_3">Figure 4b</ref> compares the remaining four datasets in SCROLLS, which typi-cally have shorter outputs, to the popular SQuAD <ref type="bibr" target="#b21">(Rajpurkar et al., 2016)</ref> and Natural Questions <ref type="bibr">(Kwiatkowski et al., 2019)</ref> datasets. <ref type="bibr">17</ref> While the answer bigrams in SQuAD and Natural Questions typically spread across distances of under 5 words, the output bigrams in SCROLLS datasets are usually separated by hundreds of words. NarrativeQA also seems to contain many examples where the answer bigrams cluster close together, but also a significant subset of examples where the answer's bigrams are dispersed across huge distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA &amp; NLI Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments to evaluate the ability of mainstream models to handle the various long text challenges presented by SCROLLS. Our code is based on the Transformers library <ref type="bibr" target="#b30">(Wolf et al., 2020)</ref>, and is available online. 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We finetune two pretrained transformer variants as baselines, as well as naive heuristic baselines to establish the floor performance on each task. Hyperparameters are detailed in Appendix D.</p><p>BART As a standard transformer baseline, we use the pretrained BART-base 18 model <ref type="bibr">(Lewis et al., 2020)</ref>. BART is a transformer encoderdecoder pretrained by reconstructing noised texts, which achieved state-of-the-art results on several summarization datasets when released. BART was pretrained on sequences of up to 1,024 tokens; we therefore truncate all inputs by retaining only their 1,024-token prefix. To examine the effect of available input length, we also consider truncating BART's inputs at 256 and 512 tokens.</p><p>Longformer Encoder-Decoder (LED) We experiment with LED-base, 19 the encoder-decoder version of the efficient transformer architecture Longformer <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>. Longformer avoids computing quadratic-complexity attention via sliding-window attention, where each word only attends to a constant number of nearby tokens, in addition to a few tokens that compute global attention over the entire input. LED is initialized with BART's parameters, without further pretraining. In our experiments, we use a sliding window of 1,024 tokens, and restrict the total input length to 16,384 tokens via truncation, following Beltagy et al. We also experiment with maximum sequence lengths of 1,024 and 4,096 tokens. While the original work on LED selects the globally-attending tokens on a per-task basis, we follow their summarization setting throughout all tasks, which enables global attention only for the first token.</p><p>Heuristic Baselines We use simple heuristics to find the lower bound of performance on each dataset. For most datasets, we use the fixed-length prefix heuristic, akin to the LEAD baseline in the summarization literature. Specifically, we compute the average output-input length ratio ? over the training set (in characters), and then produce the first ? ? n characters from the given input at inference time (where n is the input's length). For QuALITY, we use the majority class (which is just above one quarter). For ContractNLI, we use the per-hypothesis majority class, as the same 17 hypotheses are shared across all documents. <ref type="table" target="#tab_4">Table 2</ref> shows the baselines' performance on SCROLLS. A few trends are apparent:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>More Context Improves Performance We experiment with three context lengths for each model. As the model receives more context, its average SCROLLS score increases. For BART, increasing the input length from 256 tokens to 1,024 increases performance by 2.66 points, while LED grows by 2.1 points when enlarging its maximal sequence length from 1,024 tokens to 16,384. This trend is relatively consistent across datasets for BART, but less so for LED (e.g., QMSum and ContractNLI).</p><p>BART versus LED Although LED does achieve the highest SCROLLS score when given 16,384 to-  kens per sequence, BART arrives within 0.15 points of the top score despite being limited to only 1,024 tokens. This is surprising, given the substantial difference in input lengths. Moreover, when controlling for the number of input tokens, BART outperforms LED by almost two points, suggesting that LED might be under-optimized. Inspecting the dataset-level results reveals that LED (16k) significantly outperforms BART (1k) in two datasets, GovReport and NarrativeQA, which are coincidentally the largest datasets in SCROLLS by number of examples. Thus, it is possible that since LED is initialized with BART's parameters (without long-text pretraining), it requires a substantial amount of data and fine-tuning to adapt its parameters to sliding window attention and potentially longer inputs.</p><p>Overall, our experiments highlight the importance of measuring not only whether an architecture can efficiently process long sequences, but also whether it can effectively model their semanticsprecisely what SCROLLS is designed to do.</p><p>How Far is SCROLLS from being Solved? The heuristic baselines set a lower bound average score of 19.35, which the model baselines are able to improve upon by 7 to 10 points. While it is difficult to establish an accurate human performance ceiling on SCROLLS, especially when considering the summarization datasets, we do have some indicators that it is probably much higher than the current baselines. <ref type="bibr" target="#b8">Dasigi et al. (2021)</ref> study a subset of Qasper that has multiple annotated answers, and find their overlap to be 60.9% F1, more than double our best baseline. Likewise, human agreement on QuALITY was measured at 93.5% EM <ref type="bibr">(Pang et al., 2021)</ref>. We also compute the inter-annotator agreement (F1) on NarrativeQA's test set (where each question has two answers), arriving at around 58.7% F1, compared to our best baseline of 18.5% F1. Overall, it seems that contemporary off-theshelf models struggle with these tasks, challenging future work to make progress on SCROLLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new benchmark that places the spotlight on naturally long texts and their intricacies. SCROLLS fills a current gap around evaluating efficient transformer architectures and their alternatives on natural language tasks, and at the same time provides a testing ground for new pretraining schemes that target long language sequences. We hope that SCROLLS inspires the NLP community to go beyond single sentences and paragraphs, and meet the challenges of processing and reasoning over longer discourses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>The main limitation of SCROLLS is the evaluation of long output texts, specifically in summarization. Since ROUGE only accounts for ngram overlaps, it might downvalue paraphrases of the reference summary that contain the same semantic content. Establishing unbiased, automated metrics for long generations that correlate well with humans judgments is an emerging field of research, and we may indeed decide to replace or complement ROUGE with model-based evaluation in the future.</p><p>A second limitation is that SCROLLS is monolingual. Model evaluation over languages other than English has major significance, affecting the usage of language processing technology in applications worldwide. SCROLLS is limited in that sense, but takes an initial step in standardizing evaluation over long text in general. A natural future direction is establishing benchmarks focusing on other languages as well. A Dataset Format   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters</head><p>We finetune each of the baseline models on every dataset separately using AdamW <ref type="bibr" target="#b15">(Loshchilov and Hutter, 2019)</ref> with ? = (0.9, 0.98), ? = 1e-6, mixed precision (fp16), and gradient checkpointing. We achieve an effective batch size of 131,072 (2 17 ) tokens by processing 16,384 tokens per GPU across 8 NVIDIA V100 (32GB) GPUs either in parallel or via gradient accumulation. The summarization datasets are trained for 10 epochs, while Qasper, QuALITY, and ContractNLI are trained for 20; NarrativeQA (the largest dataset) is trained for 2 epochs. We tune the maximum learning rate over each validation set, selecting from 6 possible values: 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4. The learning rate is warmed up from zero during the first 10% of the learning schedule, and then linearly decays back to zero throughout the remaining 90%. We also apply 0.1 dropout throughout each network. During inference, we generate outputs using greedy decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The bilingual lexicon induction task aims to automatically build word translation dictionaries across different languages, which is beneficial for various natural language processing tasks such as cross-lingual information...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>German-English, French-English, and Japanese-English</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NarrativeQA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering Input</head><p>What is the first heist that Dignan and Anthony commit?</p><p>&lt;b&gt;BOTTLE ROCKET&lt;/b&gt; screenplay by Wes Anderson and Owen Wilson &lt;b&gt;EXT. ALLEY. DAY&lt;/b&gt; ANTHONY and DIGNAN walk down an alley behind a convenience store. Anthony's nineteen. He's got on a...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>As a practice heist they break into Anthony's family's home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QuALITY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple-Choice Question Answering Input</head><p>Why did the beings come to Earth? (A) it was the next planet for them to destroy (B) they wanted all of Earth's resources (C) they wanted to take over Earth (D) they were curious about Earth's creatures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Phone Me in Central Park" By JAMES McCONNELL</head><p>There should be an epitaph for...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>it was the next planet for them to destroy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ContractNLI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Language Inference Input</head><p>Agreement shall not grant Receiving Party any right to Confidential Information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT</head><p>This NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT ("Agreement") is made by and between: (i) the Office of the United Nations High Commissioner...  <ref type="bibr">(web)</ref> to be synonymous; they are not. Rather, the web is one portion of the Internet, and a medium through which information may be accessed. In conceptualizing the web, some may view it as consisting solely of the websites accessible through a traditional search engine such as Google. However, this content-known as the "Surface Web"-is only one portion of the web. The Deep Web refers to "a class of content on the Internet that, for various technical reasons, is not indexed by search engines," and thus would not be accessible through a traditional search engine. ...[3,791 words]... the FBI has put resources into developing malware that can compromise servers in an attempt to identify certain users of Tor. Since 2002, the FBI has reportedly used a "computer and internet protocol address verifier" (CIPAV) to "identify suspects who are disguising their location using proxy servers or anonymity services, like Tor." It has been using this program to target "hackers, online sexual predators, extortionists, and others." Law enforcement has also reportedly been working with companies to develop additional technologies to investigate crimes and identify victims on the Dark Web. In addition to developing technology to infiltrate and deanonymize services such as Tor, law enforcement may rely upon more traditional crime fighting techniques; some have suggested that law enforcement can still rely upon mistakes by criminals or flaws in technology to target nefarious actors. For instance, in 2013 the FBI took down the Silk Road, then the "cyberunderworld's largest black market." Reportedly, "missteps" by the site's operator led to its demise; ...[979 words]... <ref type="figure">Figure 5</ref>: An example from GovReport, a dataset of government reports and their expert-written summaries. This example shows the spread of the relevant information in the document, exemplified by the first and last sentences of the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Entailment</head><p>What did the group discuss about budget balancing?</p><p>--Answer --The use of the LCD screen and the advanced chip cost the team half of the expenditure. Due to the budget limit, the team had to abandon some other designs such as the rubber material and the double-curved structure. The USB connection was not feasible for now as well. For the location function, a transmitter, a receiver and speakers could be incorporated on a TV instead  <ref type="figure">Figure 6</ref>: An example from QMSum, a query-basedsummarization dataset over meeting transcripts. Information relevant for generating the last two sentences in the answer is spread in different locations in the transcript.</p><p>What approaches without reinforcement learning have been tried?</p><p>--Answer -classification, regression, neural methods --Article --...[142 words]... The main contributions of this paper are: We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. . <ref type="figure">..[1,006</ref>   <ref type="figure">Figure 7</ref>: An example from the Qasper dataset, which includes question answering over scientific papers. The evidence for the first part of the reference answer appears in the introduction, while the indication that neural models were also experimented with exists further in the document, in a description of the results table.</p><p>Whose initials are on the bottom of the burnt letter to Sir Charles?</p><p>--Answer --Laura Lyons --Story --...[35,871 words]... "Well, Sir Henry, your uncle had a letter that morning. He had usually a great many letters, for he was a public man and well known for his kind heart, so that everyone who was in trouble was glad to turn to him. But that morning, as it chanced, there was only this one letter, so I took the more notice of it. It was from Coombe Tracey, and it was addressed in a woman's hand." "Well?" "Well, sir, I thought no more of the matter, and never would have done had it not been for my wife. Only a few weeks ago she was cleaning out Sir Charles's study-it had never been touched since his death-and she found the ashes of a burned letter in the back of the grate. The greater part of it was charred to pieces, but one little slip, the end of a page, hung together, and the writing could still be read, though it was gray on a black ground. It seemed to us to be a postscript at the end of the letter and it said: 'Please, please, as you are a gentleman, burn this letter, and be at the gate by ten o clock. Beneath it were signed the initials L. L." ...[861 words]... but among the farmers or gentry there is no one whose initials are those. Wait a bit though," he added after a pause. "There is Laura Lyons-her initials are L. L.-but she lives in Coombe Tracey." ...[1,983 words]... "Did you ever write to Sir Charles asking him to meet you?" I continued. Mrs. Lyons flushed with anger again. "Really, sir, this is a very extraordinary question." "I am sorry, madam, but I must repeat it." "Then I answer, certainly not." ...[97 words]... "You do Sir Charles an injustice. He did burn the letter. But sometimes a letter may be legible even when burned. You acknowledge now that you wrote it?" "Yes, I did write it," she cried, pouring out her soul in a torrent of words. "I did write it. Why should I deny it? I have no reason to be ashamed of it. I wished him to help me. ...[19,996 words]... <ref type="figure">Figure 8</ref>: An example from NarrativeQA, where the task is to answer questions about books and movie scripts. In this question about The Hound of the Baskervilles, the answer is first discussed in several places without certainty, where even the final reveal is preceded by an explicit distractor.</p><p>All Confidential Information shall be expressly identified by the Disclosing Party.</p><p>--Label --Contradiction --Contract --...[427 words]... 3.1.4 "Confidential Information" means, without limiting the generality of the term: -3.1.4.1 technical, scientific, commercial, financial and market information, trade partners, potential clients, trade leads and trade secrets, and all other information in whatever form, whether in writing or not, whether or not subject to or protected by common law or statutory laws relating to copyright, patent, trademarks, registered or unregistered, or otherwise, disclosed or communicated to the Receiving Party or acquired by the Receiving Party from the Disclosing Party pursuant to this Agreement or the Discussions; ...[1,661 words]... If the Recipient is uncertain as to whether any information is Confidential Information, the Recipient shall treat such information as confidential until the contrary is agreed by the Disclosing Party in writing. ...[4,178 words]... <ref type="figure">Figure 9</ref>: An example from ContractNLI, a natural language inference dataset over non-disclosure agreements (NDAs). Here, the challenge of finding the evidence, residing in the middle of a long document, is further amplified by the hypothesis being only implicitly contradicted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The text says "The expert frowned horribly." What makes the expert's smile so horrible? (A) The frown indicates that he's close to detecting Korvin's true motivations. (B) The frown indicates that he knows that Korvin switched the wires on the lie detector.(C) The frown is a signal to the Ruler that Korvin is lying. (D) The frown is physically horrible because the Tr'en have fifty-eight, pointed teeth. --Story --...[607 words]... It was a ritual, Korvin had learned. "You are of the Tr'en," he replied. The green being nodded. "I am Didyak of the Tr'en," he said. ...[257 words]... Didyak beamed at him. The sight was remarkably unpleasant, involving as it did the disclosure of the Tr'en fifty-eight teeth, mostly pointed. Korvin stared back impassively. "I have been ordered to come to you," Didyak said, "by the Ruler. The Ruler wishes to talk with you." ...[1,366 words]... "They can be treated mathematically," one of the experts, a small emerald-green being, told Korvin thinly. "Of course, you would not understand the mathematics." ...[33 words]... The expert frowned horribly, showing all of his teeth. Korvin did his best not to react. "Your plan is a failure," the expert said, "and you call this a good thing." ...[1,808 words]...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An example from the QuALITY dataset, where the task is to answer multiple-choice questions about a given story or document. In this example, answering the question correctly requires reasoning over four different snippets that are separated by long token sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The spread of reference-text bigrams in the input texts, measured by the standard deviation of the position of each bigram's first occurrence in the input document. SCROLLS datasets (blue), other popular datasets (pink).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>An overview of the datasets in SCROLLS and their statistics. Summ refers to summarization, QB-Summ means query-based summarization, and MC-QA abbreviates multiple-choice question answering. The number of examples includes train, validation, and test sets.</figDesc><table><row><cell>strive to maintain a diversity of tasks, covering sum-</cell></row><row><cell>marization and query-based summarization, open</cell></row><row><cell>ended and multiple-choice question answering, as</cell></row><row><cell>well as natural language inference.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>032 words]... Howard: Hello. Sheldon: Howard, I'm sick. ...[40 words]... Howard: It's my own fault, I forgot the protocol we put in place after the great ear infection of '06. Leonard: You call Koothrappali, we need to find a place to lay low for the next eighteen to twenty four hours. Howard: Stand by. Ma, can my friends come over? Howard's Mother: I just had the carpets steamed. Howard: That's a negatory. But there's a Planet of the Apes marathon at the New Art today. Leonard: Five movies, two hours apiece. It's a start. ...[660 words]... Sheldon: Based on what happened next, I assume it means "would you like an enema?" Penny: Okay, sweetie, I'll take care of you, what do you need? ...[766 words]... Penny: You deliberately stuck me with Sheldon. Leonard: Well, I had to, you see what he's like. ...[142 words]...</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>/ 18.6 / 22.7 27.2 / 4.9 / 16.7 30.2 / 8.7 / 20.7   </figDesc><table><row><cell cols="2">Model (Input)</cell><cell>GovRep ROUGE-1/2/L</cell><cell>SumScr ROUGE-1/2/L</cell><cell>QMSum ROUGE-1/2/L</cell><cell cols="2">Qspr Nrtv F1 F1</cell><cell>QALT EM-T/H</cell><cell>CNLI EM</cell><cell>Avg</cell></row><row><cell>Naive</cell><cell cols="3">-45.3 / 17.9 / 20.8 19.6 / 1.8 / 11.0</cell><cell>14.2 / 2.0 / 9.3</cell><cell>3.4</cell><cell>1.5</cell><cell>25.2 / 26.1</cell><cell>66.0</cell><cell>19.35</cell></row><row><cell></cell><cell cols="4">256 41.9 / 14.2 / 20.3 24.5 / 3.8 / 15.3 29.9 / 8.3 / 20.4</cell><cell>23.3</cell><cell cols="2">14.0 26.0 / 25.8</cell><cell>69.8</cell><cell>26.35</cell></row><row><cell>BART</cell><cell cols="4">512 45.6 / 16.9 / 21.8 26.3 / 5.1 / 16.2 29.5 / 8.2 / 20.1</cell><cell>24.7</cell><cell cols="2">14.5 26.8 / 27.4</cell><cell>71.6</cell><cell>27.58</cell></row><row><cell></cell><cell cols="5">1024 47.9 26.3</cell><cell cols="2">15.4 26.0 / 25.9</cell><cell>77.4</cell><cell>29.01</cell></row><row><cell></cell><cell cols="4">1024 40.9 / 16.1 / 23.1 22.7 / 3.6 / 15.1 24.6 / 6.5 / 19.0</cell><cell>24.4</cell><cell cols="2">15.2 26.6 / 27.2</cell><cell>73.4</cell><cell>27.06</cell></row><row><cell>LED</cell><cell cols="4">4096 52.5 / 23.3 / 26.8 23.0 / 4.1 / 15.1 26.6 / 6.9 / 19.9</cell><cell>25.0</cell><cell cols="2">16.3 26.6 / 27.3</cell><cell>71.5</cell><cell>28.30</cell></row><row><cell></cell><cell cols="4">16384 56.2 / 26.6 / 28.8 24.2 / 4.5 / 15.4 25.1 / 6.7 / 18.8</cell><cell>26.6</cell><cell cols="2">18.5 25.8 / 25.4</cell><cell>71.5</cell><cell>29.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Baseline results on SCROLLS, using naive heuristics, BART, and Longformer Encoder-Decoder (LED), and various input length limits. The final SCROLLS score (Avg) is computed by averaging over each dataset's overall performance score. For QuALITY (QALT), we use the EM score calculated over the full test set (EM-T), without up-weighting the performance on the hard subset (EM-H). For datasets evaluated with ROUGE, we aggregate the different ROUGE scores via geometric mean to produce a single score per dataset, which is then used when calculating the final average SCROLLS score.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 1-13, Hong Kong, China. Association for Computational Linguistics. Quentin Fournier, Ga?tan Marceau Caron, and Daniel Aloise. 2021. A practical survey on faster and lighter transformers.</figDesc><table><row><cell>Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-</cell></row><row><cell>stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,</cell></row><row><cell>and Phil Blunsom. 2015. Teaching machines to read</cell></row><row><cell>and comprehend. In Advances in Neural Informa-</cell></row><row><cell>tion Processing Systems, volume 28. Curran Asso-</cell></row><row><cell>ciates, Inc.</cell></row><row><cell>Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng</cell></row><row><cell>Ji, and Lu Wang. 2021. Efficient attentions for long</cell></row><row><cell>document summarization. In Proceedings of the</cell></row><row><cell>2021 Conference of the North American Chapter of</cell></row><row><cell>the Association for Computational Linguistics: Hu-</cell></row><row><cell>man Language Technologies, pages 1419-1436, On-</cell></row><row><cell>line. Association for Computational Linguistics.</cell></row><row><cell>Nancy Ide and Keith Suderman. 2004. The Ameri-</cell></row><row><cell>can national corpus first release. In Proceedings of</cell></row><row><cell>the Fourth International Conference on Language</cell></row><row><cell>Resources and Evaluation (LREC'04), Lisbon, Por-</cell></row><row><cell>tugal. European Language Resources Association</cell></row><row><cell>(ELRA).</cell></row><row><cell>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,</cell></row><row><cell>N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-</cell></row><row><cell>cke, and C. Wooters. 2003. The icsi meeting corpus.</cell></row><row><cell>In 2003 IEEE International Conference on Acous-</cell></row><row><cell>tics, Speech, and Signal Processing, 2003. Proceed-</cell></row><row><cell>ings. (ICASSP '03)., volume 1, pages I-I.</cell></row><row><cell>Urvashi Khandelwal, He He, Peng Qi, and Dan Juraf-</cell></row><row><cell>sky. 2018. Sharp nearby, fuzzy far away: How neu-</cell></row><row><cell>ral language models use context. In Proceedings</cell></row><row><cell>of the 56th Annual Meeting of the Association for</cell></row><row><cell>Computational Linguistics (Volume 1: Long Papers),</cell></row><row><cell>pages 284-294, Melbourne, Australia. Association</cell></row><row><cell>for Computational Linguistics.</cell></row><row><cell>Tom?? Ko?isk?, Jonathan Schwarz, Phil Blunsom,</cell></row><row><cell>Chris Dyer, Karl Moritz Hermann, G?bor Melis, and</cell></row><row><cell>Edward Grefenstette. 2018. The NarrativeQA read-</cell></row><row><cell>ing comprehension challenge. Transactions of the</cell></row><row><cell>Association for Computational Linguistics, 6:317-</cell></row><row><cell>328.</cell></row><row><cell>Yuta Koreeda and Christopher D. Manning. 2021. Con-</cell></row><row><cell>tractnli: A dataset for document-level natural lan-</cell></row><row><cell>guage inference for contracts.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>shows an example from each dataset in SCROLLS.</figDesc><table><row><cell>B Dataset Splits</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">#Examples Train Valid</cell><cell>Test</cell></row><row><cell>GovReport</cell><cell>17,457</cell><cell>972</cell><cell>973</cell></row><row><cell>SummScreenFD</cell><cell>3,673</cell><cell>338</cell><cell>337</cell></row><row><cell>QMSum</cell><cell>1,257</cell><cell>272</cell><cell>281</cell></row><row><cell>Qasper</cell><cell cols="2">2,567 1,726</cell><cell>1,399</cell></row><row><cell>NarrativeQA</cell><cell cols="3">55,003 5,878 10,306</cell></row><row><cell>QuALITY</cell><cell cols="2">2,523 2,086</cell><cell>2,128</cell></row><row><cell>ContractNLI</cell><cell cols="2">7,191 1,037</cell><cell>2,091</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The number of examples in each train, validation, and test set.</figDesc><table><row><cell cols="2">C Original Datasets Results</cell><cell></cell></row><row><cell>Dataset</cell><cell>Metric</cell><cell>Score</cell></row><row><cell>GovReport</cell><cell>ROUGE (1/2)</cell><cell>56.9 / 22.6</cell></row><row><cell cols="2">SummScreenFD ROUGE (1/2)</cell><cell>25.9 / 4.2</cell></row><row><cell>QMSum</cell><cell>ROUGE (1/2)</cell><cell>29.2 / 6.4</cell></row><row><cell>Qasper</cell><cell>F1</cell><cell>32.8</cell></row><row><cell>NarrativeQA</cell><cell>BLEU</cell><cell>15.53</cell></row><row><cell>QuALITY</cell><cell cols="2">Acc (Total/Hard) 30.7 / 29.3</cell></row><row><cell>ContractNLI</cell><cell>Acc</cell><cell>87.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Results reported by the datasets' authors,</cell></row><row><cell>achieved by sequence-to-sequence baselines (when ap-</cell></row><row><cell>plicable). Most results are incomparable to the results</cell></row><row><cell>in SCROLLS as the data was cleaned, filtered and refor-</cell></row><row><cell>matted.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>An example from each one of the SCROLLS datasets, shown in the benchmark's text-to-text format. In this illustration, we truncate the examples' inputs and outputs for brevity.</figDesc><table><row><cell>The layers of the Internet go far beyond the surface content</cell></row><row><cell>that many can easily access in their daily searches.</cell></row><row><cell>...[486 words]...</cell></row><row><cell>Reportedly, officials are continuously working on expand-</cell></row><row><cell>ing techniques to deanonymize activity on the Dark Web</cell></row><row><cell>and identify malicious actors online.</cell></row><row><cell>--Document --</cell></row><row><cell>...[346 words]...</cell></row><row><cell>Many may consider the Internet and World Wide Web</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Transmitter , receiver , speakers . Plus the extra device itself that's gonna be on a T_V_ . ...[4,651 words]...</figDesc><table><row><cell>--Meeting Transcript --</cell></row><row><cell>...[1,813 words]...</cell></row><row><cell>Even then as well , um there was no criteria technically</cell></row><row><cell>defined for a joystick so I've used what I think's appropri-</cell></row><row><cell>ate . With any luck that won't mean that we've incurred</cell></row><row><cell>more cost than we can actually afford to . It blows a lot</cell></row><row><cell>of our really good ideas kind of slightly to one side , for</cell></row><row><cell>example the possibility of having a U_S_B_ connection is</cell></row><row><cell>definitely not viable now . Um .</cell></row><row><cell>...[656 words]...</cell></row><row><cell>Marketing: We don't even have uh speakers here . The</cell></row><row><cell>{disfmarker} like uh we uh {disfmarker} what about speak-</cell></row><row><cell>ers and transmitters and stuff like that ? Have we factored</cell></row><row><cell>that in ?</cell></row><row><cell>Industrial Designer: Mm .</cell></row><row><cell>Project Manager: Uh no , we haven't , not {disfmarker}</cell></row><row><cell>Marketing:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>words]... The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). ...[1,398 words]...</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://crsreports.congress.gov/ 3 https://www.gao.gov/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://transcripts.foreverdreaming.org 5 http://tvmegasite.net/ 6 https://www.tvmaze.com/ 7 https://groups.inf.ed.ac.uk/ami/icsi/index.shtml 8 https://record.assembly.wales 9 https://www.ourcommons.ca/Committees/en/Home</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">http://www.gutenberg.org 11 http://www.imsdb.com, http://www.dailyscript.com/, http://www.awesomefilm.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/tau-nlp/scrolls 14 We discuss the limitations of using ROUGE to evaluate summarization in Section 7.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">For the purposes of this analysis, we consider the output of ContractNLI to be the hypothesis, and the input to be the premise. For question answering datasets, the question is omitted.16  This metric is inspired by the analysis ofHuang et al.  (2021)  for GovReport, which also used bigram statistics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">We use the version of Natural Questions that takes entire Wikipedia articles as inputs and short answers as outputs.18 https://huggingface.co/facebook/bart-base 19 https://huggingface.co/allenai/led-base-16384</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Qualitative Analysis</head><p>We manually analyze examples from each of the datasets in the benchmark demonstrating cases that require contextualizing and synthesizing information over long ranges of text. <ref type="bibr">Figures 5,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8 and 9</ref> showcase gold references, relevant parts from input documents required to generate those references, and queries when exist, from GovReport, QMSum, Qasper, NarrativeQA and ContractNLI. Together with the SummscreenFD example in <ref type="figure">Fig</ref>  <ref type="figure">Figure 3</ref> they illustrate cases where important information is spread across multiple sections of the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GovReport</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization Input</head><p>Introduction</p><p>The United States has an abundance of natural resources. For much of the nation's history, energy availability was not a concern as commerce and industry needs could be met by domestic supplies. However, industrialization and population growth, and the continuing development of a consumer-oriented society, led to growing dependence...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Energy is crucial to the operation of a modern industrial and services economy. Concerns about the availability and cost of energy and about environmental impacts of fossil energy use have led to the establishment of...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SummScreenFD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization Input</head><p>Ted's kitchen Ted from 2030: Kids, when it comes to love, the best relationships are the ones that just come naturally. Ted: My first solo batch. Victoria: Um, I think those need to stay in the oven a while longer. Here's a professional tip. If it's still runny, it's not a cupcake. It's a beverage...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Just as things are going well between Ted and Victoria, the latter is offered a surprising but incredible opportunity to be a fellow at a culinary institute in Germany. As the couple discuss the viability of long-distance...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QMSum</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-Based Summarization Input</head><p>What did the team discuss during the product evaluation about its feature to solve customers' concerns?</p><p>Project Manager: Yep. Soon as I get this. Okay. This is our last meeting. Um I'll go ahead...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Generally speaking, the team agreed that the product was intuitive and had successfully incorporated main aims that the team had. The team believed the customers were not likely to lose the remote control since it was...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qasper</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering Input</head><p>Which languages are used in the multi-lingual caption model?</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Biesialska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Joanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljube?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation<address><addrLine>Santanu Pal, Matt Post</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="55" />
		</imprint>
	</monogr>
	<note>and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2019 conference on machine translation (WMT19)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="61" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ami meeting corpus: A pre-announcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Bourban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mael</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslav</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasilis</forename><surname>Karaiskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Kronenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lincoln</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on machine learning for multimodal interaction</title>
		<meeting><address><addrLine>Agnes Lisowska, Iain McCowan, Wilfried Post, Dennis Reidsma, and Pierre Wellner; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Summscreen: A dataset for abstractive screenplay summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Rethinking attention with performers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A dataset of information-seeking questions and answers anchored in research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4599" to="4610" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An american national corpus: a proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Macleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5801</idno>
		<title level="m">MRQA 2019 shared task: Evaluating generalization in reading</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Datasets: A community library for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>?a?ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavitvya</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delangue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran?ois Lagunas, Alexander Rush, and Thomas Wolf; Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Richard Yuanzhe Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelica</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishakh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08608</idno>
		<title level="m">He He, and Samuel R. Bowman. 2021. QuAL-ITY: Question answering with long input texts, yes! arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shortformer: Better language modeling using shorter inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.427</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5493" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00353</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do long-range language models actually use long-range context?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="807" to="822" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2020b. Efficient transformers: A survey</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">QMSum: A new benchmark for querybased multi-domain meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutethia</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5905" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
