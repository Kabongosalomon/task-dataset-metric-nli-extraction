<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SLAMP: Stochastic Latent Appearance and Motion Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adil</forename><surname>Kaan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ko? University Is Bank AI Center</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akan</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
							<email>aerdem@ku.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="institution">Hacettepe University Computer Vision Lab</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ko? University Is Bank AI Center</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G?ney</surname></persName>
							<email>fguney@ku.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Ko? University Is Bank AI Center</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SLAMP: Stochastic Latent Appearance and Motion Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://kuis-ai.github.io/slamp</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Videos contain visual information enriched by motion. Motion is a useful cue for reasoning about human activities or interactions between objects in a video. Given a few initial frames of a video, our goal is to predict several frames into the future, as realistically as possible. By looking at a few frames, humans can predict what will happen next. Surprisingly, they can even attribute semantic meanings to random dots and recognize motion patterns <ref type="bibr" target="#b16">[17]</ref>. This shows the importance of motion to infer the dynamics of the video and to predict the future frames.</p><p>Motion cues have been heavily utilized for future frame prediction in computer vision. A common approach is to factorize the video into static and dynamic components <ref type="bibr">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">33]</ref>. First, most of the previous methods are deterministic and fail to model the uncertainty of the future. Second, motion is typically interpreted as local <ref type="figure">Figure 1</ref>: Comparison of the first prediction frames (11th) SLAMP (left) vs. state-of-the-art method, SRVP <ref type="bibr" target="#b8">[9]</ref> (right) on KITTI <ref type="bibr" target="#b10">[11]</ref> (top) and Cityscapes <ref type="bibr" target="#b3">[4]</ref> (bottom) datasets. Our method can predict both foreground and background objects better than SRVP. Full sequence predictions can be seen in <ref type="figure">Fig. 32</ref> and 33. changes from one frame to the next. However, changes in motion follow certain patterns when observed over some time interval. Consider scenarios where objects move with near-constant velocity, or humans repeating atomic actions in videos. Regularities in motion can be very informative for future frame prediction. In this work, we propose to explicitly model the change in motion, or the motion history, for predicting future frames.</p><p>Stochastic methods have been proposed to model the inherent uncertainty of the future in videos. Earlier methods encode the dynamics of the video in stochastic latent variables which are decoded to future frames in a deterministic way <ref type="bibr" target="#b4">[5]</ref>. We first assume that both appearance and motion are encoded in the stochastic latent variables and decode them separately into appearance and motion predictions in a deterministic way. Inspired by the previous deterministic methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>, we also estimate a mask relating the two. Both appearance and motion decoders are expected to predict the full frame but they might fail due to occlusions around motion boundaries. Intuitively, we predict a probabilistic mask from the results of the appearance and motion decoders to combine them into a more accurate final prediction. Our model learns to use motion cues in the dynamic parts and relies on appearance in the occluded regions. ... <ref type="figure">Figure 2</ref>: Generative Model of SLAMP. The graphical model shows the generation process of SLAMP with motion history. There are two separate latent variables for appearance z p t and motion z f t generating frames x p t and x f t (black). Information is propagated between time-steps through the recurrence between frame predictions (blue), corresponding latent variables (green), and from frame predictions to latent variables (red). The final predictionx t is a weighted combination of the x p t and x f t according to the mask m(x p t , x f t ). Note that predictions at a time-step depend on all of the previous time-steps recurrently, but only the connections between consecutive ones are shown for clarity. The proposed stochastic model with deterministic decoders cannot fully utilize the motion history, even when motion is explicitly decoded. In this work, we propose a model to recognize regularities in motion and remember them in the motion history to improve future frame predictions. We factorize stochastic latent variables as static and dynamic components to model the motion history in addition to the appearance history. We learn two separate distributions representing appearance and motion and then decode static and dynamic parts from the respective ones.</p><p>Our model outperforms all the previous work and performs comparably to the state-of-the-art method, SRVP, <ref type="bibr" target="#b8">[9]</ref> without any limiting assumptions on the changes in the static component on the generic video prediction datasets, MNIST, KTH and BAIR. However, our model outperforms all the previous work, including SRVP, on two challenging real-world autonomous driving datasets with dynamic background and complex object motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Appearance-Motion Decomposition: The previous work explored motion cues for video generation either explicitly with optical flow <ref type="bibr">[35,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> or implicitly with temporal differences <ref type="bibr" target="#b23">[24]</ref> or pixel-level transformations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">33]</ref>. There are some common factors among these methods such as using recurrent models [29, <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref>, specific processing of dynamic parts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, utilizing a mask <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>, and adversarial training <ref type="bibr">[33,</ref><ref type="bibr" target="#b24">25]</ref>. We also use recurrent models, predict a mask, and separately process motion, but in a stochastic way.</p><p>The previous work which explored motion for video gen-eration are mostly deterministic, therefore failing to capture uncertainty of the future. There are a couple of attempts to learn multiple future trajectories from a single image with a conditional variational autoencoder <ref type="bibr">[34]</ref> or to capture motion uncertainty with a probabilistic motion encoder <ref type="bibr" target="#b21">[22]</ref>. The latter work uses separate decoders for flow and frame similar to our approach, however, predicts them only from the latent vector. We incorporate information from previous frames with additional modelling of the motion history.</p><p>Stochastic Video Generation: SV2P <ref type="bibr" target="#b0">[1]</ref> and SVG <ref type="bibr" target="#b4">[5]</ref> are the first to model the stochasticity in video sequences using latent variables. The input from past frames are encoded in a posterior distribution to generate the future frames. In a stochastic framework, learning is performed by maximizing the likelihood of the observed data and minimizing the distance of the posterior distribution to a prior distribution, either fixed <ref type="bibr" target="#b0">[1]</ref> or learned from previous frames <ref type="bibr" target="#b4">[5]</ref>. Since time-variance in the model is proven crucial by the previous work, we sample a latent variable at every time step <ref type="bibr" target="#b4">[5]</ref>. Sampled random variables are fed to a frame predictor, modelled recurrently using an LSTM. We model appearance and motion distributions separately and train two frame predictors for static and dynamic parts. Typically, each distribution, including the prior and the posterior, is modeled with a recurrent model such as an LSTM. Villegas et al. <ref type="bibr">[32]</ref> replace the linear LSTMs with convolutional ones at the cost of increasing the number of parameters. Castrejon et al. <ref type="bibr" target="#b2">[3]</ref> introduce a hierarchical representation to model latent variables at different scales, by introducing additional complexity. Lee et al. <ref type="bibr" target="#b19">[20]</ref> incorporate an adversarial loss into the stochastic framework to generate sharper images, at the cost of less diverse results. Our model with linear LSTMs can generate diverse and sharp-looking results without any adversarial losses, by incorporating motion information successfully into the stochastic framework. Recent methods model dynamics of the keypoints to avoid errors in pixel space and achieve stable learning <ref type="bibr">[26]</ref>. This offers an interesting solution for videos with static background and moving foreground objects that can be represented with keypoints. Our model can generalize to videos with changing background without needing keypoints to represent objects.</p><p>Optical flow has been used before in future prediction <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. Li et al. <ref type="bibr" target="#b20">[21]</ref> generate future frames from a still image by using optical flow generated by an off-theshelf model, whereas we compute flow as part of prediction. Lu et al. <ref type="bibr" target="#b24">[25]</ref> use optical flow for video extrapolation and interpolation without modeling stochasticity. Long-term video extrapolation results show the limitation of this work in terms of predicting future due to relatively small motion magnitudes considered in extrapolation. Differently from flow, Xue et al. <ref type="bibr">[36]</ref> model the motion as image differences using cross convolutions.</p><p>State-Space Models: Stochastic models are typically autoregressive, i.e. the next frame is predicted based on the frames generated by the model. As opposed to interleaving process of auto-regressive models, state-space models separate the frame generation from the modelling of dynamics <ref type="bibr" target="#b12">[13]</ref>. State-of-the-art method SRVP <ref type="bibr" target="#b8">[9]</ref> proposes a state-space model for video generation with deterministic state transitions representing residual change between the frames. This way, dynamics are modelled with latent state variables which are independent of previously generated frames. Although independent latent states are computationally appealing, they cannot model the motion history of the video. In addition, content variable designed to model static background cannot handle changes in the background. We can generate long sequences with complex motion patterns by explicitly modelling the motion history without any limiting assumptions about the dynamics of the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stochastic Video Prediction</head><p>Given the previous frames x 1:t?1 until time t, our goal is to predict the target frame x t . For that purpose, we assume that we have access to the target frame x t during training and use it to capture the dynamics of the video in stochastic latent variables z t . By learning to approximate the distribution over z t , we can decode the future frame x t from z t and the previous frames x 1:t?1 at test time.</p><p>Using all the frames including the target frame, we compute a posterior distribution q ? (z t |x 1:t ) and sample a latent variable z t from this distribution at each time step. The stochastic process of the video is captured by the latent variable z t . In other words, it should contain information accumulated over the previous frames rather than only condensing the information on the current frame. This is achieved by encouraging q ? (z t |x 1:t ) to be close to a prior distribution p(z) in terms of KL-divergence. The prior can be sampled from a fixed Gaussian at each time step or can be learned from previous frames up to the target frame p ? (z t |x 1:t?1 ). We prefer the latter one as it is shown to work better by learning a prior that varies across time <ref type="bibr" target="#b4">[5]</ref>.</p><p>The target frame x t is predicted based on the previous frames x 1:t?1 and the latent vectors z 1:t .</p><p>In practice, we only use the latest frame x t?1 and the latent vector z t as input and dependencies from further previous frames are propagated with a recurrent model. The output of the frame predictor g t contains the information required to decode x t . Typically, g t is decoded to a fixed-variance Gaussian distribution whose mean is the predicted target framex t <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SLAMP</head><p>We call the predicted target frame, appearance prediction x p t in the pixel space. In addition to x p t , we also estimate optical flow f t?1:t from the previous frame t ? 1 to the target frame t. The flow f t?1:t represents the motion of the pixels from the previous frame to the target frame. We reconstruct the target frame x f t from the estimated optical flow via differentiable warping <ref type="bibr" target="#b14">[15]</ref>. Finally, we estimate a mask m(x p t , x f t ) from the two frame estimations to combine them into the final estimationx t :</p><formula xml:id="formula_0">x t = m(x p t , x f t ) x p t + (1 ? m(x p t , x f t )) x f t<label>(1)</label></formula><p>where denotes element-wise Hadamard product and x f t is the result of warping the source frame to the target frame according to the estimated flow field f t?1:t . Especially in the dynamic parts with moving objects, the target frame can be reconstructed accurately using motion information. In the occluded regions where motion is unreliable, the model learns to rely on the appearance prediction. The mask prediction learns a weighting between the appearance and the motion predictions for combining them. We call this model SLAMP-Baseline because it is limited in the sense that it only considers the motion with respect to the previous frame while decoding the output. In SLAMP, we extend the stochasticity in the appearance space to the motion space as well. This way, we can model appearance changes and motion patterns in the video explicitly and make better predictions of future. <ref type="figure">Fig. 3</ref> shows an illustration of SLAMP (see Appendix Section A for SLAMP-Baseline).</p><p>In order to represent appearance and motion, we compute two separate posterior distributions q ? p (z p t |x 1:t ) and q ? f (z f t |x 1:t ), respectively. We sample two latent variables z p t and z f t from these distributions in the pixel space and the flow space. This allows a decomposition of the video into static and dynamic components. Intuitively, we expect the dynamic component to focus on changes and the static to what remains constant from the previous frames to the target frame. If the background is moving according to a camera motion, the static component can model the change in the background assuming that it remains constant throughout the video, e.g. ego-motion of a car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Motion History:</head><p>The latent variable z f t should contain motion information accumulated over the previous frames rather than local temporal changes between the last frame and the target frame. We achieve this by encouraging q ? f (z f t |x 1:t ) to be close to a prior distribution in terms of KL-divergence. Similar to <ref type="bibr" target="#b4">[5]</ref>, we learn the motion prior conditioned on previous frames up to the target frame: p ? f (z f t |x 1:t?1 ). We repeat the same for the static part represented by z p t with posterior q ? p (z p t |x 1:t ) and the learned prior p ? p (z p t |x 1:t?1 ).  <ref type="figure">Figure 3</ref>: SLAMP. This figure shows the components of our SLAMP model including the prediction model, inference and learned prior models for pixel and then flow from left to right. Observations x t are mapped to the latent space by using a pixel encoder for appearance on each frame and and a motion encoder for motion between consecutive frames. The blue boxes show encoders, yellow and green ones decoders, gray ones recurrent posterior, prior, and predictor models, and lastly red ones show loss functions during training. Note that L 2 loss is applied three times for appearance prediction x p t , motion prediction x f t , and the combination of the twox t according to the mask prediction m(x p t , x f t ). We only show L2 loss between the actual frame x t and the final predicted framex t in the figure. For inference, only the prediction model and learned prior models are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Variational Inference</head><p>For our basic formulation (SLAMP-Baseline), the derivation of the loss function is straightforward and provided in Appendix Section B. For SLAMP, the conditional joint probability corresponding to the graphical model in <ref type="figure">Fig. 2</ref> is:</p><formula xml:id="formula_1">p(x 1:T ) = T t=1 p(x t |x 1:t?1 , z p t , z f t ) (2) p(z p t |x 1:t?1 , z p t?1 ) p(z f t |x 1:t?1 , z f t?1 )</formula><p>The true distribution over the latent variables z p t and z f t is intractable. We train time-dependent inference networks q ? p (z p t |x 1:T ) and q ? f (z f t |x 1:T ) to approximate the true distribution with conditional Gaussian distributions. In order to optimize the likelihood of p(x 1:T ), we need to infer latent variables z p t and z f t , which correspond to uncertainty of static and dynamic parts in future frames, respectively. We use a variational inference model to infer the latent variables.</p><p>Since z p t and z f t are independent across time, we can decompose Kullback-Leibler terms into individual time steps. We train the model by optimizing the variational lower bound (see Appendix Section B for the derivation):</p><formula xml:id="formula_2">log p ? (x) ? L ?,? p ,? f ,? p ,? f (x 1:T ) (3) = t E z p 1:t ?q ? p z f 1:t ?q ? f log p ? (x t |x 1:t?1 , z p 1:t , z f 1:t ) ? ? D KL (q(z p t |x 1:t ) || p(z p t |x 1:t?1 )) + D KL (q(z f t |x 1:t ) || p(z f t |x 1:t?1 ))</formula><p>The likelihood p ? , can be interpreted as an L 2 penalty between the actual frame x t and the estimationx t as defined in <ref type="bibr" target="#b0">(1)</ref>. We apply the L 2 loss to the predictions of appearance and motion components as well.</p><p>The posterior terms for uncertainty are estimated as an expectation over q ? p (z p t |x 1:t ), q ? f (z f t |x 1:t ). As in <ref type="bibr" target="#b4">[5]</ref>, we also learn the prior distributions from the previous frames up to the target frame as p ? p (z p t |x 1:t?1 ), p ? f (z f t |x 1:t?1 ). We train the model using the re-parameterization trick <ref type="bibr" target="#b18">[19]</ref>. We classically choose the posteriors to be factorized Gaussian so that all the KL divergences can be computed analytically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Architecture</head><p>We encode the frames with a feed-forward convolutional architecture to obtain appearance features at each time-step. In SLAMP, we also encode consecutive frame pairs into a feature vector representing the motion between them. We then train linear LSTMs to infer posterior and prior distributions at each time-step from encoded appearance and motion features.</p><p>Stochastic video prediction model with a learned prior <ref type="bibr" target="#b4">[5]</ref> is a special case of our baseline model with a single pixel decoder, we also add motion and mask decoders. Next, we describe the steps of the generation process for the dynamic part.</p><p>At each time step, we encode x t?1 and x t into h f t , representing the motion from the previous frame to the target frame. The posterior LSTM is updated based on the h f t :</p><formula xml:id="formula_3">h f t = MotionEnc(x t?1 , x t ) (4) ? ? f (t) , ? ? f (t) = LSTM ? f (h f t )</formula><p>For the prior, we use the motion representation h f t?1 from the previous time step, i.e. the motion from the frame t ? 2 to the frame t ? 1, to update the prior LSTM:</p><formula xml:id="formula_4">h f t?1 = MotionEnc(x t?2 , x t?1 ) (5) ? ? f (t) , ? ? f (t) = LSTM ? f (h f t?1 )</formula><p>At the first time-step where there is no previous motion, we assume zero-motion by estimating the motion from the previous frame to itself. The predictor LSTMs are updated according to encoded features and sampled latent variables:</p><formula xml:id="formula_5">g f t = LSTM ? f (h f t?1 , z f t ) (6) ? ? f = FlowDec(g f t )</formula><p>There is a difference between the train time and inference time in terms of the distribution the latent variables are sampled from. At train time, latent variables are sampled from the posterior distribution. At test time, they are sampled from the posterior for the conditioning frames and from the prior for the following frames. The output of the predictor LSTMs are decoded into appearance and motion predictions separately and combined into the final prediction using the mask prediction (Eq. (1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the performance of the proposed approach and compare it to the previous methods on three standard video prediction datasets including Stochastic Moving MNIST, KTH Actions [28] and BAIR Robot Hand <ref type="bibr" target="#b5">[6]</ref>. We specifically compare our baseline model (SLAMP-Baseline) and our model (SLAMP) to SVG <ref type="bibr" target="#b4">[5]</ref> which is a special case of our baseline with a single pixel decoder, SAVP <ref type="bibr" target="#b19">[20]</ref>, SV2P <ref type="bibr" target="#b0">[1]</ref>, and lastly to SRVP <ref type="bibr" target="#b8">[9]</ref>. We also compare our model to SVG <ref type="bibr" target="#b4">[5]</ref> and SRVP <ref type="bibr" target="#b8">[9]</ref> on two different challenging real world datasets, KITTI <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref>, with moving background and complex object motion. We follow the evaluation setting introduced in [5] by generating 100 samples for each test sequence and report the results according to the best one in terms of average performance over the frames. Our experimental setup including training details and parameter settings can be found in Appendix Section C. We also share the code for reproducibility. Evaluation Metrics: We compare the performance using three frame-wise metrics and a video-level one. Peak Signalto-Noise Ratio (PSNR), higher better, based on L 2 distance between the frames penalizes differences in dynamics but also favors blur predictions. Structured Similarity (SSIM),    Stochastic Moving MNIST: This dataset contains up to two MNIST digits moving linearly and bouncing from walls with a random velocity as introduced in <ref type="bibr" target="#b4">[5]</ref>. Following the same training and evaluation settings as in the previous work, we condition on the first 5 frames during training and learn to predict the next 10 frames. During testing, we again condition on the first 5 frames but predict the next 20 frames. <ref type="figure" target="#fig_0">Fig. 4</ref> shows quantitative results on MNIST in comparison to SVG <ref type="bibr" target="#b4">[5]</ref> and SRVP <ref type="bibr" target="#b8">[9]</ref> in terms of PSNR and SSIM, omitting LPIPS as in SRVP. Our baseline model with a motion decoder (SLAMP-Baseline) already outperforms SVG on both metrics. SLAMP further improves the results by utilizing the motion history and reaches a comparable performance to the state of the art model SRVP. This shows the benefit of separating the video into static and dynamic parts in both state-space models (SRVP) and auto-regressive models (ours, SLAMP). This way, models can better handle challenging cases such as crossing digits as shown next.</p><p>We qualitatively compare SLAMP to SLAMP-Baseline on MNIST in <ref type="figure" target="#fig_2">Fig. 5</ref>. The figure shows predictions of static and dynamic parts as appearance and motion predictions, as well the final prediction as the combination of the two. According to the mask prediction, the final prediction mostly relies on the dynamic part shown as black on the mask and uses the static component only near the motion boundaries. Moreover, optical flow prediction does not fit the shape of the digits but expands as a region until touching the motion region of the other digit. This is due to the uniform black background. Moving a black pixel in the background randomly is very likely to result in another black pixel in the background, which means zero-loss for the warping result. Both models can predict optical flow correctly for the most part and resort to the appearance result in the occluded regions. However, continuity in motion is better captured by SLAMP with the colliding digits whereas the baseline model cannot recover from it, leading to blur results, far from the ground truth. Note that we pick the best sample for both  . We expect our model with motion history to perform very well by exploiting regularity in human actions on KTH. Following the same training and evaluation settings used in the previous work, we condition on the first 10 frames and learn to predict the next 10 frames. During testing, we again condition on the first 10 frames but predict the next 30 frames. <ref type="figure">Fig. 6</ref> and <ref type="table" target="#tab_1">Table 1</ref> show quantitative results on KTH in comparison to previous approaches. Both our baseline and SLAMP models outperform previous approaches and perform comparably to SRVP, in all metrics including FVD. A detailed visualization of all three frame predictions as well as flow and mask are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Flow predictions are much more fine-grained than MNIST by capturing fast motion of small objects such as hands or thin objects such as legs (see Appendix Section E). The mask decoder learns to identify regions around the motion boundaries which cannot be matched with flow due to occlusions and assigns more weight to the appearance prediction in these regions.</p><p>On KTH, the subject might appear after the conditioning frames. These challenging cases can be problematic for some previous work as shown in SRVP <ref type="bibr" target="#b8">[9]</ref>. Our model can generate samples close to the ground truth despite very little information on the conditioning frames as shown in <ref type="figure" target="#fig_6">Fig. 8</ref>. The figure shows the best sample in terms of LPIPS, please see Appendix Section E for a diverse set of samples with subjects of various poses appearing at different time steps. Cityscapes <ref type="bibr" target="#b3">[4]</ref> robot hand moving and pushing objects on a table <ref type="bibr" target="#b5">[6]</ref>. Due to uncertainty in the movements of the robot arm, BAIR is a standard dataset for evaluating stochastic video prediction models. Following the training and evaluation settings used in the previous work, we condition on the first 2 frames and learn to predict the next 10 frames. During testing, we again condition on the first 2 frames but predict the next 28 frames. We show quantitative results on BAIR in <ref type="figure">Fig. 6</ref> and Table 1. Our baseline model achieves comparable results to SRVP, outperforming other methods in all metrics except SV2P <ref type="bibr" target="#b0">[1]</ref> in PSNR and SAVP <ref type="bibr" target="#b19">[20]</ref> in FVD. With 2 conditioning frames only, SLAMP cannot utilize the motion history and performs similarly to the baseline model on BAIR (see Appendix Section D). This is simply due to the fact that there is only one flow field to condition on, in other words, no motion history. Therefore, we only show the results of the baseline model on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BAIR Robot Hand: This dataset contains videos of a</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-World Driving Datasets:</head><p>We perform experiments on two challenging autonomous driving datasets: KITTI <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref> with various challenges. Both datasets contain everyday real-world scenes with complex dynamics due to both background and foreground motion. KITTI is recorded in one town in Germany while Cityscapes is recorded in 50 European cities, leading to higher diversity.</p><p>Cityscapes primarily focuses on semantic understanding of urban street scenes, therefore contains a larger number of dynamic foreground objects compared to KITTI. However, motion lengths are larger on KITTI due to lower frame-rate. On both datasets, we condition on 10 frames and predict 10 frames into the future to train our models. Then at test time, we predict 20 frames conditioned on 10 frames.</p><p>As shown in <ref type="table" target="#tab_3">Table 2</ref>, SLAMP outperforms both methods on all of the metrics on both datasets, which shows its ability to generalize to the sequences with moving background. Even SVG <ref type="bibr" target="#b4">[5]</ref> performs better than the state of  the art SRVP <ref type="bibr" target="#b8">[9]</ref> in LPIPS metric for KITTI and on both SSIM and LPIPS for Cityscapes, which shows the limitations of SRVP on scenes with dynamic backgrounds. We also perform a qualitative comparison to these methods in <ref type="figure" target="#fig_7">Fig. 1 and Fig. 9</ref>. SLAMP can better preserve the scene structure thanks to explicit modeling of ego-motion history in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Latent Space:</head><p>We visualize stochastic latent variables of the dynamic component on KTH compared to the static and SVG. (see <ref type="figure" target="#fig_0">Fig. 14 and Fig. 15</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a stochastic video prediction framework to decompose video content into appearance and dynamic components. Our baseline model with deterministic motion and mask decoders outperforms SVG, which is a special case of our baseline model. Our model with motion history, SLAMP, further improves the results and reaches the performance of the state of the art method SRVP on the previously used datasets. Moreover, it outperforms both SVG and SRVP on two real-world autonomous driving datasets with dynamic background and complex motion. We show that motion his-tory enriches model's capacity to predict future, leading to better predictions in challenging cases.</p><p>Our model with motion history cannot realize its full potential in standard settings of stochastic video prediction datasets. A fair comparison is not possible on BAIR due to the little number of conditioning frames. BAIR holds a great promise with changing background but infrequent, small changes are not reflected in current evaluation metrics.</p><p>An interesting direction is stochastic motion decomposition, maybe with hierarchical latent variables, for modelling camera motion and motion of each object in the scene separately. In this part, we provide additional illustrations, derivations, and results for our paper "SLAMP: Stochastic Latent Appearance and Motion Prediction". We first show the model illustrations of our proposed model (SLAMP) and our baseline model (SLAMP-Baseline) in comparison to the previous work by Denton et al. <ref type="bibr" target="#b4">[5]</ref> (SVG) in Section A. In Section B, we provide the full derivations of the variational inference, evidence lower bounds of our baseline model (Section B.1) and our proposed model (Section B.2). We explain the architectural choices and training details in Section C. In Section D, we present detailed versions of the quantitative results in the main paper. In addition, we present the ablation experiments for our model's mask component. We evaluate and compare the predictions of static, dynamic heads of the model, simple averaging of the two without a mask, and our full model with learned mask. In Section E, we first provide the color wheel to interpret optical flow predictions, and a comparison of static and dynamic latent variables. We then present several qualitative results both with details as in the main paper and with random samples, showing the diversity of the generated samples on all datasets.</p><p>For video examples, please visit https://kuis-ai.github.io/slamp/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Illustrations</head><p>In <ref type="figure" target="#fig_8">Fig. 10</ref>, we provide the inference procedure of our model SLAMP, in addition to the training procedure provided in the main paper. Moreover, we present graphical illustrations of the training <ref type="figure">(Fig. 11</ref>) and the inference procedures ( <ref type="figure" target="#fig_11">Fig. 12</ref>) of our baseline model, SLAMP-Baseline, in comparison to SVG <ref type="bibr" target="#b4">[5]</ref>.   <ref type="figure">Figure 11</ref>: Illustration of the Training Procedure for SVG (left) and SLAMP-Baseline (right). The main difference between SVG and SLAMP-Baseline is that SLAMP-Baseline has three decoders instead of one pixel decoder. In SLAMP-Baseline, in addition to the appearance prediction x p t , we also estimate flow f t?1:t and warp the previous frame according to the estimated flow to obtain motion prediction x f t . Mask decoder takes appearance and motion predictions as input and generates a weighted combination of the two,x t as the final prediction. Note that SVG corresponds to only appearance prediction case of our baseline model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivations</head><p>Here, we provide derivations of inference steps and variational lower bounds of the baseline method, SLAMP-Baseline (Section B.1), and our method SLAMP (Section B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Derivation of the ELBO for SLAMP-Baseline</head><p>We first derive the variational lower bound for the baseline model with one posterior and one learned prior distribution.</p><formula xml:id="formula_6">log p ? (x) = log z p ? (x|z) p(z|x) (7) = log z p ? (x|z) p(z|x) q ? (z|x) q ? (z|x) = log E q ? (z|x) p ? (x|z) p(z|x) q ? (z|x) ? E q ? (z|x) log p ? (x|z) p(z|x) q ? (z|x) = E q ? (z|x) log p ? (x|z) ? E q ? (z|x) log q ? (z|x) p(z|x) = E q ? (z|x) log p ? (x|z) ? D KL (q ? (z|x) || p(z|x))</formula><p>We model the posterior distribution with a recurrent network. The recurrent network outputs a different posterior distribution, q ? (z t |x 1:t ), at every time step. Due to independence of latent variables across time, z = [z 1 , z 2 , . . . , z T ], we can derive the estimation of posterior distribution across time steps as follows:</p><formula xml:id="formula_7">q ? (z|x) = t q ? (z t |x 1:t )<label>(8)</label></formula><p>Since the latent variables, z = [z 1 , z 2 , ? ? ? , z T ], are independent across time, we can further decompose Kullback-Leibler term in the evidence lower bound into individual time steps:</p><formula xml:id="formula_8">D KL ( q ? (z|x)|| p(z|x 1:t?1 )) (9) = z q ? (z|x) log q ? (z|x) p(z|x 1:t?1 ) = z1 ? ? ? z T q ? (z 1 |x 1 ) ? ? ? q ? (z T |x 1:T ) log q ? (z 1 |x 1 ) ? ? ? q ? (z t |x 1:T ) p(z 1 |x 1 ) ? ? ? p(z T |x 1:T ?1 ) = z1 ? ? ? z T q ? (z 1 |x 1 ) ? ? ? q ? (z T |x 1:T ) t log q ? (z t |x 1:t ) p(z t |x 1:t?1 ) = t z1 ? ? ? z T q ? (z 1 |x 1 ) ? ? ? q ? (z T |x 1:T ) log q ? (z t |x 1:t ) p(z t |x 1:t?1 )</formula><p>And because</p><p>x p(x) = 1, this simplifies to:</p><formula xml:id="formula_9">= t zt q ? (z t |x 1:t ) log q ? (z t |x 1:t ) p(z t |x 1:t?1 ) = t D KL ( q ? (z t |x 1:t )|| p(z t |x 1:t?1 ))</formula><p>At each time step, our model predicts x t , conditioned on x t?1 , z p t , z f t . Since our model has recurrence connections, it considers not only x t?1 , z p t and z f t , but also x 1:t?2 , z p 1:t?1 and z f 1:t?1 . Therefore, we can further write our inference as:</p><formula xml:id="formula_10">log p ? (x|z p , z f ) = log t p ? (x t |x 1:t?1 , z p 1:t , z f 1:t ) (14) = t log p ? (x t |x 1:t?1 , z p 1:t , z f 1:t )</formula><p>Combining all of them leads to the following variational lower bound:</p><formula xml:id="formula_11">log p ? (x) ? L ?,? p ,? f (x 1:T ) (15) = E z p ?q ? p z f ?q ? f log p ? (x|z p , z f ) ? D KL (q ? p (z p |x) || p ? p (z p |x)) ? D KL (q ? f (z f |x) || p ? f (z f |x)) = t E z p ?q ? p z f ?q ? f log p ? (x t |x 1:t?1 , z p 1:t , z f 1:t ) ? D KL (q ? p (z p t |x 1:t || p ? p (z p t |x 1:t?1 )) ? D KL (q ? f (z f t |x 1:t || p ? f (z f t |x 1:t?1 ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>We provide training details including scheduled sampling (Section C.1), architecture details (Section C.2), and the hyper-parameters used in the optimization (Section C.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Scheduled Sampling</head><p>Scheduled sampling proposed for sequence prediction <ref type="bibr" target="#b1">[2]</ref> has been proven useful for several tasks where predictions need to be made based on the generated results from the previous time steps. We also experiment with scheduled sampling as part of our training procedure. Scheduled sampling prevents the model from conditioning on ground-truth perfect samples which are not available at test time. This is achieved by allowing the model to slowly encounter generated samples instead of ground-truth perfect samples. The ratio of ground-truth perfect samples over generated samples is decreased throughout the training. Specifically, we apply inverse sigmoid decay. We report the scores with and without scheduled sampling for the proposed models, both SLAMP-Baseline and SLAMP, on all datasets. As can be seen from <ref type="table" target="#tab_5">Table 3</ref>, scheduled sampling is not crucial but it improves the results on KTH, especially for SLAMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Architecture Details</head><p>Encoders and Decoders: For all encoders and decoders, we use the same architectures as the previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>: a DCGAN [27] generator and discriminator for MNIST, and a VGG16 architecture [30] for KTH and BAIR datasets. In all datasets, we encode the image into an appearance feature vector of size h appearance = 128 and the two consecutive images into a motion feature vector of size h motion = 128. Compared to SVG, there are two more decoders for predicting flow and mask in our models. For flow decoder, we use the same decoder with two output channels representing motion in horizontal and vertical direction. See below for the details of the mask decoder. For SLAMP, motion encoder takes concatenated frame pair as input and outputs a feature vector encoding motion from one frame to the next.</p><p>Similar to previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, we also use skip connections but with a minor modification. In the previous work, the skip connection from either the last conditioning frame or last generated frame is used. Instead, we take the running average of all the skip connections from seen or generated frames. For example, at time step 15, we use the average of previous 14 skip connections that are generated.</p><p>Mask Predictor: For mask predictor, we use a 5-layer CNN with 2 Squeeze and Excitation layers (SE-Layer) <ref type="bibr" target="#b13">[14]</ref> after each two convolutional layers. In the CNN, we use 64-channel filters at each layer and do not reduce the resolution by using 3 ? 3 kernels with padding. We simply concatenate the output of pixel decoder and warped prediction along their channel axis and feed it into mask predictor which outputs a one-channel image. We apply sigmoid at the end to map the output to the range between 0 and 1, representing the weight to combine the appearance and the motion predictions. LSTMs and Latent Variables: For prior, posterior, and frame predictor LSTMs, we use the settings proposed in SVG <ref type="bibr" target="#b4">[5]</ref>. All LSTMs have 256 neurons and all prior and posterior LSTMs have one layer whereas the frame predictor LSTMs have two layers. For the size of the latent variables, we use 20, 50, 64 for MNIST, KTH, and BAIR, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Optimization Hyper-Parameters</head><p>All the models are trained with Adam optimizer <ref type="bibr" target="#b17">[18]</ref>, with decay rates ? 1 = 0.9 and ? 2 = 0.999. We train each model for 300 epochs where each epoch consists of 1000 updates, unless otherwise is specified. We take the model which performs the best in the validation set. We will share the trained models upon publication for replicating the results. Dataset-specific parameters for each dataset are as follows:</p><p>MNIST: The batch size is chosen to be 32, learning rate is 3 ? 10 ?4 and ? = 1 ? 10 ?4 . We continue training the models on MNIST with a lower learning rate, 1 ? 10 ?5 , a lower ? = 5 ? 10 ?5 , and a lower ? 1 = 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KTH:</head><p>The batch size is chosen to be 20, learning rate is 1 ? 10 ?4 and ? = 1 ? 10 ?6 . We apply scheduled sampling with inverse sigmoid decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BAIR:</head><p>The batch size is chosen to be 20, learning rate is 1 ? 10 ?4 and ? = 1 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training details for KITTI and Cityscapes:</head><p>We use 92 ? 310 image resolution for KITTI and 128 ? 256 for Cityscapes. We replaced LSTMs with ConvLSTMs and used 3 ? 10 intermediate feature size for KITTI, 4 ? 8 for Cityscapes. We used a shared encoder to downsample the image first and then, use two separate encoders for pixel and motion encoders to make model less powerful. We increased the number of layers in the shared encoder to downsample the higher resolution image, and preserve the VGG-basedd structure.</p><p>For SVG, we use the same settings as SLAMP. For SRVP, we use the same shared encoder and use a channel pooling at the end to make the convolutional feature vector compatible with the rest of the architecture.</p><p>We train all the models until the models see 2.4M video samples. We use the largest batch size that we could use and choose the learning rate 1 ? 10 ?4 for all the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed Quantitative Results</head><p>In this section, we provide a detailed version of the quantitative results presented in the main paper in <ref type="figure">Figure 3</ref> and 5. We compare the performance of SLAMP-Baseline and SLAMP to the previous work in terms of PNSR, SSIM, and LPIPS averaged over all time steps on MNIST <ref type="table">(Table 4)</ref>, KTH <ref type="table" target="#tab_6">(Table 5)</ref>, and BAIR <ref type="table" target="#tab_2">(Table 6)</ref> datasets. Confirming the results in the main paper, the proposed model SLAMP with motion history outperforms both the baseline model, SLAMP-Baseline, and the previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref> and performs comparably to the state of the art model SRVP <ref type="bibr" target="#b8">[9]</ref>. See the main paper for a detailed analysis. <ref type="table">Table 4</ref>: Results on MNIST. This table compares the results of SLAMP and SLAMP-Baseline to the previous work on MNIST dataset. Following the previous work, we report the results as the mean and the 95%-confidence interval in terms of PSNR and SSIM. Bold and underlined scores indicate the best and the second best performing method, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>PSNR SSIM SVG <ref type="bibr" target="#b4">[5]</ref> 14.50 ? 0.04 0.7090 ? 0.0015 SRVP <ref type="bibr" target="#b8">[9]</ref> 16.93 ? 0.07 0.7799 ? 0.0020 SLAMP-Baseline 16.83 ? 0.06 0.7537 ? 0.0018 SLAMP 18.07 ? 0.08 0.7736 ? 0.0019  <ref type="bibr" target="#b0">[1]</ref> 28.19 ? 0.31 0.8141 ? 0.0050 0.2049 ? 0.0053 SAVP <ref type="bibr" target="#b19">[20]</ref> 26.51 ? 0.29 0.7564 ? 0.0062 0.1120 ? 0.0039 SVG <ref type="bibr" target="#b4">[5]</ref> 28.06 ? 0.29 0.8438 ? 0.0054 0.0923 ? 0.0038 SRVP <ref type="bibr" target="#b8">[9]</ref> 29.69 ? 0.32 0.8697 ? 0.0046 0.0736 ? 0.0029 SLAMP-Baseline 29.20 ? 0.28 0.8633 ? 0.0048 0.0951 ? 0.0036 SLAMP 29.39 ? 0.30 0.8646 ? 0.0050 0.0795 ? 0.0034  <ref type="bibr" target="#b19">[20]</ref> 18.44 ? 0.25 0.7887 ? 0.0092 0.0634 ? 0.0026 SVG <ref type="bibr" target="#b4">[5]</ref> 18.95 ? 0.26 0.8058 ? 0.0088 0.0609 ? 0.0034 SRVP <ref type="bibr" target="#b8">[9]</ref> 19.59 ? 0.27 0.8196 ? 0.0084 0.0574 ? 0.0032 SLAMP-Baseline 19.60 ? 0.26 0.8175 ? 0.0084 0.0596 ? 0.0032 SLAMP 19.67 ? 0.26 0.8161 ? 0.0086 0.0639 ? 0.0037</p><p>In addition, we provide detailed results corresponding to the components of our model. We evaluate the result of the static head, the dynamic head, and simply their average without the learned mask and compare them to our full model with the learned mask in <ref type="table" target="#tab_8">Table 7</ref>. Our full model, SLAMP, performs the best in all datasets according to all three evaluation metrics by combining the two predictions according to the mask prediction. to the evaluation of the prediction of the static head directly, Dynamic refers to the evaluation of the prediction of the dynamic head which uses optical flow to predict the next frame, Average refers to average of static and dynamic heads without using the mask prediction. The last row for each dataset show the results of our model which uses the mask prediction to fuse the static and dynamic predictions. Following the previous work, we report the results as the mean and the 95%-confidence interval in terms of PSNR, SSIM, and LPIPS on all the datasets except LPIPS on MNIST.   <ref type="figure" target="#fig_12">Fig. 13</ref> shows the color wheel used to visualize the optical flow with false coloring. Colors show the direction of motion and the intensity of color in the visualizations show the magnitude of motion, i.e. intense colors for large motions. By following the usual practice in optical flow, we predict flow from the target frame to the current frame and apply inverse warping to obtain the target frame. Therefore, the direction of motion is also inverse, i.e. the opposite direction on the wheel shows the motion from the current frame to the target frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Comparison of Static and Dynamic Latent Variables</head><p>In <ref type="figure" target="#fig_6">Fig. 8</ref> of the main paper, we provide a visualization of stochastic latent variables of the dynamic component on KTH using t-SNE. Here, we provide both the static and the dynamic components for a comparison. The same colors from the main paper show the semantic classes of video frames plotted. As can be seen from <ref type="figure" target="#fig_0">Fig. 14, static</ref> variables on the right are more scattered and do not from clusters according to semantic classes as in the dynamic variables on the left (and in the main paper). This shows that our model can learn video dynamics according to semantic classes with separate modelling of the dynamic component.  In <ref type="figure" target="#fig_2">Fig. 15</ref>, we visualize the latent variables of SVG <ref type="bibr" target="#b4">[5]</ref> to show the difference between our architecture's latent variables and SVG's latent variables. In our architecture, dynamic branch learns the similar repetitive movements whereas static branch learns the general image information. Therefore, dynamic latent variables form cluster around similar movements. However, in SVG, there is only one branch to predict the future frames, which only encodes the general image information rather than motion cues. Therefore, samples do not form clusters according to semantic classes as in our case for dynamic latent variables shown on the left in <ref type="figure" target="#fig_0">Fig. 14</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Diversity of Generated Samples</head><p>As proposed in SAVP <ref type="bibr" target="#b19">[20]</ref>, as a measure of diversity, we visualize the average over 100 generated samples. According to this measure, if a model is able to generate diverse results, generated samples should differ where there is motion, e.g. a moving object appearing in different positions and moving in different directions, leading to blurring out of the moving object. Therefore, we expect to see the background without moving objects in the average of the generated samples. The average samples confirm this for our model as shown for MNIST <ref type="figure" target="#fig_5">Fig. 16, KTH Fig. 17</ref>, and BAIR <ref type="figure" target="#fig_6">Fig. 18</ref>.</p><p>There is a special case on KTH which further supports our diversity claim as shown in <ref type="figure" target="#fig_5">Fig. 7</ref> of the main paper. When subject appears after conditioning frames, our model can handle stochasticity of this challenging case and can generate diverse sequences. We show the best prediction and three random predictions in <ref type="figure" target="#fig_7">Fig. 19</ref>. Generated samples differ in terms of pose and speed of the subject as well as the time step that the subject appears. Average of 100 samples <ref type="figure" target="#fig_5">Figure 17</ref>: Diversity on KTH. Since the running person appears after the conditioning frames, the model should generate different results for each sample. The average of the generated samples does not contain any human because our model can generate diverse results, e.g. person in various poses appearing at different time steps with different speed.</p><p>Ground Truth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Additional Qualitative Results</head><p>For each dataset, we show random examples with details of the best sample as well as three random samples generated. The detailed visualizations show appearance and motion prediction separately as well as the mask prediction and optical flow with false coloring. Random sample visualizations show the best sample and three random samples. We also provide full sequences of the samples in <ref type="figure">Fig. 1</ref> in <ref type="figure">Fig. 32</ref> and 33. Optical Flow <ref type="figure">Figure 20</ref>: No Overlapping Digits. This figure show a regular case with two non-overlapping digits. Note that predicted flow is from the target frame to the current frame since we apply inverse warping. The correctness of the optical flow estimation can be verified by inspecting <ref type="figure" target="#fig_12">Fig. 13</ref>.</p><p>Ground Truth t = 1 t = 3 t = 5 t = 6 t = 8 t = 9 t = 11 t = 13 t = 15 t = 17 t = 19 t = 22 t = 24</p><p>Final Prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appearance Prediction</head><p>Motion Prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Prediction</head><p>Optical Flow <ref type="figure">Figure 21</ref>: Stationary Digit. This figure shows a case where a digit, i.e. 0, is not moving. As can be seen from the last row, optical flow is correctly estimated as zero for that digit. The digits start overlapping at around t = 13. Our model can successfully handle this challenging case by preserving the appearance of digits. <ref type="figure">Figure 32</ref>: Full sequence of Cityscapes in <ref type="figure">Fig. 1</ref> We show the full sequence comparisons with baseline method, SVG <ref type="bibr" target="#b4">[5]</ref>, and state-of-the-art method, SRVP <ref type="bibr" target="#b8">[9]</ref>. Our model can model the ego-motion while both SRVP and SVG suffers from it significantly. t = 1 t = 10 t = 11 t = 13 t = 16 t = 19 t = 21 t = 26 t = 29 SVG <ref type="bibr" target="#b4">[5]</ref> SRVP <ref type="bibr" target="#b8">[9]</ref> Ours <ref type="figure">Figure 33</ref>: Full sequence of KITTI in <ref type="figure">Fig. 1</ref> We show the full sequence comparisons with baseline method, SVG <ref type="bibr" target="#b4">[5]</ref>, and state-of-the-art method, SRVP <ref type="bibr" target="#b8">[9]</ref>. Our model can model both the ego-motion and independently moving objects while both SRVP and SVG cannot reconstruct the future frames successfully.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Quantitative Results on MNIST. This figure compares SLAMP to SLAMP-Baseline, SVG [5], and SRVP [9] on MNIST in terms of PSNR (left) and SSIM (right). SLAMP clearly outperforms our baseline model and SVG, and performs comparably to SRVP. Vertical bars mark the length of the training sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>SLAMP-Baseline (left) vs. SLAMP (right) on MNIST. The top row shows the ground truth, followed by the frame predictions by the final, the appearance, the motion, and the last two rows show the mask and the optical flow predictions with false coloring. In this challenging case with bouncing and collisions, the baseline confuses the digits and cannot predict last frames correctly whereas SLAMP can generate predictions very close to the ground truth by learning smooth transitions in the motion history, as can be seen from optical flow predictions. SeeFig. 13for the color wheel showing the direction of flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative Results on KTH We visualize the results of SLAMP on KTH dataset. The top row shows the ground truth, followed by the frame predictions by the final, the appearance, the motion, and the last two rows show the mask and the optical flow predictions. The mask prediction combines the appearance prediction (white) and the motion prediction (black) into the final prediction. models among 100 samples according to LPIPS.KTH Action Dataset: KTH dataset contains real videoswhere people perform a single action such as walking, running, boxing, etc. in front of a static camera[28]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Subject Appearing after the Conditioning Frames. This figure shows a case where the subject appears after conditioning frames on KTH with ground truth (top) and a generated sample by our model (bottom). This shows our model's ability to capture dynamics of the dataset by generating samples close to the ground truth, even conditioned on empty frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative Comparison. We compare SLAMP to SVG<ref type="bibr" target="#b4">[5]</ref> and SRVP<ref type="bibr" target="#b8">[9]</ref> on KITTI (top) and Cityscapes (bottom). Our model can better capture the changes due to ego-motion thanks to explicit modeling of motion history.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Illustration of the Inference Procedure for SLAMP. This figure illustrates the difference between the inference time and the train time in terms of the distributions the latent variables are sampled from. While at train time, latent variables are sampled from the posterior distribution, at test time, they are sampled from the posterior for the conditioning frames and from the prior for the following frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Illustration of the Inference Procedure for SVG (left) and SLAMP-Baseline (right). This figure illustrates the inference time in comparison to the train time in terms of the distribution the latent variables are sampled from. While at train time, latent variables are sampled from the posterior distribution, at test time, they are sampled from the posterior for the conditioning frames and from the prior for the following frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Optical Flow False Coloring. Colors on the wheel indicate the direction of motion in 2D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Dynamic (left) vs. Static (right) Latent Variables. This figure shows the T-SNE visualization of dynamic and static latent variables on 300 test videos from KTH dataset. In dynamic latent variables, different classes with similar repetitive movements such as walking, running, and jogging are clustered together. However, in static latent variables, points are more scattered and do not form clusters according to semantic actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Latent Variables of SVG. This figure shows the T-SNE visualization of latent variables of SVG [5] on 300 test videos from KTH dataset. The latent variables form a normal distribution when visualized with T-SNE because SVG method uses a standard normal distribution as a fixed prior in KTH dataset. The model learns the general image information instead of motions groupings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 22 :</head><label>22</label><figDesc>Overlapping Digits. This figure shows a challenging case where two digits cross each other and continue moving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FVD Scores on KTH and BAIR. This table compares all the methods in terms of FVD scores with their 95%-confidence intervals over five different samples from the models. Our model is the second best on KTH and among top three methods on BAIR.</figDesc><table><row><cell>Dataset</cell><cell>KTH</cell><cell>BAIR</cell></row><row><cell>SV2P</cell><cell cols="2">636 ? 1 965 ? 17</cell></row><row><cell>SAVP</cell><cell>374 ? 3</cell><cell>152 ? 9</cell></row><row><cell>SVG</cell><cell>377 ? 6</cell><cell>255 ? 4</cell></row><row><cell>SRVP</cell><cell>222 ? 3</cell><cell>163? 4</cell></row><row><cell cols="2">SLAMP-Baseline 236 ? 2</cell><cell>245 ? 5</cell></row><row><cell>SLAMP</cell><cell>228 ? 5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Figure 6 :</head><label>6</label><figDesc>Quantitative Results on KTH and BAIR. We compare our results to previous work in terms of PSNR, SSIM, and LPIPS metrics with respect to the time steps on KTH (top), and BAIR (bottom) datasets, with 95%confidence intervals. Vertical bars mark the length of training sequences. SLAMP outperforms previous work including SVG<ref type="bibr" target="#b4">[5]</ref>, SAVP<ref type="bibr" target="#b19">[20]</ref>, SV2P<ref type="bibr" target="#b0">[1]</ref> and performs comparably to the state of the art method SRVP [9] on both datasets.higher better, compares local patches to measure similarity in structure spatially. Learned Perceptual Image Patch Similarity (LPIPS) [37], lower better, measures the distance between learned features extracted by a CNN trained for image classification. Frechet Video Distance (FVD) [31], lower better, compares temporal dynamics of generated videos to the ground truth in terms of representations computed for action recognition.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results with a Moving Background. We evaluate our model SLAMP in comparison to SVG and SRVP on KITTI<ref type="bibr" target="#b10">[11]</ref> and Cityscapes<ref type="bibr" target="#b3">[4]</ref> datasets by conditioning on 10 frames and predicting 20 frames into the future. ? 0.70 0.329 ? 0.030 0.594 ? 0.034 SRVP<ref type="bibr" target="#b8">[9]</ref> 13.41 ? 0.42 0.336 ? 0.034 0.635 ? 0.021 SLAMP 13.46 ? 0.74 0.337 ? 0.034 0.537 ? 0.042</figDesc><table><row><cell>Models</cell><cell>PSNR (?)</cell><cell>SSIM (?)</cell><cell>LPIPS (?)</cell></row><row><cell>SVG [5]</cell><cell cols="2">12.70 KITTI [12, 11]</cell><cell></cell></row><row><cell>Models</cell><cell>PSNR (?)</cell><cell>SSIM (?)</cell><cell>LPIPS (?)</cell></row><row><cell>SVG [5]</cell><cell cols="3">20.42 ? 0.63 0.606 ? 0.023 0.340 ? 0.022</cell></row><row><cell cols="4">SRVP [9] 20.97 ? 0.43 0.603 ? 0.016 0.447 ? 0.014</cell></row><row><cell>SLAMP</cell><cell cols="3">21.73 ? 0.76 0.649 ? 0.025 0.2941 ? 0.022</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>prediction. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), July 2017. 1, 2 [26] Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin P Murphy, and Honglak Lee. Unsupervised learning of object structure and dynamics from videos. In Advances in Neural Information Pro-Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2018. 6</figDesc><table><row><cell>In Advances In Neural Information Processing Systems,</cell></row><row><cell>2016. 2</cell></row><row><cell>[37]</cell></row><row><cell>cessing Systems (NeurIPS), 2019. 2</cell></row><row><cell>[27] Alec Radford, Luke Metz, and Soumith Chintala. Un-</cell></row><row><cell>supervised representation learning with deep convolu-</cell></row><row><cell>tional generative adversarial networks. In 4th Interna-</cell></row><row><cell>tional Conference on Learning Representations, 2016.</cell></row><row><cell>15</cell></row><row><cell>[28] Christian Sch?ldt, Ivan Laptev, and Barbara Caputo.</cell></row><row><cell>Recognizing human actions: A local svm approach.</cell></row><row><cell>In Proc. IEEE Conf. on Computer Vision and Pattern</cell></row><row><cell>Recognition (CVPR), 2004. 5, 7</cell></row><row><cell>-</cell></row><row><cell>sentations, 2015. 15</cell></row><row><cell>[31] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Ku-</cell></row><row><cell>rach, Raphael Marinier, Marcin Michalski, and Sylvain</cell></row><row><cell>Gelly. Towards accurate generative models of video:</cell></row><row><cell>A new metric &amp; challenges. arXiv.org, 2019. 6</cell></row><row><cell>[32] Ruben Villegas, Arkanath Pathak, Harini Kannan, Du-</cell></row><row><cell>mitru Erhan, Quoc V Le, and Honglak Lee. High</cell></row><row><cell>fidelity video prediction with large stochastic recurrent</cell></row><row><cell>neural networks. In Advances in Neural Information</cell></row><row><cell>Processing Systems (NeurIPS), 2019. 2</cell></row><row><cell>[33] C. Vondrick and A. Torralba. Generating the future</cell></row><row><cell>with adversarial transformers. In Proc. IEEE Conf.</cell></row><row><cell>on Computer Vision and Pattern Recognition (CVPR),</cell></row><row><cell>2017. 1, 2</cell></row><row><cell>[34] Jacob Walker, Carl Doersch, Abhinav Gupta, and Mar-</cell></row><row><cell>tial Hebert. An uncertain future: Forecasting from</cell></row><row><cell>static images using variational autoencoders. In Proc.</cell></row><row><cell>of the European Conf. on Computer Vision (ECCV),</cell></row><row><cell>2016. 2</cell></row><row><cell>[35] Jacob Walker, Abhinav Gupta, and Martial Hebert.</cell></row><row><cell>Dense optical flow prediction from a static image. In</cell></row><row><cell>Proc. of the IEEE International Conf. on Computer</cell></row><row><cell>Vision (ICCV), 2015. 1, 2</cell></row></table><note>[29] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai kin Wong, and Wang chun Woo. Convo- lutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems (NeurIPS), 2015. 2 [30] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni- tion. In International Conference on Learning Repre[36] Tianfan Xue, Jiajun Wu, Katherine L Bouman, and William T Freeman. Visual dynamics: Probabilistic fu- ture frame synthesis via cross convolutional networks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study. This table shows the quantitative results comparing SLAMP-Baseline and SLAMP with scheduled sampling (+ SS) and without during training. Following the previous work, we report the results as the mean and the 95%-confidence interval in terms of PSNR, SSIM, and LPIPS on all the datasets except LPIPS on MNIST.</figDesc><table><row><cell></cell><cell>Models</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>MNIST</cell><cell cols="3">SLAMP-Baseline SLAMP-Baseline + SS 16.32 ? 0.06 0.7343 ? 0.0016 16.83 ? 0.06 0.7537 ? 0.0017 SLAMP 18.07 ? 0.07 0.7736 ? 0.0019</cell><cell>---</cell></row><row><cell></cell><cell>SLAMP + SS</cell><cell cols="2">17.54 ? 0.08 0.7567 ? 0.0018</cell><cell>-</cell></row><row><cell></cell><cell>SLAMP-Baseline</cell><cell cols="3">28.47 ? 0.27 0.8527 ? 0.0053 0.0896 ? 0.0038</cell></row><row><cell>KTH</cell><cell cols="4">SLAMP-Baseline + SS 29.20 ? 0.28 0.8633 ? 0.0048 0.0951 ? 0.0036 SLAMP 28.91 ? 0.28 0.8604 ? 0.0049 0.0860 ? 0.0037</cell></row><row><cell></cell><cell>SLAMP + SS</cell><cell cols="3">29.39 ? 0.30 0.8646 ? 0.0049 0.0795 ? 0.0033</cell></row><row><cell></cell><cell>SLAMP-Baseline</cell><cell cols="3">19.60 ? 0.26 0.8175 ? 0.0083 0.0596 ? 0.0031</cell></row><row><cell>BAIR</cell><cell cols="4">SLAMP-Baseline + SS 19.55 ? 0.26 0.8171 ? 0.0083 0.0634 ? 0.0034 SLAMP 19.67 ? 0.26 0.8161 ? 0.0086 0.0639 ? 0.0037</cell></row><row><cell></cell><cell>SLAMP + SS</cell><cell cols="3">19.75 ? 0.26 0.8160 ? 0.0084 0.0661 ? 0.0035</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results on KTH. This table compares the results of SLAMP and SLAMP-Baseline to the previous work on KTH dataset. Following the previous work, we report the results as the mean and the 95%-confidence interval in terms of PSNR, SSIM, and LPIPS. Bold and underlined scores indicate the best and the second best performing method, respectively.</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>SV2P</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on BAIR. This table compares the results of SLAMP and SLAMP-Baseline to the previous work on BAIR dataset. Following the previous work, we report the results as the mean and the 95%-confidence interval in terms of PSNR, SSIM, and LPIPS. Bold and underlined scores indicate the best and the second best performing method, respectively.</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS</cell></row><row><cell>SV2P [1]</cell><cell cols="3">20.39 ? 0.27 0.8169 ? 0.0086 0.0912 ? 0.0053</cell></row><row><cell>SAVP</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Mask Ablation Study. This table shows the quantitative results comparing the components of SLAMP model. Static refers</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>.</figDesc><table><row><cell>jogging</cell><cell>running</cell><cell>handwaving</cell><cell>boxing</cell><cell>walking</cell><cell>handclapping</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = 1 t = 10 t = 11 t = 13 t = 16 t = 19 t = 21 t = 26 t = 29SVG<ref type="bibr" target="#b4">[5]</ref> SRVP<ref type="bibr" target="#b8">[9]</ref> Ours t = 1 t = 10 t = 11 t = 13 t = 16 t = 19 t = 21 t = 26 t = 29SVG<ref type="bibr" target="#b4">[5]</ref> SRVP<ref type="bibr" target="#b8">[9]</ref> Ours</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = 1 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14 t = 16 t = 18 t = 20 t = 21 t = 23 t = 24Average of 100 samplesFigure 16: Diversity on MNIST. After a digit hits the wall, it can move in any direction. Our model successfully models the stochasticity of this case and generates diverse results, resulting in blurry average images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = 1 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14 t = 16 t = 18 t = 21 t = 24 t = 26 t = 29Average of 100 samplesFigure 18: Diversity on BAIR. The robot hand can move in any direction at each time step, therefore the generated samples should differ from each other in terms of the position of the robot hand. The moving robot hand becomes invisible in the average images after the first few frames, which is an indication of the diversity of the generated samples.Ground Truth t = 1 t = 4 t = 6 t = 8 t = 11 t = 14 t = 18 t = 22 t = 24 t = 27 t = 30 t = 34 t = 37 t = 39 Best sample Random sample 1 Random sample 2Random sample 3Figure 19: Subject Appearing after Conditioning Frames on KTH. We show the best sample and three random samples for the case where the subject appears after conditioning frames. Our model generates different results at each random sample by learning dataset dynamics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = 1 t = 10 t = 11 t = 13 t = 16 t = 19 t = 21 t = 26 t = 29SVG<ref type="bibr" target="#b4">[5]</ref> SRVP<ref type="bibr" target="#b8">[9]</ref> Ours</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Jean-Yves Franceschi and Edouard Delasalles for providing technical and numerical details for the baseline performances, and Deniz Yuret for helpful discussions and comments. K. Akan was supported by KUIS AI Center fellowship, F. G?ney by TUBITAK 2232 International Fellowship for Outstanding Researchers Programme, E. Erdem in part by GEBIP 2018 Award of the Turkish Academy of Sciences, A. Erdem by BAGEP 2021 Award of the Science Academy.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>At each time step, our model predicts x t , conditioned on x t?1 and z t . Since our model has recurrence connections, it considers not only x t?1 and z t , but also x 1:t?2 and z 1:t?1 . Therefore, we can further write our inference as: log p ? (x|z) = log t p ? (x t |x 1:t?1 , z 1:t ) (10) = t log p ? (x t |x 1:t?1 , z 1:t )</p><p>Combining all of them leads to the following variational lower bound:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Derivation of the ELBO for SLAMP</head><p>In this section, we derive the variational lower bound for the proposed model with two posterior and two learned prior distributions.</p><p>We model the posterior distributions with two recurrent networks. The recurrent networks output two different posterior distributions, q ? p (z p t |x 1:t ) and q ? f (z f t |x 1:t ), at every time step. Due to the independence of the latent variables across time,</p><p>, we can derive the estimation of posterior distributions across time steps as follows:</p><p>Since the latent variables,</p><p>. . , z f T ], are independent across time and independent from each other, we can further decompose Kullback-Leibler terms in the evidence lower bound into individual time steps as in Eq. (9).  Best sample Random sample 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random sample 2</head><p>Random sample 3 <ref type="figure">Figure 24</ref>: Random Samples. We show the best sample and three random samples generated. All of the predictions are sharp-looking and different than each other, which proves that our model can generate diverse results. Best sample Random sample 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random sample 2</head><p>Random sample 3 <ref type="figure">Figure 25</ref>: Random Samples. We show the best sample and three random samples generated. The first and the third samples cannot preserve the shape of the digits, however, the best sample and the second sample are still sharp-looking. Note that predicted flow is from the target frame to the current frame since we apply inverse warping. The correctness of the optical flow estimation can be verified by checking <ref type="figure">Fig. 13</ref>. Our model focuses on the motion prediction for moving hands but it recovers the occluded motion boundaries from the appearance prediction.     We show the best sample and three random samples generated. Random samples look very similar in the beginning due to regular motion in the conditioning frames. Towards the end of the sequence, samples start looking different which shows that our model can generate diverse results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved conditional vrnns for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Annual Conference on Robot Learning</title>
		<meeting><address><addrLine>Mountain View, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-13" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cubic lstms for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>of the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic latent residual video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Delasalles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Disentangling propagation and generation for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal difference variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>Stochastic adversarial video prediction. arXiv.org</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flow-grounded spatialtemporal video prediction from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="600" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR</title>
		<meeting>of the International Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Flexible spatio-temporal networks for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
