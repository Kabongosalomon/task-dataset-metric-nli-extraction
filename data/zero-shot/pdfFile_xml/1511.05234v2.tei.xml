<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UMass Lowell</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@cs.uml.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UMass Lowell</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Spatial Attention</term>
					<term>Memory Net- work</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single "hop" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA <ref type="bibr" target="#b1">[2]</ref>, and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer <ref type="bibr" target="#b2">[3]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is an emerging interdisciplinary research problem at the intersection of computer vision, natural language processing and artificial intelligence. It has many real-life applications, such as automatic querying of surveillance video <ref type="bibr" target="#b3">[4]</ref> or assisting the visually impaired <ref type="bibr" target="#b4">[5]</ref>. Compared to the recently popular image captioning task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, VQA requires a deeper understanding of the image, but is considerably easier to evaluate. It also puts more focus on artificial intelligence, namely the inference process needed to produce the answer to the visual question.</p><p>What is the child standing on ? skateboard What color is the phone booth ? blue <ref type="figure">Fig. 1</ref>. We propose a Spatial Memory Network for VQA (SMem-VQA) that answers questions about images using spatial inference. The figure shows the inference process of our two-hop model on examples from the VQA dataset <ref type="bibr" target="#b1">[2]</ref>. In the first hop (middle), the attention process captures the correspondence between individual words in the question and image regions. High attention regions (bright areas) are marked with bounding boxes and the corresponding words are highlighted using the same color. In the second hop (right), the fine-grained evidence gathered in the first hop, as well as an embedding of the entire question, are used to collect more exact evidence to predict the answer. (Best viewed in color.)</p><p>In one of the early works <ref type="bibr" target="#b0">[1]</ref>, VQA is seen as a Turing test proxy. The authors propose an approach based on handcrafted features using a semantic parse of the question and scene analysis of the image combined in a latent-world Bayesian framework. More recently, several end-to-end deep neural networks that learn features directly from data have been applied to this problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Most of these are directly adapted from captioning models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, and utilize a recurrent LSTM network, which takes the question and Convolutional Neural Net (CNN) image features as input, and outputs the answer. Though the deep learning methods in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> have shown great improvement compared to the handcrafted feature method <ref type="bibr" target="#b0">[1]</ref>, they have their own drawbacks. These models based on the LSTM reading in both the question and the image features do not show a clear improvement compared to an LSTM reading in the question only <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Furthermore, the rather complicated LSTM models obtain similar or worse accuracy to a baseline model which concatenates CNN features and a bag-of-words question embedding to predict the answer, see the IMG+BOW model in <ref type="bibr" target="#b10">[11]</ref> and the iBOWIMG model in <ref type="bibr" target="#b2">[3]</ref>.</p><p>A major drawback of existing models is that they do not have any explicit notion of object position, and do not support the computation of intermediate results based on spatial attention. Our intuition is that answering visual questions often involves looking at different spatial regions and comparing their contents and/or locations. For example, to answer the questions in <ref type="figure">Fig. 1</ref>, we need to look at a portion of the image, such as the child or the phone booth.</p><p>Similarly, to answer the question "Is there a cat in the basket?" in <ref type="figure" target="#fig_0">Fig. 2</ref>, we can first find the basket and the cat objects, and then compare their locations.</p><p>We propose a new deep learning approach to VQA that incorporates explicit spatial attention, which we call the Spatial Memory Network VQA (SMem-VQA). Our approach is based on memory networks, which have recently been proposed for text Question Answering (QA) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Memory networks combine learned text embeddings with an attention mechanism and multi-step inference. The text QA memory network stores textual knowledge in its "memory" in the form of sentences, and selects relevant sentences to infer the answer. However, in VQA, the knowledge is in the form of an image, thus the memory and the question come from different modalities. We adapt the end-to-end memory network <ref type="bibr" target="#b12">[13]</ref> to solve visual question answering by storing the convolutional network outputs obtained from different receptive fields into the memory, which explicitly allows spatial attention over the image. We also propose to repeat the process of gathering evidence from attended regions, enabling the model to update the answer based on several attention steps, or "hops". The entire model is trained end-to-end and the evidence for the computed answer can be visualized using the attention weights.</p><p>To summarize our contributions, in this paper we propose a novel multi-hop memory network with spatial attention for the VQA task which allows one to visualize the spatial inference process used by the deep network (a CAFFE <ref type="bibr" target="#b13">[14]</ref> implementation will be made available), design an attention architecture in the first hop which uses each word embedding to capture fine-grained alignment between the image and question, create a series of synthetic questions that explicitly require spatial inference to analyze the working principles of the network, and show that it learns logical inference rules by visualizing the attention weights, provide an extensive evaluation of several existing models and our own model on the same publicly available datasets.</p><p>Sec. 2 introduces relevant work on memory networks and attention models. Sec. 3 describes our design of the multi-hop memory network architecture for visual question answering (SMem-VQA). Sec. 4 visualizes the inference rules learned by the network for synthetic spatial questions and shows the experimental results on DAQUAR <ref type="bibr" target="#b0">[1]</ref> and VQA <ref type="bibr" target="#b1">[2]</ref> datasets. Sec. 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Before the popularity of visual question answering (VQA), text question answering (QA) had already been established as a mature research problem in the area of natural language processing. Previous QA methods include searching for the key words of the question in a search engine <ref type="bibr" target="#b14">[15]</ref>; parsing the question as a knowledge base (KB) query <ref type="bibr" target="#b15">[16]</ref>; or embedding the question and using a similarity measurement to find evidence for the answer <ref type="bibr" target="#b16">[17]</ref>. Recently, memory networks were proposed for solving the QA problem. <ref type="bibr" target="#b11">[12]</ref> first introduces the memory network as a general model that consists of a memory and four components: input feature map, generalization, output feature map and response. The model is investigated in the context of question answering, where the long-term memory acts as a dynamic knowledge base and the output is a textual response. <ref type="bibr" target="#b12">[13]</ref> proposes a competitive memory network model that uses less supervision, called end-to-end memory network, which has a recurrent attention model over a large external memory. The Neural Turing Machine (NTM) <ref type="bibr" target="#b17">[18]</ref> couples a neural network to external memory and interacts with it by attentional processes to infer simple algorithms such as copying, sorting, and associative recall from input and output examples. In this paper, we solve the VQA problem using a multimodal memory network architecture that applies a spatial attention mechanism over an input image guided by an input text question.</p><p>The neural attention mechanism has been widely used in different areas of computer vision and natural language processing, see for example the attention models in image captioning <ref type="bibr" target="#b18">[19]</ref>, video description generation <ref type="bibr" target="#b19">[20]</ref>, machine translation <ref type="bibr" target="#b20">[21]</ref> <ref type="bibr" target="#b21">[22]</ref> and machine reading systems <ref type="bibr" target="#b22">[23]</ref>. Most methods use the soft attention mechanism first proposed in <ref type="bibr" target="#b20">[21]</ref>, which adds a layer to the network that predicts soft weights and uses them to compute a weighted combination of the items in memory. The two main types of soft attention mechanisms differ in the function that aligns the input feature vector and the candidate feature vectors in order to compute the soft attention weights. The first type uses an alignment function based on "concatenation" of the input and each candidate (we use the term "concatenation" as described <ref type="bibr" target="#b21">[22]</ref>), and the second type uses an alignment function based on the dot product of the input and each candidate. The "concatenation" alignment function adds one input vector (e.g. hidden state vector of the LSTM) to each candidate feature vector, embeds the resulting vectors into scalar values, and then applies the softmax function to generate the attention weight for each candidate. <ref type="bibr" target="#b18">[19]</ref>[20] <ref type="bibr" target="#b20">[21]</ref>[23] use the "concatenation" alignment function in their soft attention models and <ref type="bibr" target="#b23">[24]</ref> gives a literature review of such models applied to different tasks. On the other hand, the dot product alignment function first projects both inputs to a common vector embedding space, then takes the dot product of the two input vectors, and applies a softmax function to the resulting scalar value to produce the attention weight for each candidate. The end-to-end memory network <ref type="bibr" target="#b12">[13]</ref> uses the dot product alignment function. In <ref type="bibr" target="#b21">[22]</ref>, the authors compare these two alignment functions in an attention model for the neural machine translation task, and find that their implementation of the "concatenation" alignment function does not yield good performance on their task. Motivated by this, in this paper we use the dot product alignment function in our Spatial Memory Network.</p><p>VQA is related to image captioning. Several early papers about VQA directly adapt the image captioning models to solve the VQA problem [10] <ref type="bibr" target="#b10">[11]</ref> by generating the answer using a recurrent LSTM network conditioned on the CNN output. But these models' performance is still limited [10] <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b24">[25]</ref> proposes a new dataset and uses a similar attention model to that in image captioning <ref type="bibr" target="#b18">[19]</ref>, but does not give results on the more common VQA benchmark <ref type="bibr" target="#b1">[2]</ref>, and our own implementation of this model is less accurate on <ref type="bibr" target="#b1">[2]</ref> than other baseline models. <ref type="bibr" target="#b2">[3]</ref> summarizes several recent papers reporting results on the VQA dataset <ref type="bibr" target="#b1">[2]</ref> on arxiv.org and gives a simple but strong baseline model (iBOWIMG) on this dataset. This simple baseline concatenates the image features with the bag of word embedding question representation and feeds them into a softmax classifier to predict the answer. The iBOWIMG model beats most VQA models considered in the paper. Here, we compare our proposed model to the VQA models (namely, the ACK model <ref type="bibr" target="#b25">[26]</ref> and the DPPnet model <ref type="bibr" target="#b26">[27]</ref>) which have comparable or better results than the iBOWIMG model. The ACK model in <ref type="bibr" target="#b25">[26]</ref> is essentially the same as the LSTM model in <ref type="bibr" target="#b10">[11]</ref>, except that it uses image attribute features, the generated image caption and relevant external knowledge from a knowledge base as the input to the LSTM's first time step. The DPPnet model in <ref type="bibr" target="#b26">[27]</ref> tackles VQA by learning a convolutional neural network (CNN) with some parameters predicted from a separate parameter prediction network. Their parameter prediction network uses a Gate Recurrent Unit (GRU) to generate a question representation, and based on this question input, maps the predicted weights to CNN via hashing. Neither of these models <ref type="bibr" target="#b25">[26]</ref>[27] contain a spatial attention mechanism, and they both use external data in addition to the VQA dataset <ref type="bibr" target="#b1">[2]</ref>, e.g. the knowledge base in <ref type="bibr" target="#b25">[26]</ref> and the large-scale text corpus used to pre-train the GRU question representation <ref type="bibr" target="#b26">[27]</ref>. In this paper, we explore a complementary approach of spatial attention to both improve performance and visualize the network's inference process, and obtain improved results without using external data compared to the iBOWIMG model <ref type="bibr" target="#b2">[3]</ref> as well as the ACK model <ref type="bibr" target="#b25">[26]</ref> and the DPPnet model <ref type="bibr" target="#b26">[27]</ref> which use external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spatial Memory Network for VQA</head><p>We first give an overview of the proposed SMem-VQA network, illustrated in The input to our network is a question comprised of a variable-length sequence of words, and an image of fixed size. Each word in the question is first represented as a one-hot vector in the size of the vocabulary, with a value of one only in the corresponding word position and zeros in the other positions. Each one-hot vector is then embedded into a real-valued word vector,</p><formula xml:id="formula_0">V = {v j | v j ? R N ; j = 1, ? ? ? , T },</formula><p>where T is the maximum number of words in the question and N is the dimensionality of the embedding space. Sentences with length less than T are padded with special ?1 value, which are embedded to all-zero word vector.</p><p>The words in questions are used to compute attention over the visual memory, which contains extracted image features. The input image is processed by a convolutional neural network (CNN) to extract high-level M -dimensional visual features on a grid of spatial locations. Specifically, we use S = {s i | s i ? R M ; i = 1, ? ? ? , L} to represent the spatial CNN features at each of the L grid locations. In this paper, the spatial feature outputs of the last convolutional layer of GoogLeNet (inception 5b/output) <ref type="bibr" target="#b27">[28]</ref> are used as the visual features for the image. The resulting spatial attention weights Watt are then used to compute a weighted sum over the visual features embedded via a separate "evidence" transformation WE, e.g., selecting evidence for the cat concept at the basket location. Finally, the weighted evidence vector Satt is combined with the full question embedding Q to predict the answer. An additional hop can repeat the process to gather more evidence (Sec. 3.2).</p><p>The convolutional image feature vectors at each location are embedded into a common semantic space with the word vectors. Two different embeddings are used: the "attention" embedding W A and the "evidence" embedding W E . The attention embedding projects each visual feature vector such that its combination with the embedded question words generates the attention weight at that location. The evidence embedding detects the presence of semantic concepts or objects, and the embedding results are multiplied with attention weights and summed over all locations to generate the visual evidence vector S att .</p><p>Finally, the visual evidence vector is combined with the question representation and used to predict the answer for the given image and question. In the next section, we describe the one-hop Spatial Memory network model and the specific attention mechanism it uses in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Guided Spatial Attention in One-Hop Model</head><p>Rather than using the bag-of-words question representation to guide attention, the attention architecture in the first hop ( <ref type="figure" target="#fig_0">Fig. 2(b)</ref>) uses each word vector separately to extract correlated visual features in memory. The intuition is that the BOW representation may be too coarse, and letting each word select a related region may provide more fine-grained attention. The correlation matrix C ? R T ?L between word vectors V and visual features S is computed as</p><formula xml:id="formula_1">C = V ? (S ? W A + b A ) T<label>(1)</label></formula><p>where W A ? R M ?N contains the attention embedding weights of visual features S, and b A ? R L?N is the bias term. This correlation matrix is the dot product result of each word embedding and each spatial location's visual feature, thus each value in correlation matrix C measures the similarity between each word and each location's visual feature. The spatial attention weights W att are calculated by taking maximum over the word dimension T for the correlation matrix C, selecting the highest correlation value for each spatial location, and then applying the softmax function</p><formula xml:id="formula_2">W att = softmax( max i=1,??? ,T (C i )), C i ? R L<label>(2)</label></formula><p>The resulting attention weights W att ? R L are high for selected locations and low for other locations, with the sum of weights equal to 1. For instance, in the example shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the question "Is there a cat in the basket?" produces high attention weights for the location of the basket because of the high correlation of the word vector for basket with the visual features at that location. The evidence embedding W E projects visual features S to produce high activations for certain semantic concepts. E.g., in <ref type="figure" target="#fig_0">Fig. 2</ref>, it has high activations in the region containing the cat. The results of this evidence embedding are then multiplied by the generated attention weights W att , and summed to produce the selected visual "evidence" vector S att ? R N ,</p><formula xml:id="formula_3">S att = W att ? (S ? W E + b E )<label>(3)</label></formula><p>where W E ? R M ?N are the evidence embedding weights of the visual features S, and b E ? R L?N is the bias term. In our running example, this step accumulates cat presence features at the basket location. Finally, the sum of this evidence vector S att and the question embedding Q is used to predict the answer for the given image and question. For the question representation Q, we choose the bag-of-words (BOW). Other question representations, such as an LSTM, can also be used, however, BOW has fewer parameters yet has shown good performance. As noted in <ref type="bibr" target="#b28">[29]</ref>, the simple BOW model performs roughly as well if not better than the sequence-based LSTM for the VQA task. Specifically, we compute</p><formula xml:id="formula_4">Q = W Q ? V + b Q<label>(4)</label></formula><p>where W Q ? R T represents the BOW weights for word vectors V , and b Q ? R N is the bias term. The final prediction P is</p><formula xml:id="formula_5">P = softmax(W P ? f (S att + Q) + b P )<label>(5)</label></formula><p>where W P ? R K?N , bias term b P ? R K , and K represents the number of possible prediction answers. f is the activation function, and we use ReLU here. In our running example, this step adds the evidence gathered for cat near the basket location to the question, and, since the cat was not found, predicts the answer "no". The attention and evidence computation steps can be optionally repeated in another hop, before predicting the final answer, as detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Attention in Two-Hop Model</head><p>We can repeat hops to promote deeper inference, gathering additional evidence at each hop. Recall that the visual evidence vector S att is added to the question representation Q in the first hop to produce an updated question vector,</p><formula xml:id="formula_6">O hop1 = S att + Q<label>(6)</label></formula><p>On the next hop, this vector O hop1 ? R N is used in place of the individual word vectors V to extract additional correlated visual features to the whole question from memory and update the visual evidence.</p><p>The correlation matrix C in the first hop provides fine-grained local evidence from each word vectors V in the question, while the correlation vector C hop2 in next hop considers the global evidence from the whole question representation Q. The correlation vector C hop2 ? R L in the second hop is calculated by</p><formula xml:id="formula_7">C hop2 = (S ? W E + b E ) ? O hop1<label>(7)</label></formula><p>where W E ? R M ?N should be the attention embedding weights of visual features S in the second hop and b E ? R L?N should be the bias term. Since the attention embedding weights in the second hop are shared with the evidence embedding in the first hop, so we directly use W E and b E from first hop here. The attention weights in the second hop W att2 are obtained by applying the softmax function to the correlation vector C hop2 .</p><formula xml:id="formula_8">W att2 = softmax(C hop2 )<label>(8)</label></formula><p>Then, the correlated visual information in the second hop S att2 ? R N is extracted using attention weights W att2 .</p><formula xml:id="formula_9">S att2 = W att2 ? (S ? W E2 + b E2 )<label>(9)</label></formula><p>where W E2 ? R M ?N are the evidence embedding weights of visual features S in the second hop, and b E2 ? R L?N is the bias term. The final answer P is predicted by combining the whole question representation Q, the local visual evidence S att from each word vector in the first hop and the global visual evidence S att2 from the whole question in the second hop,</p><formula xml:id="formula_10">P = softmax(W P ? f (O hop1 + S att2 ) + b P )<label>(10)</label></formula><p>where W P ? R K?N , bias term b P ? R K , and K represents the number of possible prediction answers. f is activation function. More hops can be added in this manner. The entire network is differentiable and is trained using stochastic gradient descent via standard backpropagation, allowing image feature extraction, image embedding, word embedding and answer prediction to be jointly optimized on the training image/question/answer triples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct a series of experiments to evaluate our model. To explore whether the model learns to perform the spatial inference necessary for answering visual questions that explicitly require spatial reasoning, we design a set of experiments using synthetic visual question/answer data in Sec. 4.1. The experimental results of our model in standard datasets (DAQUAR <ref type="bibr" target="#b0">[1]</ref> and VQA <ref type="bibr" target="#b1">[2]</ref> datasets) are reported in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exploring Attention on Synthetic Data</head><p>The questions in the public VQA datasets are quite varied and difficult and often require common sense knowledge to answer (e.g., "Does this man have 20/20 vision?" about a person wearing glasses). Furthermore, past work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> showed that the question text alone (no image) is a very strong predictor of the answer. Therefore, before evaluating on standard datasets, we would first like to understand how the proposed model uses spatial attention to answer simple visual questions where the answer cannot be predicted from question alone. Our visualization demonstrates that the attention mechanism does learn to attend to objects and gather evidence via certain inference rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Absolute Position Recognition</head><p>We investigate whether the model has the ability to recognize the absolute location of the object in the image. We explore this by designing a simple task where an object (a red square) appears in some region of a white-background image, and the question is "Is there a red square on the [top|bottom|left|right]?" For each image, we randomly place the square in one of the four regions, and generate the four questions above, together with three "no" answers and one "yes" answer. The generated data is split into training and testing sets.</p><p>Due to the simplicity of this synthetic dataset, the SMem-VQA one-hop model achieves 100% test accuracy. However, the baseline model (iBOWIMG) <ref type="bibr" target="#b2">[3]</ref>  cannot infer the answer and only obtains accuracy of around 75%, which is the prior probability of the answer "no" in the training set. The SMem-VQA one-hop model is equivalent to the iBOWIMG model if the attention weights in our onehop model are set equally for each location, since the iBOWIMG model uses the mean pool of the convolutional feature (inception 5b/output) in GoogLeNet that we use in SMem-VQA model. We check the visualization of the attention weights and find that the relationship between the high attention position and the answer can be expressed by logical expressions. We show the attention weights of several typical examples in <ref type="figure" target="#fig_2">Fig. 3</ref> which reflect two logic rules: 1) Look at the position specified in question (top|bottom|right|left), if it contains a square, then answer "yes"; if it does not contain a square, then answer "no". 2) Look at the region where there is a square, then answer "yes" for the question about that position and "no" for the questions about the other three positions.</p><p>In the iBOWIMG model, the mean-pooled GoogLeNet visual features lose spatial information and thus cannot distinguish images with a square in different positions. On the contrary, our SMem-VQA model can pay high attention to different regions according to the question, and generate an answer based on the selected region, using some learned inference rules. This experiment demonstrates that the attention mechanism in our model is able to make absolute spatial location inference based on the spatial attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Position Recognition</head><p>In order to check whether the model has the ability to infer the position of one object relative to another object, we collect all the cat images from the MS COCO Detection dataset <ref type="bibr" target="#b29">[30]</ref>, and add a red square on the [top|bottom|left|right] of the bounding box of the cat in the images. For each generated image, we create four questions, "Is there a red square on the [top|bottom|left|right] of the cat?" together with three "no" answers and one "yes" answer. We select 2639 training cat images and 1395 testing cat images from MS COCO Detection dataset.</p><p>Our SMem-VQA one-hop model achieves 96% test accuracy on this synthetic task, while the baseline model (iBOWIMG) accuracy is around 75%. We also check that another simple baseline that predicts the answer based on the abso- <ref type="table">Table 1</ref>. Accuracy results on the DAQUAR dataset (in percentage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAQUAR</head><p>Multi-World <ref type="bibr" target="#b0">[1]</ref> 12.73 Neural-Image-QA <ref type="bibr" target="#b9">[10]</ref> 29.27 Question LSTM <ref type="bibr" target="#b9">[10]</ref> 32.32 VIS+LSTM <ref type="bibr" target="#b10">[11]</ref> 34.41 Question BOW <ref type="bibr" target="#b10">[11]</ref> 32.67 IMG+BOW <ref type="bibr" target="#b10">[11]</ref> 34.17 SMem-VQA One-Hop 36.03 SMem-VQA Two-Hop 40.07 lute position of the square in the image gets around 70% accuracy. We visualize the evidence embedding W E features and the attention weights W att of several typical examples in <ref type="figure" target="#fig_3">Fig. 4</ref>. The evidence embedding W E has high activations on the cat and the red square, while the attention weights pay high attention to certain locations around the cat. We can analyze the attention in the correctly predicted examples using the same rules as in absolute position recognition experiment. These rules still work, but the position is relative to the cat object: 1) Check the specified position relative to the cat, if it finds the square, then answer "yes", otherwise "no"; 2) Find the square, then answer "yes" for the specified position, and answer "no" for the other positions around the cat. We also check the images where our model makes mistakes, and find that the mistakes mainly occur in images with more than one cats. The red square appears near only one of the cats in the image, but our model might make mistakes by focusing on the other cats. We conclude that our SMem-VQA model can infer the relative spatial position based on the spatial attention around the specified object, which can also be represented by some logical inference rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Standard Datasets</head><p>Results on DAQUAR The DAQUAR dataset is a relatively small dataset which builds on the NYU Depth Dataset V2 <ref type="bibr" target="#b30">[31]</ref>. We use the reduced DAQUAR dataset <ref type="bibr" target="#b0">[1]</ref>. The evaluation metric for this dataset is 0-1 accuracy. The embedding dimension is 512 for our models running on the DAQUAR dataset. We use several reported models on DAQUAR as baselines, which are listed below:</p><p>? Multi-World <ref type="bibr" target="#b0">[1]</ref>: an approach based on handcrafted features using a semantic parse of the question and scene analysis of the image combined in a latent-world Bayesian framework.</p><p>? Neural-Image-QA <ref type="bibr" target="#b9">[10]</ref>: uses an LSTM to encode the question and then decode the hidden information into the answer. The image CNN feature vector is shown at each time step of the encoding phase.</p><p>? Question LSTM <ref type="bibr" target="#b9">[10]</ref>: only shows the question to the LSTM to predict the answer without any image information.</p><p>? VIS+LSTM <ref type="bibr" target="#b10">[11]</ref>: similar to Neural-Image-QA, but only shows the image features to the LSTM at the first time step, and the question in the remaining time steps to predict the answer. ? Question BOW <ref type="bibr" target="#b10">[11]</ref>: only uses the BOW question representation and a single hidden layer neural network to predict the answer, without any image features.</p><p>? IMG+BOW <ref type="bibr" target="#b10">[11]</ref>: concatenates the BOW question representation with image features, and then uses a single hidden layer neural network to predict the answer. This model is similar to the iBOWIMG baseline model in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Results of our SMem-VQA model on the DAQUAR dataset and the baseline model results reported in previous work are shown in Tab. 1. From the DAQUAR result in Tab. 1, we see that models based on deep features significantly outperform the Multi-World approach based on hand-crafted features. Modeling the question only with either the LSTM model or Question BOW model does equally well in comparison, indicating the the question text contains important prior information for predicting the answer. Also, on this dataset, the VIS+LSTM model achieves better accuracy than Neural-Image-QA model; the former shows the image only at the first timestep of the LSTM, while the latter does so at each timestep. In comparison, both our One-Hop model and Two-Hop spatial attention models outperform the IMG+BOW, as well as the other baseline models. A major advantage of our model is the ability to visualize the inference process in the deep network. To illustrate this, two attention weights visualization examples in SMem-VQA One-Hop and Two-Hop models on DAQUAR dataset are shown in <ref type="figure" target="#fig_4">Fig. 5 (bottom row)</ref>.</p><p>Results on VQA The VQA dataset is a recent large dataset based on MS COCO <ref type="bibr" target="#b29">[30]</ref>. We use the full release (V1.0) open-ended dataset, which contains a train set and a val set. Following standard practice, we choose the top 1000 answers in train and val sets as possible prediction answers, and only keep the examples whose answers belong to these 1000 answers as training data. The question vocabulary size is 7477 with the word frequency of at least three. Because of the larger training size, the embedding dimension is 1000 on the VQA dataset. We report the test-dev and test-standard results from the VQA evaluation server. The server evaluation uses the evaluation met- ric introduced by <ref type="bibr" target="#b1">[2]</ref>, which gives partial credit to certain synonym answers: Acc(ans) = min {(# humans that said ans)/3, 1}.</p><p>For the attention models, we do not mirror the input image when using the CNN to extract convolutional features, since this might cause confusion about the spatial locations of objects in the input image. The optimization algorithm used is stochastic gradient descent (SGD) with a minibatch of size 50 and momentum of 0.9. The base learning rate is set to be 0.01 which is halved every six epoches. Regularization, dropout and L2 norm are cross-validated and used.</p><p>For the VQA dataset, we use the simple iBOWIMG model in <ref type="bibr" target="#b2">[3]</ref> as one baseline model, which beats most existing VQA models currently on arxiv.org. We also compare to two models in <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b26">[27]</ref> which have comparable or better results to the iBOWIMG model. These three baseline models as well the best model in VQA dataset paper <ref type="bibr" target="#b1">[2]</ref> are listed in the following: ? LSTM Q+I <ref type="bibr" target="#b1">[2]</ref>: uses the element-wise multiplication of the LSTM encoding of the question and the image feature vector to predict the answer. This is the best model in the VQA dataset paper.</p><p>? ACK <ref type="bibr" target="#b25">[26]</ref>: shows the image attribute features, the generated image caption and relevant external knowledge from knowledge base to the LSTM at the first time step, and the question in the remaining time steps to predict the answer.</p><p>? DPPnet <ref type="bibr" target="#b26">[27]</ref>: uses the Gated Recurrent Unit (GRU) representation of question to predict certain parameters for a CNN classification network. They pre-train the GRU for question representation on a large-scale text corpus to improve the GRU generalization performance.</p><p>? iBOWIMG <ref type="bibr" target="#b2">[3]</ref>: concatenates the BOW question representation with image feature (GoogLeNet), and uses a softmax classification to predict the answer.</p><p>The overall accuracy and per-answer category accuracy for our SMem-VQA models and the four baseline models on VQA dataset are shown in Tab. 2. From the table, we can see that the SMem-VQA One-Hop model obtains slightly better results compared to the iBOWIMG model. However, the SMem-VQA Two-Hop model achieves an improvement of 2.27% on test-dev and 2.35% on test-standard compared to the iBOWIMG model, demonstrating the value of spatial attention. The SMem-VQA Two-Hop model also shows best performance in the per-answer category accuracy. The SMem-VQA Two-Hop model has slightly better result than the DPPnet model. The DPPnet model uses a large-scale text corpus to pre-train the Gated Recurrent Unit (GRU) network for question representation. Similar pre-training work on extra data to improve model accuracy has been done in <ref type="bibr" target="#b31">[32]</ref>. Considering the fact that our model does not use extra data to pretrain the word embeddings, its results are very competitive. We also experiment with adding a third hop into our model on the VQA dataset, but the result does not improve further.</p><p>The attention weights visualization examples for the SMem-VQA One-Hop and Two-Hop models on the VQA dataset are shown in <ref type="figure" target="#fig_4">Fig. 5 (top row)</ref>. From the visualization, we can see that the two-hop model collects supplementary evidence for inferring the answer, which may be necessary to achieve an improvement on these complicated real-world datasets. We also visualize the finegrained alignment in the first hop of our SMem-VQA Two-Hop model in <ref type="figure" target="#fig_5">Fig. 6</ref>. The correlation vector values (blue bars) measure the correlation between image regions and each word vector in the question. Higher values indicate stronger correlation of that particular word with the specific location's image features. We observe that the fine-grained visual evidence collected using each local word vector, together with the global visual evidence from the whole question, complement each other to infer the correct answer for the given image and question, as shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed the Spatial Memory Network for VQA, a memory network architecture with a spatial attention mechanism adapted to the visual question answering task. We proposed a set of synthetic spatial questions and demonstrated that our model learns inference rules based on spatial attention through attention weight visualization. Evaluation on the challenging DAQUAR and VQA datasets showed improved results over previously published models. Our model can be used to visualize the inference steps learned by the deep network, giving some insight into its processing. Future work may include further exploring the inference ability of our SMem-VQA model and exploring other VQA attention models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 (</head><label>2</label><figDesc>a). Sec. 3.1 details the word-guided spatial attention process of the first hop shown inFig. 2 (b), and Sec. 3.2 describes adding a second hop into SMem-VQA network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Our proposed Spatial Memory Network for Visual Question Answering (SMem-VQA). (a) Overview. First, the CNN activation vectors S = {si} at image locations i are projected into the semantic space of the question word vectors vj using the "attention" visual embedding WA (Sec. 3). The results are then used to infer spatial attention weights Watt using the word-guided attention process shown in (b). (b) Word-guided attention. This process predicts attention determined by the question word that has the maximum correlation with embedded visual features at each location, e.g. choosing the word basket to attend to the location of the basket in the above image (Sec. 3.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Absolute position experiment: for each image and question pair, we show the original image (left) and the attention weights Watt (right). The attention follows the following rules. The first rule (top row) looks at the position specified in question (top|bottom|right|left), if it contains a square, answer "yes"; otherwise answer "no". The second rule (bottom row) looks at the region where there is a square, and answers "yes" if the question contains that position and "no" for the other three positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Relative position experiment: for each image and question pair, we show the original image (left), the evidence embedding WE of the convolutional layer (middle) and the attention weights Watt (right). The evidence embedding WE has high activations on both cat and red square. The attention weights follow similar inference rules as inFig. 3, with the difference that the attention position is around the cat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of the spatial attention weights in the SMem-VQA One-Hop and Two-Hop models on VQA (top row) and DAQUAR (bottom row) datasets. For each image and question pair, we show the original image, the attention weights Watt of the One-Hop model, and the two attention weights Watt and Watt2 of the Two-Hop model in order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of the original image (left), the spatial attention weights Watt in the first hop (middle) and one correlation vector from the correlation matrix C for the location with highest attention weight in the SMem-VQA Two-Hop model on the VQA dataset. Higher values in the correlation vector indicate stronger correlation of that word with the chosen location's image features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Test-dev and test-standard results on the Open-Ended VQA dataset (in percentage). Models with * use external training data in addition to the VQA dataset.<ref type="bibr" target="#b25">[26]</ref> 55.72 79.<ref type="bibr" target="#b22">23</ref> 36.13 40.08 55.98 79.05 36.10 40.61 DPPnet * [27] 57.22 80.71 37.24 41.69 57.36 80.28 36.92 42.Hop 57.99 80.87 37.32 43.12 58.24 80.8 37.53 43.48</figDesc><table><row><cell></cell><cell>test-dev</cell><cell></cell><cell cols="2">test-standard</cell><cell></cell></row><row><cell></cell><cell cols="5">Overall yes/no number others Overall yes/no number others</cell></row><row><cell>LSTM Q+I [2]</cell><cell>53.74 78.94 35.24 36.42</cell><cell>54.06</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">ACK  24</cell></row><row><cell>iBOWIMG [3]</cell><cell>55.72 76.55 35.03 42.62</cell><cell cols="4">55.89 76.76 34.98 42.62</cell></row><row><cell>SMem-VQA One-Hop</cell><cell>56.56 78.98 35.93 42.09</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SMem-VQA Two-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno>abs/1410.0210</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1505.00468</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint video and text parsing for understanding events and answering queries. MultiMedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="70" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Increasing the bandwidth of crowdsourced visual question answering to better support blind users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international ACM SIGACCESS conference on Computers &amp; accessibility</title>
		<meeting>the 16th international ACM SIGACCESS conference on Computers &amp; accessibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<title level="m">Long-term recurrent convolutional networks for visual recognition and description</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4952</idno>
		<title level="m">From captions to visual concepts and back</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01121</idno>
		<title level="m">Ask your neurons: A neural-based approach to answering questions about images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural language questions for the web of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
		<title level="m">Question answering with subgraph embeddings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Describing multimedia content using attentionbased encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03416</idno>
		<title level="m">Visual7w: Grounded question answering in images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ask me anything: Freeform visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06973</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05756</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07394</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
