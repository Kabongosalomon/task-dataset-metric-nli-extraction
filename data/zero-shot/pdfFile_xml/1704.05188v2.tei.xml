<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Self-Taught Learning for Weakly Supervised Object Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>wliu@ee.columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Self-Taught Learning for Weakly Supervised Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing weakly supervised localization (WSL) approaches learn detectors by finding positive bounding boxes based on features learned with image-level supervision. However, those features do not contain spatial location related information and usually provide poor-quality positive samples for training a detector. To overcome this issue, we propose a deep self-taught learning approach, which makes the detector learn the object-level features reliable for acquiring tight positive samples and afterwards re-train itself based on them. Consequently, the detector progressively improves its detection ability and localizes more informative positive samples. To implement such self-taught learning, we propose a seed sample acquisition method via image-to-object transferring and dense subgraph discovery to find reliable positive samples for initializing the detector. An online supportive sample harvesting scheme is further proposed to dynamically select the most confident tight positive samples and train the detector in a mutual boosting way. To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process. Extensive experiments on PASCAL 2007 and 2012 show that our approach outperforms the state-ofthe-arts, strongly validating its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weakly Supervised Localization (WSL) refers to learning to localize objects within images with only image-level annotations that simply indicate the presence of an object category. WSL is gaining increasing importance in largescale vision applications because it does not require expensive bounding box annotations like its fully-supervised counterpart <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> in the model training phase.</p><p>WSL is a challenging problem due to the insufficiency of information for learning a good detector. Correctly identifying the reliable positive samples (bounding boxes) from a collection of candidates is thus of critical importance. Most <ref type="figure">Figure 1</ref>: An illustration of deep self-taught learning for weakly supervised object localization. Given image-level supervision, seed positive proposals are first obtained as initial positive samples for a CNN detector. The CNN detector is then trained with self-taught learning which alternates between training and online supportive sample harvesting relying on the relative improvement of CNN scores predicted by the detector. previous WSL methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> discover high-confident positive samples from the images with positive annotations by applying multiple instance learning (MIL) or other similar algorithms. Recent WSL methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9]</ref> also combine deep convolutional neural network (CNN) models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> with MIL, considering that CNN architectures can provide more powerful image representations. However, the representation provided by a CNN tailored to classification does not contain any specific information about object spatial locations and is thus not suitable for object-level localization tasks, leading to marginal benefits for learning a high-quality object detector.</p><p>Moreover, such methods only perform off-line MIL to mine confident class-specific object proposals before train-ing the detector, where the strong discriminating power of the learned object-level CNN detector is not fully leveraged to mine high-quality proposals for detector learning.</p><p>In this paper, we propose to make a weak detector "train" itself through exploiting a novel deep self-taught learning approach such that it progressively gains a stronger ability for object detection and solves the WSL problem, as illustrated in <ref type="figure">Fig. 1</ref>. This is a new WSL paradigm and can address the above issues of the existing methods.</p><p>Given several seed positive proposals, self-taught learning enables the detector to spontaneously harvest the most confident tight positive proposals (called supportive samples) in an online manner, through examining their predicted scores from the detector itself. By fully exploiting the strong discriminating ability of the regional CNN detector (e.g., Fast R-CNN <ref type="bibr" target="#b2">[3]</ref>), supportive samples of higher quality can be identified, compared with the ones provided by the conventional CNN plus MIL approaches. However, one key problem with the above online supportive sample harvesting strategy for self-taught learning is that some poor seed positive samples may be easily fitted by the CNN detector due to its strong learning ability and hence trap the CNN detector in poor local optima. To address this critical problem pertaining to self-taught learning, we propose a novel relative improvement metric for facilitating supportive sample harvesting. The relative improvement of scores can effectively filter those suspicious samples whose high predicted scores are from undesired overfitting, thereby helping identify authentic samples of high-quality.</p><p>The very first step of the above self-taught learning process is to acquire high-quality seed positive samples. We propose an image-to-object transferring scheme to find reliable seed positive samples. Concretely, we first select the object proposals with high responses 1 to the target class obtained by training a multi-label classification network. Selecting samples in this way roughly establishes a correspondence between image-level annotations and objectlevel high-response proposals. Then we propose to employ a dense subgraph discovery method to select a few dense spatially distributed proposals as the seed positive samples, by exploiting the spatial correlations for selected proposals as above. Comprehensive experiments demonstrate the effectiveness of our proposed approach for acquiring reliable seed samples, and the obtained seed samples are indeed beneficial for the following self-taught learning procedure to tackle WSL problems.</p><p>To sum up, we make the following contributions to WSL in this work:</p><p>1. We propose a novel deep self-taught learning approach to progressively harvest high-quality positive samples guided by the detector itself, therefore significantly improving the quality of positive samples during detector training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A novel relative score improvement based selection strategy is proposed to prevent the detector from being trapped in poor local optima resulting from the overfitting to seed positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>To acquire high-quality seed positives, we propose a novel image-to-object transferring technique to learn the spatial-aware features tailored to WSL. To further incorporate the spatial correlations between the selected object samples, a novel dense subgraph discovery based method is proposed to mine the most confident class-specific samples from a set of spatially highly correlated candidate samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous works on WSL can be roughly categorized into MIL based methods and end-to-end CNN models.</p><p>Actually, the majority of existing methods formulate WSL as an MIL problem. Given weak image-level supervisory information, these methods typically alternate between learning a discriminative representation of the object and selecting the positive object samples in positive images based on this representation. However, this results in a nonconvex optimization problem, so these methods are prone to being trapped in local optima, and their solutions are sensitive to the initial positive samples. Many efforts have been made to address the above issue. Deselaers et al. <ref type="bibr" target="#b17">[18]</ref> initialized object locations using the objectness method <ref type="bibr" target="#b18">[19]</ref>. Siva et al. <ref type="bibr" target="#b19">[20]</ref> selected positive samples by maximizing the distances between the positive samples and those in negative images. Bilen et al. <ref type="bibr" target="#b6">[7]</ref> proposed a smoothed version of MIL that softly labels object proposals instead of choosing the highest scoring ones. Song et al. <ref type="bibr" target="#b20">[21]</ref> proposed a graphbased method to initialize the object locations by solving a submodular cover problem. Wang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a latent semantic clustering method to select the most discriminative cluster for each class based on Probability Latent Semantic Analysis (pLSA).</p><p>Apart from improving the initial quality of positive samples, some work also focuses on improving optimization during iterative training. Singh et al. <ref type="bibr" target="#b22">[23]</ref> iteratively trained SVM classifiers on a subset of the initial positive samples, and evaluated them on another set to update the training samples. Bilen et al. <ref type="bibr" target="#b6">[7]</ref> proposed a posterior regularization formulation that regularizes the latent (object location) space by penalizing unlikely configurations based on symmetry and mutual exclusion of objects. Cinbis et al. <ref type="bibr" target="#b7">[8]</ref> proposed a multi-fold training strategy to alleviate the local optimum issue.</p><p>End-to-end CNN models are also used for WSL. Bilen et al. <ref type="bibr" target="#b23">[24]</ref> proposed an end-to-end CNN model with two streams, one for classification and the other for localization, which outputs final scores for the proposals by the elementwise multiplication on the results of the two streams. Kantorov et al. <ref type="bibr" target="#b24">[25]</ref> proposed a context-aware CNN model trained with contrast-based contextual guidance, resulting in refined boundaries of detected objects.</p><p>Perhaps <ref type="bibr" target="#b8">[9]</ref> is the closest work to ours. <ref type="bibr" target="#b8">[9]</ref> first trains a whole-image multi-label classification network and then selects confident class-specific proposals with a mask-out strategy and MIL. Finally, a Fast R-CNN detector is trained on these proposals. However, the whole-image classification in <ref type="bibr" target="#b8">[9]</ref> may not provide suitable features for object localization which requires tight spatial coverage of the whole object instance. Additionally, SVM is used in MIL in <ref type="bibr" target="#b8">[9]</ref>, which has the inferior discriminating ability to the regional CNN detector. In contrast, our approach overcomes this weakness by performing image-to-object transferring during multi-label image classification and online supportive sample harvesting in regional CNN detector learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Self-Taught Learning for WSL</head><p>In this section, the proposed deep self-taught learning approach for WSL will be detailed. We first describe the image-to-object transferring and dense subgraph discovery based methods used to acquire high-quality seed positive samples for detector self-taught learning. Then, online supportive sample harvesting is presented, which progressively improves the quality of the positive samples, where the detector dynamically harvests the most informative positive samples during learning, guided by the relative CNN score improvement from the detector itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Seed Sample Acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Image-to-Object Transfer</head><p>We propose an image-to-object transferring approach to identify reliable seed samples with highest class-specific likelihood, given only image-level annotations. Considering that each positive image contains at least one positive object proposal that contributes significantly to each class, we train a multi-label classification CNN model as the first step to identify seed samples. We follow the method Hypothesis-CNN-Pooling (HCP) <ref type="bibr" target="#b25">[26]</ref> in multi-label classification to mine the proposals which contribute most to image-level classification. Specifically, HCP accepts a number of input proposals and feeds them into the CNN classification network. Then cross-proposal max-pooling is performed in the integrative prediction stage for each class.</p><p>More formally, assume that {v i } n i=1 is the output response vector of the i-th proposal from the CNN, and that {v j i } c j=1 is the output response of the j-th class in v i . The final integrative prediction for an image on the j-th class is With cross-proposal max-pooling, the highest predicted response corresponding to the object of the target class will be reserved, while the responses from the negative objects will be ignored. In this way, the image-level classification error will only be back-propagated through the most confident proposal such that the network achieves spatialawareness during training. This fills the gap between the image-level annotation and the object-level features, thus providing more discriminative features for the object-level detection task. More details of HCP can be found in <ref type="bibr" target="#b25">[26]</ref>.</p><formula xml:id="formula_0">v j = max(v j 1 , v j 2 , . . . , v j n ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Reliable Seed Proposal Generation</head><p>After image-to-object transferring, the top N proposals with the highest predicted responses to the target class are selected as confident candidate proposals. However, highresponse does not imply tight spatial coverage of the true object. Our experimental observation demonstrates that the proposals with some context or containing only the key discriminative part also have high responses to the target class in the above image-to-object transferring. Another key observation is that although some proposals contain part of the object or context, they may crowd the object (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>To incorporate the spatial correlation, we formulate it as a dense subgraph discovery (DSD) problem, i.e., selecting the most spatially concentrated ones in the candidate proposal pool that contains the N high-response proposals. Mathematically, let G = (V, E) be an undirected unweighted graph whose nodes V correspond to the top N <ref type="figure">Figure 3</ref>: An illustration of graph G whose nodes are the proposals in the N -candidate proposal pool. Each candidate proposal is connected to the others with IoU ? 0.5 in this example. By dense subgraph discovery, two spatially concentrated proposals are selected among all the proposals, framed in red boxes.</p><p>high-response proposals. The edges E = {e(v i , v j )} are formed by connecting each proposal (node) to its neighboring proposals which have Intersection-over-Union (IoU) larger than a pre-defined threshold T . The visualization of an example graph G is shown in <ref type="figure">Fig. 3</ref>. We propose a greedy algorithm to discover the dense subgraph of G. The greedy algorithm iteratively selects the node with a greatest degree (number of connections to other nodes) and then prunes the node as well as all its connected neighbors. The algorithm repeats the finding-pruning iterations until the number of the remained nodes is less than a pre-defined number k. All the pruned nodes in the iterations form the dense subgraph. The procedure is detailed in Algorithm 1.</p><formula xml:id="formula_1">Algorithm 1 Dense Subgraph Discovery over Graph G Input: An undirected graph G = (V, E). Initialization: V = ?. while |V |&gt;k do v max = arg max i d i , where d i = j?V e(v i , v j ); V neighbor = {v|e(v, v max ) = 1}; V = V ? {v max }; V = V \V neighbor ; end while Output: A set of nodes V constituting the dense sub- graph.</formula><p>Compared to other two ways of selecting spatially concentrated proposals, i.e., clustering and non-maximal suppression (NMS), DSD has the following appealing advantages. First, it can provide an adaptive number of proposals instead of requiring a pre-specified fixed number as clustering. This is highly desired in solving the WSL problem as images may have different numbers of object instances. Second, DSD does not rely on the predicted response, avoiding the unfavorable case, in which poor localized proposals with the highest responses are selected. This is a common issue with NMS, which cannot filter the proposals containing only a key discriminative part or context.</p><p>Among the selected spatially concentrated proposals, the one with the highest predicted response to the target class is selected as the seed positive sample for this image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online Supportive Sample Harvesting</head><p>After obtaining the seed positive proposals, we further seek higher-quality positive samples by taking advantage of the object-level CNN detector. In particular, we implement self-taught learning to improve the ability of the object-level regional CNN detector progressively.</p><p>We propose a novel online supportive sample harvesting (OSSH) strategy to progressively harvest the high-quality positive samples such that the quality of positive samples can be significantly improved. In this way, the ability of the detector can be substantially enhanced with the provided new informative samples. Fast R-CNN is used as our regional CNN detector. We observe that a regional CNN detector (Fast R-CNN) trained on seed samples is sufficiently powerful for selecting the most confident tight positives for further training itself.</p><p>Alternating between training and re-localization shares the similar spirit with the usual MIL that continuously updates SVM to mine high-quality positive samples. Although more powerful by using Fast R-CNN, one risk is that it is easily trapped in poor local optima caused by poor initial seeds due to its stronger fitting capacity.</p><p>To address this issue, we propose to online select the most confident and tight positive samples based on relative improvement (RI) of output CNN scores, instead of relying on the static absolute CNN score at certain training iterations. Specifically, for a training image, we rank all of its N proposals in a descending order of RI over the last epoch. The proposal with the maximal RI is chosen as the positive training sample for the current epoch. For an image, we denote the Fast R-CNN predicted score for the i-th proposal at the t-th epoch (after training Fast R-CNN on this image) as A t i . To compute the RI, we also denote its Fast R-CNN score at the (t+1)-th epoch (but before training Fast R-CNN on this image) as B t+1 i . Then among the N candidate proposals, the proposal P * t+1 with the largest RI is selected for the (t+1)-th training epoch:</p><formula xml:id="formula_2">P * t+1 = arg max i (B t+1 i ? A t i ).</formula><p>We propose to use RI for proposal selection based on the following observations on the WSL problem. The high predicted score of a proposal may result from model overfitting to this proposal or the increasing detection ability of the Fast R-CNN model. We need to untangle these two factors as the former is not desired. Bad seed samples hardly obtain RI from the increasing detection ability of Fast R-CNN during training. In contrast, high-quality positive samples not selected as seeds mostly gain RI due to the improved detection ability of the model. Therefore, RI is a reliable metric for identifying high-quality positive samples. <ref type="figure" target="#fig_1">Fig. 4</ref> shows intuitive examples to justify the observations. In the Example (a) of <ref type="figure" target="#fig_1">Fig. 4</ref>, the score of the false initial training proposal gains improvement mostly from the overfitting to itself, and can hardly increase during training on other images (e.g., "1+" to "2-", "2+" to "3-"), especially in later epochs (e.g., "3+" to "4-", "4+" to "5-"). The highquality candidate proposal (i.e., candidate proposal 1) gains score improvement mostly during training on other images. The score of the low-quality candidate proposal (i.e., candidate proposal 2 which contains context) improves during the increasing of the generalization power of the CNN model in early epochs (e.g., "1+" to "2-"), but decreases in later epochs (e.g., "3+" to "4-", "4+" to "5-") when the CNN gains strong discrimination between the target class and background. In the Example (b) of <ref type="figure" target="#fig_1">Fig. 4</ref>, the low-quality seed training proposal has large score improvement when training on other images in early epochs (e.g., "1+" to "2-"), similar to candidate proposal 2, but can only gain score improvement from the overfitting to itself in later epochs.</p><p>Therefore, RI from the increasing detection ability of Fast R-CNN reliably reflects the quality of the proposal. To ensure the adequate positive samples from other images for training between two consecutive training on this image, e.g., at the t-th and (t+1)-th epoch, we fix the order of training images fed into the network in each epoch. This guarantees the model to be trained by all the rest images of the target class between two consecutive training on the particular image.</p><p>Finally, we introduce negative rejection (NR) performed after several epochs of online supportive sample harvesting (OSSH). Specifically, we perform NR by ranking all the positive samples with the highest predicted score from Fast R-CNN in each image in the order of their predicted CNN scores, and then remove 10% samples with the minimal CNN scores and their corresponding images in the subsequent Fast R-CNN training. This is inspired by the observation that even the best positive samples selected from the difficult positive images are of unsatisfactory quality (low IoU to true objects).</p><p>For data augmentation, apart from the selected proposals with the maximal relative score improvement, all the proposals in this image that overlap with the selected proposal by IoU ? 0.5 are also treated as positives to train the detector at that epoch. The proposals which have IoU ? [0.1, 0.5) overlap with the selected proposal are negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We evaluate our approach on PASCAL VOC 2007 and 2012 datasets <ref type="bibr" target="#b27">[28]</ref> which are the most widely-used benchmarks in weakly supervised object detection. For PAS-CAL 2007, we train the model on the trainval set (containing 5, 011 images) and evaluate on the test set (containing 4, 952 images). For PASCAL 2012, we first train the model on the train set (containing 5, 717 images) and evaluate on the val set (containing 5, 823 images). Additionally, we also train our model on the PASCAL 2012 trainval set (containing 11, 540 images) and evaluate on the test set (containing 1, 0991 images).</p><p>We use two metrics in the evaluation of our approach. First, standard detection mean average precision (mAP) defined by <ref type="bibr" target="#b27">[28]</ref> is evaluated on the PASCAL 2007 test set,      <ref type="bibr" target="#b28">[29]</ref> which is a standard metric for measuring localization accuracy on a training set. CorLoc is the percentage of images, where the most confident detected bounding box overlaps (IoU? 0.5) with a ground-truth box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We train the HCP multi-label classification model with the settings following <ref type="bibr" target="#b25">[26]</ref>. In all the experiments, 100 proposals with the highest responses to the target class are chosen to form the candidate proposal pool to balance the performance and efficiency. In dense subgraph discovery, we fix the values of T and k to 0.8 and 5 for all the experiments, as it is empirically shown that the localization performance will not change much when T is greater than 0.7 or when k ranges from 3 to 8. In the Fast R-CNN training with online supportive sample harvesting, the model is fine-tuned from the pre-trained model on ImageNet <ref type="bibr" target="#b29">[30]</ref>. The batch size is set to 2 such that the overfitting to a certain image resulting from the training on that mini-batch is obvious. The order of training images is fixed in all the epochs. The learning rate is set to 0.001 initially and decreased by a factor of 10 after every 6 epochs. We use the object proposals generated by Edge Boxes <ref type="bibr" target="#b30">[31]</ref>, and adopt the VGG-16 network <ref type="bibr" target="#b31">[32]</ref> in the Fast R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To validate the effectiveness of our two components, i.e., dense subgraph discovery and online supportive sample harvesting, we conduct ablation studies by accumulatively adding each of them to our baseline, i.e., HCP. The baseline HCP selects the proposal with the highest response to the target class as the positive sample in each image. In all the ablation versions of our method, Fast R-CNN is trained with the proposals with IoU? 0.5 to their respective positive samples. From <ref type="table" target="#tab_0">Table 1</ref>, one can observe that DSD improves CorLoc by nearly 4% compared to only using HCP to select positive proposals. OSSH1, OSSH2 and OSSH3 indicate performing online supportive sample harvesting in the first 1, 2 and 3 epochs from the 2nd epoch of training Fast R-CNN (note in the 1st epoch, seed positives from DSD are used in training). 12% of improvement on Cor-Loc brought by OSSH1 shows that performing OSSH only 1 time for a certain image adequately discovers the tight positive proposal in the candidate pool. It can be seen that later OSSH has a less benefit to CorLoc than the OSSH in the 2nd epoch, showing that high-quality positive proposals gain consistent CNN score improvements in each of these epochs and thus can be easily picked out in the first time of OSSH. <ref type="table" target="#tab_1">Table 2</ref> shows that mAP has similar trends to Cor-Loc. DSD and OSSH1 bring around 3% and 9% improvements in mAP respectively, validating their effectiveness. NR is also beneficial to the detector and contributes 1% mAP improvement by discarding the false positives from the difficult images. <ref type="table" target="#tab_2">Table 3</ref> also shows significant improvements of mAP after adding DSD and OSSH to the baseline method on the PASCAL 2012 val set.</p><p>To validate the advantage of using relative CNN score improvement, we conduct comparison experiments with using absolute CNN scores to harvest confident positive samples in OSSH. After epochs of OSSH, the proposals with the highest predicted score in each image are selected as confident positive samples. From <ref type="table" target="#tab_5">Table 6</ref>, it is found that relative score improvement consistently outperforms absolute CNN scores in all cases, especially when OSSH is performed in more epochs. Using absolute CNN scores, the improvements of OSSH in the later two epochs are much less than using relative score improvement. This further demonstrates that the detector is more easily trapped in poor local optima when selecting positive samples based on absolute CNN scores, since the detector highly overfits seed positive samples and thus seed positive samples can obtain high predicted scores after the first 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-The-Arts</head><p>We compare our approach to the state-of-the-art methods. <ref type="table" target="#tab_0">Table 1</ref> shows the CorLoc comparison on the PAS-CAL 2007 trainval set. Our approach achieves the highest result 56.1%, compared to all the MIL-based methods (i.e., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>) and the end-to-end WSL network (i.e., <ref type="bibr" target="#b24">[25]</ref>). <ref type="table" target="#tab_1">Table 2</ref> shows the comparison in terms of AP on the PAS-CAL 2007 test set using the model trained on the PAS-CAL 2007 trainval set. Our approach achieves 41.7% mAP which also outperforms all the state-of-the-arts, due to the high CorLoc achieved on the corresponding training set (Table 1). With more training data (the PASCAL 2007 trainval set and PASCAL 2012 trainval set), mAP can be further boosted to 43.7% by our approach. <ref type="table" target="#tab_2">Table 3</ref> shows the AP comparison on the PASCAL 2012 val set with the state-ofthe-art method <ref type="bibr" target="#b8">[9]</ref>. Both our model and theirs are trained on only the PASCAL 2012 train set. Our approach consistently keeps higher performance, surpassing <ref type="bibr" target="#b8">[9]</ref> by almost 10% in terms of mAP. <ref type="table" target="#tab_3">Table 4</ref> gives the comparison between our approach and the state-of-the-art method <ref type="bibr" target="#b24">[25]</ref> in terms of CorLoc on the PASCAL 2012 trainval set. The proposed approach significantly outperforms <ref type="bibr" target="#b24">[25]</ref> by 4% in CorLoc. <ref type="table" target="#tab_4">Table 5</ref> shows AP on the PASCAL 2012 test set of our approach and <ref type="bibr" target="#b24">[25]</ref> using the models trained on the PASCAL 2012 trainval set. An advantage of 3% on mAP is achieved by our approach. With more training data (the PASCAL 2007 trainval set and PASCAL 2012 trainval set), mAP can be further improved to 39.4% by our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>We illustrate examples of detected objects in different ablation versions of our approach in <ref type="figure" target="#fig_2">Fig. 5</ref>. We observe that in some cases the baseline HCP localizes only the key discriminative part of the object, and the localization accuracy can be progressively improved by adding DSD and OSSH to it. Note that in the fifth example which is in the final row of <ref type="figure" target="#fig_2">Fig. 5</ref>, the detected objects by HCP and HCP+DSD are false positive samples which are used as seed positive samples in training the Fast R-CNN detector. By performing OSSH for one epoch, the ground-truth object can be roughly localized, and more epochs of OSSH help precisely select the tight positive proposals, which validates the importance of using relative score improvement in OSSH to avoid the detector being trapped in poor local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a deep self-taught learning approach for weakly supervised object localization. Our approach first acquires effective seed positive object proposals by examining their response scores to the target class from a classification network, and then mining the spatially concentrated samples via dense subgraph discovery. Then by virtue of online supportive sample harvesting augmented with a new relative CNN score improvement metric, our approach can successfully detect positive samples of improved quality. The experiments demonstrate the superiority of our approach to the state-of-the-art methods. On PASCAL 2007 and 2012, the proposed approach consistently outperforms them by an obvious margin in all the evaluation scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of candidate proposals with the highest responses to the corresponding class. Top 10 proposals for each image are shown. The top-ranked proposals may contain context or only a key discriminative part of the object. However, these top-ranked proposals are mostly spatially concentrated around the true object instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>CNN score on the target class vs. number of epochs during training Fast R-CNN for different proposals. The training proposals are the seed positive samples to train Fast R-CNN. "1-" and "1+" indicate the CNN score right before and after training on this image in the 1st epoch, respectively. Similar meanings apply to the symbols in other epochs. High-quality proposals which are not used as training samples mainly gain score improvement from the increasing detection ability of Fast R-CNN, while the score improvement of false positive training samples mostly comes from the overfitting to themselves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative examples of detected objects in different ablation versions of our approach. From the 1st to the 5th column: HCP, HCP+DSD, HCP+DSD+OSSH1, HCP+DSD+OSSH2 and HCP+DSD+OSSH3. Green and red bounding boxes represent the ground-truth object bounding boxes and the bounding boxes of the detected objects, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Correct localization (CorLoc) (%) of our method and other state-of-the-art methods on the PASCAL 2007 trainval set. OSSH1 performs OSSH only in the 2nd epoch, OSSH2 performs OSSH in the 2nd and 3rd epochs, and OSSH3 performs OSSH in the 2nd, 3rd and 4th epochs. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Avg.</figDesc><table><row><cell>Cinbis et al. [8]</cell><cell>57.2 62.2 50.9 37.9 23.9 64.8 74.4 24.8 29.7 64.1 40.8 37.3 55.6</cell><cell>68.1</cell><cell>25.5</cell><cell>38.5 65.2 35.8 56.6 33.5 47.3</cell></row><row><cell>Bilen et al. [27]</cell><cell>66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5</cell><cell>60.0</cell><cell>19.8</cell><cell>39.2 41.7 30.1 50.2 44.1 43.7</cell></row><row><cell>Wang et al. [22]</cell><cell>80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3</cell><cell>69.5</cell><cell>14.1</cell><cell>38.3 58.8 47.2 49.1 60.9 48.5</cell></row><row><cell cols="2">Kantorov et al. [25] 83.3 68.6 54.7 23.4 18.3 73.6 74.1 54.1 8.6 65.1 47.1 59.5 67.0</cell><cell>83.5</cell><cell>35.3</cell><cell>39.9 67.0 49.7 63.5 65.2 55.1</cell></row><row><cell>Li et al. [9]</cell><cell>78.2 67.1 61.8 38.1 36.1 61.8 78.8 55.2 28.5 68.8 18.5 49.2 64.1</cell><cell>73.5</cell><cell>21.4</cell><cell>47.4 64.6 22.3 60.9 52.3 52.4</cell></row><row><cell>HCP</cell><cell>54.4 37.2 42.1 28.1 13.8 47.8 49.6 40.6 16.4 38.7 13.8 34.5 22.2</cell><cell>36.4</cell><cell>10.8</cell><cell>36.4 42.3 20.8 46.1 49.3 34.1</cell></row><row><cell>HCP+DSD</cell><cell>56.9 36.0 45.4 26.5 15.7 49.8 54.5 53.1 15.9 45.6 13.4 37.5 38.1</cell><cell>42.1</cell><cell>16.2</cell><cell>34.2 45.4 29.7 55.6 46.1 37.9</cell></row><row><cell cols="2">HCP+DSD+OSSH1 70.2 60.0 53.9 26.1 28.3 58.9 75.4 58.9 14.8 63.4 17.9 52.6 51.7</cell><cell>67.0</cell><cell>19.7</cell><cell>46.3 63.9 42.4 67.0 65.1 50.2</cell></row><row><cell cols="2">HCP+DSD+OSSH2 73.9 56.0 52.1 26.9 34.0 66.6 80.0 59.5 13.1 70.2 22.9 55.7 60.6</cell><cell>83.8</cell><cell>22.0</cell><cell>51.5 71.1 50.4 71.2 74.4 54.9</cell></row><row><cell cols="2">HCP+DSD+OSSH3 72.7 55.3 53.0 27.8 35.2 68.6 81.9 60.7 11.6 71.6 29.7 54.3 64.3</cell><cell>88.2</cell><cell>22.2</cell><cell>53.7 72.2 52.6 68.9 75.5 56.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detection average precision (AP) (%) of our method and other state-of-the-art methods (trained on the PASCAL 2007 trainval set) on the PASCAL 2007 test set. OSSH1, OSSH2 and OSSH3 have the same meanings as Table 1. 07+12 means training on the PASCAL 2007 trainval and 2012 trainval sets.</figDesc><table><row><cell>Cinbis et al. [8]</cell><cell>38.1 47.6 28.2 13.9 13.2 45.2 48.0 19.3 17.1 27.7 17.3 19.0 30.1 45.4</cell><cell cols="2">13.5 17.0 28.8 24.8 38.2 15.0 27.4</cell></row><row><cell>Song et al. [21]</cell><cell>27.6 41.9 19.7 9.1 10.4 35.8 39.1 33.6 0.6 20.9 10.0 27.7 29.4 39.2</cell><cell>9.1</cell><cell>19.3 20.5 17.1 35.6 7.1 22.7</cell></row><row><cell>Bilen et al. [27]</cell><cell>46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2 7.8 28.3 12.7 21.5 30.1 42.4</cell><cell>7.8</cell><cell>20.0 26.8 20.8 35.8 29.6 27.7</cell></row><row><cell>Wang et al. [22]</cell><cell>48.9 42.3 26.1 11.3 11.9 41.3 40.9 34.7 10.8 34.7 18.8 34.4 35.4 52.7</cell><cell cols="2">19.1 17.4 35.9 33.3 34.8 46.5 31.6</cell></row><row><cell>Kantorov et al. [25]</cell><cell>57.1 52.0 31.5 7.6 11.5 55.0 53.1 34.1 1.7 33.1 49.2 42.0 47.3 56.6</cell><cell cols="2">15.3 12.8 24.8 48.9 44.4 47.8 36.3</cell></row><row><cell>Li et al. [9]</cell><cell>54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8</cell><cell cols="2">16.2 29.9 40.7 15.9 55.3 40.2 39.5</cell></row><row><cell>HCP</cell><cell>42.6 40.8 26.5 21.0 5.7 41.7 47.8 34.2 10.8 27.2 12.3 28.9 12.5 27.9</cell><cell>1.8</cell><cell>18.2 29.0 12.5 45.5 47.1 26.7</cell></row><row><cell>HCP+DSD</cell><cell>45.7 41.0 26.8 23.1 5.0 51.4 51.5 43.3 10.4 37.6 10.2 29.2 23.0 39.1</cell><cell>3.1</cell><cell>16.8 33.5 13.6 47.2 40.5 29.6</cell></row><row><cell>HCP+DSD+OSSH1</cell><cell>52.5 56.9 35.5 18.5 13.8 59.5 62.4 51.7 7.0 53.1 14.9 38.3 34.6 60.0</cell><cell>5.7</cell><cell>15.1 49.7 36.0 55.7 54.6 38.8</cell></row><row><cell>HCP+DSD+OSSH2</cell><cell>52.9 53.6 32.4 20.3 14.8 59.2 64.8 50.3 3.3 51.2 16.7 42.5 44.4 62.9</cell><cell>6.1</cell><cell>19.1 47.2 42.0 57.1 62.4 40.2</cell></row><row><cell>HCP+DSD+OSSH3</cell><cell>49.6 47.0 33.6 21.7 15.7 60.4 66.0 51.7 5.6 54.1 24.5 38.4 45.2 65.0</cell><cell>6.1</cell><cell>18.5 53.3 46.0 52.5 61.5 40.8</cell></row><row><cell>HCP+DSD+OSSH3+NR</cell><cell>52.2 47.1 35.0 26.7 15.4 61.3 66.0 54.3 3.0 53.6 24.7 43.6 48.4 65.8</cell><cell>6.6</cell><cell>18.8 51.9 43.6 53.6 62.4 41.7</cell></row><row><cell cols="2">HCP+DSD+OSSH3+NR (07+12) 54.2 52.0 35.2 25.9 15.0 59.6 67.9 58.7 10.1 67.4 27.3 37.8 54.8 67.3</cell><cell>5.1</cell><cell>19.7 52.6 43.5 56.9 62.5 43.7</cell></row></table><note>method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Detection average precision (AP) (%) of our method and other state-of-the-art methods (trained on the PASCAL 2012 train set) on the PASCAL 2012 val set. OSSH1, OSSH2 and OSSH3 have the same meanings as Table 1. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</figDesc><table><row><cell>Li et al. [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.1</cell></row><row><cell>HCP</cell><cell cols="13">49.3 33.3 24.7 14.0 11.8 37.9 30.2 35.7 6.9 26.6 6.9 25.4 14.1</cell><cell>29.4</cell><cell>1.1</cell><cell cols="6">18.1 25.7 13.4 44.1 45.4 24.7</cell></row><row><cell>HCP+DSD</cell><cell cols="13">55.3 39.3 25.3 14.3 10.6 50.4 35.6 45.4 11.4 31.3 2.3 30.6 29.7</cell><cell>35.3</cell><cell>5.0</cell><cell cols="6">14.2 28.1 13.8 47.1 41.1 28.3</cell></row><row><cell>HCP+DSD+OSSH1</cell><cell cols="13">60.7 54.0 36.5 14.4 19.5 57.5 45.5 47.7 11.1 39.9 2.8 43.4 38.2</cell><cell>55.5</cell><cell>4.3</cell><cell cols="6">18.6 40.5 31.1 56.6 52.0 36.5</cell></row><row><cell>HCP+DSD+OSSH2</cell><cell cols="13">57.7 55.9 34.8 17.4 18.3 57.8 48.6 51.0 9.7 40.8 7.2 42.5 47.2</cell><cell>62.2</cell><cell>4.6</cell><cell cols="6">18.4 43.0 36.8 55.7 57.8 38.4</cell></row><row><cell>HCP+DSD+OSSH3</cell><cell cols="13">61.0 53.8 30.3 18.1 18.6 57.4 51.1 53.1 6.1 40.7 12.1 38.2 48.2</cell><cell>65.5</cell><cell>4.8</cell><cell cols="6">20.9 45.5 34.0 54.1 57.3 38.5</cell></row><row><cell cols="14">HCP+DSD+OSSH3+NR 60.9 53.3 31.0 16.4 18.2 58.2 50.5 55.6 9.1 42.1 12.1 43.4 45.3</cell><cell>64.6</cell><cell>7.4</cell><cell cols="6">19.3 44.8 39.3 51.4 57.2 39.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Correct localization (CorLoc) (%) of our method and other state-of-the-art ones on the PASCAL 2012 trainval set.</figDesc><table><row><cell>method</cell><cell>aero bike bird boat bottle bus car</cell><cell cols="4">cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>Avg.</cell></row><row><cell cols="3">Kantorov et al. [25] 78.3 70.8 52.5 34.7 36.6 80.0 58.7 38.6 27.7 71.2 32.3 48.7 76.2</cell><cell>77.4</cell><cell>16.0</cell><cell>48.4 69.9 47.5 66.9 62.9 54.8</cell></row><row><cell cols="3">HCP+DSD+OSSH3 82.4 68.1 54.5 38.9 35.9 84.7 73.1 64.8 17.1 78.3 22.5 57.0 70.8</cell><cell>86.6</cell><cell>18.7</cell><cell>49.7 80.7 45.3 70.1 77.3 58.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Detection average precision (AP) (%) of our method and other state-of-the-art methods (trained on the PASCAL 2012 trainval set) on the PASCAL 2012 test set. 07+12 means training on the PASCAL 2007 trainval and 2012 trainval sets. method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP PASCAL 2012 val set and PASCAL 2012 test set with their respective training models stated above. Second, on the training sets (i.e., the PASCAL 2007 trainval set and PASCAL 2012 trainval set), we report Correct Localization (CorLoc)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Correct localization (CorLoc) (%) on the PASCAL 2007 trainval set of using relative CNN score improvement and absolute CNN score in OSSH. The comparison is conducted in 3 cases: performing OSSH in the first 1, 2 and 3 epochs from the 2nd epoch in training Fast R-CNN.</figDesc><table><row><cell>Epochs of OSSH</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>absolute CNN score</cell><cell cols="3">48.8 52.3 53.2</cell></row><row><cell cols="4">relative score improvement 50.2 54.9 56.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Throughout this paper, response and CNN score refer to the final probability output after softmax normalization to the target class.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Zequn Jie is partially supported by Tencent AI Lab. The work of Jiashi Feng was partially supported by National University of Singapore startup grant R-263-000-C08-133 and Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reversible recursive instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tree-structured reinforcement learning for sequential object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00949</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised localization of novel objects using appearance transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transfer learning by ranking for weakly supervised object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hcp: A flexible cnn framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
