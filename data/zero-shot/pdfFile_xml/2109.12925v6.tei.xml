<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">under consideration at Pattern Recognition Letters HarrisZ + : Harris Corner Selection for Next-Gen Image Matching Pipelines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Bellavia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Mathematics and Computer Science</orgName>
								<orgName type="institution">Universit? Degli Studi di Palermo</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">Visual Recognition Group</orgName>
								<orgName type="institution">CTU in</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">under consideration at Pattern Recognition Letters HarrisZ + : Harris Corner Selection for Next-Gen Image Matching Pipelines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to its role in many computer vision tasks, image matching has been subjected to an active investigation by researchers, which has lead to better and more discriminant feature descriptors and to more robust matching strategies, also thanks to the advent of the deep learning and the increased computational power of the modern hardware. Despite of these achievements, the keypoint extraction process at the base of the image matching pipeline has not seen equivalent progresses. This paper presents HarrisZ + , an upgrade to the HarrisZ corner detector, optimized to synergically take advance of the recent improvements of the other steps of the image matching pipeline. HarrisZ + does not only consists of a tuning of the setup parameters, but introduces further refinements to the selection criteria delineated by HarrisZ, so providing more, yet discriminative, keypoints, which are better distributed on the image and with higher localization accuracy. The image matching pipeline including HarrisZ + , together with the other modern components, obtained in different recent matching benchmarks state-of-the-art results among the classic image matching pipelines. These results are quite close to those obtained by the more recent fully deep end-to-end trainable approaches and show that there is still a proper margin of improvement that can be granted by the research in classic image matching methods. (Fabio Bellavia)    building blocks are still actively investigated and successfully used as the base of modern and competitive image matching pipeline for 3D reconstruction in sparse Structure-from-Motion (SfM, Sch?nberger and Frahm (2016)) and Simultaneous Localization and Mapping (SLAM, Mur-Artal et al. (2015)).</p><p>Although besieged by deep keypoint detectors, handcrafted detectors are still able to provide state-of-the-art results in SfM and SLAM applications <ref type="bibr" target="#b16">(Jin et al., 2021)</ref>. The keypoint extraction is the first step of 3D reconstruction pipeline. A crucial aspect in devising a better image matching schema is to adapt the core detector to the recent advancements obtained on the successive steps of the pipeline. On one hand, recent local image descriptors are become robust and able to cope with higher degrees of image deformations and noise <ref type="bibr" target="#b22">(Mishchuk et al., 2017)</ref>. On the other hand, the last matching strategies exploiting local spatial constraints (Bellavia, 2022)  and those based on the RANdom SAmple Consensus (RANSAC, Fischler and Bolles  (1981)) to exploit global model constraints according to scene geometry <ref type="bibr" target="#b7">(Cavalli et al., 2020)</ref>, have been shown to tolerate much better the presence of outlier matches. Moreover, the increased computational power offered by recent GPU allows arXiv:2109.12925v6 [cs.CV] 1 May 2022</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image keypoint extraction has played a relevant role in computer vision since the early days <ref type="bibr" target="#b34">(Szeliski, 2021)</ref>. A keypoint is generally defined in a broad sense as a local region on the image which can be correctly re-localized and distinguished from others after a transformation of the image, i.e. keypoints must be repeatable and discriminable. The precision and the computational efficiency of the keypoint localization, the sparseness of the keypoint distribution over the images, and the kind of image transformations the keypoint must tolerate generally depend on the application purpose <ref type="bibr" target="#b37">(Tuytelaars and Mikolajczyk, 2008</ref>).</p><p>With the increasing success of deep learning approaches in replacing the handcrafted ones, keypoint extraction more or less implicitly has been hidden within the layers of end-to-end trainable networks <ref type="bibr" target="#b10">(Dusmanu et al., 2019;</ref><ref type="bibr" target="#b35">Tian et al., 2020;</ref><ref type="bibr" target="#b9">DeTone et al., 2018;</ref><ref type="bibr" target="#b33">Sun et al., 2021)</ref> or apparently removed <ref type="bibr" target="#b36">(Truong et al., 2021)</ref>. Nevertheless, keypoint detectors as standalone to process in parallel more keypoints and matches in a reasonable time depending on the application. It is not a coincidence that the blob-like Difference-of-Gaussian (DoG) keypoints of the Scale Invariant Feature Transform (SIFT) detector <ref type="bibr" target="#b19">(Lowe, 2004)</ref>, "unchained" to output more keypoints by an appropriate removal of the setup thresholds, have achieved among the best results in the previous Image Matching Challenge 1 (IMC2020, <ref type="bibr" target="#b16">Jin et al. (2021)</ref>).</p><p>This paper presents HarrisZ + , an upgrade to the corner-based HarrisZ detector <ref type="bibr" target="#b5">(Bellavia et al., 2011)</ref> for next-gen image matching pipelines. The aim of HarrisZ + is to provide more keypoints, better localized and distributed over the image, yet characterized by high repeatability and discriminability. Unlike the case of the unchained SIFT keypoints, HarrisZ + does not only consists of a simple tuning of the setup parameters so as to obtain more keypoints as output, but introduces further refinements to the selection criteria delineated by HarrisZ. According to the last Image Matching Challenge 2 (IMC2021) and SimLoc-Match Image Matching contest 3 , the image matching pipelines based on HarrisZ + obtain state-of-art results among the classic image matching pipelines, closely following the more recent fully deep end-to-end trainable approaches. This proves that there is still a valuable margin of improvement towards the research of classic image matching methods.</p><p>The rest of the paper is organized as follows: Sec. 2 presents the related work, Sec. 3 describes the proposed HarrisZ + updates after introducing the original HarrisZ detector, and Sec. 4 reports the experimental evaluation. Conclusion and future work are outlined in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Corners and blobs are the two principal kinds of keypoints generally recognized in the literature <ref type="bibr" target="#b34">(Szeliski, 2021)</ref>, whose the Harris <ref type="bibr" target="#b14">(Harris and Stephens, 1988</ref>) and SIFT <ref type="bibr" target="#b19">(Lowe, 2004)</ref> detectors may respectively represent the most popular and effective extractors. Corners usually tend to be identified with junctions and blobs with homogeneous image regions, but in practice this distinction only holds for simple synthetic images and not for complex, natural images. The Harris detector is based on the filter response to a function of the eigenvalues of the autocorrelation matrix of the image intensity gradient. Other operators based on the autocorrelation matrix eigenvalues have been proposed to extract corners <ref type="bibr" target="#b12">(F?rstner, 1986;</ref><ref type="bibr" target="#b32">Shi and Tomasi, 1994)</ref>, but the one defining the Harris corner detector is maybe the most commonly employed. The Hessian matrix has been also used to design keypoint detectors according to its determinant <ref type="bibr" target="#b3">(Beaudet, 1978)</ref>. Differently from the Harris corner detector, the SIFT detector is based on the filter response to DoG, as approximation of the Laplacian of Gaussian (LoG), which corresponds to the trace of the Hessian matrix. Filterbased detectors are generally invariant to illumination changes since keypoints are selected as local maxima of the filter response. Nevertheless, in practice, an absolute threshold on the 1 https://www.cs.ubc.ca/research/image-matching-challenge/2020/ 2 https://www.cs.ubc.ca/research/image-matching-challenge/ 3 https://simlocmatch.com/ filter response is generally set to suppress the detection of spurious noise as keypoints, which can decrease detector capability in case of low-contrast images. Pre-processing the input image by contrast enhancement techniques can be helpful in this situation <ref type="bibr" target="#b17">(Lecca et al., 2020)</ref>, notwithstanding that noise gets enhanced too. Concerning the robustness to geometric image transformations, in their original definition, detectors are generally only rotational invariant, while the scale depends on the filter windows size. Multi-scale approaches have been devised to solve scale issues, as well as to improve the robustness of the detector to noise <ref type="bibr" target="#b21">(Mikolajczyk and Schmid, 2004)</ref>. The Gaussian scale-space <ref type="bibr" target="#b18">(Lindeberg, 1994)</ref> is generally employed for this aim, yet non-linear scale-space definitions have been successfully applied too, as for the KAZE features <ref type="bibr" target="#b0">(Alcantarilla et al., 2012)</ref>. The keypoint scale is generally selected according to the maximum filter response among the scales, where the Laplacian filter is a common choice. Additional robustness can be required in case of severe viewpoint changes, which is generally achieved by upgrading the scale and orientation associated with the keypoint to a local affine transformation approximating the unknown original perspective one. The affine transformation can be obtained locally and iteratively <ref type="bibr" target="#b21">(Mikolajczyk and Schmid, 2004)</ref>, explicitly computing global synthetic warps of the image <ref type="bibr" target="#b25">(Morel and Yu, 2009</ref>), but also by implicitly defining the keypoint filter response as a tensor <ref type="bibr" target="#b41">(Zhang and Sun, 2020)</ref>. This geometric information characterizes the region around the keypoint and is used to generate the local image patch according to a reference system upon which to compute the associated keypoint descriptor. Recent state-of-the-art solutions based on deep learning such as AffNet <ref type="bibr" target="#b24">(Mishkin et al., 2018)</ref> only require to provide the keypoint scale and location to get affine-normalized patches.</p><p>Fast and efficient keypoint extraction may be required by real-time applications on when processing a huge amount of images. One of the two main approaches in this sense is to discretize the kernel to allow a fast computation of the filter response thanks to the integral images. This solution was originally employed by the Speeded-Up Robust Feature (SURF) detector <ref type="bibr" target="#b2">(Bay et al., 2008)</ref> as alternative to SIFT. The other solution is to characterize the keypoint according to a comparison of the keypoint central pixel with respect to the other pixels in the local window. The Features from Accelerated Segment Test (FAST) detector <ref type="bibr" target="#b29">(Rosten et al., 2010)</ref> is maybe the most known solution in this sense. In addition, FAST makes use of decision trees to further speed-up the computation. Worth to be mentioned among the handcrafted keypoint detector is the Maximally Stable Extremal Region (MSER) detector <ref type="bibr" target="#b20">(Matas et al., 2002)</ref>, which extracts blob-like structures that remain almost stable in a region growing process.</p><p>With the partial exception of FAST, machine learning has intervened into the keypoint detector design only recently. After some initial attempts <ref type="bibr" target="#b39">(Verdie et al., 2015)</ref>, excluding the deep Key.Net detector (Barroso-Laguna et al., 2019) which employs both handcrafted and learned filter layers, deep detectors only appeared as components of end-to-end image matching architectures. The structure of deep detectors generally combine in order convolutional and max pooling layers followed by nonlinear activation functions, which strongly resemble the struc-ture of filter-based handcrafted detectors, with respectively a filter definition, the local maxima selection of its response and a thresholding on them. The Learned Invariant Feature Transform (LIFT) network <ref type="bibr" target="#b40">(Yi et al., 2016)</ref> is the first example of a complete end-to-end deep image matching architecture. Due to its complexity, LIFT cannot be trained as a whole from scratch and the training process requires handcrafted keypoints as bootstrap. An alternative solution was proposed in <ref type="bibr">Super-Point (DeTone et al., 2018)</ref>, where the training process becomes self-supervised thanks to a pre-training phase with synthetic images and the homographic adaptation. Unlike LIFT which is patch-based, SuperPoint relies on a fully convolutional network to process the whole image as input. The Detect-and-Describe (D2-Net, <ref type="bibr" target="#b10">Dusmanu et al. (2019)</ref>) and the Describe-to-Detect (D2D, <ref type="bibr" target="#b35">Tian et al. (2020)</ref>) networks are other end-to-end local feature architectures, which operate in a different way to the standard computational flow where it is the descriptor that must be adapted to the keypoint detector. Specifically, D2-Net defines dense feature maps that simultaneously serve to derive the keypoints and their descriptors, while in D2D keypoints must be adapted to the information content of the descriptors. The DIScrete Keypoints (DISK) network <ref type="bibr" target="#b38">(Tyszkiewicz et al., 2020)</ref> leverages the principles of reinforcement learning to ease the complexity of the end-to-end training caused by the sparseness of the keypoints and achieved state-of-the-art results. More recently, the integration of the keypoint spatial constraints into the network design has greatly improved the final matching output. This has been done implementing attentional graph neural networks in SuperGlue <ref type="bibr" target="#b30">(Sarlin et al., 2020)</ref>, using coarse-tofine schemas with the Local Feature TRasformer (LoFTR, <ref type="bibr" target="#b33">Sun et al. (2021)</ref>), or considering dense descriptor approaches not employing any sort of keypoints <ref type="bibr" target="#b36">(Truong et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HarrisZ +</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From HarrisZ</head><p>The original HarrisZ <ref type="bibr" target="#b5">(Bellavia et al., 2011)</ref> extracts corners from an image, represented as a matrix I ? R m?n , at different scales of the Gaussian scale-space. Defining the horizontal and vertical gradient derivatives of I as the image convolution with the central difference kernel K = [?1 0 1], i.e.</p><formula xml:id="formula_0">I x = I * K, and I y = I * K<label>(1)</label></formula><p>where * indicates the convolution, the scale-space derivatives are defined as <ref type="bibr" target="#b21">(Mikolajczyk and Schmid, 2004</ref>)</p><formula xml:id="formula_1">I ?,x = G ? * I x and I ?,y = G ? * I y<label>(2)</label></formula><p>where ? denotes the differentiation scale which identifies the current working scale and G ? is a Gaussian kernel with mean zero and standard deviation ?. Unlike the standard Harris corner detector <ref type="bibr" target="#b14">(Harris and Stephens, 1988)</ref>, before computing the autocorrelation matrix both the scale-space derivatives get enhanced by a pixel-wise multiplication according to a smoothed raw edge mask M ?</p><formula xml:id="formula_2">E ?,x = I ?,x M ? and E ?,y = I ?,y M ?<label>(3)</label></formula><p>where M ? is computed from the gradient magnitude</p><formula xml:id="formula_3">I ?,m = I 2 ?,x + I 2 ?,y 1 2<label>(4)</label></formula><p>by globally threshold on the mean I ?,m computed on the whole I ?,m , i.e.</p><formula xml:id="formula_4">M ? = G ? * V I ?,m &gt; I ?,m<label>(5)</label></formula><p>where V(?) is the indicator function. The enhanced derivatives decrease the noise while strengthening the edges according to the working scale ?, providing a sort of non-linear scale-space. The autocorrelation matrix at the pixel x is then computed as</p><formula xml:id="formula_5">? ? (x) = G s? * E 2 ?,x (x) G s? * E ?,x (x) E ?,y (x) G s? * E ?,x (x) E ?,y (x) G s? * E 2 ?,y (x)<label>(6)</label></formula><p>where s? defines the integration scale, i.e. the window size of the filter, on the basis of the differentiation scale ? and the constant s = 2 1 2 . Actually, in the computation of the corner filter response H ? , only the determinant D ? and the trace T ? of ? ? are involved, respectively equal to the product and the sum of the eigenvalues of ? ? . These can be directly computed by simple pixel-wise operations on Gaussian-smoothed maps as</p><formula xml:id="formula_6">D ? = (G s? * E 2 ?,x )(G s? * E 2 ?,y ) ? (G s? * (E ?,x E ?,y )) 2 (7) T ? = (G s? * E 2 ?,x ) + (G s? * E 2 ?,y )<label>(8)</label></formula><p>In the original definition given by Harris and Stephens, the corner filter response is given b?</p><formula xml:id="formula_7">H ? = D ? ? c T 2 ? (9)</formula><p>where c is a user-given coefficient. The HarrisZ filter response H ? replaces c with an implicit adaptive reformulation according to the z-score normalization Z(?) acting globally on the whole image</p><formula xml:id="formula_8">H ? = Z(D ? ) ? Z(T 2 ? )<label>(10)</label></formula><p>where</p><formula xml:id="formula_9">Z(Q) = Q ? Q ? Q<label>(11)</label></formula><p>being Q a generic image with mean Q and standard deviation ? Q . According to the statistical properties of H ? considering the whole input image I, a pixel x can be roughly classified as a flat region if H ? (x) is almost 0, or otherwise as an edge or as a corner if H ? (x) is respectively lower or higher than 0. Moreover, a corner must lie onto the edge mask M ? . These two conditions can be coded as the binary mask C ?</p><formula xml:id="formula_10">C ? = V(H ? &gt; 0) V(M ? &gt; 0.31)<label>(12)</label></formula><p>where multiplication is intended pixel-wise and the 0.31 value is introduced to take into account the current scale in the binarization of M ? , whose formal derivation can be found in the original manuscript. The binary mask C ? provides the initial set of corner candidates according to the input image instead of requiring a user-defined threshold which generally needs to be adjusted according to the input, as happens for the original Harris detector. C ? provides a sort of implicit adaptive contrast enhancement of the input image since both H ? and M ? rely on global statistics for their definitions. Note that recent state-ofthe-art deep architectures use masks to filter keypoints with the aim of removing disturbing elements of the scene such as sky or people 2 .</p><p>Finally, corners corresponding to C ? are selected so that only local maxima on H ? will survive. Differently from the standard multi-scale approach employed for instance in the Harris-affine detector, where the convolution kernels are maintained fixed while the image is down-scaled, HarrisZ keeps the input image size fixed while increasing the kernel size, as for SURF <ref type="bibr" target="#b2">(Bay et al., 2008)</ref>, in order to increase the detector accuracy. The final set of keypoints for the scale ? is selected by sorting first the C ? pixel locations according to the decreasing values of H ? and then, in order, greedily removing a candidate keypoint if its distance from the already selected keypoints is less than 3? . This strategy can resemble in some aspect the Adaptive Non-Maximal Suppression (ANMS, <ref type="bibr" target="#b6">Brown et al. (2005)</ref>).</p><p>Keypoint localization precision is then increased with parabolic sub-pixel precision on H ? . The entire process is repeated for different scales</p><formula xml:id="formula_11">? ? {2 i 2 : 3 ? i ? 8}<label>(13)</label></formula><p>Corners are filtered one last time according to the ratio of the corresponding autocorrelation matrix eigenvalues, which must be higher than 0.75 to further exclude accidental detected edges. Finally, corners are considered altogether among the different scales and sorted first by the decreasing scale values index i and then by H ? if the maximum number of allowed output keypoints is exceeded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">To HarrisZ +</head><p>HarrisZ + differs from the original HarrisZ by the fine tuning of the parameters in combination with fine implementation changes: the devil is in the details. These updates are strongly related to the advances introduced in the successive steps of the matching pipeline, each one leading to state-of-the-art improvements. In particular:</p><p>i AffNet <ref type="bibr" target="#b24">(Mishkin et al., 2018)</ref> provides a better patch normalization than the canonical one of SIFT, which gives only the scale 4 , and it also performs better with respect to the affine transformation estimated by the autocorrelation matrix <ref type="bibr" target="#b21">(Mikolajczyk and Schmid, 2004)</ref>. ii The last version of HardNet <ref type="bibr" target="#b27">(Pultar, 2020)</ref> provides a more discriminant and robust keypoint descriptor than SIFT. iii Blob matching strategy with the inclusion of spatial constraints based on the Delaunay Triangulation Matching (DTM, Bellavia <ref type="formula" target="#formula_1">(2022)</ref>) retains more correct matches while discarding wrong matches better than the simple Nearest Neighbour Ratio (NNR) strategy <ref type="bibr" target="#b19">(Lowe, 2004)</ref>. iv Filtering matches by multiple local RANSAC has proven to be useful in removing wrong matches with the Adaptive Locally-Affine Matching (AdaLAM, <ref type="bibr" target="#b7">Cavalli et al. (2020)</ref>). v The Degenerate SAmple Consensus (DegenSAC, <ref type="bibr" target="#b8">(Chum et al., 2005)</ref>) checks for degenerate model configurations in RANSAC and obtains better results than the standalone RANSAC.</p><p>Furthermore, the computational power and memory availability of recent hardware greatly increased in the last decade. On this basis, HarrisZ + adds incrementally the following updates and upgrades with respect to HarrisZ:</p><p>1. In the original HarrisZ, keypoints are scored first by their decreasing scale indexes i and then by H ? so as to extract the most robust keypoint since higher scales imply wider patch support regions. Nevertheless, the keypoint localization accuracy at high scales degrades. The potential loss in the discriminative power of the local keypoint can be compensated by the choice of a better keypoint descriptor, so HarrisZ + scores keypoints first by H ? and then by the decreasing scale index i. Furthermore, the First Geometric Inconsistent Nearest Neighbor (FGINN, <ref type="bibr" target="#b23">Mishkin et al. (2015)</ref>) is employed instead of NNR for matching, since FGINN can handle better <ref type="bibr" target="#b16">Jin et al. (2021)</ref> than NNR matches in case of spatially close keypoints, which are promoted by the new keypoint ranking. Specifically, if a keypoint k 1 on an image I 1 matches with a keypoint k 2 on the other image I 2 , this means that their descriptor distance is the minimum d among the distances of remaining keypoints in I 2 with k 1 . In order to filter out unreliable matches, NNR normalizes d by the minimum distance d between k 1 and the remaining keypoints in I 2 excluding k 2 and thresholds this ratio at a value typically in [0.8, 0.95]. This greatly increases precision among the tentative matches, but often reduces recall by filtering out correct matches. FGINN restricts the minimum d only to keypoints far more than 10 px with respect to k 2 , so closely detected points do not hurt each others. 2. For ranking keypoints in the case where a constraint on their maximum number has been imposed, a strategy inspired by the greedy local maxima selection of Sec. 3.1 is employed to better distribute the corners over the images, avoiding image regions with clusters of keypoints which can reasonably be associated to the same true corners. If the input image is of size m ? n px and a constraint of having no more than k keypoints is imposed, the minimum required distance q between two keypoints is set as the diameter of a circle with area mn k/2 , i.e. q = (2 3 mn)</p><formula xml:id="formula_12">1 2 /?k<label>(14)</label></formula><p>Keypoints are then sorted as indicated above, and the greedy local maxima selection using q as distance threshold is performed. If less than k keypoints survive, the process is repeated again on the discarded keypoints. Notice that only roughly k 2 keypoints are expected to be selected in the first iteration, so it is more likely that in the second iteration some keypoints close to those found in the first iteration will be chosen. According to preliminary evaluations, this strategy leads to better matching results, probably due to the fact of allowing to some extent closer keypoint, which generally have some variation within their descriptors, introduces a further chance to match the right descriptors. 3. The scale index i for ? in Eq. 13 is set to 0 ? i ? 4.</p><p>The original scale index range 3 ? i ? 8 was chosen to limit to about 2000 keypoints or less the detector output for common input image resolution (1024?768 px), but also to extract the most robust keypoints, since higher scales imply wider patch support regions, but at the expense of the keypoint localization accuracy. The new range can extract more than 8000 keypoints, which according to recent evaluations greatly increase the matching power <ref type="bibr" target="#b16">(Jin et al., 2021)</ref>. Moreover, at finer scale the keypoint localization accuracy is improved. Again, the potential loss in the of discriminative power of the patches is compensated by the choice a better keypoint descriptor. 4. For color images, the original HarrisZ detector uses the common approach of performing a grayscale conversion of the input using only the luminance channel <ref type="bibr" target="#b13">(Gonzalez and Woods, 2008)</ref>. Another popular choice for graylevel conversion is the use of the value channel of the HSV decomposition, corresponding to take the maximum value over the RGB channels. This further grayscale conversion can highlight different image structures, including edges. According to this observation, the edge mask M ? in Eq. 5 is computed from further enhanced gradient derivatives I x and I y of I which take into account both the luminance L and value V channels in the grayscale conversion of the input image I. Specifically, the base horizontal derivatives of L and V are computed as for Eq. 1</p><formula xml:id="formula_13">L x = L * K and V x = V * K<label>(15)</label></formula><p>so that the maximum absolute gradient value at each pixel location x is chosen</p><formula xml:id="formula_14">I x (x) = L x (x) if |L x (x)| &gt; |V x (x)| V x (x) otherwise<label>(16)</label></formula><p>and likewise for I y from the vertical derivatives L y and V y . As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, I x and I y produce a better edge mask M ? but, according to our preliminary experiments, they should not be used for computing the filter response H ? since they lead to worse results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>In the actual setup, keypoint patches at scale index i = 0 will get a support region of 25 ? 25 px, while HardNet descriptor input patches are 32 ? 32 px <ref type="bibr" target="#b22">(Mishchuk et al., 2017)</ref>. Upsampling would be required in this case, which can introduce noise and spurious details affecting the descriptor robustness. In order to avoid patch upsampling, the final scale associated to the keypoints extracted at i = 0 at the end of the computation is replaced with the scale value associated to i = 1, which provides 35 ? 35 px patches. Patch downsampling does not negatively affect the HardNet descriptor. 6. As keypoints for i = 0, 1 get the same final scale ?,</p><p>i.e. both scale indexes output 35 ? 35 px patches, local maxima on their union is considering following again the greedy approach with a distance of 1 px. This avoids almost-duplicated keypoint patches. 7. Before being processed as described in Sec. 3.1, the input image for scales with indexes i = 0, 1 is doubled using Lanczos interpolation so as the scale value ? to reflect the image size adjustment. Extracted keypoints are then reported back to the original reference system by halving the coordinate and scale values. This modification aims at introducing sub-pixel quality details and hence to finer characterizing keypoints, also under the observation that discretized Gaussian kernels with standard deviations around 1 pixel for practical purposes provide basically the same convolution output (? = 1, 1.4 px for i = 0, 1). Besides Lanczos, the bilinear and bicubic interpolations were considered, but they provided worse results. <ref type="figure">Figure 2</ref> shows the differences between HarrisZ and HarrisZ + , underling the relations between scales and localization accuracy, as well as the different distribution of the corners on the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HarrisZ</head><p>HarrisZ + <ref type="figure">Fig. 2</ref>: Comparison between HarrisZ and HarrisZ + on example images, for each keypoint the ellipses associated to the affine transformation determined by ? ? is shown in green (see text for details, best viewed in color and zoomed in).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>The evaluation relies on the IMC and SimLocMatch contests. The IMC benchmark <ref type="bibr" target="#b16">(Jin et al., 2021)</ref> evaluates HarrisZ + as the upstream module of the image matching pipeline steps i-v described in Sec. 3.2, considering the camera pose error obtained on different scenes. In detail, the error is defined in terms of the mean Average Accuracy (mAA), which takes into account the error achieved at the end of the matching pipeline after the stereo or multiview camera pose estimation. mAA is computed by integrating the maximum angular error between the rotation and translation vectors of the final estimated fundamental matrix up to a threshold of 10 ? for each tested im-age pairs of the scene. For this aim, reliable pseudo groundtruths obtained by supersets of the image sequences are employed. mAA correlates well with the matching score, but not with repeatability, as the latter is sensitive to the number of keypoints and does not generally agree with the desired and expected results. The IMC benchmark datasets are Phototourism, PragueParks and GoogleUrban, all made up of real images and including validation and test sets. Overall, the complexity from one dataset to another is increasing: each Phototourism scene consists of a main large-scale foreground object almost taken in frontal position, PragueParks includes small-scale scenes with multiple complex objects and higher camera pose variations, and GoogleUrban presents challenge middle-scale urban scene environments with a low level of image overlap. The test sets of the IMC datasets contain respectively 10, 3 and 17 image sequences. Except GoogleUrban, where each image sequence contains 64 or 75 images, other datasets include 100 images for each sequence. Besides the three Phototourism validation sequences employed to check and adjust HarrisZ + steps, the other validation sets were only used to setup the best Degen-SAC threshold and not to fine tune any network parameter in the pipeline, unlike other competitors. Lastly, following the IMC protocol, evaluation is done by restricting the maximum number of allowed keypoints to 2048 (2K) and 8000 (8K).</p><p>Differently from IMC, the SimLocMatch benchmark relies on rendered images from synthetic scenes. In this way not only the poses are available for validating the results, but also the exact depth maps so that the evaluation can be directly focused on matches instead of using indirectly the camera pose estimation. The SimLocMatch dataset includes seven scenes representing man-made outdoor and indoor environments for a total of about 10000 images, with different simulated lighting conditions. In the evaluation, matches from roughly 80000 image pairs are checked and, unlike IMC, these include negative image pairs, i.e. with no matches between the images. As evaluation criterion, the SimLocMatch benchmark considers the matching score and the number of wrong matches per image pair, respectively having higher and lower values in the case of better methods.</p><p>Notice that both benchmarks only consider almost upright images, i.e. with no relevant relative rotation, which is the most common user application scenario and represents the standard training data for deep architecture. As rotation invariance decrease matchability, the former is disabled for a fair comparison in all methods.</p><p>The Matlab code for both HarrisZ and HarrisZ + corner detectors is freely available to download, as well as the full image matching pipeline implementation 5 which makes use of the code available from the respective authors or through the Kornia library <ref type="bibr" target="#b28">(Riba et al., 2020)</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the results on the IMC2021 Phototourism validation set obtained by incrementally incorporating the additions presented in Sec. 3.2 to get HarrisZ + from HarrisZ, the final 5 https://sites.google.com/view/fbellavia/ HarrisZ + corresponds to the last row. For this evaluation, the successive steps of the pipeline consider the AffNet for patch extraction, HardNet8 for computing the descriptors and Degen-SAC to estimate the pose (listed as i, ii and v in Sec. 3.2). Spatial local filtering (iii and iv) is excluded to better evaluate the keypoint extraction process. This "ablation study" only considers the stereo setup, excluding multiview pose optimization by bundle adjustment with COLMAP <ref type="bibr" target="#b31">(Sch?nberger and Frahm, 2016)</ref>. Each addition from HarrisZ can be applied independently with moderate improvements, however only when taken altogether the proposed changes get an overall significant gain. Specifically, according to the results, better localized keypoints provided by lower scales (listed as 1 and 3 in Sec. 3.2) are more critical in 8K setup. On the contrary, when the keypoint budget is reduced to 2K, providing an uniform distribution of keypoints among the image (2) becomes relevant. The remaining extensions (4-7) serve mainly as refinements and although each of them only introduces relatively small improvements, the final mAA increase globally of roughly 0.1 and 0.05 respectively for the 2K and 8K setups.  <ref type="table" target="#tab_1">Table 2</ref> reports the full mAA results for both the stereo and multiview setups of the last IMC2021, which adds two further datasets with respect to the Phototourism one presented in the previous IMC2020. The HarrisZ + pipeline including also blob matching plus DTM spatial filtering (iii in Sec. 3.2) with or without AdaLAM (iv) is evaluated. All the pipelines include DegenSAC (v) as final step, followed by bundle adjustment in case of the multiview setup. The standard upright SIFT matching is included for reference, while the pipeline that replaces HarrisZ + with the unchained SIFT, i.e. the DoG detector, obtained results among the state-of-the-art in the previous IMC2020 together with DISK. The remaining entries obtained the best results in IMC2021 and employ in addition learned matching strategies. Among the non-learned matching approaches, the pipelines including HarrisZ + provide the best results on each dataset with the exception of Phototourism, were results are almost aligned, with some advantages of DISK on the 2K keypoint restriction. Among the compared handcrafted keypoint detectors, HarrisZ + obtains the best results followed by DoG and then SIFT, implying the goodness of the proposed approach, while adding spatial filtering to the pipeline seems to only improve the results in the multiview setup. Ex-  <ref type="figure">Figure 3</ref> reports the SimLocMatch results. HarrisZ + and DoG are limited to 8K keypoints, while the COrrespondence TRansformer (COTR, <ref type="bibr" target="#b15">Jiang et al. (2021)</ref>) and the Probabilistic Dense Correspondence Network (PDC-Net, <ref type="bibr" target="#b36">Truong et al. (2021)</ref>) are included for completeness. Confirming the IMC2021 results, HarriZ + pipeline completed with spatial local match filtering is the only pipeline able to obtain results very close to those of the approaches employing matching networks based on transformers, i.e. LoFTR and (SuperPoint and Disk) + SuperGlue. In particular, it can be noted that besides achieving a high matching score, also the number of wrongly output matches, normalized by the number of inliers for visualization purposes, is low for HarrisZ + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Concerning running times, non-optimized HarrisZ + Matlab code requires on average one second to run on a Intel i9-10900K system with 64 GB RAM and no GPU in case of images with resolutions going from 640 ? 480 to 1900 ? 1200 px, which is roughly five times the original HarrisZ runtime. The bottleneck is the doubled image size for the first two scale indexes. Moreover, HarrisZ + keypoint uniform ranking is a global process performed at the end of the computation, which can strongly accentuate the computational times in case of a high number of candidate keypoints, as for high-textured images. Nevertheless, these running times are still reasonable for off-line applications.</p><p>As final consideration, classical modular pipeline design, as that HarrisZ + was designed for, can still offer practical advantages with respect to modern end-to-end deep design for specific applications in terms computational cost or tuning. Adding rotation invariance to the HarrisZ + pipeline only requires to simply plug OriNet <ref type="bibr" target="#b24">(Mishkin et al., 2018)</ref> or similar modules before AffNet, while both SuperGlue and LoFTR would require retraining and maybe to add some layers. In any case retrain of a whole end-to-end matching network needs a more expensive system configuration than that mentioned above (with the inclusion of a consumer-grade GPU). On this system, HarrisZ + pipeline was also tested to work with high resolution images up to 9000 ? 6732 px, such those of aerial photogrammetric surveys, while LoFTR was unable to process 2000 ? 1500 px input images on a consumer-grade PC either on GPU or CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>This paper presents HarrisZ + corner detector as an upgrade of HarrisZ to be employed with next-gen image matching pipeline. By introducing several changes, which by themselves only slightly affect the final output, HarrisZ + is able to achieve stateof-the-art results among handcrafted keypoint detectors, providing the basis of competitive image pipelines with respect to the recent end-to-end deep architectures exploiting coarse-tofine matching strategies thorough transformers. The key idea that guided the HarrisZ + design is the need of a synergic optimization between the different modules of the pipeline to globally improve the final results, so as to compensate weakness and boost strength points of the whole pipeline.</p><p>It is also authors' opinion that research on classical matching methods is still appropriate and effective despite of the overwhelming presence of deep methods. As a further contribution of this paper, on one hand the results achieved through HarrisZ + shows that there is still a worthwhile margin of improvement in classic research direction, and on the other hand the gained insight during the development can be employed in turn to design better deep networks as well as to provide more valid training data.</p><p>Future work will includes the introduction of handcrafted a coarse-to-fine matching at the end of the pipeline with further optimizations of HarrisZ + to better adapt to these changes and to improve the computational efficiency, investigations about the possibility to apply HarrisZ + -like selection to DoG keypoints and about using HarrisZ + keypoints with deep matching architectures based on transformers, such as SuperGlue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Computation of the mask M ? for a color image I. Mask portions retained in any case are in yellow, while mask regions considered only with or without the integration of the HSV value channel are respectively in green and red (see text for details, best viewed in color and zoomed in).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Incremental additions towards HarrisZ + and corresponding mAA@10 ? on IMC2021 Phototourism validation set. The # column refers to the last change described in Sec. 3.1 sequentially included from the original HarrisZ, the final HarrisZ + corresponds to the last row (see text for details)</figDesc><table><row><cell>Incremental additions</cell><cell>#</cell><cell cols="2">mAA@10 ?</cell></row><row><cell></cell><cell></cell><cell>2K</cell><cell>8K</cell></row><row><cell>Original HarrisZ</cell><cell cols="3">-0.4619 0.5980</cell></row><row><cell>H ? ranking + FGINN</cell><cell cols="3">1 0.4955 0.6209</cell></row><row><cell>Uniform distribution of corners</cell><cell cols="3">2 0.5335 0.6203</cell></row><row><cell>Scale indexes i = 0, ..., 4</cell><cell cols="3">3 0.5410 0.6422</cell></row><row><cell>Color image mask M ?</cell><cell cols="3">4 0.5390 0.6515</cell></row><row><cell>Output scale ? adjustment for i = 0</cell><cell cols="3">5 0.5493 0.6520</cell></row><row><cell>Duplicates removal for i = 0, 1</cell><cell cols="3">6 0.5636 0.6522</cell></row><row><cell cols="4">Doubling input image size for i = 0, 1 7 0.5681 0.6565</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>mAA@10 ? results on IMC2021 (see text for details). AffNet + HardNet8 0.4753 0.5578 0.6358 0.7368 0.3252 0.3567 0.4788 0.5504 + Blob + DTM 0.4449 0.5515 0.6176 0.7387 0.2981 0.3343 0.4535 0.5415 + Blob + DTM + AdaLAM 0.4369 0.5521 0.5918 0.7423 0.2963 0.3327 0.4417 0.5424GoogleUrban dataset and the Phototourism stereo setup, when comparing the pipelines based on HarrisZ + with respect to the best ones relying on learned matching strategies, results are quite comparable. The performance differences are mostly noticeable within the most challenging GoogleUrban dataset, and are probably due to the ability of these architectures to employ, more or less implicitly, a coarse-to-fine image matching strategy, missing in other approaches.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Phototourism</cell><cell cols="2">PragueParks</cell><cell cols="2">GoogleUrban</cell><cell></cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell>2K</cell><cell>8K</cell><cell>2K</cell><cell>8K</cell><cell>2K</cell><cell>8K</cell><cell>2K</cell><cell>8K</cell></row><row><cell>Stereo</cell><cell>HarrisZ + + Upright SIFT DoG + AffNet + HardNet8</cell><cell cols="8">0.3827 0.5122 0.4136 0.5369 0.2542 0.2693 0.3501 0.4394 n/a 0.5573 n/a 0.5977 n/a 0.3009 n/a 0.4853</cell></row><row><cell></cell><cell>DISK</cell><cell cols="8">0.5121 0.5583 0.4589 0.5622 0.2763 0.3295 0.4157 0.4833</cell></row><row><cell></cell><cell>LoFTR</cell><cell>n/a</cell><cell>0.6090</cell><cell>n/a</cell><cell>0.7546</cell><cell>n/a</cell><cell>0.4060</cell><cell>n/a</cell><cell>0.5898</cell></row><row><cell></cell><cell>(SuperPoint and DISK) + SuperGlue</cell><cell>n/a</cell><cell>0.6397</cell><cell>n/a</cell><cell>0.8070</cell><cell>n/a</cell><cell>0.4395</cell><cell>n/a</cell><cell>0.6285</cell></row><row><cell></cell><cell>HarrisZ + + AffNet + HardNet8</cell><cell cols="8">0.6786 0.7367 0.4676 0.4729 0.1602 0.2025 0.4354 0.4707</cell></row><row><cell></cell><cell>+ Blob + DTM</cell><cell cols="8">0.7046 0.7606 0.4583 0.4843 0.1625 0.2060 0.4418 0.4836</cell></row><row><cell>Multiview</cell><cell>+ Blob + DTM + AdaLAM Upright SIFT DoG + AffNet + HardNet8 DISK</cell><cell cols="8">0.7132 0.7580 0.4618 0.4711 0.1521 0.2084 0.4423 0.4791 0.5545 0.6850 0.3608 0.4810 0.0520 0.0902 0.3224 0.4187 n/a 0.7269 n/a 0.4670 n/a 0.1643 n/a 0.4527 0.7296 0.7445 0.4357 0.4590 0.1275 0.1833 0.4309 0.4623</cell></row><row><cell></cell><cell>LoFTR</cell><cell>n/a</cell><cell>0.7610</cell><cell>n/a</cell><cell>0.4712</cell><cell>n/a</cell><cell>0.3023</cell><cell>n/a</cell><cell>0.5115</cell></row><row><cell></cell><cell>(SuperPoint and DISK) + SuperGlue</cell><cell>n/a</cell><cell>0.7857</cell><cell>n/a</cell><cell>0.4988</cell><cell>n/a</cell><cell>0.3374</cell><cell>n/a</cell><cell>0.5406</cell></row><row><cell>cluding the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">patch orientation is not taken into account since only almost aligned images are considered, see Sec.4</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">KAZE features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Key.Net: Keypoint detection by handcrafted and learned CNN filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barroso-Laguna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SURF: Speeded up robust features. Computer Vision and Image Understanding 110</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rotationally invariant image operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beaudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2022. SIFT matching by context exposed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bellavia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving Harris corner selection strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bellavia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tegolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valenti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IET Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-image matching using multiscale oriented patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AdaLAM: Revisiting handcrafted outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-View Geometry Estimation Unaffected by a Dominant Plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SuperPoint: Selfsupervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A feature based correspondence algorithm for image matching. International Archives of Photogrammetry and Remote Sensing 26</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>F?rstner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Alvey Vision Conference</title>
		<meeting>the Alvey Vision Conference</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">COTR: Correspondence transformer for matching across images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image matching across wide baselines: From paper to practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comprehensive evaluation of image enhancement for unsupervised image description and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IET Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scale-Space Theory in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the British Machine Vision Conference (BMVC)</title>
		<meeting>eeding of the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Working Hard to Know Your Neighbor&apos;s Margins: Local Descriptor Learning Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of the Conf. on Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MODS: Fast and robust method for two-view matching. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ASIFT: A new framework for fully affine invariant image comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving the HardNet descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pultar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">9699</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv ePrint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Winter Conf. on Applications of Computer Vision (WACV)</title>
		<meeting>of the IEEE/CVF Winter Conf. on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster and better: A machine learning approach to corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SuperGlue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structure-from-Motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LoFTR: Detector-free local feature matching with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Computer Vision: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">D2D: Keypoint extraction with describe to detect approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Laguna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PDC-Net: Learning accurate dense correspondences and when to trust them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Local invariant feature detectors: a survey. Foundations and Trends in Computer Graphics and Vision 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DISK: Learning local features with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Tyszkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">TILDE: A temporally invariant learned detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LIFT: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Corner detection using multi-directional structure tensor with multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
