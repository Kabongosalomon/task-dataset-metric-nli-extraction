<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Intel AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present PartNet: a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. We benchmark four state-ofthe-art 3D deep learning algorithms for fine-grained semantic segmentation and three baseline methods for hierarchical semantic segmentation. We also propose a novel method for part instance segmentation and demonstrate its superior performance over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Being able to parse objects into parts is critical for humans to understand and interact with the world. People recognize, categorize, and organize objects based on the knowledge of their parts <ref type="bibr" target="#b7">[8]</ref>. Many actions that people take in the real world require detection of parts and reasoning over parts. For instance, we open doors using doorknobs and pull out drawers by grasping their handles. Teaching machines to analyze parts is thus essential for many vision, graphics, and robotics applications, such as predicting object functionality <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, human-object interactions <ref type="bibr" target="#b15">[16]</ref>, simulation <ref type="bibr" target="#b17">[18]</ref>, shape editing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14]</ref>, and shape generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>To enable part-level object understanding by learning approaches, 3D data with part annotations are in high demand. Many cutting-edge learning algorithms, especially for 3D understanding <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b28">29]</ref>, intuitive physics <ref type="bibr" target="#b24">[25]</ref>, and reinforcement learning <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b27">28]</ref>, require such data to train the networks and benchmark the performances. Researchers are also increasingly interested in synthesizing dynamic data through physical simulation engines <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b27">28]</ref>. Creation of large-scale animatable scenes will require a large amount of 3D data with affordances and mobility information. Object parts serve as a critical stepping stone to access this information. Thus it is necessary to have a big 3D object dataset with part annotation. With the availability of the existing 3D shape datasets with part annotations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">43]</ref>, we witness increasing research interests and advances in 3D part-level object understanding. Recently, a variety of learning methods have been proposed to push the state-of-the-art for 3D shape segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">21]</ref>. However, existing datasets only provide part annotations on relatively small numbers of object instances <ref type="bibr" target="#b4">[5]</ref>, or on coarse yet non-hierarchical part annotations <ref type="bibr" target="#b43">[43]</ref>, restricting the applications that involves understanding fine-grained and hierarchical shape segmentation.</p><p>In this paper, we introduce PartNet: a consistent, largescale dataset on top of ShapeNet <ref type="bibr" target="#b2">[3]</ref> with fine-grained, hierarchical, instance-level 3D part information. Collecting such fine-grained and hierarchical segmentation is challenging. The boundary between fine-grained part concepts are more obscure than defining coarse parts. Thus, we define a common set of part concepts by carefully examining the 3D objects to annotate, balancing over several criteria: welldefined, consistent, compact, hierarchical, atomic and complete. Shape segmentation involves multiple levels of granularity. Coarse parts describe more global semantics and fine-grained parts convey richer geometric and semantic details. We organize expert-defined part concepts in hierarchical segmentation templates to guide annotation. PartNet provides a large-scale benchmark for many partlevel object understanding tasks. In this paper, we focus on three fundamental and challenging shape segmentation tasks: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. We benchmark four state-of-the-art algorithms on fine-grained semantic segmentation and propose three baseline methods for hierarchical semantic segmentation. We propose the task of part instance segmentation using PartNet. By taking advantages of rich shape structures, we propose a novel method that outperforms the existing baseline algorithm by a clear margin.</p><p>PartNet contains highly structured, fine-grained and heterogeneous parts. Our experiments reveals that existing algorithms developed for coarse and homogeneous part understanding cannot work well on PartNet. First, small and fine-grained parts, e.g. door handles and keyboard buttons, are abundant and present new challenges for part recognition. Second, many geometrically similar but semantically different parts requires more global shape context to distinguish. Third, understanding the heterogeneous variation of shapes and parts necessitate hierarchical understanding. We expect that PartNet could serve as a better platform for part-level object understanding in the next few years.</p><p>In summary, we make the following contributions:</p><p>? We introduce PartNet, consisting of 573,585 finegrained part annotations for 26,671 shapes across 24 object categories. To the best of our knowledge, it is the first large-scale dataset with fine-grained, hierarchical, instance-level part annotations;</p><p>? We propose three part-level object understanding tasks to demonstrate the usefulness of this data: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation.</p><p>? We benchmark four state-of-the-art algorithms for semantic segmentation and three baseline methods for hierarchical segmentation using PartNet;</p><p>? We propose the task of part instance segmentation on PartNet and describe a novel method that outperforms the existing baseline method by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Understanding shape parts is a long-standing problem in computer vision and graphics. Lacking large-scale annotated datasets, early research efforts evaluated algorithm results qualitatively and conducted quantitative comparison on small sets of 3D models. Attene et al. <ref type="bibr" target="#b0">[1]</ref> compared 5 mesh segmentation algorithms using 11 3D surface meshes and presented side-by-side qualitative comparison. Chen et al. <ref type="bibr" target="#b4">[5]</ref> collected 380 surface meshes from 19 object categories with instance-level part decomposition for each shape and proposed quantitative evaluation metrics for shape segmentation. Concurrently, Benhabiles et al. <ref type="bibr" target="#b1">[2]</ref> proposed similar evaluation criteria and methodology. Kalogerakis et al. <ref type="bibr" target="#b14">[15]</ref> further assigned semantic labels to the segmented components. Shape cosegmentation benchmarks <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b8">9]</ref> were proposed to study co-segmentation among similar shapes.</p><p>Recent advances in deep learning have demonstrated the power and efficiency of data-driven methods on 3D shape understanding tasks such as classification, segmentation and generation. ShapeNet <ref type="bibr" target="#b2">[3]</ref> collected a large-scale synthetic 3D CAD models from online open-sourced 3D repositories, including more than 3,000,000 models and 3,135 object categories. Yi et al. <ref type="bibr" target="#b43">[43]</ref> took an active learning approach to annotate the ShapeNet models with semantic segmentation for 31,963 shapes covering 16 object categories. In their dataset, each object is usually decomposed into 2?5 coarse semantic parts. PartNet provides more fine-grained part annotations that contains 18 parts per shape on average.</p><p>Many recent works studied fine-grained and hierarchical shape segmentation. Yi et al. <ref type="bibr" target="#b42">[42]</ref> leveraged the noisy part decomposition inputs in the CAD model designs and trained per-category models to learn consistent shape hierarchy. Chang et al. <ref type="bibr" target="#b3">[4]</ref> collected 27,477 part instances from 2,278 models covering 90 object categories and studied the part properties related to language. Wang et al. <ref type="bibr" target="#b35">[35]</ref> proposed multi-component labeling benchmark containing  1,016 3D models from ShapeNet <ref type="bibr" target="#b2">[3]</ref> from 10 object categories with manually annotated fine-grained level part semantics and studied to learn neural networks for grouping and labeling fine-grained part components. PartNet proposes a large-scale dataset with 573,585 fine-grained and hierarchical shape part annotations covering 26,671 models from 24 object categories. There are also many previous works that attempted to understand parts by their functionality and articulation. Hu et al. <ref type="bibr" target="#b10">[11]</ref> constructed a dataset of 608 objects from 15 object categories annotated with the object functionality and introduced a co-analysis method to learns category-wise object functionality. Hu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a dataset of 368 mobility units with diverse types of articulation and learned to predict part mobility information from a single static segmented 3D mesh. In PartNet, we assign consistent semantic labels that entail such functionality and articulation information for part components within each object category, which potentially makes PartNet support such research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Annotation</head><p>The data annotation is performed in a hierarchical manner. Expert-defined hierarchical part templates are provided to guarantee labeling consistency among multiple annotators. We design a single-thread question-answering 3D GUI to guide the annotation. We hire 66 professional annotators and train them for the annotation. The average annotation time per shape is 8 minutes, and at least one pass of verification is performed for each annotation to ensure accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Expert-Defined Part Hierarchy</head><p>Shape segmentation naturally involves hierarchical understanding. People understand shapes at different segmentation granularity. Coarse parts convey global semantics while fine-grained parts provide more detailed understanding. Moreover, fine-grained part concepts are more obscure to define than coarse parts. Different annotators have different knowledge and background so that they may name parts differently when using free-form annotation <ref type="bibr" target="#b3">[4]</ref>. To address the issues, we introduce And-Or-Graph-style hierarchical templates and collect part annotations according to the pre-defined templates.</p><p>Due to the lack of well-acknowledged rules of thumb to define good templates, the task of designing hierarchical part templates for a category becomes a non-trivial task. Furthermore, the requirement for the designed template to cover all variations of shapes and parts, makes the problem more challenging. Below we summarize the criteria that we use to guide our template design:</p><p>? Well-defined: Part concepts are well-delineated such that parts are identifiable by multiple annotators; ? Consistent: Part concepts are shared and reused across different parts, shapes and object categories; ? Compact: There is no unnecessary part concept and part concepts are reused when it is possible; ? Hierarchical: Part concepts are organized in a taxonomy to cover both coarse and fine-grained parts; ? Atomic: Leaf nodes in the part taxonomy consist of primitive, non-decomposable shapes; <ref type="figure">Figure 3</ref>. We show the expert-defined hierarchical template for lamp (middle) and the instantiations for a table lamp (left) and a ceiling lamp (right). The And-nodes are drawn in solid lines and Or-nodes in dash lines. The template is deep and comprehensive to cover structurally different types of lamps. In the meantime, the same part concepts, such as light bulb and lamp shade, are shared across the different types.</p><p>? Complete: The part taxonomy covers a heterogeneous variety of shapes as completely as possible. Guided by these general principles, we build an And-Or-Graph-style part template for each object category. The templates are defined by experts after examining a broad variety of objects in the category. Each template is designed in a hierarchical manner from the coarse semantic parts to the fine-grained primitive-level components. <ref type="figure">Figure 3</ref> (middle) shows the lamp template. And-nodes segment a part into small subcomponents. Or-nodes indicate subcategorization for the current part. The combination of And-nodes and Ornodes allows us to cover structurally different shapes using the same template while sharing as much common part labels as possible. As in <ref type="figure">Figure 3</ref> (left) and (right), both table lamps and ceiling lamps are explained by the same template through the first-level Or-node for lamp types.</p><p>Despite the depth and comprehensiveness of these templates, it is still impossible to cover all cases. Thus, we allow our annotators to improve upon the structure of the template and to annotate parts that are out of the scope of our definition. We also conduct template refinements to resolve part ambiguity after we obtain the data annotation according to the original templates. To systematically identify ambiguities, we reserve a subset of shapes from each class and collect multiple human annotations for each shape. We compute the confusion matrix among different annotators and address data inconsistencies. For example, we merge two concepts with high confusion scores or remove a part if it is frequently segmented in the wrong way. We provide more details about this in the supplementary material. Based on the template hierarchy, the annotation process is designed to be a single-thread question-answering workflow, traversing the template graph in a depth-first manner, as shown in <ref type="figure" target="#fig_1">Figure 4</ref> (b). Starting from the root node, the annotator is asked a sequence of questions. The answers automatically construct the final hierarchical segmentation for the current shape instance. For each question, the annotator is asked to mark the number of subparts (And-node) or pick one among all subtypes (Or-node) for a given part. For each leaf node part, the annotator annotates the part geometry in the 3D interface. To help them understand the part definition and specification, we provide rich textual definitions and visual examples for each part. In addition, our interface supports cross-section and visibility control to annotate the interior structure of a 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Annotation Interface</head><p>The collected 3D CAD models often include original mesh subgroups and part information. Some of the grouping information is detailed enough to determine the final segmentation we need. Considering this, we provide the annotators with the original groupings at the beginning of the annotation, to speed up annotation. The annotators can simply select multiple predefined pieces to form a part of the final segmentation. We also provide mesh cutting tools to split large pieces into smaller ones following <ref type="bibr" target="#b4">[5]</ref>, when the original groupings are coarser than the desired segmentation, as shown in <ref type="figure" target="#fig_1">Figure 4</ref> (c). The annotators draw boundary lines on the remeshed watertight surface <ref type="bibr" target="#b12">[13]</ref> and the mesh cutting algorithm automatically splits the mesh into multiple smaller subcomponents.</p><p>In contrast to prior work, our UI is designed for operating directly on 3D models and collecting fine-grained and hierarchical part instances. Compared to Yi et al. <ref type="bibr" target="#b43">[43]</ref> where the annotation is performed in 2D, our approach allows the annotators to directly annotate on the 3D shapes and thus be able to pick up more subtle part details that are hidden from 2D renderings. Chang et al. <ref type="bibr" target="#b3">[4]</ref> proposes a 3D UI that paints regions on mesh surfaces for part labeling. However, their interface is limited to existing over-segmentations on part components and does not support hierarchical annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Start</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What parts does it have?</head><p>What type is it?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotate in 3D</head><p>Go to Next Level Go to Next Level "Leaf" Node "AND" Node "OR" Node Click + Grouping</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Cutting</head><p>Is it defined in 3D model?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Preview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part Definition</head><formula xml:id="formula_0">(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PartNet Dataset</head><p>The final PartNet dataset provides fine-grained and hierarchical instance-level part segmentation annotation for 26, 671 shapes with 573, 585 part instances from 24 object categories. Most of the shapes and object categories are from ShapeNetCore <ref type="bibr" target="#b2">[3]</ref>. We supplement 3 object categories that are commonly present in indoor scenes (i.e. scissors, refrigerators, and doors) and augment 7 of the existing categories with more 3D models from 3D Warehouse 1 . <ref type="figure">Figure 2</ref> and <ref type="table" target="#tab_2">Table 2</ref> show the PartNet data and statistics. More visualization and statistics are included in supplemental material. Our templates define hierarchical segmentation with 3 depth in median and 7 maximum. In total, we annotate 573, 585 fine-grained part instances, with a median of 14 parts per shape and a maximum of 230. To study annotation consistency, we also collect a subset of 771 shapes and ask for multiple annotations per shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tasks and Benchmarks</head><p>We benchmark three part-level object understanding tasks using PartNet: fine-grained semantic segmentation, hierarchical semantic segmentation and instance segmentation. Four state-of-the-art algorithms for semantic segmentation are evaluated and three baseline methods are proposed for hierarchical segmentation. Moreover, we propose a novel method for instance segmentation that outperforms the existing baseline method.</p><p>Data Preparation. In this section, we only consider parts that can be fully determined by their shape geometry 2 . We ignore the parts in evaluation that require additional information to identify, such as the glass parts on the cabinet doors which opacity is needed to identify, and the buttons on microwaves that texture information is desired to distinguish it from the main frame. We also remove rarely ap-peared parts from the evaluation, as the lacking of samples is insufficient for training and evaluating networks.</p><p>We sample 10, 000 points from each CAD model with furthest point sampling and use the 3D coordinates as the neural network inputs for all the experiments in the paper. The proposed dataset is split into train, validation and test sets with the ratio 70%: 10%: 20%. The shapes with multiple human annotations are not used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fine-grained Semantic Segmentation</head><p>Recent advances of 3D semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">21]</ref> have accomplished promising achievement in coarse-level segmentation on the ShapeNet Part dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">43]</ref>. However, few work focus on the fine-grained 3D semantic segmentation, due to the lack of large-scale fine-grained dataset. With the help of the proposed PartNet dataset, researchers can now work on this more challenging task with little overhead.</p><p>Fine-grained 3D semantic segmentation requires recognizing and distinguishing small and similar semantic parts. For example, door handles are usually small, 77 out of 10, 000 points on average in PartNet, but semantically important on doors. Beds have several geometrically similar parts such as side vertical bars, post bars and base legs. To recognize the subtle part details, segmentation systems need to understand them locally, through discriminative features, and globally, in the context of the whole shape.</p><p>Benchmark Algorithms. We benchmark four state-of-theart semantic segmentation algorithms on the fine-grained PartNet segmentation: PointNet <ref type="bibr" target="#b28">[29]</ref>, PointNet++ <ref type="bibr" target="#b29">[30]</ref>, Spi-derCNN <ref type="bibr" target="#b40">[40]</ref> and PointCNN <ref type="bibr" target="#b23">[24]</ref> 3 . PointNet <ref type="bibr" target="#b28">[29]</ref> takes unordered point sets as inputs and extracts features for shape classification and segmentation. To better learn local geometric features, the follow-up work PointNet++ <ref type="bibr" target="#b29">[30]</ref> proposes a hierarchical feature extraction scheme. Spider-CNN <ref type="bibr" target="#b40">[40]</ref> extends traditional convolution operations on 2D  <ref type="table">Table 3</ref>. Fine-grained semantic segmentation results (part-category mIoU %). Algorithm P, P + , S and C refer to PointNet <ref type="bibr" target="#b28">[29]</ref>, PointNet++ <ref type="bibr" target="#b29">[30]</ref>, SpiderCNN <ref type="bibr" target="#b40">[40]</ref> and PointCNN <ref type="bibr" target="#b23">[24]</ref>, respectively. The number 1, 2 and 3 refer to the three levels of segmentation: coarse-, middle-and fine-grained. We put short lines for the levels that are not defined. images to 3D point clouds by parameterizing a family of convolutional filters. To organize the unordered points into latent canonical order, PointCNN <ref type="bibr" target="#b23">[24]</ref> proposes to learn Xtransformation, and applies X -convolution operations on the canonical points.</p><p>We train the four methods on the dataset, using the default network architectures and hyperparameters described in their papers. Instead of training a single network for all object categories as done in most of these papers, we train a network for each category at each segmentation level. We input only the 3D coordinates for fair comparison <ref type="bibr" target="#b3">4</ref> and train the networks until convergence. More training details are described in the supplementary material.</p><p>Evaluation and Results. We evaluate the algorithms at three segmentation levels for each object category: coarse-, middle-and fine-grained. The coarse level approximately corresponds to the granularity in Yi et al. <ref type="bibr" target="#b43">[43]</ref>. The finegrained level refers to the segmentation down to leaf levels in the segmentation hierarchies. For structurally deep hierarchies, we define the middle level in between. Among 24 object categories, all of them have the coarse levels, while 9 have the middle levels and 17 have the fine levels. Overall, we define 50 segmentation levels for 24 object categories.</p><p>In <ref type="table">Table 3</ref>, we report the semantic segmentation performances at multiple levels of granularity on PartNet. We use the mean Intersection-over-Union (mIoU) scores as the evaluation metric. After removing unlabeled ground-truth points, for each object category, we first calculate the IoU between the predicted point set and the ground-truth point set for each semantic part category across all test shapes. Then, we average the per-part-category IoUs to compute the mIoU for the object category. We further calculate the average mIoU across different levels for each object category and finally report the average cross all object categories.</p><p>Unsurprisingly, performance for all four algorithms drop by a large margin from the coarse level to the fine-grained level. <ref type="figure" target="#fig_4">Figure 5</ref> shows qualitative results from PointCNN. The method does not perform well on small parts, such as the door handle on the door example, and visually similar parts, such as stair steps and the horizontal bars on the bed frame. How to learn discriminative features that better capture both local geometry and global context for these issues would be an interest topic for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Hierarchical Semantic Segmentation</head><p>Shape segmentation is hierarchical by its nature. From coarse semantics to fine-grained details, hierarchical understanding on 3D objects develops a holistic and comprehensive reasoning on the shape components. For this purpose, we study hierarchical semantic segmentation problem that predicts semantic part labels in the entire shape hierarchies that cover both coarse-and fine-grained part concepts.</p><p>A key problem towards hierarchical segmentation is how to leverage the rich part relationships on the given shape templates in the learning procedure. Recognizing a chair base as a finer-level swivel base significantly reduces the solution space for detecting more fine-grained parts such as central supporting bars, star-base legs and wheels. On the other hand, the lack of a chair back increases the possibility that the object is a stool. Different from Sec. 5.1 where we consider the problem at each segmentation level separately, hierarchical segmentation requires a holistic understanding on the entire part hierarchy.</p><p>Benchmark Algorithms. We propose three baseline methods to tackle hierarchical segmentation: bottom-up, topdown and ensemble. The bottom-up method considers only the leaf-node parts during training and groups the prediction of the children nodes to parent nodes as defined in the hierarchies in the bottom-up inference. The top-down method learns a multi-labeling task over all part semantic labels on the tree and conducts a top-down inference by classifying coarser-level nodes first and then finer-level ones. For the ensemble method, we train flat segmentation at multiple levels as defined in Sec. 5.1 and conduct joint inference by calculating the average log-likelihood scores over all the root-to-leaf paths on the tree. We use PointNet++ <ref type="bibr" target="#b29">[30]</ref> as the backbone network in this work, and other methods listed in Sec. 5.1 can also be used. More architecture and training details are described in the supplementary material.</p><p>Evaluation and Results. <ref type="table" target="#tab_9">Table 8</ref> demonstrates the performances of the three baseline methods. We calculate mIoU for each part category and compute the average over all the tree nodes as the evaluation metric. The experimental results show that the three methods perform similarly with small performance gaps. The ensemble method performs slightly better over the other two, especially for the categories with rich structural and sub-categorization variation, such as chair, table and clock. The bottom-up method only considers leaf-node parts in the training. Although the template structure is not directly used, the parent-node semantics of leaf nodes are implicitly encoded in the leaf-node part definitions. For example, the vertical bars for chair backs and chair arms are two different leaf nodes. The top-down method explicitly leverages the tree structures in both the training and the testing phases. However, prediction errors are accumulated through topdown inference. The ensemble method decouples the hierarchical segmentation task into individual tasks at multiple levels and performs joint inference, taking the predictions at all levels into consideration. Though demonstrating better performances, it has more hyper-parameters and requires longer training time for the multiple networks.  <ref type="figure">Figure 6</ref>. The proposed detection-by-segmentation method for instance segmentation. The network learns to predict three components: the semantic label for each point, a set of disjoint instance masks and their confidence scores for part instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Instance Segmentation</head><p>The goal of instance segmentation is to detect every individual part instance and segment it out from the context of the shape. Many applications in computer graphics, vision and robotics, including manufacturing, assembly, interaction and manipulation, require the instance-level part recognition. Compared to detecting objects from scenes, parts on objects usually have stronger and more intertwined structural relationships. The existence of many visuallysimilar but semantically-different parts makes the part detection problem challenging. To the best of our knowledge, this work is the first to provide a large-scale shape part instance-level segmentation benchmark.</p><p>Given a shape point cloud as input, the task of part instance segmentation is to provide several disjoint masks over the entire point cloud, each of which corresponds to an individual part instance on the object. We adopt the part semantics from the defined segmentation levels in Sec. 5.1. The detected masks should have no overlaps, but they together do not necessarily cover the entire point cloud, as some points may not belong to any part of interests.</p><p>Benchmark Algorithms. We propose a novel detectionby-segmentation network to address instance segmentation. We illustrate our network architecture in <ref type="figure">Figure 6</ref>. We use PointNet++ <ref type="bibr" target="#b29">[30]</ref> as the backbone network for extracting features and predicting both semantic segmentation for each point and K instance segmentation masks {? i ? [0, 1] N |i = 1, 2, ? ? ? , K} over the input point cloud of size N . Moreover, we train a separate mask? other for the points without semantic labels in the ground-truth. A softmax activation layer is applied to encourage the mutual exclusiveness among different masks so that? 1 +? 2 + ? ? ? +? K +? other = 1. To train the network, we apply Hungarian algorithm <ref type="bibr" target="#b19">[20]</ref> to find a bipartite matching M : {i ? M(i)|i = 1, 2, ? ? ? , T } between the prediction 7 <ref type="table">Table Trash</ref> Vase S1 55. <ref type="bibr" target="#b6">7</ref>   <ref type="table">Table 5</ref>. Instance segmentation results (part-category mAP %, IoU threshold 0.5). Algorithm S and O refer to SGPN <ref type="bibr" target="#b34">[34]</ref> and our proposed method respectively. The number 1, 2 and 3 refer to the three levels of segmentation: coarse-, middle-and fine-grained. We put short lines for the levels that are not defined. masks {? i |i = 1, 2, ? ? ? , K} and the ground-true masks {y i |i = 1, 2, ? ? ? , T }, and regress each prediction? M(t) to the matched ground-truth mask y t . We employ a relaxed version of IoU <ref type="bibr" target="#b18">[19]</ref> defined as IoU(p, q) = p, q /( p 1 + q 1 ? p, q ), as the metric for Hungarian algorithm. In the meanwhile, a separate branch is trained to predict confidence scores for the predicted masks {C i |i = 1, 2, ? ? ? , K}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg Bag Bed Bott Bowl Chair Clock Dish Disp Door Ear Fauc Hat Key Knife Lamp Lap Micro Mug Frid Scis Stora</head><p>The loss function is defined as L = L sem + ? ins L ins + ? other L other + ? conf L conf + ? l21 L l21 , combining five terms: a cross-entropy semantic segmentation loss L sem , an IoU loss for mask regression L ins = T i=1 IoU(? M(i) , y i ), an IoU loss for the unlabeled points L other = IoU(? other , y other ), a prediction-confidence loss</p><formula xml:id="formula_1">L conf = T i=1 (C M(i) ? IoU(? M(i)</formula><p>, y i )) 2 and a l 2,1 -norm regularization term L l21 = K i=1 ? i 2 + ? other 2 to encourage unused prediction masks to vanish <ref type="bibr" target="#b32">[32]</ref>. We use N = 10, 000, K = 200, ? ins = 1.0, ? other = 1.0, ? conf = 1.0 and ? l21 = 0.1 for all the experiments.</p><p>We compare the proposed method with SGPN <ref type="bibr" target="#b34">[34]</ref>, which learns similarity scores among all pairs of points and detect part instances by grouping points that share similar features. We follow most of the default settings and hyperparameters described in their paper. We first pre-train Point-Net++ semantic segmentation branch and then fine-tune it for improving the per-point feature similarity matrix and confidence maps. We use margin values of 1 and 2 for the double-hinge loss as suggested by the authors of <ref type="bibr" target="#b34">[34]</ref>, instead of 10 and 80 in the original paper. We feed 10,000 points to the network at a time, and use a batch-size of 32 in the pre-training and 1 in the fine-tuning.</p><p>Evaluation and Results. <ref type="table">Table 9</ref> reports the per-category mean Average Precision (mAP) scores for SPGN and our proposed method. For each object category, the mAP score calculates the AP for each semantic part category across all test shapes and averages the AP across all part categories. Finally, we take the average of the mAP scores across different levels of segmentation within each object category and then report the average over all object categories. We compute the IoU between each prediction mask and the closest ground-truth mask and regard a prediction mask as true positive when IoU is larger than 0.5.   <ref type="figure" target="#fig_6">Figure 7</ref> shows qualitative comparisons for our proposed method and SGPN. Our proposed method produces more robust and cleaner instance predictions. After learning for point features, SGPN has a post-processing stage that merges points with similar features as one component. This process involves many thresholding hyper-parameters. Even though most parameters are automatically inferred from the validation data, SPGN still suffers from predicting partial or noisy instances in the case of bad thresholding. Our proposed method learns structural priors within each object category that is more instance-aware and robust in predicting complete instances. We observe that training for a set of disjoint masks across multiple shapes gives us consistent part instances. We show the learned part correspondence in <ref type="figure" target="#fig_7">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce PartNet: a large-scale benchmark for finegrained, hierarchical, and instance-level 3D shape segmentation. It contains 573, 585 part annotations for 26, 671</p><p>ShapeNet <ref type="bibr" target="#b2">[3]</ref> models from 24 object categories. Based on the dataset, we propose three shape segmentation benchmarks: fine-grained semantic segmentation, hierarchical semantic segmentation and instance segmentation. We benchmark four state-of-the-art algorithms for semantic segmentation and propose a novel method for instance segmentation that outperforms the existing baseline method. Our dataset enables future research directions such as collecting more geometric and semantic annotation on parts, investigating shape grammars for synthesis and animating object articulation in virtual environments for robotic learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>This document provides additional dataset visualization and statistics (Sec B), hierarchical template design details and visualization (Sec C), and the architectures and training details for the three shape segmentation tasks (Sec D), to the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Dataset Visualization and Statistics</head><p>We present more visualization and statistics over the proposed PartNet dataset. <ref type="figure" target="#fig_0">Figure 13</ref> and 14 show more visualization for finegrained instance-level segmentation annotations in PartNet. We observe the complexity of the annotated segmentation and the heterogeneous variation of shapes within each object category. <ref type="figure" target="#fig_0">Figure 15, 16 and 17</ref> show more visualization for example hierarchical instance-level segmentation annotations in PartNet. We visualize the tree-structure of the hierarchical segmentation annotation with the 2D part renderings associated to the tree nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. More Fine-grained Segmentation Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. More Hierarchical Segmentation Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Shape Statistics</head><p>We report the statistics for the number of annotations, unique shapes and shapes that we collect multiple human annotations in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Part Statistics</head><p>We report the statistics for the number of part semantics for each object category in <ref type="figure" target="#fig_0">Figure 10</ref>. We also present the <ref type="figure">Figure 9</ref>. PartNet shape statistics. We report the statistics for the number of annotations, unique shapes and shapes that we collect multiple human annotations. <ref type="figure" target="#fig_0">Figure 10</ref>. PartNet part semantics statistics. We report the statistics for the number of part semantics for each object category. statistics for the maximum and median number of part instances per shape for each object category in <ref type="figure" target="#fig_0">Figure 11</ref>. We report the statistics for the maximum and median tree depth for each object category in <ref type="figure" target="#fig_0">Figure 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Template Design Details and Visualization</head><p>We provide more details and visualization for the expertdefined hierarchical templates to guide the hierarchical segmentation annotation and the template refinement procedure to resolve annotation inconsistencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Template Design Details</head><p>We design templates according to the rules of thumb that we describe in the main paper. We also consulted many on- <ref type="figure" target="#fig_0">Figure 11</ref>. PartNet part instance statistics. We report the statistics for the maximum and median number of part instances per shape for each object category. <ref type="figure" target="#fig_0">Figure 12</ref>. PartNet tree depth statistics. We report the statistics for the maximum and median tree depth for each object category. line references 5 that describe object parts (often for manufacturing and assembly) and previous works that relate language to the shapes <ref type="bibr" target="#b3">[4]</ref> as guides for the design of our template. To ensure that our templates cover most of the shape variations and part semantics of each object category, we generated a t-SNE [26] visualization for the entire shape space to study the shape variation. We trained an autoencoder based on the shape geometry within each object category to obtain shape embeddings for the t-SNE visualization.</p><p>Although we try to cover the most common part semantics in our templates, it is still not easy to cover all possible object parts. Thus, we allow annotators to deviate from the templates and define their own parts and segmentation structures. Among all the annotated part instances, 1.3% of 5 E.g. http://www.props.eric-hart.com/resources/parts-of-a-chair/.  <ref type="table">Table 6</ref>. The average confusion scores and the standard deviations for multiple annotations (%). We report the average confusion scores and the standard deviations by calculating over the entries on the diagonal of the confusion matrix for each object category using the small subset of shapes that we collect multiple human annotations. Rows O and R respectively refer to the scores before and after the template refinement process. them are defined by the annotators. In the raw annotation, 13.1% of shapes contained user-defined part labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg Bag Bed Bott Bowl Chair Clock Dish Disp Door Ear Fauc Hat Key Knife Lamp Lap Micro Mug Frid Scis Stora</head><p>Our analysis shows that our template designs are able to cover most of the ShapeNet <ref type="bibr" target="#b2">[3]</ref> shapes. Of the 27,260 shapes we collected in total, our annotators successfully labeled 26,671 of them, giving our templates a coverage rate of at least 97.8% for ShapeNet shapes. While template coverage is a potential issue, the remaining 2.2% were not annotated mainly due to other issues such as poor mesh quality, classification error, error during mesh splitting, etc.</p><p>We design hierarchical templates that cover both the coarse-level part semantics and fine-grained part details down to the primitive level, e.g. chair back vertical bar and bed base surface panel. Most primitive-level parts are atomic such that they are very unlikely to be further divided for end applications. If an application requires different segmentation hierarchy or level of segmentation than the ones we already provide in our template, developers and researchers can try to build up their own segmentation based upon the atomic primitives we obtain in PartNet.</p><p>Moreover, we try our best to make the shared part concepts among different shapes and even different object categories share the same part labels. For example, we use the part label leg for table, chair, lamp base, etc. and the part label wheel for both chair swivel base wheel and refrigerator base wheel. Such part concept sharing provides rich part correspondences within a specific object category and across multiple object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Template Refinement Details</head><p>Fine-grained shape segmentation is challenging to annotate due to the subtle concept gaps among similar part semantics. Even though we provide detailed textual and visual explanation for our pre-defined parts, we still observe some annotation inconsistencies across multiple annotators. To quantitatively diagnose such issues, we reserve a small subset of shapes for which we collect multiple human annotations. Then, we compute the confusion scores among the predefined parts across the multiple annotations and conduct careful template refinement to reduce the part ambiguity.</p><p>There are primarily three sources of such inconsistencies: boundary ambiguity, granularity ambiguity and part labeling ambiguity. Boundary ambiguity refers to the un-clear boundary between two parts, which is also commonly seen in previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">43]</ref>. For example, the boundary between the bottle neck and the bottle body is not that clear for wine bottles. Granularity ambiguity means that different annotators have different understanding about the segmentation granularity of the defined parts. One example is that, for a curvy and continuous chair arm, one can regard it as a whole piece or imagine the separation of armrest and arm support. The most common type of ambiguity in our dataset is the part labeling ambiguity. The fine-grained part concepts, though intended to be different category-wise, may apply to the same part on a given object. For example, a connecting structure between the seat and the base of a chair can be considered as chair seat support or chair base connector.</p><p>We study the mutual human agreement on the multiple annotation subset. We consider the parts defined at the leaf node level of segmentation on the hierarchy and compute the confusion matrix across multiple human annotations <ref type="bibr" target="#b5">6</ref> . The ideal confusion matrix should be close to the diagonal matrix without any part-level ambiguity. In our analysis, we observe human disagreement among some of our initial part definitions. To address the ambiguity, we either merge two similar concepts with high confusion scores or remove the hard-to-distinguish parts from evaluation. For example, we find our annotators often mix up the annotation for regular tables and desks due to the similarity in the two concepts. Thus, we merge the desk subtype into the regular table subtype to address this issue. In other cases, some small parts such as the buttons on the displays are very tricky to segment out from the main display frame. Since they may not be reliably segmented out, we decided to remove such unclear segmentation from evaluation. <ref type="table">Table 6</ref> compares the annotation consistency before and after the template refinement process. We compute the confusion matrices at the most fine-grained segmentation level. After the template refinement, the data consistency score is 83.3% on average, having 13.5% improvement over the raw annotation. The template refinement process improves the annotation consistency by a clear margin. This also reflects the complexity of the task in terms of annotating finegrained part concepts. Future works may investigate how to  <ref type="table">Table 7</ref>. Fine-grained semantic segmentation results (shape mIoU %). Algorithm P, P + , S and C refer to PointNet <ref type="bibr" target="#b28">[29]</ref>, PointNet++ <ref type="bibr" target="#b29">[30]</ref>, SpiderCNN <ref type="bibr" target="#b40">[40]</ref> and PointCNN <ref type="bibr" target="#b23">[24]</ref>, respectively. The number 1, 2 and 3 refer to the three levels of segmentation: coarse-, middleand fine-grained. We put short lines for the levels that are not defined.</p><p>further design better templates with less part ambiguities. <ref type="figure" target="#fig_0">Figure 18</ref>, 19 and 20 show more visualization for the expert-designed hierarchical templates after resolving the data inconsistency and conducting template refinements. We show the lamp template in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. More Visualization of Hierarchical Templates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tasks and Benchmarks</head><p>In this section, we provide more details about the architectures and training details for the benchmark algorithms. We also present additional evaluation metrics, shape mean Intersection-over-Union (shape mIoU) and shape mean Average-Precision (Shape mAP), and report the quantitative results using these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Fine-grained Semantic Segmentation</head><p>More Architecture and Training Details We follow the default architectures and training hyper-parameters used in the original papers: PointNet <ref type="bibr" target="#b28">[29]</ref>, PointNet++ <ref type="bibr" target="#b29">[30]</ref>, Spi-derCNN <ref type="bibr" target="#b40">[40]</ref> and PointCNN <ref type="bibr" target="#b23">[24]</ref>, except the following few modifications:</p><p>? Instead of training one network for all object categories as done in the four prior works, we train separate networks for each object category at each segmentation level. This is mainly to handle the increase in the number of parts for fine-grained part segmentation. Originally, there are only 50 parts for all 16 object categories using the coarse ShapeNet Part dataset <ref type="bibr" target="#b43">[43]</ref>. Now, using PartNet, there could be 480 different part semantics in total. Also, due to the data imbalance among different object categories, training a single network may overfit to the big categories.</p><p>? We change the input point cloud size to 10,000. The original papers usually sample 1,000, 2,000 or 4,000 points and input to the networks. PartNet suggests to use at least 10,000 to guarantee enough point sampling over small fine-grained parts, e.g. a door handle, or a small button.</p><p>? We reduce the batch sizes for training the networks if necessary. Since we use point cloud size 10,000, to fit the training in NVIDIA TITAN XP GPU 12G memory, we need to adjust the training batch size accordingly. For PointNet <ref type="bibr" target="#b28">[29]</ref>, PointNet++ <ref type="bibr" target="#b29">[30]</ref>, SpiderCNN <ref type="bibr" target="#b40">[40]</ref> and PointCNN <ref type="bibr" target="#b23">[24]</ref>, we use batch size of 24, 24, 2 and 4 respectively.</p><p>? We only input 3D coordinates as inputs to all the networks for fair comparison. Although the 3D CAD models in ShapeNet <ref type="bibr" target="#b2">[3]</ref> usually provide additional features, e.g. opacity, point normals, textures and material information, there is no guarantee for the quality of such information. Thus, we choose not to use them as the inputs. Also, only using pure geometry potentially increase the network generalizability to unseen objects or real scans <ref type="bibr" target="#b28">[29]</ref>. PointNet++ <ref type="bibr" target="#b29">[30]</ref> and Spi-derCNN <ref type="bibr" target="#b40">[40]</ref> by defaults take advantage of the point normals as additional inputs. In this paper, we remove such inputs to the networks. However, point normals can be estimated from the point clouds. We leave this as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape mIoU Metric and Results</head><p>We introduce the shape mean Intersect-over-Union (Shape mIoU) evaluation metric as a secondary metric to the Part-category mIoU metric in the main paper. Shape mIoU metric considers shapes as evaluation units and measures how an algorithm segment  an average shape in the object category. In contrast, Partcategory mIoU reports the average performance over all part semantics and indicates how an algorithm performs for any given part category. Shape mIoU is widely used on ShapeNet Part dataset <ref type="bibr" target="#b43">[43]</ref> for 3D shape coarse semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b23">24]</ref>. We propose a slightly different version for finegrained semantic segmentation. For each test shape, we first compute the IoU for each part semantics that either presents in the ground-truth or is predicted by the algorithm, and then we calculate the mean IoU for this shape. We remove the ground-truth unlabeled points from the evaluation. Finally, we calculate the Shape mIoU by averaging mIoU over all test shape instances.</p><p>We benchmark the four algorithms using Shape mIoU in <ref type="table">Table 7</ref>. Besides the Shape mIoU scores for each object category at each segmentation level, we also report the average across levels for each object categories and further calculate the average over all object categories.</p><p>We observe that PointNet++ <ref type="bibr" target="#b29">[30]</ref> achieves the best performance using the Shape mIoU metric, while PointCNN <ref type="bibr" target="#b23">[24]</ref> performs the best using the Part-category mIoU metric. The Part-category mIoU metric considers all part semantics equally while the Shape mIoU metric considers all shapes equally. We observe an unbalanced counts for different part semantics in most object categories, e.g. there are much more chair legs than chair wheels. To achieve good numbers on Part-category mIoU, a segmentation algorithm needs to perform equally well on both frequent parts and rare parts, while the Shape mIoU metric bias over the frequently observed parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Hierarchical Semantic Segmentation</head><p>We describe the architecture and training details for the three baseline methods we propose for hierarchical semantic segmentation in the main paper. All three methods use PointNet++ <ref type="bibr" target="#b29">[30]</ref> segmentation network as the network backbone. The difference of the three methods is mainly on the training and inference strategies to enforce the tree knowledge to the final prediction.</p><p>The Bottom-up Method The bottom-up method learns a network to perform segmentation at the most fine-grained leaf part semantics. We use the PointNet++ <ref type="bibr" target="#b29">[30]</ref> segmentation network with a softmax activation layer as the network architecture. At inference time, we use the ground-truth tree hierarchy to gather the prediction for the parent nodes. The parent node prediction is the sum of all its children node predictions. Even though we only train for the leaf node parts, the parent history is implicitly encoded. For example, we define vertical bars for both chair back and chair arm, but they are two different leaf node parts: chair back vertical bar and chair arm vertical bar.</p><p>In the ground-truth annotation, all the points in the point cloud belong to the root node. Each point is assigned a path of labels from the root node to some intermediate node in the tree. The paths for most points are all the way down to the leaf levels while some points may not. For example, a point on a bed blanket (removed from evaluation since it cannot be distinguished without color information) may be assigned with labels {bed, bed unit, sleeping area} in the ground-truth annotation. The part sleeping area is not a leaf part. For such cases, we introduce an additional leaf node other for each parent node in the tree and consider them in the training.</p><p>The Top-down Method The top-down method learns a multi-labeling task for all the part semantics in the tree, considering both the leaf nodes and the parent nodes. Compared to the bottom-up method, the top-down method takes advantage of the tree structures at training time.</p><p>Assuming there are T tree nodes in the hierarchy, we train a PointNet++ <ref type="bibr" target="#b29">[30]</ref> segmentation network for a T -way classification for each point. We apply a softmax activation layer to enforce label mutual exclusiveness. For a point with the ground-truth labels {y 1 , y 2 , y 3 } and prediction softmax scores {s i |i = 1, 2, ? ? ? , T }, we train the labels using a multi-labeling cross-entropy loss</p><formula xml:id="formula_2">L = ? log(s y1 ) ? log(s y2 ) ? log(s y3 )<label>(1)</label></formula><p>to increase the values of all the three label predictions over the rest labels.</p><p>The Ensemble Method The ensemble method trains multiple neural networks at different levels of segmentation as defined in the fine-grained semantic segmentation task.</p><p>The key idea is that conducting segmentation at the coarse-, middle-and fine-grained levels separately may learn different features that work the best at each level. Compared to the bottom-up method that we only train at the most finegrained level, additional signal at the coarse level helps distinguish the coarse-level part semantics more easily. For <ref type="table">Table 9</ref>. Instance segmentation results (shape mAP %, IoU threshold 0.5). Algorithm S and O refer to SGPN <ref type="bibr" target="#b34">[34]</ref> and our proposed method respectively. The number 1, 2 and 3 refer to the three levels of segmentation: coarse-, middle-and fine-grained. We put short lines for the levels that are not defined.</p><p>example, the local geometric features for both chair back vertical bars and chair arm vertical bars may be very similar, but the coarse-level semantics may distinguish chair backs and chair arms better.</p><p>During the training, we train 2?3 networks at multiple levels of segmentation. At the inference time, we perform a joint inference considering the prediction scores from all the networks. We use a path-voting strategy: for each path from the root node to the leaf node, we calculate the average loglikelihood over the network prediction scores after applying the softmax activations, and select the path with the highest score as the joint label predictions.</p><p>Shape mIoU Metric and Results Similar to Sec D.1, we define Shape mIoU for hierarchical segmentation. The mIoU for each shape is calculated over the part semantics in the entire hierarchical template that are either predicted by the network or included in the ground-truth. The unrelated parts are not taken into consideration. <ref type="table" target="#tab_9">Table 8</ref> shows the quantitative evaluation for the three baseline methods. We observe similar performance for the three methods, with the ensemble method works slightly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Instance Segmentation</head><p>More Architecture and Training Details To train our proposed method, we use batch size 32, learning rate 0.001, and the default batch normalization settings used in the PointNet++ <ref type="bibr" target="#b29">[30]</ref>.</p><p>For SGPN <ref type="bibr" target="#b34">[34]</ref>, we use two-stage training as suggested by the authors of <ref type="bibr" target="#b34">[34]</ref>. We first pretrain the PointNet++ semantic segmentation branch using batch size 32 and learning rate 0.001, with the default batch normalization as in PointNet++. And then, we jointly train for the semantic segmentation, similarity score matrix and confidence scores with batch size 1 and learning rate 0.0001. As suggested in the original SGPN paper, for the first five epochs of the joint training, we only turn on the loss for training the similarity scores matrix. The rest training epochs are done with the full losses switched on. We have to use batch size 1 because the input point cloud has the size of 10,000 and thus the similarity score matrix forms a 10, 000 ? 10, 000 ma-trix, which occupies too much GPU memory. Our proposed method is more memory-efficient, compared to SGPN. We also observe that our training is much faster than SPGN. We train all the networks until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape mAP Metric and Results</head><p>We define Shape mean Average-Precision (Shape mAP) metric as a secondary metric to the Part-category mAP metric in the main paper. Similar to the Shape mIoU scores we use in Sec D.1 and D.2, Shape mAP reports the part instance segmentation performance on an average shape in a object category. It averages across the test shapes, instead of averaging across all different part semantics, as benchmarked by Part-category mAP in the main paper.</p><p>To calculate Shape mAP for a test shape, we consider the AP for the part semantics that occur either in the groundtruth or the prediction for the given shape and compute their average as the mean AP score. Then, we average the mAP across all test shapes within a object category. <ref type="table">Table 9</ref> reports the part instance segmentation performance under the Shape mAP scores. We see a clear performance improvement of the proposed method over SGPN.   <ref type="figure" target="#fig_0">Figure 14</ref>. Fine-grained instance-level segmentation visualization (2/2). We present visualization for example fine-grained instancelevel segmentation annotations for storage furniture, keyboard, knife, laptop, lamp, microwave, mug, refrigerator, scissors, table, trash can, and vase. 19 <ref type="figure" target="#fig_0">Figure 17</ref>. Hierarchical instance-level segmentation visualization (3/3). We present visualization for example hierarchical instancelevel segmentation annotations for scissors, microwave, knife (cutting instrument), hat, bowl, bottle, mug, bag, and refrigerator. The lamp examples are shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines. 20 <ref type="figure" target="#fig_0">Figure 19</ref>. Template visualization (2/3). We present the templates for storage furniture, faucet, clock, bed, knife (cutting instrument), and trash can. The lamp template is shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines. <ref type="figure">Figure 20</ref>. Template visualization (3/3). We present the templates for earphone, bottle, scissors, door (door set), display, dishwasher, microwave, refrigerator, laptop, vase (pot), hat, bowl, bag, mug, and keyboard. The lamp template is shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chair</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>PartNet dataset and three benchmarking tasks. Left: we show example annotations at three levels of segmentation in the hierarchy. Right: we propose three fundamental and challenging segmentation tasks and establish benchmarks using PartNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 (</head><label>4</label><figDesc>a) shows our web-based annotation interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>We show our annotation interface with its components, the proposed question-answering workflow and the mesh cutting interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results for semantic segmentation. The top row shows the ground-truth and the bottom row shows the PointCNN prediction. The black points indicate unlabeled points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results for instance segmentation. Our method produces more robust and cleaner results than SGPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Learned instance correspondences. The corresponding parts are marked with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 .</head><label>13</label><figDesc>Fine-grained instance-level segmentation visualization (1/2). We present visualization for example fine-grained instance-level segmentation annotations for chair, bag, bed, bottle, bowl, clock, dishwasher, display, door, earphone, faucet, and hat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 . 18 Figure 16 .</head><label>151816</label><figDesc>Hierarchical instance-level segmentation visualization (1/3). We present visualization for example hierarchical instance-level segmentation annotations for bed, clock, storage furniture, faucet, table, and chair. The lamp examples are shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines. Hierarchical instance-level segmentation visualization (2/3). We present visualization for example hierarchical instancelevel segmentation annotations for dishwasher, laptop, display, trash can, door (door set), earphone, vase (pot), and keyboard. The lamp examples are shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison to the other shape part datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Shape</cell><cell>#Part</cell><cell cols="6">#Category Granularity Semantics Hierarchical Instance-level Consistent</cell></row><row><cell>Chen et al. [5]</cell><cell>380</cell><cell>4,300</cell><cell>19</cell><cell>Fine-grained</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row><row><cell>MCL [35]</cell><cell>1,016</cell><cell>7,537</cell><cell>10</cell><cell>Fine-grained</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Chang et al. [4]</cell><cell>2,278</cell><cell>27,477</cell><cell>90</cell><cell>Fine-grained</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Yi et al. [43]</cell><cell>31,963</cell><cell>80,323</cell><cell>16</cell><cell>Coarse</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell>PartNet (ours)</cell><cell>26,671</cell><cell>573,585</cell><cell>24</cell><cell>Fine-grained</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>PartNet dataset. We visualize example shapes with fine-grained part annotations for the 24 object categories in PartNet.</figDesc><table><row><cell>Bag</cell><cell></cell><cell>Bed</cell><cell></cell><cell>Bowl</cell><cell></cell><cell>Clock</cell><cell cols="2">Dishwasher</cell><cell>Display</cell><cell cols="2">Door</cell><cell></cell><cell>Earphone</cell><cell cols="2">Faucet</cell><cell>Hat</cell><cell>Storage Furniture</cell></row><row><cell cols="2">Keyboard</cell><cell>Knife</cell><cell></cell><cell>Laptop</cell><cell cols="3">Lamp Microwave</cell><cell></cell><cell cols="2">Mug Refrigerator</cell><cell></cell><cell>Chair</cell><cell>Scissors</cell><cell>Table</cell><cell cols="2">Trash Can</cell><cell>Vase</cell><cell>Bottle</cell></row><row><cell cols="18">Figure 2. #A 32537 186 248 519 247 8176 624 241 1005 285 285 840 287 210 514 3408 485 268 252 247 127 2639 9906 378 1160</cell></row><row><cell cols="18">#S 26671 146 212 464 208 6400 579 201 954 245 247 708 250 174 384 2271 453 212 212 207 88 2303 8309 340 1104</cell></row><row><cell cols="5">#M 771 20 18 28 20 77</cell><cell>25</cell><cell cols="6">20 26 20 19 60 19 18 57</cell><cell>64</cell><cell>20 28</cell><cell cols="3">20 20 20 34</cell><cell>91</cell><cell>19</cell><cell>28</cell></row><row><cell cols="2">#PS 480 4</cell><cell cols="2">24 12 4</cell><cell>57</cell><cell>23</cell><cell>12 8</cell><cell>8</cell><cell cols="2">15 18 8</cell><cell>3</cell><cell>16</cell><cell>83</cell><cell>8 12</cell><cell>4</cell><cell>13 5</cell><cell>36</cell><cell>82</cell><cell>15</cell><cell>10</cell></row><row><cell cols="6">#PI 573K 664 9K 2K 615 176K 4K</cell><cell cols="6">2K 7K 2K 3K 8K 1K 20K 3K</cell><cell cols="2">50K 3K 2K</cell><cell cols="4">839 2K 981 77K 177K 8K 5K</cell></row><row><cell>P med 14</cell><cell>4</cell><cell>33 5</cell><cell>2</cell><cell>19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>All Bag Bed Bott Bowl Chair Clock Dish Disp Door Ear Fauc Hat Key Knife Lamp Lap Micro Mug Frid Scis Stora Table TrashVase</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>PartNet statistics. Row #A, #S, #M respectively show the number of shape annotations, the number of distinct shape instances and the number of shapes that we collect multiple annotations. Row #PS and #PI show the number of different part semantics and part instances that we finally collect. Row Pmed and Pmax respectively indicate the median and maximum number of part instances per shape. Row Dmed and Dmax respectively indicate the median and maximum hierarchy depth per shape, with root node as depth 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Avg Bag Bed Bott Bowl Chair Clock Dish Disp Door Ear Fauc Hat Key Knife Lamp Lap Micro Mug Frid Scis StoraTable Trash Vase P1 57.9 42.5 32.0 33.8 58.0 64.6 33.2 76.0 86.8 64.4 53.2 58.6 55.9 65.6 62.2 29.7 96.5 49.4 80.0 49.6 86.4 51.9 50.5 55.2 54.7 P2 37.3 ? 20.1 ? ? 38.2 ? 55.6 ? 38.3 ? ? ? ? ? 27.0 ? 41.7 ? 35.5 ? 44.6 34.3 ? ? P3 35.6 ? 13.4 29.5 ? 27.8 28.4 48.9 76.5 30.4 33.4 47.6 ? ? 32.9 18.9 ? 37.2 ? 33.5 ? 38.0 29.0 34.8 44.4 Avg 51.2 42.5 21.8 31.7 58.0 43.5 30.8 60.2 81.7 44.4 43.3 53.1 55.9 65.6 47.6 25.2 96.5 42.8 80.0 39.5 86.4 44.8 37.9 45.0 49.6 P + 1 65.5 59.7 51.8 53.2 67.3 68.0 48.0 80.6 89.7 59.3 68.5 64.7 62.4 62.2 64.9 39.0 96.6 55.7 83.9 51.8 87.4 58.0 69.5 64.3 64.4 P + 2 44.5 ? 38.8 ? ? 43.6 ? 55.3 ? 49.3 ? ? ? ? ? 32.6 ? 48.2 ? 41.9 ? 49.6 41.1 ? ? P + 3 42.5 ? 30.3 41.4 ? 39.2 41.6 50.1 80.7 32.6 38.4 52.4 ? ? 34.1 25.3 ? 48.5 ? 36.4 ? 40.5 33.9 46.7 49.8 Avg 58.1 59.7 40.3 47.3 67.3 50.3 44.8 62.0 85.2 47.1 53.5 58.6 62.4 62.2 49.5 32.3 96.6 50.8 83.9 43.4 87.4 49.4 48.2 55.5 57.1 S1 60.4 57.2 55.5 54.5 70.6 67.4 33.3 70.4 90.6 52.6 46.2 59.8 63.9 64.9 37.6 30.2 97.0 49.2 83.6 50.4 75.6 61.9 50.0 62.9 63.8 S2 41.7 ? 40.8 ? ? 39.6 ? 36.2 32.2 ? 30.0 24.8 50.0 80.1 30.5 37.2 44.1 ? ? 22.2 19.6 ? 43.9 ? 39.1 ? 44.6 20.1 42.4 32.4 Avg 53.6 57.2 44.2 43.4 70.6 45.7 29.1 59.8 85.4 43.7 41.7 52.0 63.9 64.9 29.9 24.9 97.0 46.9 83.6 41.4 75.6 50.8 34.9 52.7 48.1 C1 64.3 66.5 55.8 49.7 61.7 69.6 42.7 82.4 92.2 63.3 64.1 68.7 72.3 70.6 62.6 21.3 97.0 58.7 86.5 55.2 92.4 61.4 17.3 66.8 63.4 C2 46.5 ? 42.6 ? ? 47.4 ? 41.9 41.8 ? 43.9 36.3 58.7 82.5 37.8 48.9 60.5 ? ? 34.1 20.1 ? 58.2 ? 42.9 ? 49.4 21.3 53.1 58.9 Avg 59.8 66.5 46.8 45.8 61.7 53.6 39.5 68.7 87.4 50.2 56.5 64.6 72.3 70.6 48.4 21.4 97.0 59.7 86.5 46.9 92.4 56.0 22.6 60.0 61.2</figDesc><table><row><cell>? 59.0 ? 48.1 ? ?</cell><cell>? ? ? 24.9</cell><cell>? 47.6</cell><cell>? 34.8 ? 46.0 34.5</cell><cell>?</cell><cell>?</cell></row><row><cell>S3 37.0 ? 65.1 ? 49.4 ? ?</cell><cell>? ? ? 22.9</cell><cell>? 62.2</cell><cell>? 42.6 ? 57.2 29.1</cell><cell>?</cell><cell>?</cell></row><row><cell>C3 46.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>6Table 4 .</head><label>4</label><figDesc>Avg Bed Bott Chair Clock Dish Disp Door Ear Fauc Knife Lamp Micro Frid StoraTable Trash Vase Bottom-Up 51.2 40.8 56.1 47.2 38.3 61.5 84.1 52.6 54.3 63.4 52.3 36.8 48.2 41.0 46.8 38.3 53.6 54.4 Top-Down 50.8 41.1 56.2 46.5 34.3 54.5 84.7 50.6 59.5 61.4 55.6 37.1 48.8 41.6 45.2 37.0 53.5 55.6 Ensemble 51.7 42.0 54.7 48.1 44.5 58.8 84.7 51.4 57.2 61.9 51.9 37.6 47.5 41.4 47.3 44.0 52.8 53.1 Hierarchical segmentation results (part-category mIoU %). We present the hierarchical segmentation performances for three baseline methods: bottom-up, top-down and ensemble. We conduct experiments on 17 out of 24 categories with tree depth bigger than 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>38.8 29.8 61.9 56.9 72.4 20.3 72.2 89.3 49.0 57.8 63.2 68.7 20.0 63.2 32.7 100 50.6 82.2 50.6 71.7 32.9 49.2 56.8 46.6 S2 29.7 ? 15.4 ? ? 25.4 ? 58.1 ? 25.4 ? ? ? ? ? 21.7 ? 49.4 ? 22.1 ? 30.5 18.9 ? ? S3 29.5 ? 11.8 45.1 ? 19.4 18.2 38.3 78.8 15.4 35.9 37.8 ? ? 38.3 14.4 ? 32.7 ? 18.2 ? 21.5 14.6 24.9 36.5 Avg 46.8 38.8 19.0 53.5 56.9 39.1 19.3 56.2 84.0 29.9 46.9 50.5 68.7 20.0 50.7 22.9 100 44.2 82.2 30.3 71.7 28.3 27.5 40.9 41.6 O1 62.6 64.7 48.4 63.6 59.7 74.4 42.8 76.3 93.3 52.9 57.7 69.6 70.9 43.9 58.4 37.2 100 50.0 86.0 50.0 80.9 45.2 54.2 71.7 49.8 O2 37.4 ? 23.0 ? ? 35.5 ? 15.0 48.6 ? 29.0 32.3 53.3 80.1 17.2 39.4 44.7 ? ? 45.8 18.7 ? 34.8 ? 26.5 ? 27.5 23.9 33.7 52.0 Avg 54.4 64.7 28.8 56.1 59.7 46.3 37.5 64.1 86.7 36.6 48.5 57.1 70.9 43.9 52.1 27.6 100 44.2 86.0 37.2 80.9 35.9 36.4 52.7 50.9</figDesc><table><row><cell>? 62.8 ? 39.7 ? ?</cell><cell>? ? ? 26.9</cell><cell>? 47.8</cell><cell>? 35.2 ? 35.0 31.0</cell><cell>?</cell><cell>?</cell></row><row><cell>O3 36.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc>Trash Vase Oavg 69.8 54.8 70.0 87.5 87.7 59.0 62.1 67.3 85.2 64.4 74.1 69.9 86.8 77.0 75.0 44.6 61.6 71.0 91.0 65.3 88.7 68.0 40.1 51.4 72.6 Oavg 19.0 29.3 17.4 6.9 8.1 26.7 24.6 19.1 17.8 15.2 15.6 17.6 9.5 17.9 14.6 29.0 27.1 19.3 10.6 27.7 9.0 21.3 29.3 23.3 19.5 Ravg 83.3 82.1 76.2 89.3 91.7 77.8 91.1 81.5 94.0 77.0 83.0 84.7 89.3 89.6 77.8 72.7 78.3 84.4 91.7 85.1 90.2 77.1 71.4 71.0 92.3 R std 10.4 11.1 9.2 6.0 7.4 15.2 7.0 7.2 2.8 11.2 13.5 8.6 10.3 3.5 14.9 17.3 14.6 9.1 9.7 6.2 8.6 12.8 22.6 13.2 7.5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Avg Bag Bed Bott Bowl Chair Clock Dish Disp Door Ear Fauc Hat Key Knife Lamp Lap Micro Mug Frid Scis StoraTable Trash Vase P1 71.8 59.3 39.6 81.0 78.5 81.8 67.1 78.9 88.2 71.1 68.0 67.5 58.5 65.6 66.5 46.5 96.5 75.0 84.2 79.6 86.5 55.9 85.6 66.7 76.3 P2 50.1 ? 21.3 ? ? 52.4 ? 60.0 ? 47.1 ? ? ? ? ? 43.5 ? 64.3 ? 63.9 ? 48.8 50.0 ? ? P3 48.2 ? 13.0 55.3 ? 44.8 37.8 55.2 79.0 38.8 47.5 55.5 ? ? 40.0 34.7 ? 54.5 ? 53.2 ? 47.4 42.5 46.4 74.0 Avg 63.4 59.3 24.6 68.2 78.5 59.7 52.4 64.7 83.6 52.3 57.8 61.5 58.5 65.6 53.2 41.6 96.5 64.6 84.2 65.6 86.5 50.7 59.4 56.5 75.2 PP1 76.8 72.7 54.7 85.8 78.5 84.5 74.1 81.9 90.7 73.5 77.8 73.6 64.2 62.5 75.0 65.5 96.6 80.3 90.9 72.1 87.5 61.2 86.7 71.5 81.4 PP2 54.7 ? 34.8 ? ? 54.9 ? 25.1 61.0 ? 49.6 46.1 52.5 81.0 48.0 56.1 60.4 ? ? 49.1 46.0 ? 54.3 ? 50.7 ? 50.6 47.0 54.7 75.1 Avg 68.1 72.7 38.2 73.4 78.5 63.0 60.1 65.0 85.8 59.5 67.0 67.0 64.2 62.5 62.0 56.1 96.6 65.9 90.9 60.4 87.5 54.9 62.4 63.1 78.2 S1 73.9 72.9 55.9 86.1 83.4 83.8 72.1 73.3 90.4 60.4 70.6 71.5 71.6 64.6 42.1 59.1 97.1 78.6 91.6 68.7 77.0 64.2 83.8 74.4 79.5 S2 53.3 ? 37.8 ? ? 53.6 ? 27.2 52.8 ? 44.7 44.2 51.1 77.2 40.7 47.5 53.7 ? ? 27.3 35.7 ? 54.4 ? 52.4 ? 53.1 43.3 48.0 62.3 Avg 65.1 72.9 40.3 69.4 83.4 60.7 58.1 63.2 83.8 52.0 59.0 62.6 71.6 64.6 34.7 45.4 97.1 65.0 91.6 61.2 77.0 55.7 59.6 61.2 70.9 C1 75.5 72.0 55.3 83.6 75.0 83.9 65.6 81.8 91.9 68.1 74.5 71.1 66.8 70.4 68.1 55.6 97.1 83.1 92.7 78.9 92.6 58.8 85.5 67.7 71.8 Avg 66.3 72.0 40.3 71.2 75.0 61.5 50.9 66.8 86.7 54.5 65.2 65.9 66.8 70.4 47.2 44.1 97.1 68.6 92.7 62.5 92.6 55.2 55.2 59.2 69.4</figDesc><table><row><cell></cell><cell>? 60.6 ? 57.0 ? ?</cell><cell>? ? ? 56.8</cell><cell>? 63.0</cell><cell>? 58.4 ? 52.9 53.6</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">PP3 53.4 ? 65.3 ? 55.0 ? ?</cell><cell>? ? ? 41.4</cell><cell>? 62.1</cell><cell>? 62.6 ? 49.8 51.7</cell><cell>?</cell><cell>?</cell></row><row><cell>S3 48.0 C2 52.1 ? 36.6 ? ? 52.9</cell><cell>? 63.4 ? 54.9 ? ?</cell><cell>? ? ? 42.4</cell><cell>? 64.1</cell><cell>? 57.7 ? 54.4 42.7</cell><cell>?</cell><cell>?</cell></row><row><cell cols="3">C3 49.6 ? 29.1 58.7 ? 47.7 36.2 55.3 81.5 40.4 55.8 60.7 ? ? 26.4 34.4</cell><cell>? 58.7</cell><cell cols="3">? 50.8 ? 52.3 37.4 50.8 67.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Avg Bed Bott Chair Clock Dish Disp Door Ear Fauc Knife Lamp Micro Frid StoraTable Trash Vase Bottom-Up 65.9 42.0 74.3 63.8 64.1 66.3 84.2 61.4 70.0 74.2 67.1 62.7 63.0 60.8 57.8 65.7 62.8 80.9 Top-Down 65.9 42.0 73.7 62.3 65.5 64.0 85.5 63.1 71.1 73.5 68.8 63.3 62.7 58.8 57.6 66.2 63.0 79.3 Ensemble 66.6 42.9 74.4 64.3 65.5 62.7 85.8 63.7 71.7 74.0 66.7 63.4 61.9 61.5 60.6 67.5 64.0 82.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Hierarchical segmentation results (shape mIoU %). We present the hierarchical segmentation performances for three baseline methods: bottom-up, top-down and ensemble. We conduct experiments on 17 out of 24 categories with tree depth bigger than 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table Trash Can Vase</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://3dwarehouse.sketchup.com 2 Although 3D models in ShapeNet<ref type="bibr" target="#b2">[3]</ref> come with face normal, textures, material and other information, there is no guarantee for the quality of such information. Thus, we leave this as a future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">There are many other algorithm candidates:<ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref> 21]. We will host an online leadboard to report the performances.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">PointNet++<ref type="bibr" target="#b29">[30]</ref> and SpiderCNN<ref type="bibr" target="#b40">[40]</ref> use point normals as additional inputs. For fair comparison, we only input the 3D coordinates.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We consider the entire path labels as histories to the leaf nodes when computing the confusion matrix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by NSF grants CRI-1729205 and IIS-1763268, a Vannevar Bush Faculty Fellowship, a Google fellowship, and gifts from Autodesk, Google and Intel AI Lab. We especially thank Zhe Hu from Hikvision for the help on data annotation and Linfeng Zhao for the help on preparing hierarchical templates. We appreciate the 66 annotators from Hikvision, Ytuuu and Data++ on data annotation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mesh segmentation-a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mortara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patan?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spagnuolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape Modeling and Applications, 2006. SMI 2006. IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for the objective evaluation of segmentation algorithms using a ground-truth of human segmented 3D-models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Benhabiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavou?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Shape Modeling and Applications (SMI)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>pages Session-5</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linking WordNet to 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Global WordNet Conference</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A benchmark for 3D mesh segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01759</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parts of recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="65" to="96" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-segmentation of 3D shapes via subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1703" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to predict part mobility from a single static snapshot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">227</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning how objects function via co-analysis of interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predictive and generative neural networks for object functionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Eurographics State-of-the-art report)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="603" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust watertight manifold surface generation method for shapenet models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring shape variations by 3d-model decomposition and partbased recombination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thorm?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning 3D mesh segmentation and labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape2pose: Human-centric shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kdnetworks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ai2-thor: An interactive 3d environment for visual ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05474</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PointGrid: A deep network for 3D shape understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SO-Net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grass: Generative recursive autoencoders for shape structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X -transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05070</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Physical primitive decomposition. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploration of continuous variability in collections of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Virtualhome: Simulating household activities via programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SplatNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep functional dictionaries: Learning consistent semantic structures on 3D models from functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to group and label fine-grained shape components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active co-analysis of a set of shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Asafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">165</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">VoxSegNet: Volumetric CNNs for semantic part segmentation of 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00226</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03981</idno>
		<title level="m">Structure-aware generative network for 3d-shape modeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<title level="m">Spider-CNN: Deep learning on point sets with parameterized convolutional filters. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Chalet: Cornell house agent learning environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07357</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning hierarchical shape segmentation and labeling from online repositories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia</title>
		<meeting>SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Visual semantic planning using deep successor representations. arXiv preprint ArXiv:1705.08080</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Avg Bag Bed Bott Bowl Chair Clock Dish Disp Door Ear Fauc Hat Key Knife Lamp Lap Micro Mug Frid Scis Stora Table Trash Vase S1 72</title>
		<imprint/>
	</monogr>
	<note>5 62.8 38.7 76.7 83.2 91.5 41.5 81.4 91.3 71.2 81.4 82.2 71.9 23.2 78</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Template visualization (1/3). We present the templates for table and chair. The lamp template is shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
