<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dilated Neighborhood Attention Transformer NA / DiNA (dilation = 1) DiNA (dilation = 2) DiNA (dilation = 3) DiNA (dilation = 4)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab @ U of Oregon &amp; UIUC, and Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab @ U of Oregon &amp; UIUC, and Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dilated Neighborhood Attention Transformer NA / DiNA (dilation = 1) DiNA (dilation = 2) DiNA (dilation = 3) DiNA (dilation = 4)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. An illustration of a single pixel's attention span in Neighborhood Attention (NA) and Dilated Neighborhood Attention (DiNA) under different dilation values. NA localizes attention to the pixel's nearest neighbors. DiNA extends NA's local attention to a less constrained sparse global attention without additional computational burden. Transformers comprised of both NA and DiNA are capable of preserving locality, expanding the receptive field exponentially, and capturing longer-range inter-dependencies, leading to significant performance boosts in downstream vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over attention-based baselines such as NAT and Swin, as well as modern convolutional baseline Con-vNeXt. Our Large model is ahead of its Swin counterpart by 1.5% box AP in COCO object detection, 1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation, and faster in throughput. We believe combinations of NA and DiNA have the potential to empower various tasks beyond those presented in this paper. To support and encourage research in this direction, in vision and beyond, we open-source our project at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b40">[39]</ref> have made a significant contribution to AI research, starting with natural language understanding <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b32">31]</ref> before being applied to other modalities such as speech <ref type="bibr" target="#b12">[12]</ref> and vision <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b29">29]</ref>, thanks to their universal architecture built upon self attention. This success inspired efforts into attention-based models in vision, from backbone networks <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b39">38]</ref>, to more specific applications including image generation and density modeling <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b29">29]</ref>, object detection <ref type="bibr" target="#b3">[3]</ref>, image segmentation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b41">40]</ref>, and more.</p><p>Vision Transformer (ViT) <ref type="bibr" target="#b11">[11]</ref> was one of the first major demonstrations of transformers as direct alternatives to Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>, the de facto standard in vision. ViT treats an image as a sequence of patches and uses a plain transformer encoder to encode and classify images. It demonstrated competitive performance to CNNs on large scale image classification, and resulted in a surge in vision research focused on transformerbased architectures as competitors to CNNs <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36]</ref>.</p><p>Vision transformers and CNNs are different not only in terms of architecture and building blocks, but also in how they treat data. CNNs typically downsample inputs gradually as they pass through the model and construct hierarchical feature maps. This hierarchical design is crucial for vision, as objects vary in scale, and high-resolution feature maps are important to dense tasks, such as segmentation. On the other hand, transformers are known for their fixed dimensionality throughout the model, and as a result, plain ViTs downsample inputs aggressively from the very beginning to alleviate the quadratic cost of self attention, which in turn hinders the application of plain ViTs as backbones to dense vision tasks.</p><p>While research in applying plain ViTs to dense vision tasks continues <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">21]</ref>, research into hierarchical vision transformers quickly became dominant <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b42">41]</ref> and continues to grow <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b25">25]</ref>. A key advantage of these hierarchical transformer models is their ease of integration with existing hierarchical vision frameworks. Inspired by existing CNNs, hierarchical vision transformers are comprised of multiple (typically 4) levels of transformer encoders, with downsampling modules in between, and a less aggressive initial downsampling (i.e. 1/4 instead of 1/16). Earlier layers in hierarchical transformers, if using unrestricted self attention, would bear the same quadratically growing complexity and memory usage with respect to input resolution, making them intractable for higher resolution images. Therefore, hierarchical transformers typically employ certain local attention mechanisms.</p><p>Swin Transformer <ref type="bibr" target="#b26">[26]</ref>, one of the earliest hierarchical vision transformers, utilizes a Window Self Attention (WSA) module, followed by a pixel-shifted Window Self Attention (SWSA), both of which localize self attention to non-overlapping sub-windows. This reduces the cost of self attention, making its time and space complexity linear with respect to resolution. SWSA is identical to WSA, but with a shift in feature map pixels preceding it, and followed by a reverse shift. This is essential to its performance, as it allows out-of-window interactions, and therefore the expansion of its receptive field. One of the major advantages of Swin is efficiency, as pixel shifts and window partitioning are relatively cheap and easily parallelizable operations. Additionally, it involves little to no changes to the self attention module, making implementation easier. Swin went on to become the state of the art across multiple vision tasks, and followed by Swin-V2 <ref type="bibr" target="#b25">[25]</ref> to scale up to large scale pretraining.</p><p>Neighborhood Attention Transformer (NAT) <ref type="bibr" target="#b13">[13]</ref> was introduced later, with a simpler sliding-window based attention. Neighborhood Attention (NA) localizes self attention to the nearest neighbors around each token. It is conceptually similar to Stand Alone Self Attention (SASA) <ref type="bibr" target="#b33">[32]</ref>, which applies attention in the style of convolutions. Such dynamic attention operations were assumed to be extremely inefficient and challenging to parallelize <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b39">38]</ref>, until the Neighborhood Attention CUDA Extension (NATTEN) <ref type="bibr" target="#b13">[13]</ref> was released, which showed that NA can run even faster than Swin's SWSA in practice <ref type="bibr" target="#b1">1</ref> . NAT was able to significantly outperform Swin on image classification, and achieved competitive performance on downstream tasks, while also scaling up to be even faster than Swin despite the slightly different architecture.</p><p>Despite the efforts into hierarchical vision transformers with local attention, some of self attention's most important properties, including global receptive field, and the ability to model long-range inter-dependencies, are weakened as a result of this localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This leads to a simple question: How does one maintain the tractability that local attention provides in hierarchical vision transformers, while avoiding its shortcomings?</head><p>In other words, the optimal scenario is maintaining the linear complexity, while preserving the global receptive field and the ability to model long-range inter-dependencies of self attention. In this paper, we aim to answer this question and improve hierarchical transformers by extending a simple local attention mechanism, Neighborhood Attention, to Dilated Neighborhood Attention (DiNA): a flexible and powerful sparse global attention. Dilating neighborhoods in NA into larger sparse regions has multiple advantages: 1. it captures more global context, 2. allows the receptive field to grow exponentially, as opposed to linearly <ref type="bibr" target="#b47">[46]</ref>, and 3. comes at no additional computational cost. To demonstrate the effectiveness of DiNA, we propose Dilated Neighborhood Attention Transformer (DiNAT), which not only improves the existing NAT model in terms of downstream performance, it manages to outperform strong modern CNN baselines, such as ConvNeXt <ref type="bibr" target="#b27">[27]</ref>, in downstream tasks with a noticeable margin. Our main contributions can be summarized as follows:</p><p>? Introducing DiNA, a simple, flexible, and powerful sparse global attention pattern, which allows the receptive field to grow exponentially and captures longerrange context without any additional computational burden. DiNA does so while maintaining the symmetry in neighborhoods introduced in NA. It can also adapt to larger resolutions without expanding to larger window sizes.</p><p>? Analyzing theoretical receptive field sizes in models based on convolutions, localized attention, and a DiNA-based model.</p><p>? Introducing DiNAT, a new hierarchical vision transformer consisting of both dilated and non-dilated variants of Neighborhood Attention. DiNAT utilizes a gradual dilation change through the model, which extends receptive fields more optimally and helps fineto-coarse feature learning.</p><p>? Conducting extensive experiments on image classification, object detection, and instance and semantic segmentation with DiNAT, and finding that it exhibits a noticeable improvement in downstream tasks over both attention-based and convolutional baselines. Additionally, we investigate isotropic and hybrid attention variants, scaling experiments with ImageNet-22K pretraining, and the effects of different dilation values.</p><p>? Extending NATTEN, NA's CUDA extension for Py-Torch, by adding dilation support, and bfloat16 utilization, allowing the research in this direction to be extended to other tasks and applications.</p><p>While the initial experiments with DiNAT already exhibit significant improvements in downstream vision tasks, neither its performance nor applications stop here. NA's local attention and DiNA's sparse global attention complement each other: they can preserve locality, model longerrange inter-dependencies, expand the receptive field exponentially, and maintain a linear complexity. Their restriction of self attention can potentially improve convergence by avoiding self attention's possible redundant interactions, such as those with repetitive, background, or distracting tokens <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b34">33]</ref>. Combinations of local attention and sparse global attention can potentially empower various vision tasks and beyond. To support research in this direction, we are open sourcing our entire project, including our modified NATTEN extension, which can reduce runtime by orders of magnitude compared to naive implementations <ref type="bibr" target="#b13">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review dot product self attention (DPSA), the Transformer <ref type="bibr" target="#b40">[39]</ref>, and Vision Transformer <ref type="bibr" target="#b11">[11]</ref>. We then move on to localized self attention modules such as SASA <ref type="bibr" target="#b33">[32]</ref>, SWSA in Swin Transformer <ref type="bibr" target="#b26">[26]</ref>, and NA in Neighborhood Attention Transformer <ref type="bibr" target="#b13">[13]</ref>, and discuss their limitations, which are our motivation behind this work. Finally, we discuss previous uses of sparse attention mechanisms in language processing <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b35">34]</ref> and vision <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self Attention</head><p>Vaswani et al. <ref type="bibr" target="#b40">[39]</ref> define dot product attention as an operation between a query, and a set of key-value pairs. The dot product of the query and keys is scaled and sent through a softmax activation to produce attention weights. Said attention weights are then applied to the values:</p><formula xml:id="formula_0">Attention(Q, K, V ) = sof tmax QK T ? d k V,<label>(1)</label></formula><p>where ? d k is the scaling parameter, and d k is the key dimension. Dot product self attention is simply a case of this operation where the queries, keys, and values are all linear projections of the same input. Given an input X ? R n?d , where n is the number of tokens and d is the embedding dimension, this operation has a complexity of O(n 2 d) and a space complexity of O(n 2 ) for the attention weights (space depends on implementation <ref type="bibr" target="#b8">[8]</ref>). Vision Transformer (ViT) <ref type="bibr" target="#b11">[11]</ref> is one of the earliest works applying a pure transformer encoder to vision, showing the power that a large scale self attention based model bears. Follow up works extended the study with minimal changes to training techniques <ref type="bibr" target="#b36">[35]</ref>, architectural changes <ref type="bibr" target="#b37">[36]</ref>, and applications to small data regimes <ref type="bibr" target="#b14">[14]</ref>. Due to their quadratic time complexity, many works attempt to restrict the attention span in order to reduce compute, specifically when scaling to larger inputs, such as long documents in NLP <ref type="bibr" target="#b1">[1]</ref>, and large resolutions in vision <ref type="bibr" target="#b26">[26]</ref>. Restricting self attention can be done in different patterns, one of which is localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Local Attention</head><p>Stand-Alone Self Attention (SASA). SASA <ref type="bibr" target="#b33">[32]</ref> is one of the earliest local attention mechanisms that was specifically designed to be used in vision models, years before ViT <ref type="bibr" target="#b11">[11]</ref>. It sets the key-value pair to sliding windows over the feature map, therefore localizing attention for each query (pixel) to a window centered around it. Such an operation could easily replace convolutions in existing CNNs, such as ResNets, and theoretically even reduce computational complexity. Despite the promise it showed, the authors found that the resulting model runs slow, due to the inefficient implementation of this module. Works succeeding it therefore switched to alternative methods that could run more efficiently, such as blocked self attention in HaloNet <ref type="bibr" target="#b39">[38]</ref>, and Window Self Attention in Swin <ref type="bibr" target="#b26">[26]</ref>.</p><p>Shifted Window Self Attention (SWSA). Liu et al. <ref type="bibr" target="#b26">[26]</ref> proposed Window Self Attention (WSA) and its shifted variant SWSA, and used them in their hierarchical model for vision, Swin Transformer. They pointed out the inefficiency of methods such as SASA as one of their motivations behind developing Window Self Attention. The shifted variant (SWSA), as the name suggests, shifts pixels before the attention operation, and reverses the shift afterwards, to create a different window partitioning compared to the previous layer, which allows for out-of-window interactions that are crucial to a growing receptive field (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Swin initially became the state of the art in object detection and semantic segmentation. It also inspired other works that extended it to different tasks beyond the ones explored in the paper, such as generation <ref type="bibr" target="#b49">[48]</ref>, restoration <ref type="bibr" target="#b22">[22]</ref>, masked image modeling <ref type="bibr" target="#b46">[45]</ref>, video action recognition <ref type="bibr" target="#b28">[28]</ref>, and more. Additionally, the followup model, Swin-V2 <ref type="bibr" target="#b25">[25]</ref>, became the new state of the art with their largest model. It is noteworthy that Swin-V2 utilizes much larger window sizes to achieve such performance, which in turn increase time complexity and memory usage.</p><p>Neighborhood Attention (NA). NA <ref type="bibr" target="#b13">[13]</ref> was proposed as an alternative to WSA/SWSA, which was simply localizing the receptive field for each query to its nearest neighbors. Conceptually closer to SASA, NA's handling of corner pixels is what makes it approach self attention as its window size grows. Additionally, NA would bear with it a time and space complexity and number of parameters identical to that of Swin's WSA and SWSA. The primary limitation of NA, like SASA, was also the lack of an efficient implementation, as no existing deep learning or CUDA libraries implemented such an operation directly. As a result, Neighborhood Attention CUDA extension to PyTorch (NATTEN) was developed and improved over time, which allows NA to even outperform modules such as WSA/SWSA in terms of speed. The model Neighborhood Attention Transformer (NAT) is similar in its hierarchical design to Swin Transformer. The key differences, other than the attention modules, is that NAT utilizes overlapping convolutions in downsampling layers, as opposed to the patched ones used in Swin. As a result, to keep variants similar to Swin variants in terms of number of parameters and FLOPs, the models were made slightly deeper, with smaller inverted bottlenecks. NAT achieves superior results in image classification compared to Swin, and performs competitively on downstream tasks.</p><p>While local attention based models are able to perform well across different vision tasks due to its preservation of locality and efficiency, they fall short of capturing global context like self attention, which is also crucial to vision. Additionally, localized attention mechanisms utilize a smaller and slowly growing receptive field, similar to that of convolutions, compared to the full-sized receptive field in self attention. Besides self attention, several works also explored global receptive fields in vision, including but not limited to Non-local Neural Networks <ref type="bibr" target="#b43">[42]</ref>. However, operations with unrestricted global receptive field usually suf-fer from high computational complexities compared to restricted ones, which can be local, or sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sparse Attention</head><p>Child et al. <ref type="bibr" target="#b5">[5]</ref> proposed Sparse Transformers, which in addition to scaling to much deeper variants, utilized a sparse-kernel attention mechanism. Through this, the model was able to train much more efficiently on longer sequences of data. There have been other works in sparse attention, such as Longformer <ref type="bibr" target="#b1">[1]</ref>, Routing Transformers <ref type="bibr" target="#b35">[34]</ref>, and CCNet <ref type="bibr" target="#b18">[18]</ref>, all of which share a common feature: reducing the cost of self attention in cases where longer sequences of tokens are inevitable, but a global context is still necessary. Longformer <ref type="bibr" target="#b1">[1]</ref> specifically investigates using a combination of 1-D sliding window attention with and without dilation, along with global attention for specific tokens. This results in a model that is able to process long documents while maintaining the global context. CCNet <ref type="bibr" target="#b18">[18]</ref> uses axial attention to improve semantic segmentation heads by introducing global context without the quadratic cost of unrestricted self attention. More recently, MaxViT <ref type="bibr" target="#b38">[37]</ref> explored a hybrid model, which uses a combination of Window Self Attention <ref type="bibr" target="#b26">[26]</ref> (local self attention), Grid Attention (sparse self attention), and convolutions. However, the resulting model yields higher complexity and lower throughput compared to Swin <ref type="bibr" target="#b26">[26]</ref>.</p><p>While such non-local and sparse restrictions of self attention have shown to be promising, they are not wellstudied in the scope of hierarchical vision transformers. To expand the local receptive fields, and re-introduce some global context into hierarchical vision transformers, we introduce an extension of NA which spans neighborhoods over longer ranges by increasing the step size, while maintaining the overall attention span. Our Dilated Neighborhood Attention (DiNA), can serve as a sparse and global operation, that works most effectively when used in conjunction with NA as a local-only operation. We present an illustration of receptive fields in <ref type="figure">Fig. 4</ref>, where we compare fully connected layers to convolutions and dilated convolutions, and similarly self attention, to NA and DiNA. We provide empirical evidence for this claim with our hierarchical vision transformer, Dilated Neighborhood Attention Transformer (DiNAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we define DiNA as an extension to NA, analyze its effect on the receptive field, and move on to our model, DiNAT. We also provide brief details on implementation, and integration with the existing NATTEN package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dilated Neighborhood Attention</head><p>For simplicity, we keep our notations limited to singledimensional NA and DiNA. Given input X ? R n?d , whose  <ref type="figure">Figure 4</ref>. A single-dimensional illustration of receptive fields in fully connected layers, convolutional layers, and different attention mechanisms. NA and DiNA restrict self attention through sliding windows, similar to how convolutions and dilated convolutions restrict fully connected layers. These restrictions reduce computational burden, introduce useful inductive biases, and in some cases increase flexibility with respect to varying input sizes.</p><p>rows are d-dimensional token vectors, and query and key linear projections of X, Q and K, and relative positional biases between any two tokens i and j, B(i, j), we define neighborhood attention weights for the i-th token with neighborhood size k, A r i , as the matrix multiplication of the i-th token's query projection, and its k nearest neighboring tokens' key projections:</p><formula xml:id="formula_1">A k i = ? ? ? ? ? Q i K T ?1(i) + B (i,?1(i)) Q i K T ?2(i) + B (i,?2(i)) . . . Q i K T ? k (i) + B (i,? k (i)) ? ? ? ? ? ,<label>(2)</label></formula><p>where ? j (i) denotes i's j-th nearest neighbor. We similarly define neighboring values, V k i , as a matrix whose rows are the i-th token's k nearest neighboring value projections:</p><formula xml:id="formula_2">V k i = V T ?1(i) V T ?2(i) . . . V T ? k (i) T ,<label>(3)</label></formula><p>where V is a linear projection of X. Neighborhood Attention output for the i-th token with neighborhood size k is then defined as: ConvNeXt.</p><formula xml:id="formula_3">NA k (i) = sof tmax A k i ? d k V k i ,<label>(4)</label></formula><formula xml:id="formula_4">Complexity: O ndk , RF: (k ? 1) + 1. NA NA NA NA NAT. Complexity: O ndk , RF: (k ? 1) + 1. Dilation=1 Dilation=3 Dilation=1 Dilation=9</formula><p>DiNAT. We also provide the complexity of each method's primary operation. n denotes the number of tokens, d denotes the embedding dimension, and k denotes kernel/window size. Note that all denoted receptive fields are bounded by input size, n. DiNAT's receptive field is flexible and ranges from linear growth, (k ? 1) + 1, to exponential growth, k .</p><formula xml:id="formula_5">Complexity: O ndk , RF: [ (k ? 1) + 1, k ].</formula><p>where ? d k is the scaling parameter, and d k is the key dimension. To extend this definition to DiNA, given a dilation value ?, we simply define ? ? j (i) as token i's j-th nearest neighbor that satisfies: j mod ? = i mod ?. We can then define ?-dilated neighborhood attention weights for the i-th token with neighborhood size k, A (k,?) i , as follows:</p><formula xml:id="formula_6">A (k,?) i = ? ? ? ? ? ? Q i K T ? ? 1 (i) + B (i,? ? 1 (i)) Q i K T ? ? 2 (i) + B (i,? ? 2 (i))</formula><p>. . .</p><formula xml:id="formula_7">Q i K T ? ? k (i) + B (i,? ? k (i)) ? ? ? ? ? ? .<label>(5)</label></formula><p>We similarly define ?-dilated neighboring values for the i-th token with neighborhood size k, V</p><formula xml:id="formula_8">(k,?) i : V (k,?) i = V T ? ? 1 (i) V T ? ? 2 (i) . . . V T ? ? k (i) T .<label>(6)</label></formula><p>DiNA output for the i-th token neighborhood size k is then defined as:</p><formula xml:id="formula_9">DiNA ? k (i) = sof tmax A (k,?) i ? d k V (k,?) i .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Choice of Dilation</head><p>DiNA introduces a key new architectural hyperparameter: per layer dilation values. We define the upper bound for dilation value to be n k , where n is the number of tokens, and k is kernel/neighborhood size. This is simply to ensure exactly k dilated neighbors exist for each token. The lower bound is always 1, which would be equivalent to vanilla NA. Therefore, dilation value in each layer of the model will Layer structure Memory usage FLOPs Receptive Field <ref type="table">Table 1</ref>. Memory usage (weights), FLOPs, and receptive field sizes in different models. Convolutions and NA expand receptive field linearly with model depth. Window Self Attention alone would suffer from a fixed-value receptive field, but the pixel shift in SWSA expands the receptive field linearly. NA and DiNA together can expand receptive fields exponentially. Self attention has the maximum receptive field, which comes at the expense of a quadratic computational cost. Note that the denoted receptive fields have an upper bound of n.</p><formula xml:id="formula_10">? ? DWSConv-DWSConv d 2 + dk nd 2 + ndk (k ? 1) + 1 ? ? WSA-WSA 3d 2 + nk 3nd 2 + 2ndk k ? ? WSA-SWSA 3d 2 + nk 3nd 2 + 2ndk k ? ? NA-NA 3d 2 + nk 3nd 2 + 2ndk (k ? 1) + 1 ? ? NA-DiNA 3d 2 + nk 3nd 2 + 2ndk ? [ (k ? 1) + 1, k ] ? ? SA-SA 3d 2 + n 2 3nd 2 + 2n 2 d n</formula><p>be an input-dependent hyperparameter, which can take any</p><formula xml:id="formula_11">integer ? ? [1, n k ].</formula><p>Because dilation values are changeable, they provide a flexible receptive field (discussed in Sec. 3.3). It is not feasible to try out all possible combinations, therefore we explored a limited number of choices, which are discussed in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Receptive Fields</head><p>We analyze DiNA's receptive field, as it is important to understanding the power of DiNA, especially in comparison to other models. We present a comparison of receptive field sizes in different attention patterns in Tab. 1, along with FLOPs and memory usage. We also include depth-wise separable convolution (DWSConv), the key component in Con-vNeXt <ref type="bibr" target="#b27">[27]</ref>, for completeness.  We calculate receptive field size with respect to the number of layers, , kernel size k, and number of tokens n. Both convolutions and NA start out with a receptive field of size k, and expand by k ? 1 per layer (center pixel remains fixed). Swin Transformer's Window Self Attention <ref type="bibr" target="#b26">[26]</ref> on its own maintains a constant receptive field size, as the window partitioning prevents cross-window interactions, hence preventing receptive field expansion. Pixel shifted WSA resolves this issue, and expands receptive fields by exactly one window per layer, which is an expansion of k per layer.</p><p>It is worth noting that while Swin enjoys a slightly larger receptive field compared to NAT and ConvNeXt thanks to its special shifted window design, it breaks an important property: symmetry. Since Swin's feature maps are partitioned into non-overlapping windows, pixels within the same window only attend to each other, regardless of their position (whether at center or corner), leading to some pixels seeing asymmetric context around them.</p><p>Unlike the fixed receptive field growth in NAT, Swin, and ConvNeXt, DiNA's receptive field is flexible and changes with dilation. It can range anywhere from NAT's original (k ? 1) + 1 (all dilation values set to 1), to and exponentially growing receptive field of k (gradual dilation increase), which is one of the main reasons behind its power. Regardless of dilation, the first layer always yields a receptive field of size k. Given large enough dilation values, the preceding DiNA layer will yield a k-sized receptive field for each of the k in the DiNA layer, yielding a receptive field of size k 2 . As a result, DiNA and NA combinations with optimal dilation values can potentially increase receptive field exponentially to k . This comes with no surprise, as dilated convolutions have also been known for having an exponentially-growing receptive field size when using exponentially growing dilation values <ref type="bibr" target="#b47">[46]</ref>. An illustration of the increased receptive field size is also presented in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">DiNAT</head><p>For a fair evaluation of DiNA's performance, we design DiNAT to be identical to the original NAT model in terms of architecture and configuration. It uses two 3?3 convolutional layers with 2?2 strides initially, resulting in feature maps that are a quarter of the input resolution. It also uses a single 3?3 convolution with 2?2 strides to downsample between levels, which cut spatial resolution in half and double channels. DiNAT variants are also configured the same way NAT variants are, which are summarized in Tab. 2.</p><p>The key difference in DiNAT is that every other layer uses DiNA instead of NA. Dilation values for DiNA layers are set based on the task and input resolution. For ImageNet-1k at 224?224 resolution, we set dilation values to 8, 4, 2, and 1 in levels one through four respectively. In downstream tasks, because of their larger resolution, we increase dilation values to beyond that. All dilation values and other relevant architecture details are presented in Tab. II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation</head><p>We implemented DiNA on top of the existing Neighborhood Attention CUDA Extension (NATTEN), allowing ease of use and identical memory usage to NA. The latest public version of the extension includes a more efficient "tiled" implementation of Neighborhood Attention, which is what allows it to compete with methods such as Swin in terms of speed. By adding a dilation element to all the existing CUDA kernels, and re-implementing the "tiled" kernel to support dilated memory format, we managed to implement DiNA without affecting the speed of the existing NA kernels. However, it should be noted that DiNA's throughput will depend on dilation value, and is expected to be slightly slower than NA in practice. This is simply due to the break in memory access pattern, which would affect throughput overall (see <ref type="figure">Fig. Ia</ref>). We also note that these implementations are still fairly naive and don't fully utilize newer architecture standards in CUDA, such as Tensor Cores, and are therefore only working as a proof of concept. Despite this limitation, models using NA and DiNA can achieve competitive throughput levels compared to other methods that mostly utilize convolutions, linear projections, and self attention, all of which run through NVIDIA libraries that fully utilize the aforementioned standards. More information on implementation is provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted extensive experiments to study the effects of our proposed DiNAT model over existing baselines. Similar to existing methods, we pre-train models on image classification (ImageNet-1K and ImageNet-22K <ref type="bibr" target="#b9">[9]</ref>), and then transfer the learned weights to object detection and instance segmentation (MS-COCO <ref type="bibr" target="#b24">[24]</ref>), as well as semantic segmentation (ADE20K <ref type="bibr" target="#b52">[51]</ref>). We compare DiNAT to the original NAT model, Swin <ref type="bibr" target="#b26">[26]</ref>, and ConvNeXt <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head><p>We used the community standard for ImageNet training in PyTorch, timm <ref type="bibr" target="#b44">[43]</ref> (Apache License v2), which now serves as the community standard for ImageNet training in PyTorch <ref type="bibr" target="#b30">[30]</ref>, to train our model on ImageNet-1k <ref type="bibr" target="#b9">[9]</ref>. We use the same training configurations, regularization techniques, and augmentations (CutMix <ref type="bibr" target="#b48">[47]</ref>, Mixup <ref type="bibr" target="#b50">[49]</ref>, Ran-   <ref type="table">Table 4</ref>. ImageNet-1K image classification performance at 384 2 resolution. Throughput and peak memory usage are measured from forward passes with a batch size of 256 on a single NVIDIA A100 GPU. dAugment <ref type="bibr" target="#b7">[7]</ref>, and Random Erasing <ref type="bibr" target="#b51">[50]</ref>) and training techniques used in NAT <ref type="bibr" target="#b13">[13]</ref> and Swin <ref type="bibr" target="#b26">[26]</ref>. Models trained on ImageNet-1K directly are trained for 300 epochs with a batch size of 1024, and use an iteration-wise cosine learning rate schedule and a 20 epoch warmup, with a base learning rate of 1e-3, and weight decay rate of 0.05, cooled down for an additional 10 epochs. Larger variants are pre-trained on ImageNet-22K <ref type="bibr" target="#b9">[9]</ref> for 90 epochs with a batch size of 4096, but use a linear learning rate schedule and a 5 epoch warmup, with a base learning rate of 1e-3, and weight decay rate of 0.01, again following Swin <ref type="bibr" target="#b26">[26]</ref>. We fine-tune models pre-trained on ImageNet-22K to ImageNet-1K for 30 epochs, with a batch size of 512, and a linear learning rate schedule with no warmup, and a base learning rate of 5e-5, and weight decay rate of 1e-4. Final ImageNet-1K validation set accuracy levels, along with number of learnable parameters, FLOPs, throughput, and memory usage are provided in Tabs. 3 and 4. The reason for providing both FLOPs and throughput is to point out the necessity in distinguishing theoretical computational requirements, versus efficiency in practice with each method's available implementation. This is especially important in this case because NA and DiNA are based on from scratch implementations of the algorithms (NATTEN), and are not as well-optimized as ConvNeXt or Swin, which mostly run on native NVIDIA libraries designed for optimal throughput.</p><p>ImageNet-1K. DiNAT doesn't show improvement over NAT in smaller variants. Improvement over NAT-Mini is less than 0.1%, and we found that while the Tiny variant converges faster than NAT-Tiny at first, it converges to a lower accuracy of 82.7%. We noticed that despite this, Di-NAT consistently outperforms NAT across all four variants on downstream tasks. Additionally, DiNAT shows a slight improvement of at least 0.1% over NAT on Small and Base variants.</p><p>ImageNet-22K. We pre-trained our Large variant on ImageNet-22K, and fine-tuned it to ImageNet-1K at both 224?224 and 384?384 resolutions. We found that our model can successfully outperform Swin-Large at 224?224 resolution, while falling short to ConvNeXt-Large. At 384?384, our model gets very close to Swin's reported performance without an increase in kernel size. Upon increasing our kernel size to 11 <ref type="bibr" target="#b2">2</ref> , we see that our model successfully outperforms Swin's. We note that NA/DiNA are in theory limited to odd-sized kernels, which is the reason behind picking 11 2 instead of 12 <ref type="bibr" target="#b2">2</ref> . We also note that if we use Swin's original architecture, the Large variant would outperform Swin-Large even with a 7?7 kernel (see Appendix C).</p><p>Isotropic variants. To further compare NA/DiNA to plain self attention, we also explore isotropic variants of NAT and DiNAT, similar to isotropic ConvNeXt <ref type="bibr" target="#b27">[27]</ref> variants. These models simply follow ViT in design: a single Transformer encoder operating on feature maps with a fixed spatial size <ref type="bibr">(14 ? 14)</ref>, preceded by a single patch-and-embedding layer; they are not hierarchical transformers. To maintain fairness in comparison to self attention, we trained ViT models with relative positional biases (ViT + ) to ensure the models are only different in attention receptive fields. Note that ViT variants with relative positional biases have previously been explored in timm <ref type="bibr" target="#b44">[43]</ref>, but we run our own to ensure similar training settings. We present a comparison of these models and their performance on ImageNet-1k in Tab. 5. We find that isotropic variants of both NAT and DiNAT exhibit only minor throughput improvements over ViT + , which can again be attributed to the lack of fully optimized implementations. Note that these variants reduce FLOPs to almost the same number as isotropic ConvNeXt variants. They also reduce memory usage compared to ViT + noticeably. As for performance, we observe that isotropic NAT variants result in a drop in performance compared to ViT + , which is to be expected since NAT has half the receptive field size as ViT + . However, we find that isotropic DiNAT variants significantly improve upon NAT's isotropic variants, without increasing the receptive field size. This further supports our claim that a combination of NA and DiNA is more effective at producing an alternative to self attention than simply using NA throughout the model. To further study the effects of different attention mechanisms, and investigate whether or not a model fully based on self-attention always yields the best result, we experiment with hybrid isotropic models utilizing both NA/DiNA layers as well as self attention. We present those results in Tab. 6. We found that a small-scale (22M parameter) model with only half the layers performing self attention and the other half neighborhood attention can reach a similar accuracy as a similar model with all 12 layers utilizing self attention. We also found that changing the order of different attention layers can result in an approximately 0.2% change in accuracy.  <ref type="table">Table 7</ref>. COCO object detection and instance segmentation performance. ? indicates that the model was pre-trained on ImageNet-22K. Swin-L was not reported with Cascade Mask R-CNN, therefore we trained it with their official checkpoint. Throughput is measured on a single NVIDIA A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection and Instance Segmentation</head><p>To explore DiNAT's effectiveness in object detection and instance segmentation, we used its pre-trained weights as backbones for Mask R-CNN <ref type="bibr" target="#b16">[16]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b2">[2]</ref>, and trained those models on MS-COCO <ref type="bibr" target="#b24">[24]</ref>. We followed NAT <ref type="bibr" target="#b13">[13]</ref> and Swin <ref type="bibr" target="#b26">[26]</ref>'s training settings in mmdetection <ref type="bibr" target="#b4">[4]</ref> (Apache License v2), and trained with the same accelerated 3? LR schedule. The results are presented in Tab. 7. We observe that DiNAT consistently shows noticeable improvement over NAT, with little-to-no drop in throughput. There are even instances where DiNAT even surpasses NAT's throughput, but within the margin of error. Additionally, we observe that this improvement over NAT pushes DiNAT ahead of ConvNeXt <ref type="bibr" target="#b27">[27]</ref>. At scale, we see DiNAT continues to outperform both Swin and ConvNeXt with ImageNet-22K pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic Segmentation</head><p>We also trained UPerNet <ref type="bibr" target="#b45">[44]</ref> with our DiNAT as the backbone on ADE20K <ref type="bibr" target="#b52">[51]</ref>, with ImageNet-pre-trained backbones. We followed NAT's mmsegmentation <ref type="bibr" target="#b6">[6]</ref> (Apache License v2) configurations, itself following Swin's configuration for training ADE20K. The results are pre-  <ref type="table">Table 8</ref>. ADE20K semantic segmentation performance. ? indicates that the model was pre-trained on ImageNet-22K. ? indicates increased window size from the default 7 2 to 12 2 . Throughput is measured on a single NVIDIA A100 GPU.</p><p>sented in Tab. 8. Similar to detection, we find that Di-NAT exhibits a noticeable improvement over the original NAT model, with very little impact on throughput. DiNAT also maintains its place ahead of both models at scale with ImageNet-22K pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Additional experiments</head><p>In this section, we aim to study DiNAT in more depth by analyzing the effects of: dilation values, NA-DiNA order, kernel sizes, and test-time changes in dilation.</p><p>Dilation values. In Tab. 9, we present models with different dilation values, and their effect on classification, detection, instance segmentation and semantic segmentation performance levels. Note that the increased dilation <ref type="bibr" target="#b16">(16,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b2">2)</ref> is applicable to downstream tasks only, because in theory input feature maps should be larger than or equal to the product of kernel size and dilation. As a result, "8, 4, 2, 1" is the maximum applicable dilation to ImageNet at 224 ? 224 resolution. Depending on image resolution, even higher dilation values are possible. We explored a "dynamic" dilation value, where DiNA layers apply the maximum possible dilation, which is the floor of resolution divided by kernel size ("Maximum" in Tab. 9). We finally choose settle on "gradual" dilation (see illustration in <ref type="figure">Fig. 4</ref>), in which we gradually increase dilation to the maximum level defined. For instance, if maximum dilation for a specific level to 8, its layers will have dilation values 1, 2, 1, 4, 1, 6, 1, 8 (refer to Appendix B for details).  <ref type="table">Table 11</ref>. Kernel size impact on performance. Note that we set dilation to the maximum values possible in each block based on the default resolutions. Therefore, the variant with kernel size 5 has larger dilation values compared to the one with kernel size 7.</p><p>NA-DiNA vs. DiNA-NA. We also experimented with models with DiNA layers before NA layers, as opposed to our final NA before DiNA choice. While the local-global order (NA-DiNA) was our initial choice, we've also found it to be the more effective choice. We also tried a model with only DiNA modules, and found that it performs significantly worse than other combinations. This highlights the importance of having a combination of both local and sparse global attention patterns in the model. The results are summarized in Tab. 10.</p><p>Kernel size. We study the effect of kernel size on model performance in Tab. 11. We observed that a DiNAT-Tiny sees a significant decay in performance with a smaller kernel size across all three tasks. However, we find increasing kernel size beyond the default 7?7 does not result in a significant increase in return.  Test-time dilation changes. We present an analysis of sensitivity to dilation values, in which we attempt different dilation values on already trained models, and evaluate their performance. This can be particularly important to cases with varying resolutions, i.e. multi-scale testing. For DiNAT to be at its best, dilation level needs to be a nearmaximum number to expand attention to a longer range.</p><p>The results are presented in Tab. 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Local attention modules are effective at reducing complexity, and are crucial when working with a hierarchical model that gradually downsamples inputs. Nevertheless, they cannot capture longer range inter-dependencies as well as global self attention, unless their receptive field size is increased, which defeats their initial purpose of efficiency and tractability. In this paper, we propose DiNA, a natural extension to NA that expands its local attention to sparse global attention at no additional cost. We build Di-NAT with combinations of NA and DiNA, and show that it can improve performance significantly, especially in downstream tasks, without introducing any additional computational burden. While our experiments give insight into the power behind such flexible attention modules, neither their performance nor efficiency stop here. We believe that combinations of NA and DiNA will be able to empower various models in vision and beyond, wherever locality and global context matter. We open source our entire project, including our CUDA library NATTEN, and will continue to support it as a toolkit for the community to allow easy experimentation with NA and DiNA.  <ref type="figure">Figure I</ref>. Layer-wise relative speed and memory comparison between NAT and DiNAT, with respect to Swin. NAT layers, which are only two consecutive NA layers with kernel size 7 2 , are already up to 40% faster than Swin layers with the same kernel size. DiNAT layers, comprised of an NA layer followed by a DiNA layer, are slightly slower in practice due to the break in memory access pattern, but are still faster than Swin layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation notes</head><p>As discussed in Sec. 3.5, we extend the existing Neighborhood Attention CUDA Extension (NATTEN) to support dilated neighborhoods. NATTEN has a two-stage attention computation, similar to many other implementations: QK, and AV. The former computes the dot product of queries and keys, and produces attention weights, and the latter applies attention weights to the values. Scaling, softmax, and dropout are not included, as to prevent re-implementation. One of the advantages of this two-stage structure over manual implementations is that, like implementations of convolutions, sliding windows are taken directly from the source tensor, and not cached into an intermediary tensor, thus using significantly less memory. We refer readers to NATTEN documentation, and NAT <ref type="bibr" target="#b13">[13]</ref> for further details.</p><p>Dilation support. Adding dilation to NATTEN's naive kernels is mostly simple: instead of incrementing neighbors across each axis by 1, we simply instruct the kernels to increment by a variable d. NA however has a special way to handle edge/corner pixels, which requires additional changes to support dilation. The greater challenge in adding dilation to NATTEN was adding it to the "tiled" kernels that utilize shared memory. Tiled NA kernels are a more recent addition to NATTEN, and boost NA's throughput significantly. Tiled implementations of matrix multiplication and convolutions are essential in parallelizing these operations efficiently, while minimizing DRAM accesses. As the name suggests, tiled implementations divide the operation into tiles and cache tiles o inputs from the global memory into the shared memory within each threadblock. Accessing values from shared memory is typically much faster compared to directly accessing global memory, but also comes with challenges such as bank conflicts. Tiled implementations also operate with the assumption that access patterns are not broken. Introducing dilation values would break those access patterns and require a re-implementation that ensures dilated neighbors are cached instead of local neighbors. We present a layer-wise relative speed and memory usage comparison between NAT and DiNAT with respect to Swin in Scaling and brain float support. In order to train our larger models and avoid overflowing activation values in later layers of the model, we've had to switch from automatic mixed-precision training with the default half precision data type, float16, which has 5 exponent bits and 10 mantissa bits, to bfloat16, which has the advantage of having 8 exponent bits while having only 7 mantissa bits. Utilizing bfloat16 has often been recommended for cases which lead to large activations, which includes ours as we scale our model. However, switching to bfloat16 required a re-implementation of NATTEN's half precision kernels to support and utilize bfloat16 correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training settings</head><p>We provide additional details on training DiNAT in Tab. I. We also provide details on DiNAT s , which utilizes non-overlapping patch embedding and downsampling, similar to Swin <ref type="bibr" target="#b26">[26]</ref> and ConvNeXt <ref type="bibr" target="#b27">[27]</ref>. DiNAT s serves as an alternative DiNA-based model, which has an architecture identical to Swin. DiNAT s can also serve as an ablation model, since it is identical to Swin in architecture, with WSA replaced with NA, and SWSA replaced with DiNA.</p><p>One of the most important architecture-related hyperparameters in DiNA-based models is dilation values. Both DiNAT and DiNAT s use a combination of NA and DiNA layers. We typically set dilation values in DiNA layers to be the maximum possible value with respect to input resolutions, if known. For example, ImageNet classification at  224?224 is downsampled to a quarter of the original size initially, therefore Level 1 layers take feature maps of resolution 56?56 as input. With a kernel size of 7?7, the maximum possible dilation value is 56/7 = 8. Level 2 will take feature maps of resolution 28?28 as input, leading to a maximum possible dilation value of 4. Because of this, we change dilation values depending on the task and resolution. We present the final dilation values we used in classification, detection, and segmentation in Tab. II. Note that we only change dilation values for DiNA layers, since we found that fine-tuning NA layers to DiNA layers may result in a slight decrease in initial performance (see Sec. 4.4, Tab. 12).  <ref type="table" target="#tab_13">Table III</ref>. ImageNet-1K image classification performance. ? indicates increased window size from 7 2 to 12 2 . Throughput and peak memory usage are measured from forward passes with a batch size of 256 on a single A100 GPU. Note that DiNATs is identical in architecture to Swin, and only different in attention modules (WSA/SWSA replaced with NA/DiNA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional experiments</head><p>We conducted all primary experiments with both our main model, DiNAT, as well as DiNAT s . We found that DiNAT s could serve as alternatives in certain cases, as they still provide noticeable improvements over Swin in terms of speed, accuracy, and memory usage. We also found that DiNAT s -Large is slightly ahead of DiNAT-Large in ImageNet-1K classification. Classification results are provided in Tab. III, object detection and instance segmentation results are provided in Tab. IV, and semantic segmentation results are provided in Tab. VI.</p><p>In Sec. 4.4 we experimented with architecture-related hyperparameters that are introduced by DiNA: dilation values, and the ordering of NA and DiNA layers. We also complete those dilation experiments by adding DiNAT s and Swin, and presente the results in Tabs. V and VII. <ref type="table">Table IV</ref>. COCO object detection and instance segmentation performance. ? indicates that the model was pre-trained on ImageNet-22K. Swin-L was not reported with Cascade Mask R-CNN, therefore we trained it with their official checkpoint. Throughput is measured on a single A100 GPU. Note that DiNATs is identical in architecture to Swin, and only different in attention modules (WSA/SWSA replaced with NA/DiNA).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>arXiv:2209.15001v1 [cs.CV] 29 Sep 2022 Radar chart comparing Swin-L, ConvNeXt-L, and our DiNAT-L across various visual recognition tasks. Both ConvNeXt and our DiNAT surpass Swin on all tasks. DiNAT further exhibits noticeable improvements over downstream dense recognition tasks compared to ConvNeXt. We found that introducing dilation into NAT models, and gradually increasing dilation values according to image sizes significantly boosts downstream performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of attention layers in Swin Transformer and DiNAT. Swin divides inputs into non-overlapping windows and applies self attention to each window separately, and applies a pixel shift every other layer. Pixel-shifted layers mask attention weights between out-of-order regions, which restricts self attention to shifted subwindows. DiNAT applies Neighborhood Attention, a sliding window attention, and dilates it at every other layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Receptive fields in ViT, Swin, ConvNeXt, NAT, and our DiNAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>An illustration of DiNAT's architecture. It downsamples inputs to a quarter of their original spatial resolution initially, and sends them through 4 levels of DiNA Transformer encoders. Feature maps are downsampled to half their spatial size and doubled in channels between levels. DiNAT layers are similar to most Transformers: Attention followed by an MLP with normalization and skip connections in between. It also switches between local NA and sparse global DiNA at every other layer (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Neighborhood Attention Transformer Architecture</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLP ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Layer Norm</cell></row><row><cell>H ? W</cell><cell>H 4 ? W 4</cell><cell></cell><cell>H 8 ? W 8</cell><cell></cell><cell>H 16 ? W 16</cell><cell></cell><cell>H 32 ? W 32</cell><cell></cell><cell></cell><cell>Layer Norm Dilated NA ?</cell></row><row><cell>Initial Downsampler</cell><cell>DiNAT Block</cell><cell>Level Downsampler</cell><cell>DiNAT Block</cell><cell>Level Downsampler</cell><cell>DiNAT Block</cell><cell>Level Downsampler</cell><cell>DiNAT Block</cell><cell>Fully Connected</cell><cell>Flamingo</cell><cell>Layer Norm MLP ?</cell></row><row><cell></cell><cell>?N1</cell><cell></cell><cell>?N2</cell><cell></cell><cell>?N3</cell><cell></cell><cell>?N4</cell><cell></cell><cell></cell><cell>Layer Norm NA ?</cell></row><row><cell></cell><cell>Dilated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DiNAT Block</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Variant</cell><cell>Layers</cell><cell cols="2">Dim ? MLP</cell><cell># of</cell><cell>FLOPs</cell></row><row><cell></cell><cell cols="4">per level Heads ratio Params</cell></row><row><cell cols="3">3, 4, 6, 5 3, 4, 18, 5 32 ? 2 32 ? 2 ? DiNAT-Small 3, 4, 18, 5 32 ? 3 ? DiNAT-Mini ? DiNAT-Tiny 3, 4, 18, 5 32 ? 4 ? DiNAT-Base ? DiNAT-Large 3, 4, 18, 5 32 ? 6</cell><cell>3 3 2 2 2</cell><cell>20 M 28 M 51 M 90 M 200 M</cell><cell>2.7 G 4.3 G 7.8 G 13.7 G 30.6 G</cell></row></table><note>. DiNAT variants. In terms of architecture, DiNAT is identical to NAT, which follows Swin closely in overall design. Channels (heads and dim) double after every level. Kernel size is 7 2 in all variants.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Win. # of</cell><cell>FLOPs</cell><cell cols="2">Thru. Memory Top-1</cell></row><row><cell></cell><cell>Size Params</cell><cell></cell><cell cols="2">(imgs/sec) (GB)</cell><cell>(%)</cell></row><row><cell cols="3">12 2 197 M 104.0 G 198 M 101.1 G ? ConvNeXt-L 7 2 ? Swin-L 7 2 200 M 89.7 G ? DiNAT-L ? DiNAT-L 11 2 200 M 92.4 G</cell><cell>169 221 161 110</cell><cell>32.7 19.2 20.1 26.9</cell><cell>87.25 87.50 87.18 87.31</cell></row></table><note>ImageNet-1K image classification performance at 224 2 resolution. Throughput and peak memory usage are mea- sured from forward passes with a batch size of 256 on a single NVIDIA A100 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison of different layer structures in the isotropic variant. We compare different attention mechanisms in detail by creating hybrid models with both SA and NA/DiNA. All models add learnable relative positional biases to attention weights.</figDesc><table><row><cell>Model</cell><cell cols="4"># of FLOPs Thru. Memory Top-1</cell></row><row><cell></cell><cell>Params</cell><cell cols="3">(imgs/sec) (GB)</cell><cell>(%)</cell></row><row><cell cols="2">4.3 G 4.3 G 4.3 G 4.6 G ? ConvNeXt-B (iso.) 87 M 16.9 G ? ConvNeXt-S (iso.) 22 M 22 M ? NAT-S (iso.) 22 M ? DiNAT-S (iso.) 22 M ? ViT + -S 86 M 16.9 G ? NAT-B (iso.) 86 M 16.9 G ? DiNAT-B (iso.) ? ViT + -B 86 M 17.5 G</cell><cell></cell><cell>4327 3255 3160 3086 1661 1350 1316 1284</cell><cell>1.2 1.3 1.3 1.9 2.4 2.7 2.7 3.7</cell><cell>79.7 80.0 80.8 81.2 82.0 81.6 82.1 82.5</cell></row><row><cell cols="5">Table 5. ImageNet-1K Top-1 validation accuracy compar-</cell></row><row><cell cols="5">ison of ConvNeXt, NAT, and DiNAT's isotropic variants to</cell></row><row><cell cols="5">ViT. To compare self attention and NA/DiNA fairly, we ran ViT + ,</cell></row><row><cell cols="5">which uses relative positional biases in attention layers, instead</cell></row><row><cell cols="5">of the one-time absolute positional encoding in the original ViT.</cell></row><row><cell cols="5">Throughput and peak memory usage are measured from forward</cell></row><row><cell cols="5">passes with a batch size of 256 on a single NVIDIA A100 GPU.</cell></row><row><cell>Model</cell><cell cols="2">Layer structure</cell><cell>FLOPs</cell><cell>Top-1 (%)</cell></row><row><cell>? ? NAT-S (iso.) ? ? ? ? ? ? DiNAT-S (iso.) ? ? ? ? ? ? ? ? ? ? ViT + -S</cell><cell>NA-NA DiNA-DiNA DiNA-NA NA-DiNA SA-NA SA-DiNA NA-SA DiNA-SA SA-SA</cell><cell></cell><cell>4.32 G 4.32 G 4.32 G 4.32 G 4.45 G 4.45 G 4.45 G 4.45 G 4.58 G</cell><cell>80.0 77.9 80.6 80.8 81.0 81.1 81.2 80.9 81.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Layer structure impact on performance. Our final model has the local-global (NA-DiNA) order. Thru. AP b AP m Thru. mIoU Thru. 1810 imgs/sec 46.8 42.0 45.5 fps 46.3 22.9 fps ? DiNAT-T 5 2 81.3 1777 imgs/sec 47.6 42.7 45.6 fps 46.4 22.7 fps 1235 imgs/sec 48.8 43.5 39.2 fps 48.4 20.0 fps</figDesc><table><row><cell>Model</cell><cell></cell><cell>Dilation</cell><cell cols="2">ImageNet</cell><cell>MSCOCO ADE20K</cell></row><row><cell></cell><cell></cell><cell>per level</cell><cell cols="3">Top-1 (%) AP b AP m</cell><cell>mIoU</cell></row><row><cell cols="3">1, 1, 1, 1 8, 4, 2, 1 ? DiNAT-Tiny 16, 8, 4, 2 ? NAT-Tiny ? DiNAT-Tiny ? DiNAT-Tiny Maximum ? DiNAT-Tiny Gradual</cell><cell></cell><cell>83.2 82.7 -82.7 -</cell><cell>47.7 42.6 48.0 42.9 48.3 43.4 48.6 43.5 48.6 43.5</cell><cell>48.4 48.5 48.5 48.7 48.8</cell></row><row><cell cols="6">Table 9. Dilation impact on performance. Dilation values be-</cell></row><row><cell cols="6">yond "8, 4, 2, 1" are only applicable to downstream tasks, as their</cell></row><row><cell cols="6">larger resolution allows for it. Maximum dilation indicates it is set</cell></row><row><cell cols="6">to the maximum possible value based on input size. It would be</cell></row><row><cell cols="6">the same as "8, 4, 2, 1" for ImageNet. Gradual dilation indicates</cell></row><row><cell cols="6">that dilation values in DiNA layers increase gradually.</cell></row><row><cell>Variant</cell><cell></cell><cell>Layer</cell><cell></cell><cell cols="2">ImageNet MSCOCO ADE20K</cell></row><row><cell></cell><cell></cell><cell cols="4">structure Top-1 (%) AP b AP m</cell><cell>mIoU</cell></row><row><cell cols="4">NA-NA ? ? DiNAT-Tiny NA-DiNA ? ? NAT-Tiny DiNA-NA ? ? ? ? DiNA-DiNA</cell><cell>83.2 82.7 82.6 82.2</cell><cell>47.7 42.6 48.3 43.4 48.5 43.5 44.9 40.5</cell><cell>48.4 48.5 47.9 45.8</cell></row><row><cell>Model</cell><cell>Win.</cell><cell cols="2">ImageNet</cell><cell cols="2">MSCOCO</cell><cell>ADE20K</cell></row><row><cell cols="6">size Top-1 ? NAT-T 5 2 7 2 83.2 1537 imgs/sec 47.7 42.6 44.5 fps 48.4 21.4 fps 81.6 ? NAT-T 82.7 1500 imgs/sec 48.3 43.4 43.3 fps 48.5 21.3 fps ? DiNAT-T 7 2 9 2 83.1 1253 imgs/sec 48.5 43.3 39.4 fps 48.1 20.2 fps ? NAT-T ? DiNAT-T 9 2 83.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc></figDesc><table><row><cell>Test time dilation change and its impact on perfor-</cell></row><row><cell>mance. Dilation values larger than 8, 4, 2, 1 are inapplicable to</cell></row><row><cell>ImageNet at 224 2 . Highlighted rows indicate testing is done on</cell></row><row><cell>the same settings the model was trained on.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Relative memory usage w.r.t. Swin (%) WSA + SWSA (Swin) NA/DiNA + NA/DiNA (NAT/DiNAT)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>180</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">WSA + SWSA (Swin)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">NA + NA (NAT)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>160</cell><cell></cell><cell cols="3">NA + DiNA (DiNAT)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>110</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relative speed w.r.t. Swin (%)</cell><cell>80 100 120 140</cell><cell>100 %</cell><cell>114 % 114 %</cell><cell>100 %</cell><cell>140 % 142 %</cell><cell>100 %</cell><cell>131 %</cell><cell>136 %</cell><cell>100 %</cell><cell>132 % 131 %</cell><cell>100 %</cell><cell>124 %</cell><cell>120 %</cell><cell>100 %</cell><cell>122 %</cell><cell>111 %</cell><cell>100 %</cell><cell>121 %</cell><cell>108 %</cell><cell>100 %</cell><cell>121 %</cell><cell>108 %</cell><cell>100 %</cell><cell>120 %</cell><cell>108 %</cell><cell>90 100 80</cell><cell>100 %</cell><cell>95 %</cell><cell>100 %</cell><cell>89 %</cell><cell>100 %</cell><cell>84 %</cell><cell>100 %</cell><cell>81 %</cell><cell>100 %</cell><cell>81 %</cell><cell>100 %</cell><cell>81 %</cell><cell>100 %</cell><cell>80 %</cell><cell>100 %</cell><cell>80 %</cell><cell>100 %</cell><cell>81 %</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>7 ? 7</cell><cell cols="2">14 ? 14</cell><cell cols="3">28 ? 28</cell><cell cols="8">56 ? 56 Feature map resolution 112 ? 112 224 ? 224</cell><cell cols="3">448 ? 448</cell><cell cols="3">896 ? 896</cell><cell cols="3">1792 ? 1792</cell><cell></cell><cell cols="2">7 ? 7</cell><cell cols="2">14 ? 14</cell><cell cols="2">28 ? 28</cell><cell cols="6">56 ? 56 Feature map resolution 112 ? 112 224 ? 224</cell><cell cols="2">448 ? 448</cell><cell cols="2">896 ? 896</cell><cell cols="2">1792 ? 1792</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) Speed.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Memory usage.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table I. Summary of DiNAT and DiNATs configurations. Channels (heads and dim) double after every level until the final one. Default dilation values for the four levels are 8, 4, 2, and 1. Kernel size is 7?7 in all variants.</figDesc><table><row><cell>Variant</cell><cell cols="5">Downsampling Layers Dim ? MLP # of FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">per level Heads ratio Params</cell></row><row><cell>? DiNAT s -T ? DiNAT s -S ? DiNAT s -B ? DiNAT s -L ? DiNAT-M ? DiNAT-T ? DiNAT-S ? DiNAT-B ? DiNAT-L</cell><cell cols="2">Patched Patched Patched Patched Conv Conv Conv Conv Conv</cell><cell cols="2">2, 2, 6, 2 32 ? 3 2, 2, 18, 2 32 ? 3 2, 2, 18, 2 32 ? 4 2, 2, 18, 2 32 ? 6 3, 4, 6, 5 32 ? 2 3, 4, 18, 5 32 ? 2 3, 4, 18, 5 32 ? 3 3, 4, 18, 5 32 ? 4 3, 4, 18, 5 32 ? 6</cell><cell>4 4 4 4 3 3 2 2 2</cell><cell>28 M 50 M 88 M 15.4 G 4.5 G 8.7 G 197 M 34.5 G 20 M 2.7 G 28 M 4.3 G 51 M 7.8 G 90 M 13.7 G 200 M 30.6 G</cell></row><row><cell>Variant</cell><cell cols="3">Resolution Level 1 Level 2</cell><cell cols="2">Level 3</cell><cell>Level 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ImageNet classification.</cell><cell></cell></row><row><cell>? DiNATs-T ? DiNATs-S/B/L ? DiNATs-L ? DiNAT-M ? DiNAT-T/S/B/L ? DiNAT-L</cell><cell>224 2 224 2 384 2 224 2 224 2 384 2</cell><cell cols="4">1, 8 1, 8 1, 13 1, 8, 1 1, 4, 1, 4 1, 4 1, 4 1, 6 1, 8, 1 1, 4, 1, 4 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2 1, 1, 1, 1, 1 1, 2, 1, 2, 1, 2 1, 1 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2 1, 1 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3 1, 1 1, 2, 1, 2, 1, 2 1, 1, 1, 1, 1 1, 13, 1 1, 6, 1, 6 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3 1, 1, 1, 1, 1</cell></row><row><cell></cell><cell></cell><cell cols="4">MS-COCO detection and instance segmentation.</cell></row><row><cell>? DiNATs-T ? DiNATs-S/B/L ? DiNAT-M ? DiNAT-T/S/B/L</cell><cell>800 2 800 2 800 2 800 2</cell><cell cols="4">1, 28 1, 28 1, 28, 1 1, 7, 1, 14 1, 14 1, 14 1, 28, 1 1, 7, 1, 14 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7 1, 3, 1, 3, 1 1, 3, 1, 5, 1, 7 1, 3 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7 1, 3 1, 3, 1, 5, 1, 7 1, 3, 1, 3, 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ADE20K semantic segmentation.</cell><cell></cell></row><row><cell>? DiNATs-T ? DiNATs-S/B ? DiNATs-L ? DiNAT-M ? DiNAT-T/S/B ? DiNAT-L</cell><cell>512 2 512 2 640 2 512 2 512 2 640 2</cell><cell cols="4">1, 16 1, 16 1, 20 1, 16, 1 1, 4, 1, 8 1, 8 1, 8 1, 10 1, 16, 1 1, 4, 1, 8 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4 1, 2, 1, 2, 1 1, 2, 1, 3, 1, 4 1, 2 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4 1, 2 1, 2, 1, 3, 1, 4, 1, 5, 1, 2, 1, 3, 1, 4, 1, 5, 1, 5 1, 2 1, 2, 1, 3, 1, 4 1, 2, 1, 2, 1 1, 20, 1 1, 5, 1, 10 1, 2, 1, 3, 1, 4, 1, 5, 1, 2, 1, 3, 1, 4, 1, 5, 1, 5 1, 2, 1, 2, 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table II .</head><label>II</label><figDesc>Dilation values. Due to ImageNet's relatively small input resolution, level 4 layers cannot go beyond a dilation value of 1, which is equivalent to NA. Also note that at 224?224 resolution, level 4 inputs will be exactly 7?7, therefore NA will be equivalent to self attention. This is not true in downstream tasks where resolutions are noticeably higher where levels 2 and 3 have gradually increasing dilation values, which are repeated in deeper models. This corresponds to the highlighted rows in Tab. 9 labeled "Gradual". These configurations apply to all downstream experiments (excluding those in Sec. 4.4), regardless of the detector/segmentation head.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table V. Dilation impact on performance. Models listed within the same section have identical architectures and are different only in attention patterns (NATs is identical to Swin with both WSA and SWSA replaced with NA, DiNATs replaces SWSA with DiNA).</figDesc><table><row><cell>Model</cell><cell>Dilation</cell><cell cols="3">ImageNet MSCOCO ADE20K</cell></row><row><cell></cell><cell>per level</cell><cell cols="2">Top-1 (%) AP b AP m</cell><cell>mIoU</cell></row><row><cell>? Swin-Tiny ? NAT s -Tiny ? DiNAT s -Tiny ? DiNAT s -Tiny ? DiNAT s -Tiny ? DiNAT s -Tiny ? NAT-Tiny ? DiNAT-Tiny ? DiNAT-Tiny ? DiNAT-Tiny ? DiNAT-Tiny</cell><cell>Not Applicable 1, 1, 1, 1 8, 4, 2, 1 16, 8, 4, 2 Maximum Gradual 1, 1, 1, 1 8, 4, 2, 1 16, 8, 4, 2 Maximum Gradual</cell><cell>81.3 81.8 81.8 -81.8 -83.2 82.7 -82.7 -</cell><cell>46.0 41.6 46.1 41.5 46.3 41.6 46.4 41.8 46.4 41.9 46.6 42.1 47.7 42.6 48.0 42.9 48.3 43.4 48.6 43.5 48.6 43.5</cell><cell>45.8 46.2 46.7 47.1 47.0 47.4 48.4 48.5 48.5 48.7 48.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table VI .</head><label>VI</label><figDesc>ADE20K semantic segmentation performance. indicates increased window size from 7 2 to 12 2 . Throughput is measured on a single A100 GPU. Note that DiNATs is identical in architecture to Swin, and only different in attention modules (WSA/SWSA replaced with NA/DiNA).Table VII. Layer structure impact on performance. Models listed within the same section have identical architectures and are different only in attention patterns (NATs is identical to Swin with both WSA and SWSA replaced with NA, DiNATs replaces SWSA with DiNA).</figDesc><table><row><cell>Variant</cell><cell>Layer</cell><cell cols="3">ImageNet MSCOCO ADE20K</cell></row><row><cell></cell><cell>structure</cell><cell cols="2">Top-1 (%) AP b AP m</cell><cell>mIoU</cell></row><row><cell>? ? Swin-Tiny ? ? NAT s -Tiny ? ? DiNAT s -Tiny ? ? ? ? ? ? NAT-Tiny ? ? DiNAT-Tiny ? ? ? ?</cell><cell>WSA-SWSA NA-NA NA-DiNA DiNA-NA DiNA-DiNA NA-NA NA-DiNA DiNA-NA DiNA-DiNA</cell><cell>81.3 81.8 81.8. 81.5 79.7 83.2 82.7 82.6 82.2</cell><cell>46.0 41.6 46.1 41.5 46.4 41.8 46.5 41.8 39.8 36.8 47.7 42.6 48.3 43.4 48.5 43.5 44.9 40.5</cell><cell>45.8 46.2 47.1 46.9 40.7 48.4 48.5 47.9 45.8</cell></row></table><note>? indicates that the model was pre-trained on ImageNet-22K.?</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The latest NATTEN release reports up to 40% more throughput with up to 25% less memory consumption in a layer-wise comparison of NA and SWSA.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Picsart AI Research (PAIR), Meta/Facebook AI, and IARPA for their generous support that made this work possible.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Dinat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-L ?</forename></persName>
		</author>
		<idno>258 M 1276 G 14.0 55.2 73.9 60.1 47.7 71.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<idno>2020. 10</idno>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
	<note>2020. 1, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Interspeech, 2020. 1</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neighborhood attention transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07143</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno>2017. 10</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
		<idno>1989. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Not all patches are what you need: Expediting vision transformers via token reorganizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno>2022. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno>2022. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<imprint>
			<pubPlace>Zeming</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maxvit: Multi-axis vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<idno>2022. 5</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno>2022. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Styleswin: Transformer-based gan for high-resolution image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>2022. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
