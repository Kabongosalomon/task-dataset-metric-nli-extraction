<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
							<email>rujunhan@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This Effective CONtinual pre-training framework for Event Temporal reasoning (ECONET) improves the PTLMs' fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reasoning event temporal relations is crucial for natural language understanding, and facilitates many real-world applications, such as tracking biomedical histories <ref type="bibr" target="#b24">(Sun et al., 2013;</ref><ref type="bibr" target="#b0">Bethard et al., 2015</ref><ref type="bibr" target="#b1">Bethard et al., , 2016</ref><ref type="bibr" target="#b2">Bethard et al., , 2017</ref>, generating stories <ref type="bibr" target="#b28">(Yao et al., 2019;</ref><ref type="bibr" target="#b8">Goldfarb-Tarrant et al., 2020)</ref>, and forecasting social events <ref type="bibr" target="#b16">(Li et al., 2020;</ref><ref type="bibr" target="#b14">Jin et al., 2020)</ref>. In this work, we study two prominent event temporal reasoning tasks as shown in <ref type="figure" target="#fig_0">Figure 1</ref>: event relation extraction (ERE) <ref type="bibr" target="#b3">(Chambers et al., 2014;</ref><ref type="bibr" target="#b21">Ning et al., 2018;</ref><ref type="bibr" target="#b22">O'Gorman et al., 2016;</ref><ref type="bibr" target="#b19">Mostafazadeh et al., 2016)</ref> that predicts temporal relations between a pair of events, and machine reading comprehension (MRC) <ref type="bibr" target="#b20">(Ning et al., 2020;</ref><ref type="bibr" target="#b31">Zhou et al., 2019)</ref> where a passage and a question about event temporal relations is presented, and models need to provide correct answers using the information in a given passage.</p><p>Recent approaches leveraging large pre-trained language models (PTLMs) achieved state-of-theart results on a range of event temporal reasoning tasks <ref type="bibr" target="#b20">(Ning et al., 2020;</ref><ref type="bibr" target="#b23">Pereira et al., 2020;</ref><ref type="bibr" target="#b27">Wang et al., 2020;</ref><ref type="bibr" target="#b34">Zhou et al., 2020c;</ref><ref type="bibr" target="#b10">Han et al., 2019b)</ref>. Despite the progress, vanilla PTLMs do not focus on capturing event temporal knowledge that can be used to infer event relations. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, an annotator of the QA sample can easily infer from the temporal indicator "following" that "transfer" happens BEFORE "preparing the paperwork", but a fine-tuned RoBERTa model predicts that "transfer" has no such relation with the event "preparing the paperwork." Plenty of such cases exist in our error analysis on PTLMs for event temporal relation-related tasks. We hypothesize that such deficiency is caused by original PTLMs' random masks in the pre-training where temporal indicators and event triggers are under-weighted and hence not attended well enough for our downstream tasks. <ref type="table">Table 1</ref>: The full list of the temporal lexicon. Categories are created based on authors' domain knowledge and best judgment. * * 'once' can be also placed into [past] category due to its second meaning of 'previously', which we exclude to keep words unique.</p><p>TacoLM <ref type="bibr" target="#b32">(Zhou et al., 2020a)</ref> explored the idea of targeted masking and predicting textual cues of event frequency, duration and typical time, which showed improvements over vanilla PTLMs on related tasks. However, event frequency, duration and time do not directly help machines understand pairwise event temporal relations. Moreover, the mask prediction loss of TacoLM leverages a soft crossentropy objective, which is manually calibrated with external knowledge and could inadvertently introduce noise in the continual pre-training.</p><p>We propose ECONET, a continual pre-training framework combining mask prediction and contrastive loss using our masked samples. Our targeted masking strategy focuses only on event triggers and temporal indicators as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This design assists models to concentrate on events and temporal cues, and potentially strengthen models' ability to understand event temporal relations better in the downstream tasks. We further pre-train PTLMs with the following objectives jointly: the mask prediction objective trains a generator that recovers the masked temporal indicators or events, and the contrastive loss trains a discriminator that shares the representations with the generator and determines whether a predicted masked token is corrupted or original <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>. Our experiments demonstrate that ECONET is effective at improving the original PTLMs' performances on event temporal reasoning.</p><p>We briefly summarize our contributions. 1) We propose ECONET, a novel continual pre-training framework that integrates targeted masking and contrastive loss for event temporal reasoning. 2) Our training objectives effectively learn from the targeted masked samples and inject richer event temporal knowledge in PTLMs, which leads to stronger fine-tuning performances over five widely used event temporal commonsense tasks. In most target tasks, ECONET achieves SOTA results in comparison with existing methods. 3) Compared with full-scale pre-training, ECONET requires a much smaller amount of training data and can cope with various PTLMs such as <ref type="bibr">BERT and RoBERTa. 4)</ref> In-depth analysis shows that ECONET successfully transfers knowledge in terms of textual cues of event triggers and relations into the target tasks, particularly under low-resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our proposed method aims at addressing the issue in vanilla PTLMs that event triggers and temporal indicators are not adequately attended for our downstream event reasoning tasks. To achieve this goal, we propose to replace the random masking in PTLMs with a targeted masking strategy designed specifically for event triggers and temporal indicators. We also propose a continual pre-training method with mask prediction and contrastive loss that allows models to effectively learn from the targeted masked samples. The benefits of our method are manifested by stronger fine-tuning performances over downstream ERE and MRC tasks.</p><p>Our overall approach ECONET consists of three components. 1) Creating targeted self-supervised training data by masking out temporal indicators and event triggers in the input texts; 2) leveraging mask prediction and contrastive loss to continually train PTLMs, which produces an event temporal knowledge aware language model; 3) fine-tuning the enhanced language model on downstream ERE and MRC datasets. We will discuss each of these components in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-trained Masked Language Models</head><p>The current PTLMs such as BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr" target="#b17">(Liu et al., 2019)</ref> follow a random masking strategy. <ref type="figure" target="#fig_0">Figure 1</ref> shows such an example where random tokens / words are masked from the input sentences. More formally, let x = [x 1 , ..., x n ] be a sequence of input tokens and x m t ? x m represents random masked tokens. The per-sample pre-training objective is to predict the identity (x t ) of x m t with a cross-entropy loss,</p><formula xml:id="formula_0">LMLM = ? x m t ?x m I[x m t = xt] log(p(x m t |x))</formula><p>(1) <ref type="figure">Figure 2</ref>: The proposed generator-discriminator (ECONET) architecture for event temporal reasoning. The upper block is the mask prediction task for temporal indicators and the bottom block is the mask prediction task for events. Both generators and the discriminator share the same representations.</p><p>Next, we will discuss the design and creation of targeted masks, training objectives and fine-tuning approaches for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Targeted Masks Creation</head><p>Temporal Masks. We first compile a lexicon of 40 common temporal indicators listed in the <ref type="table">Ta</ref> With the temporal lexicon, we conduct string matches over the 20-year's New York Times news articles 2 and obtain over 10 million 1-2 sentence passages that contain at least 1 temporal indicators. Finally, we replace each of the matched temporal indicators with a mask token. The upper block in <ref type="figure">Figure 2</ref> shows two examples where "following" and "after" are masked from the original texts. Event Masks. We build highly accurate event detection models <ref type="bibr" target="#b11">(Han et al., 2019c;</ref><ref type="bibr" target="#b29">Zhang et al., 2021)</ref> to automatically label event trigger words in the 10 million passages mentioned above. Similarly, we replace these events with mask tokens. The bottom block in <ref type="figure">Figure 2</ref> shows two examples where events "transfer" and "resumed" are masked from the original texts.</p><p>2 NYT news articles are public from 1987-2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generator for Mask Predictions</head><p>To learn effectively from the targeted samples, we train two generators with shared representations to recover temporal and event masks. Temporal Generator. The per-sample temporal mask prediction objective is computed using crossentropy loss,</p><formula xml:id="formula_1">LT = ? x T t ?x T I[x T t = xt] log(p(x T t |x)) (2) where p(x T t |x) = Sof tmax (f T (h G (x) t )</formula><p>) and x T t ? x T is a masked temporal indicator. h G (x) is x's encoded representation using a transformer and f T is a linear layer module that maps the masked token representation into label space T consisting of the 40 temporal indicators. Event Generator. The per-sample event mask prediction objective is also computed using crossentropy loss,</p><formula xml:id="formula_2">LE = ? x E t ?x E I[x E t = xt] log(p(x E t |x)) (3) where p(x E t |x) = Sof tmax (f E (h G (x) t )) and x E t ? x E are masked events. h G (x)</formula><p>is the shared transformer encoder as in the temporal generator and f E is a linear layer module that maps the masked token representation into label space E which is a set of all event triggers in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discriminator for Contrastive Learning</head><p>We incorporate a discriminator that provides additional feedback on mask predictions, which helps correct errors made by the generators. Contrastive Loss. For a masked token x t , we design a discriminator to predict whether the recovered token by the mask prediction is original or corrupted. As shown in <ref type="figure">Figure 2</ref>, "following" and "resumed" are predicted correctly, so they are labeled as original whereas "during" and "run" are incorrectly predicted and labeled as corrupted. We train the discriminator with a contrastive loss,</p><formula xml:id="formula_3">LD = ? x t ?M y log(D(xt|x)) + (1 ? y) log(1 ? D(xt|x)) where M = x E ? x T and D(x t |x) = Sigmoid (f D (h D (x) t )</formula><p>) and y is a binary indicator of whether a mask prediction is correct or not. h D shares the same transformer encoder with h G . Perturbed Samples. Our mask predictions focus on temporal and event tokens, which are easier tasks than the original mask predictions in PTLMs. This could make the contrastive loss not so powerful as training a good discriminator requires relatively balanced original and corrupted samples. To deal with this issue, for r% of the generator's output, instead of using the recovered tokens, we replace them with a token randomly sampled from either the temporal lexicon or the event vocabulary. We fix r = 50 to make original and corrupted samples nearly balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Joint Training</head><p>To optimize the combining impact of all components in our model, the final training loss calculates the weighted sum of each individual loss, L = L T + ?L E + ?L D , where ? and ? are hyperparameters that balance different training objectives. The temporal and event masked samples are assigned a unique identifier (1 for temporal, 0 for event) so that the model knows which linear layers to feed the output of transformer into. Our overall generator-discriminator architecture resembles ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>. However, our proposed method differs from this work in 1) we use targeted masking strategy as opposed to random masks; 2) both temporal and event generators and the discriminator, i.e. h G and h D share the hidden representations, but we allow task-specific final linear layers f T , f E and f D ; 3) we do not train from scratch and instead continuing to train transformer parameters provided by PTLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Fine-tuning on Target Tasks</head><p>After training with ECONET, we fine-tune the updated MLM on the downstream tasks. ERE samples can be denoted as [P, e i , e j , r i,j ], where P is the passage and (e i , e j ) is a pair of event trigger tokens in P. As <ref type="figure" target="#fig_1">Figure 3a</ref> shows, we feed (P, e i , e j ) into an MLM (trained with ECONET). Following the setup of <ref type="bibr" target="#b9">Han et al. (2019a)</ref> and <ref type="bibr" target="#b29">Zhang et al. (2021)</ref>, we concatenate the final event representations v i , v j associated with (e i , e j ) to predict temporal relation r i,j . The relation classifier is implemented by a multi-layer perceptron (MLP).</p><p>MRC/QA samples can be denoted as [P, Q, A], where Q represents a question and A denotes answers. <ref type="figure" target="#fig_1">Figure 3b</ref> illustrates an extractive QA task where we feed the concatenated [P, Q] into an MLM. Each token x i ? P has a label with 1 indicating x i ? A and 0 otherwise. The token classifier implemented by MLP predicts labels for all x i . <ref type="figure" target="#fig_1">Figure 3c</ref> illustrates another QA task where A is a candidate answer for the question. We feed the concatenated [P, Q, A] into an MLM and the binary classifier predicts a 0/1 label of whether A is a true statement for a given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>In this section, we describe details of implementing ECONET, datasets and evaluation metrics, and discuss compared methods reported in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Event Detection Model. As mentioned briefly in Section 2, we train a highly accurate event prediction model to mask event (triggers). We experimented with two models using event annotations in TORQUE <ref type="bibr" target="#b20">(Ning et al., 2020)</ref> and TB-Dense <ref type="bibr" target="#b3">(Chambers et al., 2014)</ref>. These two event annotations both follow previous event-centric reasoning research by using a trigger word (often a verb or an noun that most clearly describes the event's occurrence) to represent an event <ref type="bibr" target="#b26">(UzZaman et al., 2013;</ref><ref type="bibr" target="#b7">Glava? et al., 2014;</ref><ref type="bibr" target="#b22">O'Gorman et al., 2016)</ref>. In both cases, we fine-tune RoBERTa LARGE on the train set and select models based on the performance on the dev set. The primary results shown in <ref type="table" target="#tab_2">Table 2</ref> uses TORQUE's annotations, but we conduct additional analysis in Section 4 to show both models produce comparable results.</p><p>Continual Pretraining. We randomly selected only 200K out of 10 million samples to speed up our experiments and found the results can be as good as using a lot more data. We used half of these 200K samples for temporal masked samples and the other half for the event masked samples. We ensure none of these sample passages overlap with the target test data. To keep the mask tokens balanced in the two training samples, we masked only 1 temporal indicator or 1 event (closest to the temporal indicator). We continued to train BERT and RoBERTa up to 250K steps with a batch size of 8. The training process takes 25 hours on a single GeForce RTX 2080 GPU with 11G memory. Note that our method requires much fewer samples and is more computation efficient than the full-scale pre-training of language models, which typically requires multiple days of training on multiple large GPUs / TPUs.</p><p>For the generator only models reported in Table 2, we excluded the contrastive loss, trained models with a batch size of 16 to fully utilize GPU memories. We leveraged the dev set of TORQUE to find the best hyper-parameters. Fine-tuning. Dev set performances were used for early-stop and average dev performances over three randoms seeds were used to pick the best hyper-parameters. Note that test set for the target tasks were never observed in any of the training process and their performances are reported in Table 2. All hyper-parameter search ranges can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We evaluate our approach on five datasets concerning temporal ERE and MRC/QA. We briefly describe these data below and list detailed statistics in Appendix A.</p><p>ERE Datasets. TB-Dense <ref type="bibr" target="#b3">(Chambers et al., 2014)</ref>, MATRES <ref type="bibr" target="#b21">(Ning et al., 2018)</ref> and RED <ref type="bibr" target="#b22">(O'Gorman et al., 2016)</ref> are all ERE datasets. Their samples follow the input format described in Section 2.6 where a pair of event (triggers) together with their context are provided. The task is to predict pairwise event temporal relations. The differences are how temporal relation labels are defined. Both TB-Dense and MATRES leverage a VAGUE label to capture relations that are hard to determine even by humans, which results in denser annotations than RED. RED contains the most fine-grained temporal relations and thus the lowest sample/relation ratio. MATRES only considers start time of events to determine their temporal order, whereas TB-Dense and RED consider start and end time, resulting in lower inter-annotator agreement.</p><p>TORQUE <ref type="bibr" target="#b20">(Ning et al., 2020)</ref> is an MRC/QA dataset where annotators first identify event triggers in given passages and then ask questions regarding event temporal relations (ordering). Correct answers are event trigger words in passages. TORQUE can be considered as reformulating temporal ERE tasks as an MRC/QA task. Therefore, both ERE datasets and TORQUE are highly correlated with our continual pre-training objectives where targeted masks of both events and temporal relation indicators are incorporated.</p><p>MCTACO <ref type="bibr" target="#b31">(Zhou et al., 2019)</ref> is another MR-C/QA dataset, but it differs from TORQUE in 1) events are not explicitly identified; 2) answers are statements with true or false labels; 3) questions contain broader temporal commonsense regarding not only temporal ordering, but also event frequency, during and typical time that may not be directly helpful for reasoning temporal relations. For example, knowing how often a pair of events happen doesn't help us figure out which event happens earlier. Since our continual pre-training focuses on temporal relations, MCTACO could the least compatible dataset in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Metrics</head><p>Three metrics are used to evaluate the fine-tuning performances.</p><p>F 1 : for TORQUE and MCTACO, we follow the data papers <ref type="bibr" target="#b20">(Ning et al., 2020)</ref> and <ref type="bibr" target="#b31">(Zhou et al., 2019)</ref> to report macro average of each question's F 1 score. For TB-Dense, MATRES and RED, we report standard micro-average F 1 scores to be consistent with the baselines. Exact-match (EM): for both MRC datasets, EM = 1 if answer predictions match perfectly with gold annotations; otherwise, EM = 0.   <ref type="bibr" target="#b10">Han et al. (2019b)</ref>. ? , ? ? , ? and ? ? only report the best single model results, and to make fair comparisons with these baselines, we report both average and best single model performances. TacoLM baseline uses the provided and recommended checkpoint for extrinsic evaluations.</p><formula xml:id="formula_4">TORQUE MCTACO TB-Dense MATRES RED Methods F 1 EM C F 1 EM F 1 F 1 F 1</formula><p>EM-consistency (C): in TORQUE, some questions can be clustered into the same group due to the data collection process. This metric reports the average EM score for a group as opposed to a question in the original EM metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Compared Methods</head><p>We compare several pre-training methods with ECONET: 1) RoBERTa LARGE is the original PTLM and we fine-tune it directly on target tasks; 2) RoBERTa LARGE + ECONET is our proposed continual pre-training method; 3) RoBERTa LARGE + Generator only uses the generator component in continual pre-training; 4) RoBERTa LARGE + random mask keeps the original PTLMs' objectives and replaces the targeted masks in ECONET with randomly masked tokens. The methods' names for continual pretraining BERT LARGE can be derived by replacing RoBERTa LARGE with BERT LARGE . We also fine-tune pre-trained TacoLM on target datasets. The current SOTA systems we compare with are provided by <ref type="bibr" target="#b20">Ning et al. (2020)</ref>, <ref type="bibr" target="#b23">Pereira et al. (2020)</ref>, <ref type="bibr" target="#b29">Zhang et al. (2021)</ref> and <ref type="bibr" target="#b10">Han et al. (2019b)</ref>. More details are presented in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, we report two baselines. The first one, TacoLM is a related work that focuses on event duration, frequency and typical time. The second one is the current SOTA results reported to the best of the authors' knowledge. We also report our own implementations of fine-tuning BERT LARGE and RoBERTa LARGE to compare fairly with ECONET. Unless pointing out specifically, all gains mentioned in the following sections are in the unit of absolute percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with Existing Systems</head><p>TORQUE. The current SOTA system reported in <ref type="bibr" target="#b20">Ning et al. (2020)</ref> fine-tunes RoBERTa LARGE and our own fine-tuned RoBERTa LARGE achieves on-par F 1 , EM and C scores. The gains of RoBERTa LARGE + ECONET against the current SOTA performances are 0.9%, 0.5% and 2.3% per F 1 , EM and C metrics. MCTACO. The current SOTA system ALICE <ref type="bibr" target="#b23">(Pereira et al., 2020)</ref> also uses RoBERTa LARGE as the text encoder, but leverages adversarial attacks on input samples. ALICE achieves 79.5% and 56.5% per F 1 and EM metrics on the test set for the best single model, and the best performances for RoBERTa LARGE + ECONET are 76.8% and 54.7% per F 1 and EM scores, which do not outperform ALICE. This gap can be caused by the fact that the majority of samples in MCTACO reason about event frequency, duration and time, which are not directly related to event temporal relations. TB-Dense + MATRES. The most recent SOTA system reported in <ref type="bibr" target="#b29">Zhang et al. (2021)</ref> uses both BERT LARGE and RoBERTa LARGE as text encoders, but leverages syntactic parsers to build large graphical attention networks on top of PTLMs. RoBERTa LARGE + ECONET's fine-tuning performances are essentially on-par with this work without additional parameters. For TB-Dense, our best model outperforms <ref type="bibr" target="#b29">Zhang et al. (2021)</ref> by 0.1% while for MATRES, our best model underperforms by 1.0% per F 1 scores. RED. The current SOTA system reported in <ref type="bibr" target="#b10">Han et al. (2019b)</ref> uses BERT BASE as word representations (no finetuning) and BiLSTM as feature extractor. The single best model achieves 34.0% F 1 score and RoBERTa LARGE + ECONET is 9.8% higher than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>The Impact of ECONET Overall Impact. ECONET in general works better than the original RoBERTa LARGE across 5 different datasets, and the improvements are more salient in TORQUE with 1.0%, 2.0% and 1.5% gains per F 1 , EM and C scores, in MCTACO with 2.4% lift over the EM score, and in TB-Dense and RED with 2.0% and 3.4% improvements respectively over F 1 scores. We observe that the improvements of ECONET over BERT LARGE is smaller and sometimes hurts the fine-tuning performances. We speculate this could be related to the property that BERT is less capable of handling temporal reasoning tasks, but we leave more rigorous investigations to future research. Impact of Contrastive Loss. Comparing the average performances of continual pre-training with generator only and with ECONET (generator + discriminator), we observe that generator alone can improve performances of RoBERTa LARGE in 3 out of 5 datasets. However, except for TB-Dense, ECONET is able to improve fine-tuning performances further, which shows the effectiveness of using the contrastive loss. Significance Tests. As current SOTA models are either not publicly available or under-perform RoBERTa LARGE , we resort to testing the statistical significance of the best single model between ECONET and RoBERTa LARGE . <ref type="table" target="#tab_14">Table 8</ref> in the appendix lists all improvements' p-values per Mc-Nemar's test <ref type="bibr" target="#b18">(McNemar, 1947)</ref>. MATRES appears to be the only one that is not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Event Models</head><p>Event trigger definitions have been consistent in previous event temporal datasets <ref type="bibr" target="#b22">(O'Gorman et al., 2016;</ref><ref type="bibr" target="#b3">Chambers et al., 2014;</ref><ref type="bibr" target="#b20">Ning et al., 2020)</ref>. Trigger detection models built on TORQUE and TB-Dense both achieve &gt; 92% F 1 scores and &gt; 95% precision scores. For the 100K pre-training data selected for event masks, we found an 84.5% overlap of triggers identified by both models. We further apply ECONET trained on both event mask data to the target tasks and achieve comparable performances shown in <ref type="table" target="#tab_15">Table 10</ref> of the appendix. These results suggest that the impact of different event annotations is minimal and triggers detected in either model can generalize to different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Ablation Studies</head><p>To better understand our proposed model, we experiment with additional continual training methods and compare their fine-tuning performances.  Random Masks. As most target datasets we use are in the news domain, to study the impact of potential domain-adaption, we continue to train PTLMs with the original objective on the same data using random masks. To compare fairly with the generator and ECONET, we only mask 1 token per training sample. The search range of hyperparameters is the same as in Section 3. As <ref type="table" target="#tab_4">Table 3</ref> and 11 (appendix) show, continual pre-training with random masks, in general, does not improve and sometimes hurt fine-tuning performances compared with fine-tuning with original PTLMs. We hypothesize that this is caused by masking a smaller fraction (1 out of ?50 average) tokens than the original 15%. RoBERTa LARGE + ECONET achieves the best fine-tuning results across the board.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Fine-tuning under Low-resource Settings</head><p>In <ref type="table" target="#tab_6">Table 4</ref>, we compare the improvements of fine-tuning RoBERTa LARGE + ECONET over RoBERTa LARGE using full and 10% of the training data. Measured by both absolute and relative percentage gains, the majority of the improvements are much more significant under low-resource settings. This suggests that the transfer of event temporal knowledge is more salient when data is scarce. We further show fine-tuning performance comparisons using different ratios of the training data in <ref type="figure">Figure 6a-6b</ref>  Sec. 2.6, for a particular ERE task (e.g. TB-Dense), we need to predict the temporal relations between a pair of event triggers e i , e j ? P i,j with associated vector representations v l,h i , v l,h j , l ? L, h ? H in an MLM. L and H are the number of layers and attention heads respectively. We further use T m ? T to denote a temporal indicator category listed in <ref type="table">Table 1</ref>, and t m,n ? T m denote a particular temporal indicator. If we let attn(v l,h i , v l,h x ) represents the attention score between an event vector and any other hidden vectors, we can aggregate the per-layer attention score between e i and t m,n as,</p><formula xml:id="formula_5">a l i,tm,n = 1 H H h attn(v l,h i , v l,h tm,n )</formula><p>. Similarly, we can compute a l j,tm,n . The final per-layer attention score for (e i , e j ) is a l tm,n = 1 2 a l i,tm,n + a l j,tm,n . To compute the attention score for the T m category, we take the average of {a l tm,n | ?t m,n ? T m and ?t m,n ? P i,j }. Note we assume a temporal indicator is a single token to simplify notations above; for multiple-token indicators, we take the average of attn(v l,h i , v l,h x?tm,n ). <ref type="figure">Figure 4</ref> shows the cumulative attention scores for temporal indicator categories, [before], <ref type="bibr">[after] and [during]</ref> in ascending order of model layers. We observe that the attention scores for RoBERTa LARGE and ECONET align well on the bottom layers, but ECONET outweighs RoBERTa LARGE in middle to top layers. Previous research report that upper layers of pre-trained language models focus more on complex semantics as opposed to shallow surface forms or syntax on the lower layers <ref type="bibr" target="#b25">(Tenney et al., 2019;</ref><ref type="bibr" target="#b13">Jawahar et al., 2019)</ref>. Thus, our findings here show another piece of evidence that targeted masking is effective at capturing temporal indicators, which could facilitate semantics tasks including temporal reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Temporal Knowledge Injection</head><p>We hypothesize in the introduction that vanilla PTLMs lack special attention to temporal indica-  tors and events, and our proposed method addresses this issue by a particular design of mask prediction strategy and a discriminator that is able to distinguish reasonable events and temporal indicators from noises. In this section, we show more details of how such a mechanism works. The heat maps in <ref type="figure" target="#fig_2">Figure 5</ref> calculate the fine-tuning performance differences between 1) RoBERTa LARGE and continual pre-training with random masks <ref type="figure" target="#fig_2">(Figure 5a)</ref>; and 2) between RoBERTa LARGE and ECONET <ref type="figure" target="#fig_2">(Figure 5b)</ref>. Each cell shows the difference for each label class in TB-Dense conditional on samples' input passage containing a temporal indicator in the categories specified in <ref type="table">Table 1</ref>. Categories with less than 50 sample matches are excluded from the analysis.</p><p>In <ref type="figure" target="#fig_2">Figure 5a</ref>, the only gains come from VAGUE, which is an undetermined class in TB-Dense to handle unclear pairwise event relations. This shows that continual pre-training with random masks works no better than original PTLMs to leverage existing temporal indicators in the input passage to distinguish positive temporal relations from unclear ones. On the other hand, in <ref type="figure" target="#fig_2">Figure 5b</ref>, having temporal indicators in general benefits much more for BEFORE, AFTER, IS_INCLUDED labels. The only exception is INCLUDES, but it is a small class with only 4% of the data.</p><p>More interestingly, notice the diagonal cells, i.e. ([before], BEFORE), ([after], AFTER) and ([during], INCLUDES) have the largest values in the respective columns. These results are intuitive as temporal indicators should be most beneficial for temporal relations associated with their categories.</p><p>Combining these two sets of results, we provide additional evidence that ECONET helps PTLMs better capture temporal indicators and thus results in stronger fine-tuning performances.</p><p>Our final analysis attempts to show why discriminator helps. We feed 1K unused masked samples into the generator of the best ECONET in <ref type="table" target="#tab_2">Table 2</ref> to predict either the masked temporal indicators or masked events. We then examine the accuracy of the discriminator for correctly and incorrectly predicted masked tokens. As shown in <ref type="table" target="#tab_2">Table 12</ref> of the appendix, the discriminator aligns well with the event generator's predictions. For the temporal generator, the discriminator disagrees substantially (82.2%) with the "incorrect" predictions, i.e. the generator predicts a supposedly wrong indicator, but the discriminator thinks it looks original.</p><p>To understand why, we randomly selected 50 disagreed samples and found that 12 of these "incorrect" predictions fall into the same temporal indicator group of the original ones and 8 of them belong to the related groups in <ref type="table">Table 1</ref>. More details and examples can be found in <ref type="table" target="#tab_4">Table 13</ref> in the appendix. This suggests that despite being nearly perfect replacements of the original masked indicators, these 40% samples are penalized as wrong predictions when training the generator. The discriminator, by disagreeing with the generator, provides opposing feedback that trains the overall model to better capture indicators with similar temporal signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Language Model Pretraining. Since the breakthrough of BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, PTLMs have become SOTA models for a variety of NLP applications. There have also been several modifications/improvements built on the original BERT model. RoBERTa <ref type="bibr" target="#b17">(Liu et al., 2019)</ref> removes the next sentence prediction in BERT and trains with longer text inputs and more steps. ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref> proposes a generator-discriminator architecture, and addresses the sample-inefficiency issue in previous PTLMs.</p><p>Recent research explored methods to continue to train PTLMs so that they can adapt better to downstream tasks. For example, TANDA <ref type="bibr" target="#b6">(Garg et al., 2019)</ref> adopts an intermediate training on modified Natural Questions dataset <ref type="bibr" target="#b15">(Kwiatkowski et al., 2019)</ref> so that it performs better for the Answer Sentence Selection task. <ref type="bibr">Zhou et al. (2020b)</ref> proposed continual training objectives that require a model to distinguish natural sentences from those with concepts randomly shuffled or generated by models, which enables language models to capture large-scale commonsense knowledge. Event Temporal Reasoning. There has been a surge of attention to event temporal reasoning research recently. Some noticeable datasets include ERE samples: TB-Dense <ref type="bibr" target="#b3">(Chambers et al., 2014)</ref>, MATRES <ref type="bibr" target="#b21">(Ning et al., 2018)</ref> and <ref type="bibr">RED (O'Gorman et al., 2016)</ref>. Previous SOTA systems on these data leveraged PTLMs and structured learning <ref type="bibr" target="#b11">(Han et al., 2019c;</ref><ref type="bibr" target="#b27">Wang et al., 2020;</ref><ref type="bibr" target="#b34">Zhou et al., 2020c;</ref> and have substantially improved model performances, though none of them tackled the issue of lacking event temporal knowledge in PTLMs. TORQUE <ref type="bibr" target="#b20">(Ning et al., 2020)</ref> and MCTACO <ref type="bibr" target="#b31">(Zhou et al., 2019)</ref> are recent MRC datasets that attempt to reason about event temporal relations using natural language rather than ERE formalism. <ref type="bibr" target="#b32">Zhou et al. (2020a)</ref> and <ref type="bibr" target="#b30">Zhao et al. (2020)</ref> are two recent works that attempt to incorporate event temporal knowledge in PTLMs. The formal one focuses on injecting temporal commonsense with targeted event time, frequency and duration masks while the latter one leverages distantly labeled pairwise event temporal relations, masks before/after indicators, and focuses on ERE application only. Our work differs from them by designing a targeted masking strategy for event triggers and comprehensive temporal indicators, proposing a continual training method with mask prediction and contrastive loss, and applying our framework on a broader range of event temporal reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In summary, we propose a continual training framework with targeted mask prediction and contrastive loss to enable PTLMs to capture event temporal knowledge. Extensive experimental results show that both the generator and discriminator components can be helpful to improve fine-tuning performances over 5 commonly used data in event temporal reasoning. The improvements of our methods are much more pronounced in low-resource settings, which points out a promising direction for few-shot learning in this research area. <ref type="table" target="#tab_10">Table 5</ref> describes basic statistics for target datasets used in this work. The numbers of train/dev/test samples for TORQUE and MCTACO are question based. There is no training set provided in MCTACO. So we train on the dev set and report the evaluation results on the test set following <ref type="bibr" target="#b23">Pereira et al. (2020)</ref>. The numbers of train/dev/test samples for TB-Dense, MATRES and RED refer to <ref type="bibr">(event, event, relation)</ref> triplets. The standard dev set is not provide by MATRES and RED, so we follow the split used in <ref type="bibr" target="#b29">Zhang et al. (2021)</ref> and <ref type="bibr" target="#b10">Han et al. (2019b)</ref>.  Downloading link for the (processed) continual pretraining data is provided in the README file of the code package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Event Detection Model</head><p>As mentioned briefly in Secion 2, we train an event prediction model using event annotations in TORQUE. We finetune RoBERTa LARGE on the training set and select models based on the performance on the dev set. The best model achieves &gt; 92% event prediction F 1 score with &gt; 95% precision score after just 1 epoch of training, which indicates that this is a highly accurate model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Reproduction Checklist</head><p>Number of parameters. We continue to train BERT LARGE and RoBERTa LARGE and so the number of parameters are the same as the original PTLMs, which is 336M.</p><p>Hyper-parameter Search Due to computation constraints, we had to limit the search range of hyper-parameters for ECONET. For learning rates, we tried (1e ?6 , 2e ?6 ); for weights on the contrastive loss (?), we tried (1.0, 2.0).</p><p>Best Hyper-parameters. In <ref type="table" target="#tab_11">Table 6 and Table 7</ref>, we provide hyper-parameters for our best performing language model using RoBERTa LARGE + ECONET and BERT LARGE + ECONET and best hyper-parameters for fine-tuning them on downstream tasks. For fine-tuning on the target datasets. We conducted grid search for learning rates in the range of (5e ?6 , 1e ?5 ) and for batch size in the range of <ref type="bibr">(2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">12)</ref>. We fine-tuned all models for 10 epochs with three random seeds (5, 7, 23).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>learning rate batch size ?    </p><formula xml:id="formula_6">ECONET 1e ?6 8 1.0 TORQUE 1e ?5 12 - MCTACO 5e ?6 4 - TB-Dense 5e ?6 4 - MATRES 5e ?6 2 - RED 5e ?6 2 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Fine-tuning under Low-resource Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Variants of ECONET</head><p>We also experimented with a variant of ECONET by first pretraining RoBERTa LARGE + Generator for a few thousands steps and then continue to pretrain with ECONET. However, this method leads worse finetuning results, which seems to contradict the suggestions in <ref type="bibr">Zhou et al. (2020b)</ref> and <ref type="bibr" target="#b4">Clark et al. (2020)</ref> that the generator needs to be first trained to obtain a good prediction distribution for the discriminator. We speculate that this is due to our temporal and event mask predictions being easier tasks than those in the previous work, which makes the "warm-up steps" for the generator not necessary.    <ref type="table" target="#tab_2">Table 12</ref> shows the alignment of between the generator and the discriminator, and <ref type="table" target="#tab_4">Table 13</ref> shows the examples of "disagreed" samples between the     Text: A letter also went home a week ago in Pelham, in Westchester County, New York, mask a threat made by a student in a neighboring town circulated in several communities within hours...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Impact of Event Models</head><p>Ex 2. original: prior to; predicted: before Text: ... An investigation revealed that rock gauges were picking up swifter rates of salt movement in the ceiling of the room, but at Wipp no one had read the computer printouts for at least one month mask the collapse.</p><p>Type II. Related Group: 8/50 (16%) Ex 3. original: in the past; predicted: before text: Mr. Douglen confessed that Lautenberg, which had won mask , was "a seasoned roach and was ready for this race...</p><p>Ex 4. original: previously; predicted: once text: Under the new legislation enacted by Parliament, divers who mask had access to only 620 miles of the 10,000 miles of Greek coast line will be able to explore ships and "archaeological parks" freely... <ref type="table" target="#tab_4">Table 13</ref>: Categories and examples of highly related "incorrect" temporal indicator predictions by the generator, but labeled as "correct" by the discriminator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Top: an example illustrating the difference between ERE and QA / MRC samples of event temporal reasoning. Bottom: our targeted masking strategy for ECONET v.s. random masking in PTLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Target ERE and QA task illustrations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Performance (F 1 score) differences by temporal indicator categories and label classes in TB-Dense. Fine-tuning on 10% TB-Dense training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Attentions scores for [during] indicators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Attentions score comparisons between RoBERTa LARGE and ECONET for all model layers. All numbers are multiplied by 100 and average over 3 random seeds for illustration purpose J Analysis of the Discriminator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TacoLM 65.4(?0.8) 37.1(?1.0) 21.0(?0.8) 69.3(?0.6) 40.5(?0.5) 64.8(?0.7) 70.9(?0.3) 40.3(?1.7) BERT LARGE 70.6(?1.2) 43.7(?1.6) 27.5(?1.2) 70.3(?0.9) 43.2(?0.6) 62.8(?1.4) 70.5(?0.9) 39.4(?0.6) + ECONET 71.4(?0.7) 44.8(?0.4) 28.5(?0.5) 69.2(?0.9) 42.3(?0.5) 63.0(?0.6) 70.4(?0.9) 40.2(?0.8) RoBERTa LARGE 75.1(?0.4) 49.6(?0.5) 35.3(?0.8) 75.5(?1.0) 50.4(?0.9) 62.8(?0.3) 78.3(?0.5) 39.4(?0.4)</figDesc><table><row><cell>+ Generator</cell><cell cols="8">75.8(?0.4) 51.2(?1.1) 35.8(?0.9) 75.1(?1.4) 50.2(?1.2) 65.2(?0.6) 77.0(?0.9) 41.0(?0.6)</cell></row><row><cell>+ ECONET</cell><cell cols="8">76.1(?0.2) 51.6(?0.4) 36.8(?0.2) 76.3(?0.3) 52.8(?1.9) 64.8(?1.4) 78.8(?0.6) 42.8(?0.7)</cell></row><row><cell>ECONET (best)</cell><cell>76.3</cell><cell>52.0</cell><cell>37.0</cell><cell>76.8</cell><cell>54.7</cell><cell>66.8</cell><cell>79.3</cell><cell>43.8</cell></row><row><cell>Current SOTA</cell><cell>75.2  *</cell><cell>51.1</cell><cell>34.5</cell><cell>79.5  ?</cell><cell>56.5</cell><cell>66.7  ? ?</cell><cell>80.3  ?</cell><cell>34.0  ? ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall experimental results. Refer to Section 3.4 for naming conventions. The SOTA performances for TORQUE * are provided by<ref type="bibr" target="#b20">Ning et al. (2020)</ref> and the numbers are average over 3 random seeds. The SOTA performances for MCTACO ? are provided by<ref type="bibr" target="#b23">Pereira et al. (2020)</ref>; TB-Dense ? ? and MATRES ? by Zhang et al.</figDesc><table /><note>(2021) and RED ? ? by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Fine-tuning performances with different pretraining methods. All numbers are average over 3 random seeds. Std. Dev. ? 1% is underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>RoBERTa LARGE + ECONET's improvements over RoBERTa LARGE using full train data v.s. 10% of train data. ? indicates absolute points improvements while ?% indicates relative gains per F 1 scores.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>in the appendix. The results demonstrate that ECONET can outperform RoBERTa LARGE consistently when fine-tuning TORQUE and RED.</figDesc><table><row><cell>5 10 15 20 25 30 Average Attention Scores</cell><cell>ECONET [before] RoBERTa [before] ECONET [after] RoBERTa [after] ECONET [during] RoBERTa [during]</cell></row><row><cell>0</cell><cell>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Layers</cell></row><row><cell cols="2">Figure 4: Cumulative attention score comparisons be-</cell></row><row><cell cols="2">tween RoBERTa LARGE and ECONET on TB-Dense</cell></row><row><cell cols="2">test data. All numbers are multiplied by 100 and aver-</cell></row><row><cell cols="2">aged over 3 random seeds for illustration clarity.</cell></row></table><note>4.6 Attention Scores on Temporal Indicators In this section, we attempt to show explicitly how ECONET enhances MLMs' attentions on temporal indicators for downstream tasks. As mentioned in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>The numbers of samples for TORQUE refers to number of questions; the numbers for MCTACO are valid question and answer pairs; and the numbers of samples for TB-Dense, MATRES and RED are all (event, event, relation) triplets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters of our best performing LM with RoBERTa LARGE + ECONET as well as best hyper-parameters for fine-tuning on downstream tasks.</figDesc><table><row><cell>Method</cell><cell cols="2">learning rate batch size</cell><cell>?</cell></row><row><cell>ECONET</cell><cell>2e ?6</cell><cell>8</cell><cell>1.0</cell></row><row><cell>TORQUE</cell><cell>1e ?5</cell><cell>12</cell><cell>-</cell></row><row><cell>MCTACO</cell><cell>1e ?5</cell><cell>2</cell><cell>-</cell></row><row><cell>TB-Dense</cell><cell>1e ?5</cell><cell>2</cell><cell>-</cell></row><row><cell>MATRES</cell><cell>5e ?6</cell><cell>4</cell><cell>-</cell></row><row><cell>RED</cell><cell>1e ?5</cell><cell>6</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters of our best performing LM with BERT LARGE + ECONET as well as best hyperparameters for fine-tuning on downstream tasks.</figDesc><table><row><cell>Dev Set Performances We show average dev set</cell></row><row><cell>performances in Table 9 corresponding to our main</cell></row><row><cell>results in Table 2.</cell></row><row><cell>We leverage McNemar's tests (McNemar, 1947)</cell></row><row><cell>to show ECONET's improvements against</cell></row><row><cell>RoBERTa LARGE . McNemar's tests compute</cell></row><row><cell>statistics by aggregating all samples' prediction</cell></row><row><cell>correctness. For ERE tasks, this value is simply</cell></row><row><cell>classification correctness; for QA tasks (TORQUE</cell></row><row><cell>and MCTACO), we use EM per question-answer</cell></row><row><cell>pairs.</cell></row></table><note>D Significance Tests.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows improvements of ECONET over</cell></row><row><cell>RoBERTa LARGE are much more salient under low-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>McNemar's tests for improvement significance between best single models of RoBERTa LARGE and ECONET on the test data. RoBERTa LARGE vs. RoBERTa LARGE + ECONET over different ratios of the training data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Tests with p-values &lt;</cell></row><row><cell cols="11">0.05 (  *  *  ) indicate strong statistical significance; tests</cell></row><row><cell cols="11">with p-values &lt; 0.1 (  *  ) indicate weak statistical signifi-</cell></row><row><cell cols="2">cance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">resource setting.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>72.5 75.0 77.5</cell><cell cols="2">RoBERTa + ECONET</cell><cell>72.8</cell><cell>73.4</cell><cell>73.6</cell><cell>74.2</cell><cell>74.1</cell><cell>74.9</cell><cell>75.1</cell><cell>76.1</cell></row><row><cell>67.5 70.0</cell><cell></cell><cell>66.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>65.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60.0 62.5</cell><cell>59.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.1</cell><cell cols="2">0.3</cell><cell cols="2">0.5</cell><cell cols="2">0.7</cell><cell cols="2">1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) TORQUE</cell><cell></cell><cell></cell></row><row><cell>35.0 37.5 40.0 42.5 45.0</cell><cell cols="2">RoBERTa + ECONET</cell><cell></cell><cell>35.2</cell><cell>37.2</cell><cell>37.6</cell><cell>38.7</cell><cell>39.4</cell><cell>39.4</cell><cell>42.8</cell></row><row><cell>27.5 30.0 32.5</cell><cell>27.2</cell><cell>29.0</cell><cell>30.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25.0</cell><cell cols="2">0.1</cell><cell cols="2">0.3</cell><cell cols="2">0.5</cell><cell cols="2">0.7</cell><cell cols="2">1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) RED</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 6:</cell><cell cols="8">Performances (F 1 scores) compari-</cell></row><row><cell cols="7">son between fine-tuning</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10</head><label>10</label><figDesc>compares results based on two event annotations.</figDesc><table><row><cell>H Ablation Studies for BERT</cell></row><row><cell>I Attention Scores</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Average Dev Performances Corresponding to Table 2. Note that for MCTACO, we train on dev set and evaluate on the test set as mentioned in Section 3, so we do not report test performance again here.</figDesc><table><row><cell></cell><cell cols="3">TORQUE TB-D RED</cell></row><row><cell cols="2">Event Annotations F 1 EM C</cell><cell>F 1</cell><cell>F 1</cell></row><row><cell>TORQUE</cell><cell cols="3">76.1 51.6 36.8 64.8 42.8</cell></row><row><cell>TB-Dense</cell><cell cols="3">76.1 51.3 36.4 65.1 42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Fine-tuning performance comparisons using event detection models trained on TORQUE v.s. TB-Dense event annotations. All numbers are average over 3 random seeds. Std. Dev. ? 1% is underlined.</figDesc><table><row><cell></cell><cell cols="3">TORQUE TB-D RED</cell></row><row><cell>Methods</cell><cell>F 1 EM C</cell><cell>F 1</cell><cell>F 1</cell></row><row><cell>BERT LARGE</cell><cell cols="3">70.6 43.7 27.5 62.8 39.4</cell></row><row><cell cols="4">+ random mask 70.6 44.1 27.2 63.4 35.3</cell></row><row><cell>+ ECONET</cell><cell cols="3">71.4 44.8 28.5 63.0 40.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Fine-tuning performances. All numbers are average over 3 random seeds. Std. Dev. ? 1% is underlined.generator and the discriminator. Detailed analysis can be found in Section 4 in the main text.</figDesc><table><row><cell></cell><cell cols="4">Temporal Generator Event Generator</cell></row><row><cell></cell><cell>Corr.</cell><cell>Incorr.</cell><cell>Corr.</cell><cell>Incorr.</cell></row><row><cell>Total #</cell><cell>837</cell><cell>163</cell><cell>26</cell><cell>974</cell></row><row><cell>Discr. Corr. #</cell><cell>816</cell><cell>29</cell><cell>25</cell><cell>964</cell></row><row><cell>Accuracy</cell><cell>97.5%</cell><cell>17.8%</cell><cell cols="2">96.2% 99.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Discriminator's alignment with generator's mask predictions in ECONET. Second column shows that discriminator strongly disagree with the "errors" made by the temporal generator.</figDesc><table /><note>Type I. Same Group: 12/50 (24%) Ex 1. original: when; predicted: while</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Reproduction code, training data and models are available here: https://github.com/PlusLabNLP/ECONET.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007 and DARPA under agreement number FA8750-19-2-0500.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2136</idno>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 6: Clinical TempEval</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 12: Clinical TempEval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Te</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1165</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 12: Clinical TempEval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="565" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense event ordering with a multi-pass architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HiEve: A corpus for extracting event hierarchies from news stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>?najder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3678" to="3683" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Content planning for neural story generation with aristotelian rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seraphina</forename><surname>Goldfarb-Tarrant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4319" to="4338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep structured neural network for event temporal relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="666" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextualized word embeddings enhanced event temporal relation extraction for story understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyue</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bashar</forename><surname>Alhafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2019), Workshop on Narrative Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint event and temporal relation extraction with shared representations and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain knowledge empowered structured neural net for end-to-end event temporal relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.461</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5717" to="5729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Forecastqa: A question answering challenge for event forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woojeong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00792</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>
		<editor>Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Connecting the dots: Event graph schema induction with path language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.50</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="684" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach. arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Note on the sampling error of the difference between correlated proportions or percentages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quinn</forename><surname>Mcnemar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caters: Causal and temporal relation scheme for semantic annotation of event structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyson</forename><surname>Grealish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TORQUE: A reading comprehension dataset of temporal ordering questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.88</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1158" to="1172" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multiaxis annotation scheme for event temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Richer event description: Integrating event coreference with temporal, causal and bridging annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Tim O&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Wright-Bettner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016)</title>
		<meeting>the 2nd Workshop on Computing News Storylines (CNS 2016)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial training for commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lis</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Evaluating temporal relations in clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<idno type="DOI">10.1136/amiajnl-2013-001628</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>i2b2 challenge</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1452</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SemEval-2013 task 1: TempEval-3: Evaluating time expressions, events, and temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naushad</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Llorens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint constrained learning for event-event relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.51</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Planand-write: Towards better automatic storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weischedel</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Extracting temporal event relation with syntacticguided temporal graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2104.09570</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Effective distant supervision for temporal relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shih-Ting Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12755</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">going on a vacation&quot; takes longer than &quot;going for a walk&quot;: A study of temporal commonsense understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3363" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal Common Sense Acquisition with Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bill Yuchen Lin, and Xiang Ren. 2020b. Pre-training text-to-text transformers for concept-centric common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Selvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyeon</forename><surname>Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Clinical temporal relation extraction with probabilistic soft logic regularization and global inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Harry</forename><surname>Caufield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peipei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08790</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
