<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matching Networks for One Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<email>vinyals@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
							<email>cblundell@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
							<email>korayk@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
							<email>wierstra@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Matching Networks for One Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans learn new concepts with very little supervision -e.g. a child can generalize the concept of "giraffe" from a single picture in a book -yet our best deep learning systems need hundreds or thousands of examples. This motivates the setting we are interested in: "one-shot" learning, which consists of learning a class from a single labelled example.</p><p>Deep learning has made major advances in areas such as speech <ref type="bibr" target="#b6">[7]</ref>, vision <ref type="bibr" target="#b12">[13]</ref> and language <ref type="bibr" target="#b15">[16]</ref>, but is notorious for requiring large datasets. Data augmentation and regularization techniques alleviate overfitting in low data regimes, but do not solve it. Furthermore, learning is still slow and based on large datasets, requiring many weight updates using stochastic gradient descent. This, in our view, is mostly due to the parametric aspect of the model, in which training examples need to be slowly learnt by the model into its parameters.</p><p>In contrast, many non-parametric models allow novel examples to be rapidly assimilated, whilst not suffering from catastrophic forgetting. Some models in this family (e.g., nearest neighbors) do not require any training but performance depends on the chosen metric <ref type="bibr" target="#b0">[1]</ref>. Previous work on metric learning in non-parametric setups <ref type="bibr" target="#b17">[18]</ref> has been influential on our model, and we aim to incorporate the best characteristics from both parametric and non-parametric models -namely, rapid acquisition of new examples while providing excellent generalisation from common examples.</p><p>The novelty of our work is twofold: at the modeling level, and at the training procedure. We propose Matching Nets (MN), a neural network which uses recent advances in attention and memory that enable rapid learning. Secondly, our training procedure is based on a simple machine learning principle: test and train conditions must match. Thus to train our network to do rapid learning, we Besides our contributions in defining a model and training criterion amenable for one-shot learning, we contribute by the definition of tasks that can be used to benchmark other approaches on both ImageNet and small scale language modeling. We hope that our results will encourage others to work on this challenging problem.</p><p>We organized the paper by first defining and explaining our model whilst linking its several components to related work. Then in the following section we briefly elaborate on some of the related work to the task and our model. In Section 4 we describe both our general setup and the experiments we performed, demonstrating strong results on one-shot learning on a variety of tasks and setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our non-parametric approach to solving one-shot learning is based on two components which we describe in the following subsections. First, our model architecture follows recent advances in neural networks augmented with memory (as discussed in Section 3). Given a (small) support set S, our model defines a function c S (or classifier) for each S, i.e. a mapping S ? c S (.). Second, we employ a training strategy which is tailored for one-shot learning from the support set S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>In recent years, many groups have investigated ways to augment neural network architectures with external memories and other components that make them more "computer-like". We draw inspiration from models such as sequence to sequence (seq2seq) with attention <ref type="bibr" target="#b1">[2]</ref>, memory networks <ref type="bibr" target="#b28">[29]</ref> and pointer networks <ref type="bibr" target="#b26">[27]</ref>.</p><p>In all these models, a neural attention mechanism, often fully differentiable, is defined to access (or read) a memory matrix which stores useful information to solve the task at hand. Typical uses of this include machine translation, speech recognition, or question answering. More generally, these architectures model P (B|A) where A and/or B can be a sequence (like in seq2seq models), or, more interestingly for us, a set <ref type="bibr" target="#b25">[26]</ref>.</p><p>Our contribution is to cast the problem of one-shot learning within the set-to-set framework <ref type="bibr" target="#b25">[26]</ref>.</p><p>The key point is that when trained, Matching Networks are able to produce sensible test labels for unobserved classes without any changes to the network. More precisely, we wish to map from a (small) support set of k examples of image-label pairs S = {(x i , y i )} k i=1 to a classifier c S (x) which, given a test examplex, defines a probability distribution over outputs?. We define the mapping S ? c S (x) to be P (?|x, S) where P is parameterised by a neural network. Thus, when given a new support set of examples S from which to one-shot learn, we simply use the parametric neural network defined by P to make predictions about the appropriate label? for each test examplex: P (?|x, S ). In general, our predicted output class for a given input unseen examplex and a support set S becomes arg max y P (y|x, S).</p><p>Our model in its simplest form computes? as follows:</p><formula xml:id="formula_0">y = k i=1 a(x, x i )y i<label>(1)</label></formula><p>where x i , y i are the samples and labels from the support set S = {(x i , y i )} k i=1 , and a is an attention mechanism which we discuss below. Note that eq. 1 essentially describes the output for a new class as a linear combination of the labels in the support set. Where the attention mechanism a is a kernel on X ? X, then (1) is akin to a kernel density estimator. Where the attention mechanism is zero for the b furthest x i fromx according to some distance metric and an appropriate constant otherwise, then (1) is equivalent to 'k ? b'-nearest neighbours (although this requires an extension to the attention mechanism that we describe in Section 2.1.2). Thus (1) subsumes both KDE and kNN methods. Another view of (1) is where a acts as an attention mechanism and the y i act as memories bound to the corresponding x i . In this case we can understand this as a particular kind of associative memory where, given an input, we "point" to the corresponding example in the support set, retrieving its label. However, unlike other attentional memory mechanisms <ref type="bibr" target="#b1">[2]</ref>, (1) is non-parametric in nature: as the support set size grows, so does the memory used. Hence the functional form defined by the classifier c S (x) is very flexible and can adapt easily to any new support set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The Attention Kernel</head><p>Equation 1 relies on choosing a(., .), the attention mechanism, which fully specifies the classifier. The simplest form that this takes (and which has very tight relationships with common attention models and kernel functions) is to use the softmax over the cosine distance c, i.e., a(x, x i ) = e c(f (x),g(xi)) / k j=1 e c(f (x),g(xj )) with embedding functions f and g being appropriate neural networks (potentially with f = g) to embedx and x i . In our experiments we shall see examples where f and g are parameterised variously as deep convolutional networks for image tasks (as in VGG <ref type="bibr" target="#b21">[22]</ref> or Inception <ref type="bibr" target="#b23">[24]</ref>) or a simple form word embedding for language tasks (see <ref type="bibr">Section 4)</ref>.</p><p>We note that, though related to metric learning, the classifier defined by Equation 1 is discriminative. For a given support set S and sample to classifyx, it is enough forx to be sufficiently aligned with pairs (x , y ) ? S such that y = y and misaligned with the rest. This kind of loss is also related to methods such as Neighborhood Component Analysis (NCA) <ref type="bibr" target="#b17">[18]</ref>, triplet loss <ref type="bibr" target="#b8">[9]</ref> or large margin nearest neighbor <ref type="bibr" target="#b27">[28]</ref>.</p><p>However, the objective that we are trying to optimize is precisely aligned with multi-way, one-shot classification, and thus we expect it to perform better than its counterparts. Additionally, the loss is simple and differentiable so that one can find the optimal parameters in an "end-to-end" fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Full Context Embeddings</head><p>The main novelty of our model lies in reinterpreting a well studied framework (neural networks with external memories) to do one-shot learning. Closely related to metric learning, the embedding functions f and g act as a lift to feature space X to achieve maximum accuracy through the classification function described in eq. 1.</p><p>Despite the fact that the classification strategy is fully conditioned on the whole support set through P (.|x, S), the embeddings on which we apply the cosine similarity to "attend", "point" or simply compute the nearest neighbor are myopic in the sense that each element x i gets embedded by g(x i ) independently of other elements in the support set S. Furthermore, S should be able to modify how we embed the test imagex through f .</p><p>We propose embedding the elements of the set through a function which takes as input the full set S in addition to x i , i.e. g becomes g(x i , S). Thus, as a function of the whole support set S, g can modify how to embed x i . This could be useful when some element x j is very close to x i , in which case it may be beneficial to change the function with which we embed x i -some evidence of this is discussed in Section 4. We use a bidirectional Long-Short Term Memory (LSTM) <ref type="bibr" target="#b7">[8]</ref> to encode x i in the context of the support set S, considered as a sequence (see appendix for a more precise definition).</p><p>The second issue can be fixed via an LSTM with read-attention over the whole set S, whose inputs are equal to x:</p><p>f (x, S) = attLSTM(f (x), g(S), K) where f (x) are the features (e.g., derived from a CNN) which are input to the LSTM (constant at each time step). K is the fixed number of unrolling steps of the LSTM, and g(S) is the set over which we attend, embedded with g. This allows for the model to potentially ignore some elements in the support set S, and adds "depth" to the computation of attention (see appendix for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Strategy</head><p>In the previous subsection we described Matching Networks which map a support set to a classification function, S ? c(x). We achieve this via a modification of the set-to-set paradigm augmented with attention, with the resulting mapping being of the form P ? (.|x, S), noting that ? are the parameters of the model (i.e. of the embedding functions f and g described previously).</p><p>The training procedure has to be chosen carefully so as to match inference at test time. Our model has to perform well with support sets S which contain classes never seen during training.</p><p>More specifically, let us define a task T as distribution over possible label sets L. Typically we consider T to uniformly weight all data sets of up to a few unique classes (e.g., 5), with a few examples per class (e.g., up to 5). In this case, a label set L sampled from a task T , L ? T , will typically have 5 to 25 examples.</p><p>To form an "episode" to compute gradients and update our model, we first sample L from T (e.g., L could be the label set {cats, dogs}). We then use L to sample the support set S and a batch B (i.e., both S and B are labelled examples of cats and dogs). The Matching Net is then trained to minimise the error predicting the labels in the batch B conditioned on the support set S. This is a form of meta-learning since the training procedure explicitly learns to learn from a given support set to minimise a loss over a batch. More precisely, the Matching Nets training objective is as follows:</p><formula xml:id="formula_1">? = arg max ? E L?T ? ? E S?L,B?L ? ? (x,y)?B log P ? (y|x, S) ? ? ? ? .<label>(2)</label></formula><p>Training ? with eq. 2 yields a model which works well when sampling S ? T from a different distribution of novel labels. Crucially, our model does not need any fine tuning on the classes it has never seen due to its non-parametric nature. Obviously, as T diverges far from the T from which we sampled to learn ?, the model will not work -we belabor this point further in Section 4.1.2.</p><p>3 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory Augmented Neural Networks</head><p>A recent surge of models which go beyond "static" classification of fixed vectors onto their classes has reshaped current research and industrial applications alike. This is most notable in the massive adoption of LSTMs <ref type="bibr" target="#b7">[8]</ref> in a variety of tasks such as speech <ref type="bibr" target="#b6">[7]</ref>, translation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref> or learning programs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. A key component which allowed for more expressive models was the introduction of "content" based attention in <ref type="bibr" target="#b1">[2]</ref>, and "computer-like" architectures such as the Neural Turing Machine <ref type="bibr" target="#b3">[4]</ref> or Memory Networks <ref type="bibr" target="#b28">[29]</ref>. Our work takes the metalearning paradigm of <ref type="bibr" target="#b20">[21]</ref>, where an LSTM learnt to learn quickly from data presented sequentially, but we treat the data as a set. The one-shot learning task we defined on the Penn Treebank <ref type="bibr" target="#b14">[15]</ref> relates to evaluation techniques and models presented in <ref type="bibr" target="#b5">[6]</ref>, and we discuss this in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metric Learning</head><p>As discussed in Section 2, there are many links between content based attention, kernel based nearest neighbor and metric learning <ref type="bibr" target="#b0">[1]</ref>. The most relevant work is Neighborhood Component Analysis (NCA) <ref type="bibr" target="#b17">[18]</ref>, and the follow up non-linear version <ref type="bibr" target="#b19">[20]</ref>. The loss is very similar to ours, except we use the whole support set S instead of pair-wise comparisons which is more amenable to one-shot learning. Follow-up work in the form of deep convolutional siamese <ref type="bibr" target="#b10">[11]</ref> networks included much more powerful non-linear mappings. Other losses which include the notion of a set (but use less powerful metrics) were proposed in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Lastly, the work in one-shot learning in <ref type="bibr" target="#b13">[14]</ref> was inspirational and also provided us with the invaluable Omniglot dataset -referred to as the "transpose" of MNIST. Other works used zero-shot learning on ImageNet, e.g. <ref type="bibr" target="#b16">[17]</ref>. However, there is not much one-shot literature on ImageNet, which we hope to amend via our benchmark and task definitions in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we describe the results of many experiments, comparing our Matching Networks model against strong baselines. All of our experiments revolve around the same basic task: an N -way k-shot learning task. Each method is providing with a set of k labelled examples from each of N classes that have not previously been trained upon. The task is then to classify a disjoint batch of unlabelled examples into one of these N classes. Thus random performance on this task stands at 1/N . We compared a number of alternative models, as baselines, to Matching Networks.</p><p>Let L denote the held-out subset of labels which we only use for one-shot. Unless otherwise specified, training is always on =L , and test in one-shot mode on L .</p><p>We ran one-shot experiments on three data sets: two image classification sets (Omniglot <ref type="bibr" target="#b13">[14]</ref> and ImageNet <ref type="bibr">[19, ILSVRC-2012]</ref>) and one language modeling (Penn Treebank). The experiments on the three data sets comprise a diverse set of qualities in terms of complexity, sizes, and modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Classification Results</head><p>For vision problems, we considered four kinds of baselines: matching on raw pixels, matching on discriminative features from a state-of-the-art classifier (Baseline Classifier), MANN <ref type="bibr" target="#b20">[21]</ref>, and our reimplementation of the Convolutional Siamese Net <ref type="bibr" target="#b10">[11]</ref>. The baseline classifier was trained to classify an image into one of the original classes present in the training data set, but excluding the N classes so as not to give it an unfair advantage (i.e., trained to classify classes in =L ). We then took this network and used the features from the last layer (before the softmax) for nearest neighbour matching, a strategy commonly used in computer vision <ref type="bibr" target="#b2">[3]</ref> which has achieved excellent results across many tasks. Following <ref type="bibr" target="#b10">[11]</ref>, the convolutional siamese nets were trained on a same-or-different task of the original training data set and then the last layer was used for nearest neighbour matching.</p><p>We also tried further fine tuning the features using only the support set S sampled from L . This yields massive overfitting, but given that our networks are highly regularized, can yield extra gains. Note that, even when fine tuning, the setup is still one-shot, as only a single example per class from L is used.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Omniglot</head><p>Omniglot <ref type="bibr" target="#b13">[14]</ref> consists of 1623 characters from 50 different alphabets. Each of these was hand drawn by 20 different people. The large number of classes (characters) with relatively few data per class <ref type="bibr" target="#b19">(20)</ref>, makes this an ideal data set for testing small-scale one-shot classification. The N -way Omniglot task setup is as follows: pick N unseen character classes, independent of alphabet, as L. Provide the model with one drawing of each of the N characters as S ? L and a batch B ? L. Following <ref type="bibr" target="#b20">[21]</ref>, we augmented the data set with random rotations by multiples of 90 degrees and used 1200 characters for training, and the remaining character classes for evaluation.</p><p>We used a simple yet powerful CNN as the embedding function -consisting of a stack of modules, each of which is a 3 ? 3 convolution with 64 filters followed by batch normalization <ref type="bibr" target="#b9">[10]</ref>, a Relu non-linearity and 2 ? 2 max-pooling. We resized all the images to 28 ? 28 so that, when we stack 4 modules, the resulting feature map is 1 ? 1 ? 64, resulting in our embedding function f (x). A fully connected layer followed by a softmax non-linearity is used to define the Baseline Classifier.</p><p>Results comparing the baselines to our model on Omniglot are shown in <ref type="table" target="#tab_1">Table 1</ref>. For both 1-shot and 5-shot, 5-way and 20-way, our model outperforms the baselines. There are no major surprises in these results: using more examples for k-shot classification helps all models, and 5-way is easier than 20-way. We note that the Baseline Classifier improves a bit when fine tuning on S , and using cosine distance versus training a small softmax from the small training set (thus requiring fine tuning) also performs well. Siamese nets fare well versus our Matching Nets when using 5 examples per class, but their performance degrades rapidly in one-shot. Fully Conditional Embeddings (FCE) did not seem to help much and were left out of the table due to space constraints.</p><p>Like the authors in <ref type="bibr" target="#b10">[11]</ref>, we also test our method trained on Omniglot on a completely disjoint taskone-shot, 10 way MNIST classification. The Baseline Classifier does about 63% accuracy whereas (as reported in their paper) the Siamese Nets do 70%. Our model achieves 72%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">ImageNet</head><p>Our experiments followed the same setup as Omniglot for testing, but we considered a rand and a dogs (harder) setup. In the rand setup, we removed 118 labels at random from the training set, then tested only on these 118 classes (which we denote as L rand ). For the dogs setup, we removed all classes in ImageNet descended from dogs (totalling 118) and trained on all non-dog classes, then tested on dog classes (L dogs ). ImageNet is a notoriously large data set which can be quite a feat of engineering and infrastructure to run experiments upon it, requiring many resources. Thus, as well as using the full ImageNet data set, we devised a new data set -miniImageNet -consisting of 60, 000 colour images of size 84 ? 84 with 100 classes, each having 600 examples. This dataset is more complex than CIFAR10 <ref type="bibr" target="#b11">[12]</ref>, but fits in memory on modern machines, making it very convenient for rapid prototyping and experimentation. This dataset is fully described in Appendix B. We used 80 classes for training and tested on the remaining 20 classes. In total, thus, we have randImageNet, dogsImageNet, and miniImageNet.</p><p>The results of the miniImageNet experiments are shown in <ref type="table" target="#tab_2">Table 2</ref>. As with Omniglot, Matching Networks outperform the baselines. However, miniImageNet is a much harder task than Omniglot which allowed us to evaluate Full Contextual Embeddings (FCE) sensibly (on Omniglot it made no difference). As we an see, FCE improves the performance of Matching Networks, with and without fine tuning, typically improving performance by around two percentage points.  Next we turned to experiments based upon full size, full scale ImageNet. Our baseline classifier for this data set was Inception <ref type="bibr" target="#b24">[25]</ref> trained to classify on all classes except those in the test set of classes (for randImageNet) or those concerning dogs (for dogsImageNet). We also compared to features from an Inception Oracle classifier trained on all classes in ImageNet, as an upper bound. Our Baseline Classifier is one of the strongest published ImageNet models at 79% top-1 accuracy on the standard ImageNet validation set. Instead of training Matching Networks from scratch on these large tasks, we initialised their feature extractors f and g with the parameters from the Inception classifier (pretrained on the appropriate subset of the data) and then further trained the resulting network on random 5-way 1-shot tasks from the training data set, incorporating Full Context Embeddings and our Matching Networks and training strategy.</p><p>The results of the randImageNet and dogsImageNet experiments are shown in <ref type="table" target="#tab_3">Table 3</ref>. The Inception Oracle (trained on all classes) performs almost perfectly when restricted to 5 classes only, which is not too surprising given its impressive top-1 accuracy. When trained solely on =L rand , Matching Nets improve upon Inception by almost 6% when tested on L rand , halving the errors. <ref type="figure" target="#fig_1">Figure 2</ref> shows two instances of 5-way one-shot learning, where Inception fails. Looking at all the errors, Inception appears to sometimes prefer an image above all others (these images tend to be cluttered like the example in the second column, or more constant in color). Matching Nets, on the other hand, manage to recover from these outliers that sometimes appear in the support set S .</p><p>Matching Nets manage to improve upon Inception on the complementary subset =L dogs (although this setup is not one-shot, as the feature extraction has been trained on these labels). However, on the much more challenging L dogs subset, our model degrades by 1%. We hypothesize this to the fact that the sampled set during training, S, comes from a random distribution of labels (from =L dogs ), whereas the testing support set S from L dogs contains similar classes, more akin to fine grained classification. Thus, we believe that if we adapted our training strategy to samples S from fine grained sets of labels instead of sampling uniformly from the leafs of the ImageNet class tree, improvements could be attained. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">One-Shot Language Modeling</head><p>We also introduce a new one-shot language task which is analogous to those examined for images. The task is as follows: given a query sentence with a missing word in it, and a support set of sentences which each have a missing word and a corresponding 1-hot label, choose the label from the support set that best matches the query sentence. Here we show a single example, though note that the words on the right are not provided and the labels for the set are given as 1-hot-of-5 vectors.  . it's not easy to roll out something that &lt;blank_token&gt; and make it pay mr. jacob says. comprehensive Query: in late new york trading yesterday the &lt;blank_token&gt; was quoted at N marks down from N marks late friday and at N yen down from N yen late friday. dollar Sentences were taken from the Penn Treebank dataset <ref type="bibr" target="#b14">[15]</ref>. On each trial, we make sure that the set and batch are populated with sentences that are non-overlapping. This means that we do not use words with very low frequency counts; e.g. if there is only a single sentence for a given word we do not use this data since the sentence would need to be in both the set and the batch. As with the image tasks, each trial consisted of a 5 way choice between the classes available in the set. We used a batch size of 20 throughout the sentence matching task and varied the set size across k=1,2,3. We ensured that the same number of sentences were available for each class in the set. We split the words into a randomly sampled 9000 for training and 1000 for testing, and we used the standard test set to report results. Thus, neither the words nor the sentences used during test time had been seen during training.</p><p>We compared our one-shot matching model to an oracle LSTM language model (LSTM-LM) <ref type="bibr" target="#b29">[30]</ref> trained on all the words. In this setup, the LSTM has an unfair advantage as it is not doing one-shot learning but seeing all the data -thus, this should be taken as an upper bound. To do so, we examined a similar setup wherein a sentence was presented to the model with a single word filled in with 5 different possible words (including the correct answer). For each of these 5 sentences the model gave a log-likelihood and the max of these was taken to be the choice of the model.</p><p>As with the other 5 way choice tasks, chance performance on this task was 20%. The LSTM language model oracle achieved an upper bound of 72.8% accuracy on the test set. Matching Networks with a simple encoding model achieve 32.4%, 36.1%, 38.2% accuracy on the task with k = 1, 2, 3 examples in the set, respectively. Future work should explore combining parametric models such as an LSTM-LM with non-parametric components such as the Matching Networks explored here.</p><p>Two related tasks are the CNN QA test of entity prediction from news articles <ref type="bibr" target="#b4">[5]</ref>, and the Children's Book Test (CBT) <ref type="bibr" target="#b5">[6]</ref>. In the CBT for example, a sequence of sentences from a book are provided as context. In the final sentence one of the words, which has appeared in a previous sentence, is missing. The task is to choose the correct word to fill in this blank from a small set of words given as possible answers, all of which occur in the preceding sentences. In our sentence matching task the sentences provided in the set are randomly drawn from the PTB corpus and are related to the sentences in the query batch only by the fact that they share a word. In contrast to CBT and CNN dataset, they provide only a generic rather than specific sequential context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we introduced Matching Networks, a new neural architecture that, by way of its corresponding training regime, is capable of state-of-the-art performance on a variety of one-shot classification tasks. There are a few key insights in this work. Firstly, one-shot learning is much easier if you train the network to do one-shot learning. Secondly, non-parametric structures in a neural network make it easier for networks to remember and adapt to new training sets in the same tasks. Combining these observations together yields Matching Networks. Further, we have defined new one-shot tasks on ImageNet, a reduced version of ImageNet (for rapid experimentation), and a language modeling task. An obvious drawback of our model is the fact that, as the support set S grows in size, the computation for each gradient update becomes more expensive. Although there are sparse and sampling-based methods to alleviate this, much of our future efforts will concentrate around this limitation. Further, as exemplified in the ImageNet dogs subtask, when the label distribution has obvious biases (such as being fine grained), our model suffers. We feel this is an area with exciting challenges which we hope to keep improving in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Description</head><p>In this section we fully specify the models which condition the embedding functions f and g on the whole support set S. Much previous work has fully described similar mechanisms, which is why we left the precise details for this appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Fully Conditional Embedding f</head><p>As described in section 2.1.2, the embedding function for an examplex in the batch B is as follows:</p><formula xml:id="formula_2">f (x, S) = attLSTM(f (x), g(S), K)</formula><p>where f is a neural network (e.g., VGG or Inception, as described in the main text). We define K to be the number of "processing" steps following work from <ref type="bibr" target="#b25">[26]</ref> from their "Process" block. g(S) represents the embedding function g applied to each element x i from the set S.</p><p>Thus, the state after k processing steps is as follows:</p><formula xml:id="formula_3">h k , c k = LSTM(f (x), [h k?1 , r k?1 ], c k?1 ) (3) h k =? k + f (x) (4) r k?1 = |S| i=1 a(h k?1 , g(x i ))g(x i ) (5) a(h k?1 , g(x i )) = softmax(h T k?1 g(x i ))<label>(6)</label></formula><p>noting that LSTM(x, h, c) follows the same LSTM implementation defined in <ref type="bibr" target="#b22">[23]</ref> with x the input, h the output (i.e., cell after the output gate), and c the cell. a is commonly referred to as "content" based attention, and the softmax in eq. 6 normalizes w.r.t. g(x i ). The read-out r k?1 from g(S) is concatenated to h k?1 . Since we do K steps of "reads", attLSTM(f (x), g(S), K) = h K where h k is as described in eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The Fully Conditional Embedding g</head><p>In section 2.1.2 we described the encoding function for the elements in the support set S, g(x i , S), as a bidirectional LSTM. More precisely, let g (x i ) be a neural network (similar to f above, e.g. a VGG or Inception model). Then we define g(x i , S) = h i + h i + g (x i ) with:</p><formula xml:id="formula_4">h i , c i = LSTM(g (x i ), h i?1 , c i?1 ) h i , c i = LSTM(g (x i ), h i+1 , c i+1 )</formula><p>where, as in above, LSTM(x, h, c) follows the same LSTM implementation defined in <ref type="bibr" target="#b22">[23]</ref> with x the input, h the output (i.e., cell after the output gate), and c the cell. Note that the recursion for h starts from i = |S|. As in eq. 4, we add a skip connection between input and outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B miniImageNet Description</head><p>To construct miniImageNet we chose 100 random classes from ImageNet, and used the first 80 for training, and the last 20 for testing. This split was used in our one-shot experiments described in section 4. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Matching Networks architecture train it by showing only a few examples per class, switching the task from minibatch to minibatch, much like how it will be tested when presented with a few examples of a new task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of two 5-way problem instance on ImageNet. The images in the set S contain classes never seen during training. Our model makes far less mistakes than the Inception baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>dollar 4 .</head><label>4</label><figDesc>we had a lot of people who threw in the &lt;blank_token&gt; today said &lt;unk&gt; ellis a partner in benjamin jacobson &amp; sons a specialist in trading ual stock on the big board.towel 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Results on the Omniglot dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on miniImageNet.</figDesc><table><row><cell>Model</cell><cell cols="2">Matching Fn Fine Tune</cell><cell>5-way Acc 1-shot 5-shot</cell></row><row><cell>PIXELS</cell><cell>Cosine</cell><cell>N</cell><cell>23.0% 26.6%</cell></row><row><cell>BASELINE CLASSIFIER</cell><cell>Cosine</cell><cell>N</cell><cell>36.6% 46.0%</cell></row><row><cell>BASELINE CLASSIFIER</cell><cell>Cosine</cell><cell>Y</cell><cell>36.2% 52.2%</cell></row><row><cell>BASELINE CLASSIFIER</cell><cell>Softmax</cell><cell>Y</cell><cell>38.4% 51.2%</cell></row><row><cell cols="2">MATCHING NETS (OURS) Cosine</cell><cell>N</cell><cell>41.2% 56.2%</cell></row><row><cell cols="2">MATCHING NETS (OURS) Cosine</cell><cell>Y</cell><cell>42.4% 58.0%</cell></row><row><cell cols="3">MATCHING NETS (OURS) Cosine (FCE) N</cell><cell>44.2% 57.0%</cell></row><row><cell cols="3">MATCHING NETS (OURS) Cosine (FCE) Y</cell><cell>46.6% 60.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on full ImageNet on rand and dogs one-shot tasks. Note that =L rand and =L dogs are sets of classes which are seen during training, but are provided for completeness.</figDesc><table><row><cell>Model</cell><cell>Matching Fn</cell><cell>Fine Tune</cell><cell cols="3">ImageNet 5-way 1-shot Acc L rand =L rand L dogs =L dogs</cell></row><row><cell>PIXELS</cell><cell>Cosine</cell><cell>N</cell><cell>42.0%</cell><cell>42.8%</cell><cell>41.4% 43.0%</cell></row><row><cell>INCEPTION CLASSIFIER</cell><cell>Cosine</cell><cell>N</cell><cell>87.6%</cell><cell>92.6%</cell><cell>59.8% 90.0%</cell></row><row><cell cols="2">MATCHING NETS (OURS) Cosine (FCE)</cell><cell>N</cell><cell cols="2">93.2% 97.0%</cell><cell>58.8% 96.4%</cell></row><row><cell>INCEPTION ORACLE</cell><cell cols="2">Softmax (Full) Y (Full)</cell><cell cols="2">? 99% ? 99%</cell><cell>? 99% ? 99%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1. an experimental vaccine can alter the immune response of people infected with the aids virus a &lt;blank_token&gt; u.s. scientist said. prominent 2. the show one of five new nbc &lt;blank_token&gt; is the second casualty of the three networks so far this fall. series 3. however since eastern first filed for chapter N protection march N it has consistently promised to pay creditors N cents on the &lt;blank_token&gt;.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1.2. Note that the last 20 class objects were never seen during training. For the exact 600 images that comprise each of the 100 classes in miniImageNet, please see the following text file: https://goo.gl/e3orz6 , department, departure, depress, designated, desk, desktop, detailing, devaluation, develops, devoe, di, dialogue, dictator, die, diesel, differ, digs, diluted, diminished, direct-mail, disappointing, discount, discrepancies, discuss, disease, disney, disruption, distributed, distributor, dive, diversified, divided, dividends, dodge, doing, domestic, dominant, domination,double-a, downgraded, downgrading, downtown, drives, drought, drunk, dunkin, earn, earthquakes, edisto, editions, educate, eggs, elaborate, elite, embarrassing, emerges, emerging, emigration, employers, empty, enactment, encourages, endorsement, enemies, engelken, enhanced, entertaining, enthusiastic, epicenter, equipped, era, erosion, esselte, est, ethical, ethiopia, eurodollar, events, everyone, exchanges, exciting, exclusively, executed, executing, executive, executives, exempt, expertise, explicit, explosion, expressed, expression, extending, extraordinary, faculty, failed, failure, fallout, faltered, fanfare, fare, farm, fast-growing, fasteners, fastest, fax, fazio, february, federated, fee, field, fifth, fighting, filipino, film, final, financiers, finished, finland, firmed, fiscal, fits, fitzwater, five-cent, fixed-income, fla, flamboyant, fleets, fleming, fletcher, flight, flights, flowers, focus, folk, following, foot, forecasting, found, fox, fray, freeway, freeways, freeze, frequency, freshman, fromstein, frustrating, fur, galileo, game, gandhi, garbage, gathered, gave, gear, gene, generale, genuine, gerard, giant, girl, gloomy, goes, golden, goodman, gov., governing, government-owned, governor, grave, greenhouse, gridlock, grim, guerrilla, guild, gun, h&amp;r, half-hour, handicapped, handy, hanging, happening, happy, harold, haunts, headed, heating, heavier, heavily, hedges, heights, heller, helping, helps, hepatitis, hess, high-definition, high-technology, hiring, hoffman, hold, hole, homeless, honduras, hooker, horizon, hot-dipped, houses, how, hubbard, hurricane, hydro-quebec, hyman, idaho, ill, illness, illustrated, immune, impeachment, implicit, impose, impression, impressive, increase, incredible, incurred, indexing, indiana, indicates, indications, influences, influx, inherent, inquiry, intensive, intentions, internationally, involves, irish, ironically, isler, itel, itt, j., jackson, jaguar, jazz, jefferson, jittery, jolted, july, jump, jury, justifies, karen, kean, keating, kent, kgb, khan, killing, knocked, knocking, koch, l.j., labs, lasts, lately, latest, lawrence, league, lean, least, leave, legitimacy, lehman, leisure, lend, leo, life, lighter, lights, linda, line, literature, live, living, longstanding, looking, looks, loral, lord, lose, lotus, louisville, lower, ltd., luis, lumpur, made, madrid, malcolm, male, manage, management, manic, manville, marcos, marked, market-makers, market-share, markets, mary, mass-market, mayor, mccormick, mcdonald, md., measured, member, members, memorandum, merabank, mercury, merely, merged, mergers, message, mich., mile, millions, mining, ministers, ministry, minorities, minutes, missile, mission, mitterrand, modified, monitor, months, moody, moore, mothers, motorola, movie, mr., much, multibillion-dollar, multiples, mundane, municipalities, muscle, mutual, mutual-fund, named, names, namibia, nashua, nathan, ncr, near-term, nec, necessary, necessity, negligence, negotiating, negotiation, nervousness, newcomers, newspaper, nice, noise, northrop, norton, nose, nothing, noticed, notification, notified, notwithstanding, november, nurses, nutritional, observed, oddly, off, offerings, official, oh, oklahoma, oldest, olympic, ones, opening, opera, optical, optimistic, or, original, orthodox, ortiz, ousted, outfit, outlook, outside, oversees, owen, oy, pachinko, packaged, painted, park, parker, part, particular, partly, patrick, patterson, payable, pc, peace, peaked, peddling, pegged, pepsico, perception, perfect, perfectly, pfizer, pharmaceutical, phelan, philippine, philippines, phony, photographic, physicians, picking, pigs, pittsburgh, place, plagued, plan, planes, planet, pleasure, poles, pool, portable, portfolio, ports, post-crash, pound, poured, poverty, precedent, preclude, pregnant, prescribed, presents, pretty, priced, privileges, procurement, products, profit-taking, projections, prominent, promise, promotional, prompted, proper, proponents, propose, prosecuted, protein, prototype, prove, proved, published, publisher, pull, pulled, pumped, pumping, pushing, quebec, quickview, quist, quite, radical, radio, rain, ranging, rank, rebates, rebel, rebound, rebuild, recent, recital, recognizes, recognizing, recorded, recorders, reduce, reduced, refinery, refrigerators, registered, regret, reinvest, rejected, rejecting, rejection, relations, relatively, relying, remark, remics, reorganization, repaired, repeatedly, reports, represent, repurchase, resembles, reserved, resisted, resolved, resort, rest, restraints, restrictions, restructured, restructuring, result, rican, right, ring, rise, robbed, robinson, robots, robust, roh, role, rolled, rose, rothschild, rough, royal, ruled, rushing, s.c, sale, salesmen, salespeople, salmonella, salvage, saul, says, scheduled, school, schwarz, seagram, second, sector, securities, seek, segment, seismic, seldom, selected, semel, sending, sentences, sentencing, session, settlement, seventh, shed, shell, sheraton, shifting, shocks, short, showed, shy, sigh, sights, signals, sir, site, sites, sitting, skinner, slashed, snapped, so-called, soldiers, solely, solo, somehow, sotheby, speak, specialist, specialize, specializing, specifically, specifications, speculate, speculated, spencer, sperry, spreading, spur, stake, standardized, standing, statistics, steady, stemmed, stern, stevens, stock-index, stockholm, straight, strategists, stream, strength, stress-related, strict, subscriber, suggestions, surplus, surprise, surprises, surrounding, syrian, taiwanese, tall, tap, tapped, task, taxation, taxed, tci, technicians, televised, temptation, testing, texans, theatre, third, this, thomas, those, thoughts, thriving, tickets, ties, tiger, tighter, tire, tisch, together, torontobased, toshiba, towers, toxin, traditional, trains, transit, trap, treated, trecker, tribune, trigger, triggering, trillion, tube, tune, turn, turnaround, typically, u.k., u.n., uncertain, underlying, underwear, underwrite, underwriter, underwriting, undo, unfortunately, unidentified, unilab, unisys, unit, unknown, unlawful, unless, unused, upheld, upon, upside, urge, usia, uv-b, valid, van, vendors, very, victim, vienna, violations, virginia, vision, visit, voluntary, w., wade, wait, wanting, ward, warner, wars, wary, wash., wealthy, wednesday, when-issued, whether, white-collar, wholly, widening, will, willingness, wilmington, win, winnebago, winners, wish, wolf, words, work, working, worse, would, yard, yards, yearly, yielding, youth, z, zones</figDesc><table><row><cell>L train =</cell></row><row><cell>n01614925, n01632777, n01641577, n01664065, n01687978, n01695060, n01729322, n01773157, n01833805, n01871265, n01877812,</cell></row><row><cell>n01978455, n01986214, n02013706, n02066245, n02071294, n02088466, n02090379, n02091635, n02096437, n02097130, n02099429,</cell></row><row><cell>n02108089, n02108915, n02109047, n02109525, n02111889, n02115641, n02123045, n02129165, n02167151, n02206856, n02264363,</cell></row><row><cell>n02279972, n02342885, n02346627, n02364673, n02454379, n02481823, n02486261, n02494079, n02655020, n02793495, n02804414,</cell></row><row><cell>n02808304, n02837789, n02895154, n02909870, n02917067, n02966687, n03000684, n03014705, n03041632, n03045698, n03065424,</cell></row><row><cell>n03180011, n03216828, n03355925, n03384352, n03424325, n03452741, n03482405, n03494278, n03594734, n03599486, n03630383,</cell></row><row><cell>n03649909, n03676483, n03690938, n03742115, n03868242, n03877472, n03976467, n03976657, n03998194, n04026417, n04069434,</cell></row><row><cell>n04111531, n04118538, n04200800</cell></row><row><cell>L test =</cell></row><row><cell>n04201297, n04204347, n04239074, n04277352, n04370456, n04409515, n04456115, n04479046, n04487394, n04525038, n04591713,</cell></row><row><cell>n04599235, n07565083, n07613480, n07695742, n07714571, n07717410, n07753275, n10148035, n12768682</cell></row><row><cell>C ImageNet Class Splits</cell></row><row><cell>Here we define the two class splits used in our full ImageNet experiments -these classes were</cell></row><row><cell>excluded for training during our one-shot experiments described in section 4.1.2.</cell></row></table><note>demonstrated</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Nal Kalchbrenner for brainstorming around the design of the function g, and Sander Dieleman and Sergio Guadarrama for their help setting up ImageNet. We would also like thank Simon Osindero for useful discussions around the tasks discussed in this paper, and Theophane Weber and Remi Munos for following some early developments. Karen Simonyan and David Silver helped with the manuscript, as well as many at Google DeepMind. Thanks also to Geoff Hinton and Alex Toshev for discussions about our results.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Locally weighted learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Atkeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep metric learning using triplet network. Similarity-Based Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ailon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<title level="m">Convolutional deep belief networks on cifar-10. Unpublished</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bm Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank. Computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mp Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Neighbourhood component analysis. NIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory networks. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">n07695742, n07697313, n07697537, n07716906, n12998815, n13133613 L dogs = n02085620</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2113978</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent neural network regularization</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">adopting, advised, advisers, advises, advising, aer, affidavits, afternoon, ag, aged, ages, agreements, airport, akzo, alaska, alcohol, alert, alliance, allied-signal, ally, altman, ambrosiano, american, amgen, amount, amounts, an, andy, angry, animals, annuities, antitrust, anybody, anyway, appointed, approaching, approvals, arabs, arafat, arbitration, argentina, arranged, arrest, artists, assembled, associations, assume, assumptions, atoms, attitudes, audio, authorities, authority, away, balls, bally, banknote, banks, banning, barely, barred, barriers, bass, battery, baum, bears, bell, belt, best, best-known, billion, binge, blamed, blanket, bloc, block, blocking, boat, bodies, boesel, bolstered, bonuses, boston, bowed, boys, bozell, bradstreet, brains, breakers, breaks, briefly, brink, brisk, broad-based, broken, bronx, brother, bsn, built, buried, burmah, burned, bursts, bush, businessland, businessman, buys, calculate, calculated, caltrans, campbell, candlestick, capitalism, captured, careers, carpeting, carried, carry-forward, casting, castle, catholic, caught, ceiling, cells, centuries, chair, chairs, challenged, chances, chandler, characters, charts, cheating, checks, cherry, chiron, cie, cie., cincinnati, circuit, civic, clara, classroom, cleanair, climate, closer, cms, cnw</title>
		<imprint/>
	</monogr>
	<note>s, 12-year, 190.58-point, 1930s, 26-week, a.c., abortion, absorbed, accelerating, acceptable, accords, accusations, achieve, acquires, actively, adapted, addition, adequate, admitting, adopt, adopted. coast, coats, cocom, cold, collected, comes, commercial, commerzbank, commissioned, committed, commute, complains, completing, computer, confirm, confiscated, confronted, conn, conn., consisting, consortium, constitute, consultant, consumer, consumers, contemporary, contra, contraceptive, contributing, convinced, cost-cutting, count, counterparts, counties, courses, cover, cracks, craft, crane, create, creating, crossing, crumbling, crusade, crusaders, cubic, curtail, curve, cushion, cut, cynthia, dairy, dam, david, davis, day, deal, dealerships, debentures, debut, deceptive, decided, decision, decisions, deck, defended, defenders, defenses, definitely, delivering, della</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
