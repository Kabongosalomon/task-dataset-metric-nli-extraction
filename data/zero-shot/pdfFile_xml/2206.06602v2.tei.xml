<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Isolation Forest for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzuo</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Deep Isolation Forest for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly Detection</term>
					<term>Isolation Forest</term>
					<term>Deep Representation</term>
					<term>Ensemble Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. This representation scheme facilitates high freedom of the partition in the original data space (equivalent to non-linear partition on subspaces of varying sizes), encouraging a unique synergy between random representations and random partition-based isolation. Extensive experiments show that our model achieves significant improvement over state-of-the-art isolation-based methods and deep detectors on tabular, graph and time series datasets; our model also inherits desired scalability from iForest.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A NOMALY detection has broad applications in various domains, such as detection of insurance fraud and financial crime, surveillance of complex systems like data centres and spacecraft, and identification of attacks and potential threats in cyberspace. Given these important applications, this task has been a popular research topic for decades, and numerous anomaly detection approaches have been introduced <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>In recent years, isolation forest (iForest) <ref type="bibr" target="#b2">[3]</ref> has been emerging as arguably the most popular anomaly detector due to its general effectiveness across different benchmarks and strong scalability. Compared to many existing methods such as distance/density-based methods, iForest better captures the key essence of anomalies, i.e., "few and different". It does not introduce extra assumptions of data characteristics, thus showing consistently effective performance across diverse datasets. Also, iForest is with linear time complexity, which is often a very appealing advantage in many industrial applications when there are large-scale data and strict requirements of time efficiency. Concretely, iForest uses an ensemble of isolation trees (iTree), in which each iTree is grown by iteratively branching. Leaf nodes are built by using random cuts in the values of randomly selected features until the data objects are isolated. Data abnormality is estimated according to the average depth traversing from the root node to the isolated leaf node in these iTrees.</p><p>Nevertheless, an explicit major issue is that it cannot handle hard anomalies (e.g., the anomalies that can be ? Hongzuo Xu, Yijie <ref type="bibr">Wang</ref>  only isolated in higher-order subspaces by looking into the combination of multiple features) because it treats all features separately and considers only one feature per isolation operation. <ref type="figure">Fig. 1</ref> exemplifies this issue with a simple toy example. Anomalies (represented as red triangles) are surrounded by ring-shaped normal samples, which cannot be isolated by either x-axis or y-axis slicing cuts. Although these anomalies might be finally isolated by multiple cuts, it results in indistinguishable isolation depth in iTrees compared to normal data. The failure of recalling these anomalies induces high false negative errors. Given that anomalies often contain critical information related to potential accidents or faults, those false negatives may cause serious consequences. Therefore, this issue has been a major bottleneck hindering the performance of iForest, particularly on datasets with high-dimensional/non-linearseparable data spaces.</p><p>Another inherent imperfection of iForest is that it assigns unexpectedly low anomaly scores to artefacts introduced by the algorithm itself, which is revealed as the "ghost region" problem in <ref type="bibr" target="#b3">[4]</ref>. To clearly demonstrate this issue, we visualise the data distribution of three 2-D synthetic datasets used in <ref type="bibr" target="#b3">[4]</ref> and anomaly score maps generated by iForest in the first two columns in <ref type="figure">Fig. 2</ref>, respectively. As can be seen from all three anomaly score maps, iForest assigns clearly lower anomaly scores to some artefact regions, i.e., the four rectangular areas centred around the presented data objects in the single-blob dataset (1st row), the upper right and bottom left clique areas in the two-blob dataset (2nd row), and the vertical rectangular areas along the sinusoid in the sinusoidal dataset (3rd row). However, these artefact regions are similar to other regions that contain no samples or have similar radial distances to the presented data objects. The unusually lower anomaly scores are due to the intrinsic algorithmic bias of iForest, i.e., only axis-parallel partitions are admitted in iTree construction. Instead, an expected anomaly score map should smoothly approximate circular Projected Space Projected Space Projected Space Projected Space <ref type="figure">Fig. 1</ref>. Illustration of the hard anomaly challenge. The left figure is a synthetic dataset, where red triangles are anomalies and blue points are normal samples. The following figures are projected spaces (i.e., random representations) created by our method. Linear data partition (in either vertical/horizontal or oblique forms) used in existing methods cannot effectively isolate these hard anomalies in the original data space. By contrast, these anomalies are possible to be exposed in newly created spaces.</p><p>iForest PID EIF DIF (ours) <ref type="figure">Fig. 2</ref>. Illustration of the algorithmic bias problem. The three figures on the far left are three synthetic datasets, and the following panels of each dataset are the anomaly score maps produced by iForest and its two state-of-the-art extensions -PID and EIF -and our method DIF. Score maps indicate anomaly score distribution in the full data space (deeper colour indicates higher abnormality). Black contour lines in each score map denote the 99th percentile of the anomaly scores of the original data, which can be seen as the boundary of data normality predicted by each anomaly detector. Red triangles represent possible anomalies. There are some artefact regions in the score maps of iForest and its two extensions, and these methods may fail to identify the anomalies since these anomalies are assigned similarly low anomaly scores as the presented normal samples (they are included in the predicted normal areas). By contrast, DIF produces more accurate and smooth score maps, and DIF can successfully assign a high abnormal degree to anomalies compared to those original data samples.</p><p>contour lines w.r.t. the density of the presented data. This problem also leads to false negative errors, i.e., iForest may fail to detect possible anomalies in these "ghost regions" such as the red triangles in <ref type="figure">Fig. 2</ref> as they are assigned similarly low anomaly scores as some of the presented normal objects and are included in the normal areas.</p><p>There have been a number of extensions of iForest introduced over the years <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These extensions attempt to devise more advanced isolation methods by (i) using novel hierarchical criteria to selectively pick splitting thresholds and/or dimensions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, or (ii) using optimal/random hyper-planes <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. These improvements successfully contribute to better detection performance, but their isolation methods are still confined to linear partition. Instead, data partition in the isolation process is expected to form non-linearly arbitrary shapes in the data space to handle hard anomalies in complicated datasets and eliminate the algorithmic bias in derived anomaly scores. We show two state-of-the-art extensions (PID <ref type="bibr" target="#b5">[6]</ref> and EIF <ref type="bibr" target="#b3">[4]</ref>) in <ref type="figure">Fig. 2</ref>. PID <ref type="bibr" target="#b5">[6]</ref> selectively chooses splitting dimensions and uses similar axis-parallel isolation operations. Thus, its score maps also encounter the same algorithmic bias problem. EIF <ref type="bibr" target="#b3">[4]</ref> works better due to the use of hyper-planes with random slopes and intercepts as branching criteria, eliminating the limits of axis-parallel partitions to some extent. However, the partitions still operate linearly, which works ineffectively on challenging datasets where only nonlinear partitions are useful in isolating anomalies from the other data. This problem is reflected by the rather wrinkled anomaly score contours in all three datasets in <ref type="figure">Fig. 2</ref>.</p><p>Therefore, the key to these limitations becomes very clear now, i.e., a step further is required to liberate the isolation method from the underlying linear constraints. To this end, this paper proposes a novel, intuitively appealing extension of iForest, DEEP ISOLATION FOREST (termed DIF). The key idea in DIF is to harness the strong representation power of neural networks to map the original data into a group of new data spaces, and non-linear isolation can be easily achieved by performing simple axis-parallel partitions upon these newly created data spaces (equivalent to nonlinear partition on subspaces of varying sizes in the original data space). Specifically, we propose a novel representation scheme, i.e., the random representation ensemble, produced by optimisation-free deep neural networks. These networks are only casually initialised and do not involve any optimisation or training process. DIF then simply utilises random axis-parallel cuts upon these representations. The randomness in our representation scheme allows high freedom of the partition in the original data space, encouraging a unique synergy effect between random representations and random partition-based isolation. This facilitates effective isolation of hard anomalies and eliminates the algorithmic bias, thus significantly enhancing the detection performance. <ref type="figure">Fig. 1</ref> shows four random representations created by DIF, where those hard anomalies are possible to be exposed and easily isolated by using a few axis-parallel cuts. Besides, as shown in <ref type="figure">Fig. 2</ref>, DIF accurately assigns anomaly scores to the presented data objects and has a smooth estimation of the anomaly scores in the other regions, alleviating the aforementioned algorithmic bias problem.</p><p>Further, in our specific implementation of DIF, we propose the Computation-Efficient Representation Ensemble method (CERE) and the Deviation-Enhanced Anomaly Scoring function (DEAS). With the help of CERE, DIF can efficiently produce the representation ensemble by taking full advantage of parallel accelerators in the mini-batch calculation, largely eliminating the computational overhead. DEAS leverages the hidden quantitative information enclosed in the mapped dense representations beyond the qualitative comparison. This additional information enables a more accurate assessment of the isolation difficulty of data objects, offering a better anomaly scoring function.</p><p>Our main contributions are summarised as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose the DEEP ISOLATION FOREST (DIF) method. A new isolation method is introduced, which enables non-linear partition on subspaces of varying sizes, offering a more effective anomaly isolation solution than the current methods that are capable of linear isolation only. The resulting approach can better handle hard anomalies in complex datasets with high-dimensional/non-linearseparable data spaces and eliminate the algorithmic bias. We also show that DIF is a high-level generalisation of iForest and its very recent extension EIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a novel representation scheme, the random representation ensemble, in which only casually initialised neural networks are required. This representation scheme facilitates high freedom of the partition in the original data space. The unique synergy between random representations and random partition-based isolation brings excellent randomness and diversity into the overall ensemble-based abnormality estimation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose the representation ensemble method CERE to ensure DIF can inherit good scalability from iForest. The anomaly scoring function DEAS is introduced to leverage additional quantitative information enclosed in mapped dense representations, enhancing the quality of anomaly scoring of DIF.</p><p>? DIF offers a data-type-agnostic anomaly detection solution. We show that DIF is versatile to detect anomalies in different types of data by simply plugging in corresponding randomly initialised neural networks in the feature mapping.</p><p>Extensive experiments on a large collection of real-world datasets, including not only tabular data but graph and time-series data, show that: (i) DIF significantly outperforms iForest and its state-of-the-art extensions (Sec. 5.2); (ii) DIF also achieves remarkable improvement compared to the ensemble of advanced deep anomaly detectors (Sec. 5.2); (iii) DIF has desired scalability on high-dimensional, largescale datasets (Sec. 5.3) and presents good robustness to anomaly contamination (Sec. 5.4); (iv) The significance of the synergy between random representations and random partition-based isolation is justified by comparing to several alternatives (Sec. 5.5); and (v) the contribution of CERE and DEAS is separately verified in our ablation study (Sec. 5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anomaly Detection</head><p>Anomaly detection has been intensively studied in the last decades by using different data characteristics like distance, density, cluster, or probability <ref type="bibr" target="#b0">[1]</ref>. Recent studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> devise deep anomaly detection models based on representation learning, while some other methods use new concepts (e.g., reconstruction <ref type="bibr" target="#b13">[14]</ref> and deviation <ref type="bibr" target="#b14">[15]</ref>) in the deep learning framework as scoring function. Surveys and comparative studies can be found in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Isolation Forest and Its Extensions</head><p>iForest <ref type="bibr" target="#b2">[3]</ref> is a very popular anomaly detection method that identifies anomalies according to the isolating difficulty in the data space. Although iForest has shown effective performance on several benchmarks, a number of its extensions have been proposed, attempting to fix its notorious vulnerabilities and obtain better detection performance.</p><p>The mainstream of these extensions focuses on devising more effective isolation methods. SCIF <ref type="bibr" target="#b4">[5]</ref> introduces a non-axis-parallel way as branching criteria, i.e., instead of selecting one feature during splitting, an optimal slicing hyper-plane is used. EIF <ref type="bibr" target="#b3">[4]</ref> also uses hyper-planes but with random slopes and intercepts. Another work <ref type="bibr" target="#b6">[7]</ref> further fixes the empty branching problem of EIF by choosing splitting thresholds from the range of projected values onto the slope direction. PID <ref type="bibr" target="#b5">[6]</ref> selectively picks dimensions that have greater variance and selects split points according to the sparsity of the partition branches. The literature <ref type="bibr" target="#b8">[9]</ref> proposes a probability-based method to find better split values than random splitting. These extensions normally achieve better performance by introducing non-axis-parallel and/or heuristic partition. However, the major problem is that they still rely on linear isolation operations, which means it is also hard to handle complicated data that require nonlinear partitions as the isolation method. Also, they suffer from the aforementioned artefact problem due to the implicit algorithmic bias hidden in the isolation process. There are also some extensions employing the nearest neighbour information into isolation process, e.g., LeSiNN <ref type="bibr" target="#b17">[18]</ref> and iNNE <ref type="bibr" target="#b18">[19]</ref>. The literature <ref type="bibr" target="#b19">[20]</ref> further uses locally-sensitive hashing to extend the isolation mechanism to any distance measures and data types. These distance-based methods introduce an extra assumption, i.e., anomalies are far from other data objects. However, this assumption does not always hold since clustered anomalies are also very close to their adjacent neighbours <ref type="bibr" target="#b4">[5]</ref>. The detection performance is also sensitive to the choice of distance metrics.</p><p>Another angle is to enhance the scoring method. The literature <ref type="bibr" target="#b20">[21]</ref> introduces a path-weighted scoring method and a probability-based aggregation function. PID <ref type="bibr" target="#b5">[6]</ref> redefines the scoring function according to the sparsity rather than depth in the tree. Similarly, these extensions are still vulnerable due to their linear isolation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Ensembles</head><p>Deep ensemble <ref type="bibr" target="#b21">[22]</ref>, a simple framework that combines prediction results of a group of independently trained networks together, has garnered much interest from the deep learning community recently. It can improve prediction accuracy and provide uncertainty estimation in a simple framework without modifying the original working pipeline. Similarly to other ensemble-based approaches, the quality of deep ensembles also largely hinges on the diversity of its members. Besides, albeit simple, deep ensembles still induce considerably larger computational costs. Thus, many related studies attempt to address these two key limitations, e.g., <ref type="bibr" target="#b22">[23]</ref> uses both weight and hyperparameter diversity, <ref type="bibr" target="#b23">[24]</ref> uses repulsive terms to ensure individual diversity, <ref type="bibr" target="#b24">[25]</ref> increases diversity by reducing spurious correlations among features by adversarially preventing features from being conditionally predictable from each other, and <ref type="bibr" target="#b25">[26]</ref> proposes a distilled model that can absorb as much function diversity inside the ensemble as possible.</p><p>Our work also involves the process of integrating neural networks. From the deep ensemble aspect, our work can successfully tackle the above two key issues. The diversity between ensemble members can be guaranteed and the calculation time efficiency can be well maintained because only initialised neural networks are required. Our work provides valuable insights into the deep ensemble research line. Given that the burgeoning of deep learning has fuelled a plethora of deep anomaly detectors that achieve continuously improved performance, our work may also foster further research about the ensemble of current successful deep anomaly detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Let D = {o 1 , ? ? ? , o N } be a dataset with N data objects, anomaly detection is to give a scoring function f : D ? R N that estimates the abnormality of each data object. Different from many existing models that only focus on an individual specific data type, we do not restrict the type of data objects in this work, which means they can be multi-dimensional vectors, time-series data, or graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The iForest Method</head><p>For the sake of clarity, we recall the basic procedure of iForest <ref type="bibr" target="#b2">[3]</ref>. iForest is designed for tabular data. A basic structure named isolation tree (iTree for short) is proposed. iTree ? is essentially a binary tree, and each node in the tree corresponds to a pool of data objects. A subset containing n data objects is used as the data pool of the root node, which is randomly subsampled from the whole dataset. iTree ? grows by recursively isolating data objects in the leaf node (i.e., a disjoint partition of data objects into two child nodes) in a top-down fashion until remaining one data object in the node or reaching the maximum depth limit. iForest uses a simple isolation method that performs a comparison between the j-th dimension of the data object o (j) and a splitting value ? as the branching criterion of each data object o, where j and ? respectively denote a randomly selected feature index and a split value within the range of available values of the j-th feature. Each data object o has a traversing path in the iTree ? . The path length h(o|? ) can be naturally viewed as an indication of the abnormal degree of o (anomalies are often easier to be isolated, and the path length is shorter). iForest constructs a forest of T iTrees</p><formula xml:id="formula_0">T ={? i } T i=1 .</formula><p>The anomaly score of data object o is calculated based on its averaged path length E ?i?T (h(o|? i )) over all of</p><formula xml:id="formula_1">the iTrees in the forest T , i.e., f iFoerst (o|T ) = 2 ?E ? i ?T h(o|? i ) C(T ) , where C(T ) is a normalising factor.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP ISOLATION FOREST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation and Insights</head><p>iForest uses a linear axis-parallel isolation method that only considers one dimension each time, and existing extension work introduces hyper-plane-based isolation that involves multiple dimensions, but similarly, only linear partition is admitted. These current isolation methods are limited to effectively handle hard anomalies that cannot be isolated using linear partitions on individual features or simple combinations of multiple features. Additionally, these existing methods generally suffer from the algorithmic bias brought by constraints hidden in their isolation strategies.</p><p>Isolation methods that are unleashed from these constraints are needed to effectively isolate those hard anomalies and avoid algorithmic bias. To this end, we introduce the DEEP ISOLATION FOREST (DIF) method. In a nutshell, DIF constructs an ensemble of representations derived from deep neural networks, and simple axis-parallel isolation is operated upon new data spaces to build iTrees in the forest. Instead of following the deep ensemble framework that combines independently trained neural networks, we use an ensemble of random representations produced by optimisation-free neural networks that only require simple casual initialisation. This new representation scheme allows high freedom of the partition in the original data space, and thus the slicing cuts can be liberated from current linear constraints. Meanwhile, a unique synergy between random representations and random partition-based isolation can facilitate the overall ensemble-based abnormality estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Formulation of DIF</head><p>DIF first produces the random representation ensemble via optimisation-free neural networks, which is defined as</p><formula xml:id="formula_2">G (D) = X u ? R d X u = ? u (D; ? u ) r u=1 (1)</formula><p>where r is the ensemble size, ? u : D ? R d is the network that maps original data into new d-dimensional data spaces, and the network weights in ? u are randomly initialised. Each representation is assigned with t iTrees, and a forest T ={? i } T i=1 containing T =r?t iTrees is constructed. iTree ? i of X is initialised by a root node with a set of projected data P 1 ? X . The k-th node with the data pool P k is branched into two leaf nodes with disjoint subsets P 2k and P 2k+1 :</p><formula xml:id="formula_3">P 2k = {x|x (j k ) ? ? k , o ? P k }, P 2k+1 = {x|x (j k ) &gt; ? k , o ? P k },<label>(2)</label></formula><p>where j k is selected uniformly at random among all the dimensions of the newly created data space {1, ? ? ? , d}, x (j k ) is the j k -th dimension of the projected data object, and ? k is a split value within the range of {x (j k ) |x ? P k }. After constructing T , the abnormality of a data object o is evaluated by the isolation difficulty in each iTree of the forest T . The scoring function is defined as</p><formula xml:id="formula_4">F (o|T ) = ? ?i?T I(o|? i ),<label>(3)</label></formula><p>where I(o|? i ) denotes a function to measure the isolation difficulty in iTree ? i , and ? denotes an integration function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation of DIF</head><p>This subsection further introduces the Computation-Efficient deep Representation Ensemble method (CERE) and the Deviation-Enhanced Anomaly Scoring function (DEAS) to efficiently and effectively implement two key components -random representation ensemble function G during the construction of the deep isolation forest and the anomaly scoring function F -in DIF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">CERE: Computation-efficient Deep Representation Ensemble Method</head><p>Successively feeding raw data into r independent networks in Eq. (1) can induce a considerably high memory and time overhead. To inherit outstanding scalability of the original iForest, we introduce CERE to efficiently implement the representation ensemble function G (D).</p><p>Let W ? R m?n be a weight matrix of a neural network layer, then following <ref type="bibr" target="#b26">[27]</ref>, we use a tuple of small random vectors p i ? R m and q i ? R n to yield a rank-one matrix via multiplication, which is used to derive the full weight matrices of each ensemble member. Formally, based on a base weight matrix W 0 , the weight matrix W i of the i-th ensemble member is generated as</p><formula xml:id="formula_5">W i = W 0 ? (p i q i ),<label>(4)</label></formula><p>where ? denotes the Hadamard product. The mapping process of incoming neurons x ? R m and the weight W i can be further derived as follows:</p><formula xml:id="formula_6">W i x = (W 0 ? p i q i ) x = W 0 (x ? p i ) ? q i .<label>(5)</label></formula><p>Given r tuples of weight vectors { p i , q i } r i=1 and a mini-batch of data X ? R b?m with mini-batch size b and dimension m, the ensemble of mapped results</p><formula xml:id="formula_7">{X W 1 , ? ? ? , X W r } can be calculated via ? ? ? ? X W 1 X W 2 ? ? ? X W r ? ? ? ? = ? ? ? ? X X ? ? ? X ? ? ? ? ? ? ? ? ? P 1 P 2 ? ? ? P r ? ? ? ? W 0 ? ? ? ? ? Q 1 Q 2 ? ? ? Q r ? ? ? ? ,<label>(6)</label></formula><p>where each row in P i and Q i is duplicated p i and q i . Let 1 b be a all-one vector with size b, P i and Q i are obtained via</p><formula xml:id="formula_8">P i = 1 b p i and Q i = 1 b q i .</formula><p>The above vectorisation derivation allows the deep representation ensemble process in DIF to be efficiently calculated. With the help of CERE, the time complexity of the ensemble process is similar to the feed-forward process of a single neural network since all the ensemble members can be computed simultaneously in a given mini-batch. Eq. (6) requires additional Hadamard product steps, but this operation is very cheap compared to matrix multiplication. One possible limitation is the batch size in Eq. <ref type="bibr" target="#b5">(6)</ref>. A minibatch of r ? t objects is simultaneously computed, which is larger than conventional settings. However, as computation within a mini-batch is automatically parallelisable, increasing the batch size incurs almost no time overhead. As for memory cost, this process is also feasible in a typical device because DIF does not involve optimisation, i.e., gradients are not calculated and saved. For example, a dataset with 10,000 features costs about 3GB of memory when using the recommended ensemble size r=50 and the batch size b=64.</p><p>Let ? be the L-layer neural network using the newlydefined feed-forward step in Eq. <ref type="bibr" target="#b5">(6)</ref>. The ensemble of representations can be directly generated as</p><formula xml:id="formula_9">G CERE (D) = ? D; ? = X i ? R d r i=1 ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">? = W l , {p (l,i) } r i=1 , {q (l,i) } r i=1 L l=1</formula><p>. Note that other operations like activation or pooling and some layers that do not use a weight matrix are processed sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">DEAS: Deviation-enhanced Anomaly Scoring Function</head><p>We further introduce a new anomaly scoring function DEAS. Recall that the standard anomaly scoring process in iForest only uses the length of the traversed path, i.e., all nodes are considered to have the same importance. The path length only provides limited information, which may not sufficiently delineate the isolation difficulty of data objects. Except for qualitative comparison in each node, additional quantitative information is readily available to be leveraged, such as relation between the feature values of data objects and the branching threshold.</p><p>Motivated by this, we utilise the deviation degree of the feature value to the branching threshold as additional weighting information to further improve the measurement of isolation difficulty. These deviation degrees are important indicators to the isolation difficulty because the feature values in newly created data spaces are typically densely distributed and these deviations reflect the local density in the projected space., e.g., a small deviation value indicates that the slicing cut is on a dense region and thus is hard to isolate the data object. Specifically, let x u be the corresponding representation of a data object o in an iTree ? i . p(x u |? i ) = {1, ? ? ? , K} is its traversed node path. we define the averaged deviation degree of x u in ? i as</p><formula xml:id="formula_11">g(x u |? i ) = 1 |p(x u |? i )| k?p(xu|?i) |x (j k ) u ? ? k |.<label>(8)</label></formula><p>We further combine the path length h(x u |? i ) = |p(x u |? i )| as in iForest and the deviation measure in Eq. <ref type="formula" target="#formula_11">(8)</ref> to specify the function in Eq. (3) by defining our deviationenhanced isolation anomaly scoring function as:</p><formula xml:id="formula_12">F DEAS (o|T ) = 2 ?E ? i ?T h(xu |? i ) C(T ) ? E ?i?T g(x u |? i ) , (9)</formula><p>where the first term is the averaged depth used as anomaly scores in iForest and the second term is the deviation-based anomaly score we introduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Algorithm of DIF</head><p>Algorithm 1 presents the procedure of the construction of deep isolation trees T . Step (2) prepares r random representations, and t isolation trees {? i } t i=1 are built upon each representation in Steps <ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref>. For each isolation tree ? i , a subset of transformed data objects P 1 is first randomly subsampled in Step (5) to initialise the root node. Each leaf node P k is then iteratively split by using a comparison branching criteria based on a randomly selected representation dimension j k and a split point ? k in Steps <ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref>.</p><p>We report the anomaly scoring procedure in Algorithm 2. Data object o is transformed to vectorised representations {x} r u=1 in Step (1) After the initialisation in Step <ref type="formula" target="#formula_5">(4)</ref>, the data object traverses each tree ? i by the criteria of each node and reaches the final node, during which the traverse path p(x u |? i ) and the accumulated difference ? are recorded in Steps <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref>. The path length h(x u |? i ) and the deviation g(x u |? i ) in iTree ? i are calculated in Step <ref type="bibr" target="#b12">(13)</ref>. The anomaly score of o is calculated and returned in Steps <ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Theoretical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Time Complexity Analysis</head><p>We first analyse the time complexity for the production of the random representation ensemble via CERE (i.e., Step 2 in Algorithm 1). Let the input data D be a tabular dataset with size N ? D. Multi-layer perceptron network is used for Algorithm for i = 1 to t do <ref type="bibr">5:</ref> Initialise an isolation tree ? i by setting the root node using P 1 ? X u , |P 1 | = n <ref type="bibr">6:</ref> while P k is a leaf node of tree ? i do <ref type="bibr">7:</ref> if |P k | &gt; 1 and the depth is smaller than J then <ref type="bibr">8:</ref> Randomly select a dimension j k ? {1, ? ? ? , d} 9:</p><p>Randomly select a split point ? k between the max and min values of dimension j k in P k 10:</p><formula xml:id="formula_13">P 2k ? {x|x (j k ) ? ? k , x ? P k } 11: P 2k+1 ? {x|x (j k ) &gt; ? k , x ? P k } 12:</formula><p>end if <ref type="bibr">13:</ref> end while <ref type="bibr">14:</ref> T ? T ? ? i 15: end for <ref type="bibr" target="#b15">16</ref> </p><formula xml:id="formula_14">for i = 1 to t do 4: Initialise k ? 1, ? ? 0, p(x u |? i ) ? ? 5:</formula><p>while |P k | &gt; 1 and not reaching J do <ref type="bibr">6:</ref> if x (j k ) u ? ? k then <ref type="bibr">7:</ref> k ? 2k 8: else <ref type="bibr">9:</ref> k ? 2k + 1 <ref type="bibr">10:</ref> end if <ref type="bibr" target="#b10">11</ref>:</p><formula xml:id="formula_15">p(x u |? i ) ? p(x u |? i ) ? k, ? ? ? + |x (j k ) u ? ? k | 12:</formula><p>end while <ref type="bibr">13:</ref> h(x u |? i ) ? |p(x u |? i )|, g(x u |? i ) ? ?/|p(x u |? i )| <ref type="bibr">14:</ref> end for 15: end for <ref type="bibr">16:</ref> </p><formula xml:id="formula_16">return F DEAS (o|T ) ? 2 ?E ? i ?T h(xu |? i ) C(T ) ?E ? ?T g(x u |? i ) ?.</formula><p>The used network ? comprises L layers, and the l-th layer is with d l hidden units, the representation dimension is d. We use CERE to implement the ensemble with r members within each mini-batch, and thus the whole feed-forward computation induces O(r?N ?(Dd 1 +d l d+ L?1 l=1 d i d l+1 )). Only feed-forward steps are required in DIF, and the number of hidden units and representation dimension is generally small. Thus, this process is linear w.r.t. both data size and dimensionality, which does not introduce much extra computational overhead than the original iForest. In terms of the subsequent process of the iTree construction, given the depth limit J, we have the maximum 2 J?1 splits (Steps (8-11)) during the growth of each iTree. For a node with n samples, each split takes O(n) complexity in determining the maximum and minimum value of the selected dimension and the assigning process. The overall process induces O(2 J?1 ? n ? r ? t). J and n often use fixed small values (typically 8 and 256 respectively). Therefore, the overall complexity is linear w.r.t. the ensemble size r ? t. As for the process in Algorithm 2, the traversing process takes a similar computation process, which has linear time complexity w.r.t. the size of testing sets and the ensemble size. Overall, according to the above analysis, the time complexity of DIF is O(N D(r ? t)). It has linear complexity w.r.t. data size, dimensionality, and ensemble size, which inherits desired scalability from iForest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">DIF as a Generalisation of iForest and EIF</head><p>EIF <ref type="bibr" target="#b3">[4]</ref> is a recent extension of iForest <ref type="bibr" target="#b2">[3]</ref>, which has been shown to be a generalisation of iForest in <ref type="bibr" target="#b3">[4]</ref>. We show that DIF can be viewed as a further higher-level generalisation of isolation methods used in both iForest and EIF. Lemma 1. DIF is a generalisation of iForest and EIF.</p><p>Proof. We explain how DIF subsumes iForest and EIF as specific cases below, i.e., the branching criteria used in iForest and EIF can be transformed into the format of DIF.</p><p>Let o ? R D be a vectorised data object. Recall that the branching criterion in DIF is</p><formula xml:id="formula_17">?(o) (j) ? ?.<label>(10)</label></formula><p>Both iForest and EIF are special cases of DIF when the neural network ? is with one linear layer parameterised by a weight matrix W, i.e., ?(o) = W o. iForest splits the node by using the criterion o (j) ? ?, while DIF degrades to iForest if the weight matrix is set as an identity matrix, i.e., W = I D .</p><p>EIF uses a slicing hyper-plane for each node branching, and the slope of its hyper-plane is a normal vector k ? R D , and k (i) ? N (0, 1), ?i ? {1, ? ? ? D}. The intercept of the hyper-plane p ? R D is uniformly selected over the range of possible values at each branching point. The branching criterion is (o ? p) ? k ? 0, which is equivalent to o ? k ? p ? k. We can fulfil exactly the same operation in DIF when the weight matrix satisfies W ? R D?1 . The elements in W should also be initialised by a normal distribution N (0, 1) to satisfy W = k. Additionally, the splitting point ? = p ? k can be understood as a standard random vector p sampled in possible values with a Gaussian noise k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussions</head><p>The power of DIF mainly depends on: (i) the strong representation ability of neural networks, (ii) the discard of optimised representations, and (iii) the synergy between random representations and random partition-based isolation, which are respectively discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Representation Ability of Neural Networks</head><p>Neural networks have very strong representation power, even for randomly initialised networks. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref> where the new data spaces are projected by the random neural networks, the randomness in these initialised networks can create highly diversified new data spaces, on which simple axis-parallel cuts can be equivalent to sophisticated  slicing cuts in the original data space. In other words, nonlinear activation functions can effectively tweak and fold partition bounds to embed non-linearity into the isolation process, even though the networks are not optimised at all. On the other hand, there have been different deep learning architectures developed for various data types, so DIF is empowered to handle diverse data types by plugging and using the data-specific network backbone (e.g., multiperceptron networks, recurrent networks, or graph neural networks) to produce the representations (see Sec. 5.1.3 for different neural networks used in DIF and Sec. 5.2 for their performance in different data types).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Optimised vs. Casually Initialised Representations</head><p>In general, we can use many representation learning networks specifically designed for anomaly detection, such as those in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, to obtain well-optimised feature representations. These representations are more expressive than randomly initialised representations if the optimisation objective well fits the input data. However, instead of using optimised representations, DIF uses the casually initialised representations due to the following two main reasons. (i) These loss functions are not versatile. It is difficult to devise one representation learning loss that can fit different data with diversified characteristics. (ii) The downstream data partition might be strongly controlled by the optimisation process. This way weakens the randomness and diversity of the feature representations, which are required in isolationbased anomaly scoring methods. These two intuitions are empirically investigated in our experiments by showing the performance of DIF on optimised representations produced by recent normality feature learning algorithms and the quality of these optimised representations (see Sec. 5.5.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Synergy between Random Representations and Random Partition-based Isolation</head><p>DIF employs a novel representation scheme, i.e., the random representation ensemble produced via optimisationfree neural networks. The parameters of these networks can be initialised by randomly sampling from widely-used initialisation distributions (e.g., normal or uniform distribution), easily yielding a set of feature representations with excellent randomness and diversity. Given a sufficiently large set of such random representations, we can largely boost the isolation power in the random data partition, making it possible to effectively isolate some really hard anomalies on some subsets of these representations. For example, as shown in <ref type="figure">Fig. 1</ref>, among a large set of new representation spaces, there are some selective new spaces where hard anomalies become easy-to-isolate data objects. Recall that isolation methods, including DIF, are based on an average measure for anomaly scoring. Thus, the anomalies would stand out in the anomaly scores as long as they are effectively isolated in some of the isolation trees. DIF utilises this unique synergy between random representations and random partition-based isolation to largely improve the isolation process and subsequently the anomaly scoring function, resulting in significantly improved effectiveness of isolation-based anomaly detection. We empirically investigate the significance of this synergy by respectively replacing random representations and random partitionbased isolation with multiple alternatives (see Sec. 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We now introduce our experimental analysis. This section is organised as follows.</p><p>? In Sec. 5.1, we first start with the experimental setup including the used datasets, competing methods, parameter settings, and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>In Sec. 5.2, 5.3, and 5.4, we evaluate the performance of our method w.r.t. effectiveness, scalability, and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>In Sec. 5.5 and 5.6, we empirically analyse our method by investigating the significance of the synergy between random representations and random partition-based isolation and the contribution of CERE and DEAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We employ a large collection of publicly available and commonly-used real-world datasets, including ten tabular datasets, four graph datasets, and four time-series datasets. Their basic information is shown in <ref type="table" target="#tab_3">Table 1</ref>.</p><p>? Tabular Data. Analysis, Backdoor, DoS, and Exploits are taken from a popular intrusion detection benchmark UNSW NB 15. Following <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref>, we select different attacks as anomalies in these three datasets against normal network traffic. R8 is a highly-imbalanced text classification dataset, where the rare class are treated as anomalies by following <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Cover is short for CoverType, which is from the ecology domain. Fraud is for fraudulent credit card transaction detection. Pageblocks and Shuttle are provided by an anomaly benchmark study <ref type="bibr" target="#b30">[31]</ref>. Thrombin is to detect unusual molecular bio-activity for drug design, which is an ultrahigh-dimensional anomaly detection dataset used in <ref type="bibr" target="#b9">[10]</ref> ? Graph Data. We employ datasets from the popular graph benchmark Tox21, which is a project on toxicity evaluation of newly synthesised or used chemical compounds. These datasets are chosen since they are inherently imbalanced and contain real anomalies. The task is to detect abnormal graphs. They are also used in the latest graph-level anomaly detection literature <ref type="bibr" target="#b31">[32]</ref>.</p><p>? Time-series Data. The four time-series datasets are taken from the UCR time-series anomaly benchmark which is released in KDD Cup 2021. We employ real datasets that have natural anomalies. Each dataset has multiple time-series sequences, and we report the average performance over entities per dataset by following mainstream studies <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Lab2Cmac is from an insect, which contains records using EPG apparatus, and anomalies indicate the insect moved its stylet to a new vein. TkeepMARS is from NASA spacecraft. gaitHunt is from a biomechanics lab, and anomalies are faults in the left foot sensor. sel840mECG is heartbeat data, where anomalies are supraventricular beats. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Competing Methods</head><p>To have a comprehensive comparison, DIF is compared with the following two types of anomaly detection approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? iForest and Its Extensions (IF-based Methods).</head><p>Apart from the popular iForest algorithm <ref type="bibr" target="#b2">[3]</ref>, its three advanced variants, i.e., EIF <ref type="bibr" target="#b3">[4]</ref>, PID <ref type="bibr" target="#b5">[6]</ref>, and LeSiNN <ref type="bibr" target="#b17">[18]</ref>, are employed. EIF slices data by using hyperplanes with random slopes and intercepts. In PID, the choice of splits is optimised based on the variance in the sparsity, and the anomaly scoring is redefined according to the sparsity. LeSiNN employs the nearest neighbour distance-based isolation ensemble, which is also known as aNNE in <ref type="bibr" target="#b34">[35]</ref>. All of these methods are originally designed for tabular data. As for graph datasets, these methods are performed upon vectorised representations generated by an unsupervised graph representation method InfoGraph <ref type="bibr" target="#b35">[36]</ref> according to <ref type="bibr" target="#b31">[32]</ref>. Following <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, they directly handle time-series datasets by treating each timestamp as a data object. methods are employed as base models, and we use the deep ensemble framework <ref type="bibr" target="#b21">[22]</ref> to construct a suite of ensemble-based deep contenders. For tabular data, we utilise four state-of-the-art deep methods including RDP <ref type="bibr" target="#b12">[13]</ref>, REPEN <ref type="bibr" target="#b9">[10]</ref>, Deep SVDD <ref type="bibr" target="#b10">[11]</ref>, and a reconstruction-based Autoencoder baseline (RECON for short) <ref type="bibr" target="#b0">[1]</ref>. We also employ the CERE ensemble method used in DIF for those methods to ensure their time efficiency and have a fair competition. As for graph data, a deep graph-level anomaly detector GLocalKD <ref type="bibr" target="#b31">[32]</ref> is used. TranAD <ref type="bibr" target="#b32">[33]</ref> is employed in the experiments on time-series data. Their ensemble versions are denoted as eRDP, eREPEN, eDSVDD, eRECON, eGLocalKD, and eTranAD. All these methods are specifically designed for anomaly detection on the corresponding data type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Parameter Settings and Implementations</head><p>DIF uses 50 representations (r=50) and 6 isolation trees per representation (t=6), with 256 as subsampling size (n=256) for each iTree. DIF processes tabular data using fullyconnected multi-layer-perceptron networks. To handle timeseries data, GRU is used. GIN network <ref type="bibr" target="#b37">[38]</ref> with the summation pooling function is utilised for graph data. To have a fair competition, all of the IF-based competing methods use 300 trees. The subsampling size is set as 256. We use the maximum extension level of EIF, i.e., the extension level is adaptively set as the dimensionality minus 1. For LeSiNN, the subsampling size is 8 by following <ref type="bibr" target="#b17">[18]</ref>. The deep anomaly detectors are trained by 50 epochs and 30 steps per epoch. REPEN, DSVDD, and RECON take an Adam optimiser with a 1e-3 learning rate and use 64 objects per mini-batch, and we empirically found that RDP can work significantly better when using 1e-4. The default/recommended settings are used in the other parameters of these competing methods.</p><p>All the anomaly detection algorithms in our experiments are implemented using Python, with iForest from scikit-learn package, EIF from eif package, and other methods from their authors' releases. The implementation of our method is publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Evaluation Metrics and Computing Infrastructure</head><p>Following the mainstream evaluation protocols of anomaly detection <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>, the detection accuracy is evaluated by two complementary metrics including Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and Area Under the Precision-Recall Curve (AUC-PR). ROC curve indicates the true positives against false positives, while PR curve summarises precision and recall of the anomaly class only. Both of these two metrics range from 0 to 1, and higher values indicate better performance. Achieving high AUC-PR is normally very challenging in real-world datasets due to the skewed and heterogeneous distributions of the anomaly class <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The reported metrics are averaged results and standard deviation over ten independent runs. The paired Wilcoxon signed rank test is used to examine the statistical significance of the performance of DIF against each competing method.    We then introduce a new metric called Anomaly Isoability Index (AII) to measure the quality of representations. We borrow the concept of triplet loss <ref type="bibr" target="#b39">[40]</ref> to count the percentage of effectively isolated anomalies among all true anomalies in each representation space, i.e.,</p><formula xml:id="formula_18">AII = P a?A median ni?N 1 |C| nj ?C ?(a|n i , n j ) &gt; 0 ,<label>(11)</label></formula><p>where ?(a|n i , n j ) = d(a, n i ) ? d(n j , n j ) denotes the difference between the Euclidean distances of the two pairs, a ? A represents any anomaly drawn from the true anomaly set in the dataset, C is a group of randomly sampled normal anchors, and N is another set of random normal samples to delegate the whole normal distribution. All the above data objects are from the target representation space. |C|=20 and |N |=1000 are used as we found empirically that these two settings are sufficiently large to compute the AII metric.</p><p>The computational time of all methods is based on a workstation with Intel Xeon Silver 4210R CPU, a single NVIDIA TITAN RTX GPU, and 64 GB RAM. <ref type="table" target="#tab_5">Table 2</ref> and 3 present the AUC-ROC and AUC-PR results of our method DIF and eight competing methods. Overall, DIF largely reduces the false negatives compared to iForest and its three extensions, resulting in substantial averaged performance improvement in detection precision and/or recall rates, and thus, DIF obtains superior AUC-PR and AUC-ROC performance. Particularly, in the average AUC-PR, DIF substantially outperforms EIF (61%), PID (186%), LeSiNN (56%), iForest (144%), eRDP (13%), eREPEN (77%), eDSVDD (19%), and eRECON (82%). DIF also obtains 4% -11% AUC-ROC improvement across these methods. <ref type="table" target="#tab_5">Table 2</ref>, DIF significantly outperforms iForest and its three advanced extensions at the 99% confidence level. In both AUC-ROC and AUC-PR, DIF is the best isolation-based detector across all the ten datasets except AUC-PR on Fraud, on which the nearest neighbour-based anomaly measure LeSiNN is more effective. The features in Fraud are the results of PCA transformation due to the confidentiality issues, and thus the distance concept used in the nearest neighbour information can well reflect the proximity relationship of data objects. By contrast, we do not rely on such prior information that is not always reliable in all the datasets. These results demonstrate a superior isolation power of DIF, which can effectively isolate anomalies that may not be possible in existing IF-based methods due to the challenges like data sparsity and non-linearity. This is particularly true on challenging high-dimensional datasets like R8, Analysis, Backdoor and DoS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness in Reducing False Negatives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Tabular Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Compared to deep ensemble-based methods, DIF performs significantly better at the 99% confidence level according to the AUC-ROC performance in <ref type="table" target="#tab_6">Table 3</ref>. DIF achieves the best performance on seven out of ten datasets in terms of both AUC-ROC and AUC-PR, and it obtains very competitive results on the rest three datasets with less than 0.01 difference in AUC-ROC. Nevertheless, the comparison results between DIF and its deep ensemble-based counterparts are very encouraging given the fact that DIF does not involve any optimisation while these deep methods need to be properly trained using pre-defined objective functions   to better expose anomalies. The superiority of DIF in this comparison owes to the representation diversity of each ensemble member and the unique synergy between random representations and random partition-based isolation in the downstream anomaly scoring process. More importantly, DIF runs significantly faster than these deep ensemblebased contenders by around two orders of magnitude (see Sec. 5.3). Its superior computational efficiency endows DIF with stronger practicability in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Time-series and Graph Data</head><p>The AUC-ROC and AUC-PR results on graph data and timeseries data are shown in <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table" target="#tab_8">Table 5</ref>, respectively, in which EIF and LeSiNN are selected as competing methods due to their preferable performance in tabular datasets over PID, with iForest also included as a baseline. SOTA graphlevel anomaly detector GLocalKD and time-series anomaly detectors TranAD are also used. As noted in Sec. 5.1.3, EIF and LeSiNN work on vectorised graph representations learned by InfoGraph <ref type="bibr" target="#b35">[36]</ref>, while they can directly work on time-series data by treating each timestamp as a data object by following <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>DIF is the best performer on three out of four datasets in both graph-level and time-series anomaly detection tasks. This performance of DIF is remarkable in that it outperforms not only the isolation-based methods on these diverse datasets but the recent SOTA methods that are specifically designed to extensively learn data-type-specific characteristics (e.g., holistic graph structure or temporal dependence) for effectively detecting these graph/sequential anomalies. By contrast, DIF performs well in a unified framework by only replacing the network structure with a different randomly initialised network backbone, offering a significantly simpler yet data-type-agnostic effective solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scalability to High-dimensional, Large-scale Data</head><p>We examine the scalability of DIF and its contenders to high-dimensional, large-scale data in this experiment. These anomaly detection algorithms are performed on a group of synthetic tabular datasets with different sizes and dimensionalities to record their training times. Nine datasets are with 5,000 data objects, and their dimensions range from <ref type="bibr" target="#b15">16</ref>   and data sizes from a minimum of 1,000 up to 256,000. IF-based anomaly detectors only need CPU devices, while deep ensemble-based methods can leverage GPU acceleration. Therefore, for the sake of a fair competition, we report the training time of DIF and its deep ensemble-based competitors on both GPU and CPU devices. <ref type="figure" target="#fig_5">Fig. 4 (top)</ref> shows the scalability test results of DIF and its competing methods on a CPU device, and <ref type="figure" target="#fig_5">Fig. 4 (bottom)</ref> reports the comparison using GPU. DIF and all the other isolation-based methods present good scalability w.r.t. both dimensionality and data size compared to deep ensemblebased methods when using CPU computation. This is owed to the subsampling-based methods applied to both data samples and dimensions in isolation-based detectors. Besides, deep ensemble-based methods can greatly benefit from GPU acceleration when handling high-dimensional data. DIF shows outstanding time efficiency compared to its deep ensemble-based counterparts since it only requires one feed-forward step instead of a large number of training epochs. These results demonstrate that DIF inherits excellent computational efficiency from the iForest. Note that DIF can obtain almost the same scalability as iForest w.r.t. the data dimensionality, as DIF performs isolation on newly pro-0% 5% 10% 0.5 0.6 0.7 0.8 0.9 1.0  jected spaces of much smaller dimensionality while iForest works on the original data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC-ROC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Robustness w.r.t. Anomaly Contamination</head><p>This experiment examines the performance of DIF and its contenders when datasets containing different anomaly contamination ratios. Time-series datasets and graph datasets have pre-defined train-test split and their training sets do not contain anomalies, and thus we use tabular datasets as our bases here. Following <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref>, we adjust the contamination ratios by injecting/removing anomalies, such that ratios range from 0% to 10%. These detection approaches are trained on adjusted datasets with controlled contamination ratios and tested on the original version.</p><p>The AUC-ROC performance is reported in <ref type="figure" target="#fig_6">Fig. 5</ref>. Generally, the performance of all the anomaly detectors downgrades with the increasing contamination ratio. Nevertheless, DIF has relatively clear superiority and stronger robustness in most of the datasets. The success of deep ensemble on out-of-distribution robustness has been proved in many recent studies <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b40">[41]</ref>, which partially explains why deep ensemble-based methods show better robustness w.r.t. anomaly contamination than IF-based methods. However, these competing methods still fail to provide consistently good robustness (e.g., eRDP, eREPEN, and eDSVDD on R8 and eRECON on Backdoor). It is mainly due to two reasons: (i) their ensemble processes may suffer from the diversity problem, and (ii) their scoring strategies and training objectives rely on strong assumptions such as the distance concept that might not hold in some datasets. It is interesting to note that eREPEN shows an uptrend in Shuttle. REPEN uses LeSiNN to estimate initial anomaly scores when generating triplet mini-batches. As LeSiNN is with good robustness on Shuttle, this initial estimation can reliably obtain more anomaly examples as positive data when the contamination ratio increases, and thus the triplet learning process might benefit from the augmentation of the positive class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Significance of the Synergy between Random Representations and Random Partition-based Isolation</head><p>We respectively replace random representations and random isolation-based anomaly scoring in DIF to investigate their synergy effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Representation Scheme</head><p>In this experiment, we evaluate the effectiveness of our novel representation scheme by comparing it with representations produced by optimised neural networks, including RDP <ref type="bibr" target="#b12">[13]</ref>, REPEN <ref type="bibr" target="#b9">[10]</ref>, DSVDD <ref type="bibr" target="#b10">[11]</ref> and Reconstructionbased Autoencoder <ref type="bibr" target="#b0">[1]</ref>. That is, we replace random representations with a group of representations learned by one of these methods, with all the other components of DIF fixed. These four variants are denoted as RDP-DIF, REPEN-DIF, DSVDD-DIF and RECON-DIF. RDP, REPEN, and DSVDD are originally designed for learning a good representation for anomaly detection, and a dense representation can be implicitly derived from the reconstruction-based Autoencoder. To enable straightforward and fair evaluation of the representations, the ensemble size of representations in these variants is also set to 50, and 6 iTrees per representation are constructed. The representations are evaluated by the AUC-ROC performance and the individual representation quality as follows. AUC-ROC Results. The AUC-ROC results are shown in the upper half of <ref type="figure" target="#fig_7">Fig. 6</ref>. DIF outperforms four optimised representation ensemble-based methods on five datasets and shows very competitive performance to the best performer on the other five datasets. Averagely, the random representation ensemble contributes to 5%, 5%, 7%, and 15% AUC-ROC improvement than RDP-DIF, REPEN-DIF, DSVDD-DIF, and RECON-DIF, respectively. Although DIF may not be the best performer on all the datasets, it is very encouraging to see this performance achieved by the ensemble of random representations when compared to those optimised representations trained by state-of-the-art learning objectives. Quality per Representation. To further analyse the mechanism behind the above results, we directly evaluate the quality of each representation produced by DIF, RDP-DIF, REPEN-DIF, DSVDD-DIF, and RECON-DIF. The representation quality is measured by Anomaly Isoability Index (AII), as introduced in Sec. 5.1.4. The AII results of each representation used in five anomaly detection methods are shown in <ref type="figure" target="#fig_7">Fig. 6 (bottom)</ref>, in which we use box plots to present the quality distribution of representations produced by 50 representations in the ensemble framework.</p><p>Based on the above experiment results, the following three remarks can be made. The proposed representation scheme in DIF achieves desired diversity and randomness, while at the same time maintaining relatively stable expressiveness in each representation, enabling excellent synergy with the downstream isolation-based anomaly scoring mechanism. This is the main driving force behind the superior performance of DIF.</p><p>? Optimised representations can be with consistently good quality on some datasets (e.g., near 80% true anomalies are well isolated by RECON-DIF on Analysis, Backdoor, and DoS), whereas the lack of diversity in representations downgrades the efficacy of this ensemble framework.</p><p>? Optimisation may even lead to worse representations on some datasets compared to random representations (e.g., R8, Cover, Pageblocks and Thrombin). This may be due to the fact that the underlying assumption (e.g., one-class assumption) in their learning objectives may not hold in those datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Scoring Strategy</head><p>As shown in the prior experiment, random representations are with good diversity and stable quality, fostering a unique excellent synergy effect in downstream ensemblebased anomaly scoring. This section tends to further justify this intuition by investigating the effectiveness of combining random representations with other anomaly scoring methods, including probability-based anomaly scoring method ECOD <ref type="bibr" target="#b41">[42]</ref>, distance-based method KNN <ref type="bibr" target="#b42">[43]</ref>, and densitybased method LOF <ref type="bibr" target="#b43">[44]</ref>. Each of these methods is used to replace the isolation-based scoring process, with all the other modules fixed. These variants are denoted as DIF-ECOD, DIF-KNN, and DIF-LOF, respectively. Similarly, we evaluate their AUC-ROC performance and the individual scoring quality as follows. AUC-ROC Results. The AUC-ROC results are shown in the upper half of <ref type="figure" target="#fig_8">Fig. 7</ref>. DIF outperforms these competing variants on seven out of ten datasets. Averagely, DIF outperforms DIF-ECOD, DIF-KNN, and DIF-LOF by 3%, 13%, and 25%, respectively. The superiority further justifies the synergy effect in DIF. Note that KNN and LOF are with very heavy computational overhead. They take around 60 hours to handle large-scale datasets Cover and Fraud. Quality per Anomaly Scoring Result. We also study the quality of individual anomaly scoring results produced by DIF and its variants on each random representation, as shown in <ref type="figure" target="#fig_8">Fig. 7 (bottom)</ref>. The quality is also estimated by AUC-ROC here. We make the following three remarks.</p><p>? These competing scoring methods can produce markedly better individual scoring results than our isolation-based scoring mechanism on Exploits, R8, and Thrombin. However, they only yield less effective or slightly better integrated results compared to DIF. These scoring methods are unable to leverage the diversity embedded in our representation scheme.</p><p>? By contrast, DIF combines data representation and anomaly scoring in a successful unified ensemble learning framework. DIF achieves better integrated performance by fully leveraging the diversity and randomness of representations, e.g., above 0.9 AUC-ROC on R8 with the maximum individual value only achieving around 0.7.</p><p>? DIF is inferior to its variants on Shuttle. It might be because anomalies in this dataset can be more easily identified by using the prior concepts used in these competing scoring methods (i.e., probability, distance, or density). However, these prior concepts may fail to work properly across all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study on CERE and DEAS</head><p>Considering detection effectiveness and computational efficiency, we propose the Deviation-Enhanced Anomaly Scoring function (DEAS) and the Computation-Efficient deep Representation Ensemble method (CERE) in the specific implementation of DIF. This experiment is conducted to examine whether DIF can have better detection performance and use less training time with the help of DEAS and CERE.</p><p>Two ablated variants are employed, i.e., w/o CERE replaces G CERE with the conventional sequential ensemble process, and w/o DEAS uses the standard scoring function used in iForest to replace our scoring function F DEAS . The AUC-ROC and AUC-PR results of DIF and w/o DEAS are shown in <ref type="table" target="#tab_11">Table 6</ref>. DIF significantly outperforms its variant w/o DEAS at the 90% confidence interval. DIF also achieves approximate 11% AUC-PR improvement over w/o DEAS. On the other hand, DIF costs considerably less training time with the help of CERE. The total training time across all the ten datasets is only approximate one-tenth of the variant w/o CERE. The AUC-ROC/AUC-PR results of w/o CERE are on par with DIF, which are omitted due to the space limitation. Based on the above comparison results, the contribution of the proposed DEAS and CERE is validated and quantitatively measured. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>This paper introduces DIF, a novel extension of iForest. DIF takes the deep neural network-based random representation ensemble as a new representation scheme, enabling significantly versatile data partition in diverse random directions on subspaces of different sizes. The anomaly scoring can be facilitated by the synergy between random representations and random partition-based isolation. This enables DIF to fulfil (i) more effective isolation of anomalies, especially hard anomalies in data with intractable sparsity and nonlinearity; (ii) liberation of isolation process from existing constraints to tackle the artefact problem; and (iii) versatile ability to handle different data types. Extensive experiments show that DIF significantly outperforms iForest and its existing extensions not only on tabular data but also on graph and time-series data. DIF also shows promising improvement compared to the ensemble of state-of-the-art deep anomaly detectors. In our future work, we plan to utilise more advanced neural network architectures for the application of DIF in other data types, such as exploring the use of deep residual networks <ref type="bibr" target="#b44">[45]</ref> and transformers <ref type="bibr" target="#b45">[46]</ref> for anomaly detection on video and image data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Original</head><label></label><figDesc>Data Space Projected Data Space Projected Data Space Projected Data Space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Data space transformation in random representations produced by casually initialised neural networks (the original data space compared to three transferred data spaces).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Ensemble of Deep Anomaly Detectors<ref type="bibr" target="#b0">1</ref> . Different state-of-the-art (SOTA) deep anomaly detection 1. The ensemble performance of these deep models generally outperforms their solitary versions, and thus we focus on the comparison between ensemble-based results in the following experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.006 0.899?0.005 0.883?0.026 0.844?0.006 0.862?0.002 0.404?0.051 0.400?0.019 0.168?0.051 0.374?0.028 0.093?0.002 Backdoor 0.918?0.002 0.900?0.006 0.863?0.022 0.916?0.004 0.840?0.003 0.453?0.051 0.422?0.020 0.163?0.055 0.448?0.030 0.082?0.004 DoS 0.932?0.003 0.905?0.004 0.861?0.025 0.900?0.004 0.856?0.004 0.440?0.023 0.420?0.013 0.154?0.047 0.409?0.028 0.105?0.004 Exploits 0.858?0.010 0.795?0.008 0.744?0.040 0.748?0.017 0.808?0.002 0.273?0.020 0.250?0.008 0.080?0.019 0.249?0.013 0.086?0.001 R8 0.930?0.008 0.864?0.020 0.904?0.004 0.883?0.014 0.779?0.000 0.145?0.031 0.123?0.017 0.086?0.005 0.074?0.006 0.082?0.000 Cover 0.972?0.010 0.966?0.008 0.896?0.013 0.981?0.005 0.922?0.004 0.246?0.069 0.192?0.049 0.044?0.006 0.353?0.055 0.067?0.003 Fraud 0.953?0.002 0.947?0.002 0.954?0.001 0.948?0.002 0.946?0.001 0.387?0.039 0.329?0.025 0.345?0.030 0.480?0.026 0.372?0.002 Pageblocks 0.903?0.010 0.850?0.005 0.896?0.005 0.882?0.005 0.829?0.012 0.547?0.020 0.466?0.011 0.511?0.019 0.397?0.009 0.542?0.032 Shuttle 0.941?0.006 0.900?0.009 0.780?0.025 0.837?0.037 0.779?0.014 0.150?0.017 0.132?0.009 0.031?0.003 0.144?0.022 0.044?0.002 Thrombin 0.913?0.003 0.869?0.019 0.914?0.004 0.366?0.019 0.911?0.000 0.468?0.020 0.368?0.053 0.399?0.056 0.016?0.001 0.457?0.000 Average 0.925?0.006 0.889?0.009 0.870?0.016 0.831?0.011 0.853?0.004 0.351?0.034 0.310?0.022 0.198?0.029 0.294?0.022 0.193?0.005 p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.005 0.964?0.003 0.956?0.001 0.960?0.003 0.956?0.008 0.591?0.056 0.419?0.014 0.416?0.002 0.403?0.011 0.420?0.006 TkeepMARS 0.926?0.010 0.913?0.007 0.905?0.002 0.919?0.005 0.947?0.013 0.609?0.036 0.335?0.030 0.281?0.003 0.390?0.018 0.331?0.018 gaitHunt 0.999?0.001 0.997?0.001 0.999?0.000 0.996?0.001 0.998?0.001 0.864?0.082 0.845?0.028 0.869?0.052 0.827?0.032 0.797?0.079 sel840mECG 0.996?0.001 0.992?0.001 0.993?0.000 0.992?0.000 0.991?0.000 0.723?0.045 0.611?0.011 0.628?0.001 0.609?0.007 0.666?0.005</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Scalability test results. (Top) The training time of all the anomaly detectors on a CPU device; and (Bottom) The results of deep ensemble-based methods (including DIF) on a GPU device.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>AUC-ROC w.r.t. different contamination ratios ? (the percentage of anomalies in the training set).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>(Top) AUC-ROC of DIF and its variants that use optimised representations, and (Bottom) the quality (measured by AII) distribution of randomised representations used in DIF and representations optimised by RDP, REPEN, DSVDD, and RECON.?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A n a l y s i s B a c k d o o r D o s E x p l o i tFig. 7 .</head><label>7</label><figDesc>(Top) AUC-ROC of DIF and its variants of using other anomaly scoring strategies upon random representations, and (Bottom) their effectiveness (measured by AUC-ROC) on each ensemble member within the randomised representation groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1</head><label></label><figDesc>Construction of Deep Isolation Trees Input: D -input dataset Output: T -forest of deep isolation trees 1: Initialise T ? ? 2: Generate representations {X u } r u=1 via G CERE 3: for u = 1 to r do</figDesc><table><row><cell>4:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>: end for 17: return T Algorithm 2</head><label></label><figDesc>Deviation-enhanced Anomaly Scoring Input: o -data object, T -set of deep isolation trees Output: anomaly score F DEAS (o|T ) 1: Generate representations {x u } r u=i via G CERE 2: for u = 1 to r do</figDesc><table /><note>3:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Dataset information of the used tabular, graph, and time-series (TS) datasets. N indicates the number of data objects, with parenthesis denoting the pre-defined training/testing size of graph and time-series datasets. D denotes the number of dimensionality in tabular datasets, the average number of nodes per graph in graph datasets, and the number of sequences in each time-series dataset. #Anom (ratio) is the number of anomalies and the corresponding ratios.</figDesc><table><row><cell></cell><cell>Data</cell><cell>N</cell><cell>D</cell><cell>#Anom (ratio)</cell></row><row><cell></cell><cell>Analysis</cell><cell>95,677</cell><cell>197</cell><cell>2,677 (2.80%)</cell></row><row><cell></cell><cell>Backdoor</cell><cell>95,329</cell><cell>197</cell><cell>2,329 (2.44%)</cell></row><row><cell></cell><cell>DoS</cell><cell>96,000</cell><cell>197</cell><cell>3,000 (3.13%)</cell></row><row><cell>Tabular</cell><cell>Exploits R8 Cover Fraud</cell><cell>96,000 3,974 286,048 284,807</cell><cell>197 9,468 11 30</cell><cell>3,000 (3.13%) 51 (1.28%) 2,747 (0.96%) 492 (0.17%)</cell></row><row><cell></cell><cell>Pageblocks</cell><cell>5,393</cell><cell>11</cell><cell>510 (9.46%)</cell></row><row><cell></cell><cell>Shuttle</cell><cell>1,013</cell><cell>10</cell><cell>13 (1.28%)</cell></row><row><cell></cell><cell>Thrombin</cell><cell>1,909</cell><cell cols="2">139,352 42 (2.20%)</cell></row><row><cell>Graph</cell><cell>HSE MMP p53 PPAR</cell><cell>8,417 (8,150/267) 7,558 (7,320/238) 8,903 (8,634/269) 8,451 (8,184/267)</cell><cell>17 18 18 17</cell><cell>10 (3.75%) 38 (0.50%) 28 (10.41%) 15 (5.62%)</cell></row><row><cell></cell><cell>Lab2Cmac</cell><cell>29,942 (5,833/24,109)</cell><cell>6</cell><cell>108 (0.45%)</cell></row><row><cell>TS</cell><cell>TkeepMARS gaitHunt</cell><cell cols="2">11,349 (3,500/7,849) 64,000 (20,133/43,867) 3 5</cell><cell>64 (0.81%) 521 (1.19%)</cell></row><row><cell></cell><cell cols="2">sel840mECG 58501 (18,500/40,001)</cell><cell>2</cell><cell>371 (0.93%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 AUC</head><label>2</label><figDesc>-ROC and AUC-PR performance (mean ? standard deviation) of DIF and IF-based competing methods on ten real-world tabular datasets. PID and EIF runs out of memory (OOM) on the ultrahigh-dimensional dataset Thrombin. The best performer is boldfaced. 931?0.006 0.910?0.005 0.820?0.019 0.903?0.008 0.782?0.017 0.404?0.051 0.198?0.022 0.075?0.007 0.183?0.028 0.063?0.006 Backdoor 0.918?0.002 0.902?0.005 0.808?0.016 0.894?0.006 0.731?0.021 0.453?0.051 0.218?0.028 0.066?0.005 0.205?0.031 0.046?0.004 DoS 0.932?0.003 0.918?0.004 0.802?0.013 0.896?0.009 0.747?0.020 0.440?0.023 0.269?0.027 0.075?0.004 0.185?0.028 0.060?0.005 Exploits 0.858?0.010 0.840?0.008 0.797?0.011 0.816?0.005 0.745?0.010 0.273?0.020 0.167?0.011 0.077?0.003 0.120?0.013 0.062?0.</figDesc><table><row><cell>Data</cell><cell>DIF (ours)</cell><cell>EIF</cell><cell>AUC-ROC PID</cell><cell>LeSiNN</cell><cell>IF</cell><cell>DIF (ours)</cell><cell>EIF</cell><cell>AUC-PR PID</cell><cell>LeSiNN</cell><cell>IF</cell></row><row><cell>Analysis</cell><cell cols="10">0.003</cell></row><row><cell>R8</cell><cell cols="10">0.930?0.008 0.854?0.006 0.881?0.018 0.859?0.001 0.853?0.016 0.145?0.031 0.101?0.009 0.078?0.011 0.094?0.000 0.075?0.008</cell></row><row><cell>Cover</cell><cell cols="10">0.972?0.010 0.872?0.017 0.939?0.007 0.885?0.008 0.888?0.017 0.246?0.069 0.040?0.006 0.069?0.006 0.051?0.004 0.055?0.008</cell></row><row><cell>Fraud</cell><cell cols="10">0.953?0.002 0.950?0.001 0.950?0.002 0.952?0.000 0.950?0.001 0.387?0.031 0.378?0.027 0.186?0.033 0.401?0.001 0.155?0.015</cell></row><row><cell cols="11">Pageblocks 0.903?0.006 0.902?0.001 0.851?0.003 0.887?0.002 0.900?0.005 0.547?0.012 0.537?0.006 0.421?0.011 0.511?0.007 0.476?0.013</cell></row><row><cell>Shuttle</cell><cell cols="10">0.941?0.006 0.843?0.009 0.864?0.017 0.805?0.005 0.862?0.019 0.150?0.017 0.061?0.003 0.059?0.008 0.048?0.001 0.075?0.014</cell></row><row><cell>Thrombin</cell><cell>0.913?0.003</cell><cell>OOM</cell><cell>OOM</cell><cell cols="3">0.912?0.000 0.905?0.002 0.468?0.020</cell><cell>OOM</cell><cell>OOM</cell><cell cols="2">0.458?0.001 0.372?0.008</cell></row><row><cell>Average</cell><cell cols="10">0.925?0.006 0.888?0.006 0.857?0.011 0.881?0.004 0.836?0.013 0.351?0.033 0.219?0.015 0.123?0.010 0.226?0.011 0.144?0.008</cell></row><row><cell>p-value</cell><cell>-</cell><cell>0.004</cell><cell>0.004</cell><cell>0.002</cell><cell>0.002</cell><cell>-</cell><cell>0.004</cell><cell>0.004</cell><cell>0.006</cell><cell>0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 AUC</head><label>3</label><figDesc>-ROC and AUC-PR performance of DIF and its deep ensemble-based competing methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc>Results on detecting abnormal graphs. 737?0.013 0.715?0.014 0.686?0.008 0.697?0.014 0.593?0.002 0.094?0.005 0.088?0.004 0.074?0.002 0.082?0.004 0.054?0.000 MMP 0.715?0.006 0.663?0.012 0.665?0.004 0.667?0.018 0.675?0.001 0.260?0.006 0.216?0.006 0.230?0.002 0.219?0.011 0.233?0.001 p53 0.680?0.008 0.597?0.017 0.566?0.005 0.619?0.013 0.640?0.001 0.177?0.006 0.138?0.004 0.129?0.001 0.143?0.004 0.150?0.000 PPAR 0.701?0.013 0.716?0.005 0.681?0.004 0.733?0.009 0.643?0.001 0.127?0.008 0.173?0.006 0.143?0.002 0.208?0.012 0.086?0.000</figDesc><table><row><cell>Data</cell><cell>DIF (ours)</cell><cell>EIF</cell><cell>AUC-ROC LeSiNN</cell><cell>iForest</cell><cell>eGLocalKD DIF (ours)</cell><cell>EIF</cell><cell>AUC-PR LeSiNN</cell><cell>iForest</cell><cell>eGLocalKD</cell></row><row><cell>HSE</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Results on detecting anomalies in time-series.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>to 4,096. The other nine datasets are with 32 features</figDesc><table><row><cell>Training Time (in seconds) Training Time (in seconds)</cell><cell>10 0 10 2 10 4 10 6 10 0 10 2 10 4 10 6</cell><cell>16 64 256 1,024 4,096 Dimensionality 10 0 1,000 4,000 16,000 64,000256,000 Data Size 10 2 10 4 10 6 16 64 256 1,024 4,096 Dimensionality 1,000 4,000 16,000 64,000256,000 Data Size 10 6 10 0 10 2 10 4</cell><cell>DIF (cpu) eRDP (cpu) eREPEN (cpu) eDSVDD (cpu) eRECON (cpu) EIF PID LeSiNN iForest DIF (gpu) eRDP (gpu) eREPEN (gpu) eDSVDD (gpu) eRECON (gpu)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6 AUC</head><label>6</label><figDesc>-ROC and AUC-PR results of DIF and w/o DEAS, and training time (in seconds) of DIF and w/o CERE. w/o DEAS is an ablated version by replacing DEAS with the standard scoring function. w/o CERE only uses the typical method to produce the representation ensemble.</figDesc><table><row><cell>Data</cell><cell cols="2">AUC-ROC DIF w/o</cell><cell>DIF</cell><cell>AUC-PR w/o</cell><cell cols="2">Time (in seconds) DIF w/o</cell></row><row><cell></cell><cell></cell><cell>DEAS</cell><cell></cell><cell>DEAS</cell><cell></cell><cell>CERE</cell></row><row><cell>Analysis</cell><cell cols="5">0.931 0.922?0.007 0.404 0.315?0.058 10.5</cell><cell>69.2</cell></row><row><cell>Backdoor</cell><cell cols="5">0.918 0.914?0.006 0.453 0.381?0.067 10.5</cell><cell>66.2</cell></row><row><cell>DoS</cell><cell cols="5">0.932 0.926?0.007 0.440 0.388?0.074 10.4</cell><cell>65.6</cell></row><row><cell>Exploits</cell><cell cols="5">0.858 0.854?0.008 0.273 0.248?0.041 10.4</cell><cell>65.8</cell></row><row><cell>R8</cell><cell cols="4">0.930 0.915?0.012 0.145 0.123?0.034</cell><cell>5.2</cell><cell>19.2</cell></row><row><cell>Cover</cell><cell cols="5">0.972 0.964?0.014 0.246 0.210?0.084 33.0</cell><cell>154.3</cell></row><row><cell>Fraud</cell><cell cols="5">0.953 0.952?0.002 0.387 0.387?0.042 33.6</cell><cell>169.1</cell></row><row><cell>Pageblocks</cell><cell cols="4">0.903 0.912?0.007 0.547 0.576?0.024</cell><cell>0.8</cell><cell>3.5</cell></row><row><cell>Shuttle</cell><cell cols="4">0.941 0.923?0.012 0.150 0.103?0.019</cell><cell>0.5</cell><cell>1.0</cell></row><row><cell>Thrombin</cell><cell cols="5">0.913 0.916?0.002 0.468 0.448?0.020 13.8</cell><cell>336.7</cell></row><row><cell>Average/Total</cell><cell cols="5">0.925 0.920?0.008 0.351 0.318?0.046 128.7</cell><cell>950.6</cell></row><row><cell>p-value</cell><cell>-</cell><cell>0.07</cell><cell>-</cell><cell>0.03</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<title level="m">Outlier analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for anomaly detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th IEEE Int. Conf. Data Mining</title>
		<meeting>8th IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extended isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Brunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1479" to="1489" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On detecting clustered anomalies using sciforest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Mach. Learn. Principles Practice Knowl. Discovery Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="274" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pidforest: anomaly detection via partial identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized isolation forest for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lesouple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baudoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spigai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Tourneret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Revisiting randomized choices in isolation forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cortes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13402</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A probabilistic generalization of isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tokovarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karczmarek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">584</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2041" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. PMLR</title>
		<meeting>Int. Conf. Mach. Learn. PMLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting random distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2950" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rca: A deep collaborative autoencoder approach for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with deviation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unifying review of deep and shallow anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Systematic construction of anomaly detection benchmarks from real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Emmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Workshop Outlier Detection Description</title>
		<meeting>ACM SIGKDD Workshop Outlier Detection Description</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lesinn: Detecting anomalies by identifying least similar nearest neighbours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th IEEE Int. Conf. Data Mining Workshops</title>
		<meeting>15th IEEE Int. Conf. Data Mining Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Isolation-based anomaly detection using nearestneighbor ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Bandaragoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="968" to="998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lshiforest: A generic framework for fast tree isolation based ensemble anomaly analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kotagiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Salcic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Data Eng</title>
		<meeting>Int. Conf. Data Eng</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="983" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced anomaly scores for isolation forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bicego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="page">108115</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperparameter ensembles for robustness and uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6514" to="6527" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Repulsive deep ensembles are bayesian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fortuin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3451" to="3465" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dice: Diversity in deep ensembles via conditional redundancy adversarial estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ram?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diversity matters when learning from ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8367" to="8377" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond outlier detection: Outlier interpretation by attentionguided triplet deviation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Web Conf., 2021</title>
		<meeting>Web Conf., 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1328" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward deep supervised anomaly detection: Reinforcement learning from partially labeled anomaly data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Less is more: Building selective anomaly ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discovery Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">O</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Micenkov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="891" to="927" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep graph-level anomaly detection by glocal knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Web Search Data Mining</title>
		<meeting>Int. Conf. Web Search Data Mining</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endowment</title>
		<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1201" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural network-based anomaly detection in multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4027" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Defying the gravity of learning curve: a characteristic of nearest neighbour anomaly detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="91" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Timeseries anomaly detection using temporal hierarchical one-class network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13" to="016" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mix: A joint learning framework for detecting both clustered and scattered outliers in mixed-type data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th IEEE Int. Conf. Data Mining</title>
		<meeting>19th IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1408" to="1413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ecod: Unsupervised outlier detection using empirical cumulative distribution functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Botta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient algorithms for mining outliers from large data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Manag. Data</title>
		<meeting>Int. Conf. Manag. Data</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Manag. Data</title>
		<meeting>Int. Conf. Manag. Data</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
