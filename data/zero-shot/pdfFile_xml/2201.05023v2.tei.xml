<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stereo Magnification with Multi-Layer Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khakhulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korzhenkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Solovev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sterkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-T</forename><surname>Ardelean</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stereo Magnification with Multi-Layer Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representing scenes with multiple semitransparent colored layers has been a popular and successful choice for real-time novel view synthesis. Existing approaches infer colors and transparency values over regularly spaced layers of planar or spherical shape. In this work, we introduce a new view synthesis approach based on multiple semitransparent layers with scene-adapted geometry. Our approach infers such representations from stereo pairs in two stages. The first stage produces the geometry of a small number of data-adaptive layers from a given pair of views. The second stage infers the color and transparency values for these layers, producing the final representation for novel view synthesis. Importantly, both stages are connected through a differentiable renderer and are trained end-to-end. In the experiments, we demonstrate the advantage of the proposed approach over the use of regularly spaced layers without adaptation to scene geometry. Despite being orders of magnitude faster during rendering, our approach also outperforms the recently proposed IBRNet system based on implicit geometry representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen rapid progress in image-based rendering and novel view synthesis, with a multitude of various methods based on neural rendering approaches <ref type="bibr" target="#b33">[33]</ref>. Among this diversity, the approaches that are based on semitransparent multi-layer representations <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40]</ref> stand out due to their combination of fast rendering time, compatibility with traditional graphics engines, and good quality of re-rendering in the vicinity of the input frames.</p><p>Existing approaches <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40]</ref> build multilayer representations over grids of regularly spaced surfaces * Most of the work was done while Victor Lempitsky was at Samsung AI Center such as planes or spheres with uniformly changing inverse depth. As the number of layers is necessarily limited by resource constraints and the risk of overfitting, this number is usually taken to be relatively small (e.g. <ref type="bibr" target="#b32">32)</ref>. The resulting semi-transparent representation may therefore only coarsely approximate the true geometry of the scene, which limits the generalization to novel views and introduces artefacts. The most recent works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17]</ref> use excessive number of spheres (up to 128) and then merge the resulting geometry using a non-learned post-processing merging step. While the merge step creates scene-adapted and compact geometric representation, it is not incorporated into the learning process of the main matching network, and degrades the quality of novel view synthesis <ref type="bibr" target="#b4">[4]</ref>.</p><p>The coarseness of layered geometry used by multilayer approaches is in contrast to more traditional imagebased rendering methods that start by estimating the nondiscretized scene geometry in the form of mesh <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b34">34]</ref>, view-dependent meshes <ref type="bibr" target="#b11">[11]</ref>, a single-layer depth map <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b38">38]</ref>. Geometry estimates may come from multiview dense stereo matching or from monocular depth. All these approaches obtain a finer approximation to scene geometry, although most of them have to use a relatively slow neural rendering step to compensate for the errors in the geometry estimation.</p><p>Our approach called StereoLayers ( <ref type="figure">Fig. 1</ref>) combines scene geometry adaptation with multi-layer representation. This model is designed for a case known as stereo magnification problem: it reconstructs the scene from as few as two input images. The proposed method starts by building a geometric proxy that is customized to a particular scene. The proxy is formed by a small number of mesh layers with continuous depth coordinate values. In the second stage, similarly to other multi-layer approaches, we estimate the transparency and color textures for each layer, resulting in the final representation of the scene. When processing a new scene, both stages take the same pair of images of that scene as input. Two deep neural networks trained on a dataset of similar scenes are used to implement these two <ref type="bibr">Figure 1</ref>. The proposed StereoLayers pipeline estimates scene-adjusted multi-layer geometry from the plane sweep volume using a pretrained geometry network, and after that estimates the color and transparency values using a pretrained coloring network. The layered geometry represents the scene as an ordered set of mesh layers. The geometry and the coloring networks are trained together end-to-end. stages. Crucially, we train both neural networks together in an end-to-end fashion using the differentiable rendering framework <ref type="bibr" target="#b16">[16]</ref>.</p><p>We compare our approach to the previously proposed methods that use regularly spaced layers on the popular RealEstate10k <ref type="bibr" target="#b40">[40]</ref> and LLFF <ref type="bibr" target="#b21">[21]</ref> datasets. In addition, we propose a more challenging new dataset for novel view synthesis benchmarking. In both cases, we observe that scene-adaptive geometry in our approach results in better novel view synthesis quality than the use of non-adaptive geometry. To put our work in a broader context, we also compare our system's performance with the IBRNet system <ref type="bibr" target="#b37">[37]</ref>, and observe the advantage of our approach, in addition to the considerably faster rendering time. In general, our approach produces very compact scene representations that are amenable for real-time rendering even on low-end devices.</p><p>To sum up, our contributions are as follows. First, we propose a new method for the geometric reconstruction of a scene from pairs of stereo. The method represents scenes using a small number of semitransparent layers with sceneadapted geometry. Unlike other related methods, ours uses two jointly (end-to-end) trained deep networks, the first of which estimates the geometry of the layers, while the second estimates the transparency and color textures of the layers. Finally, we evaluate our approach on previously proposed datasets and introduce a new challenging dataset for training and evaluation of novel view synthesis methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Representations for novel view synthesis. Over the years, different kinds of representations have been proposed for novel view synthesis. Almost without exception, when such representations are acquired from multiple images, those are registered using structure-and-motion algorithm or come from a pre-calibrated stereo-rig. Alternatively, some recent works investigate the creation of such representations from a single image <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b38">38]</ref>. The proposed representations fall into several classes, including volumetric representations that rely on volumetric rendering <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29]</ref>, mesh-based representations <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b41">41]</ref> and pointbased representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">15]</ref>. Most representations of these types require extensive computations to render a novel view, such as running a raw image through a deep convolutional rendering network <ref type="bibr" target="#b33">[33]</ref> or numerous evaluations of a scene network that has a perceptron architecture <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>An important class of representations is based on depth maps. Such depth maps can be naturally obtained using stereo matching <ref type="bibr" target="#b5">[5]</ref> or from monocular depth estimation <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b38">38]</ref>. In this class, the 3D layered inpainting approach <ref type="bibr" target="#b28">[28]</ref> is most related to our work, since after starting from a monocular depth map, it performs its segmentation into multiple layers and then applies the inpainting procedure to each layer to extend its support behind the more frontal layers. Our work has several important differences, as it uses two (rather than one) images as input and predicts the transparency of the layers. Most importantly, the estimation of the multi-layered geometry and the estimation of their colors and transparency are both implemented using deep architectures, which are trained in an end-to-end fashion.</p><p>Multi-layer semitransparent representations. In 1999, <ref type="bibr" target="#b31">[31]</ref> proposed representing scenes with multiple frontoparallel semitransparent layers and acquiring such representations through stereo-matching of a pair of input views. Twenty years later, several approaches <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30]</ref> starting from <ref type="bibr" target="#b40">[40]</ref> exploited advances in deep learning to build deep networks that directly map plane sweep volumes (i.e. tensors obtained by the "unprojection" operation) to final representations of the same kind. The rendering of semitransparent layers is well supported by modern graphics engines, thus the resulting representation is in general more suitable to interactive applications than most other representations that lead to the similar level of realism.</p><p>The multi-layer representations have been extended to wider fields of view in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b17">17]</ref> by replacing planes with spheres. Two approaches <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17]</ref> suggested to "coalesce" (merge) the groups of nearby layers into layers with sceneadapted geometry. In both cases, the grouping of layers is predefined and the merge process is non-learnable and uses simple accumulation heuristics. Consequently, <ref type="bibr" target="#b4">[4]</ref> reported <ref type="bibr">Figure 2</ref>. View extrapolations obtained by our method. The two input images are shown in the middle. The proposed method (StereoLayers) generate plausible renderings even when the baseline is magnified by a factor of 5x (as in this case).</p><p>the loss of rendering quality as a result of such merge, which is still justified in their case by increased rendering and storage efficiency.</p><p>Our research is highly related to previous works on multi-layer semitransparent representations. Unlike most works in this group, our pipeline starts with scene-adapted (non-planar, non-spherical) layer estimation and only then estimates the colors and transparencies of the layers. While <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17]</ref> also end up with scene-adapted semi-transparent layers as a representation, our approach performs the reconstruction in the opposite order (the geometry is estimated first). More importantly, unlike <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17]</ref> we estimate the geometry of layers using a neural network, which is trained jointly with the color and transparency estimation network.</p><p>In the experiments, we show that such an approach results in better view synthesis.</p><p>Single-layer new view synthesis with differentiable rendering. SynSin <ref type="bibr" target="#b38">[38]</ref> and, more recently, Worldsheet <ref type="bibr" target="#b13">[13]</ref> systems predict single-layered geometry from a single image and use differentiable rendering to learn the neural network in a way similar to our method. Our approach considers the case of two input images and focuses on multi-layer geometry. While a variant of Worldsheet considers twolayer extension, it is based on a different architecture and a different layer aggregation strategy and, most importantly, does not outperform a single-layer representation in their experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-layer representation from stereo</head><p>We consider the task of stereo magnification i.e. generate a novel view? n of the scene, based on two input views (images): a reference view I r and a side view I s . We assume that the relative camera poses ? s and ? n of the side and novel views to the reference view and the camera intrinsics K r , K s , and K n are given. To solve this task, our approach builds the scene representation that depends only on side and reference views. Afterward, such a representation can be rendered on any novel camera with standard graphic engines (without reestimating the scene representation). We now describe our approach in detail. We first explain the rendering procedure of a trained model and then discuss the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometry estimation</head><p>Given a trained model and a new stereo pair, the multilayer representation is inferred in two stages. First, the structure of the scene, such as the geometry of the mesh layers, is predicted. Then, in the second stage, the layers' opacity (alpha) and RGB color (textures) are inferred. Note that we treat the pair of input views asymmetrically, as we build the scene representation in the frustum of the reference camera.</p><p>We start by computing the plane sweep volume (PSV) <ref type="bibr" target="#b6">[6]</ref> by placing P fronto-parallel planes in the reference camera frustum and unprojecting the side view onto these planes. The planes are spaced uniformly in the inverse depth space at depths {d 1 , . . . , d P }. We sample the planes at H ? W resolution and concatenate the reference view as an additional set of three channels, resulting in H ? W ? (3P + 3)-sized tensor, which is similar to the one used in other multi-layer approaches, e.g. <ref type="bibr" target="#b40">[40]</ref>.</p><p>The input tensor is then processed by the geometry network F g . Although we consider several variants of the architectures discussed in the following, all these architectures predict L depth maps of size h ? w, which correspond to the depths along with the h ? w pencil of rays uniformly spaced in the image coordinate space of the reference view. In our experiments, we set the resolution of the layers equal to the size of the reference view, w = W , although sampling at different resolutions is also possible. The backbone of F g is similar to the depth prediction module of SynSin <ref type="bibr" target="#b38">[38]</ref>, i.e. is a UNet-like 2D-convolutional net with spectral normalization. The only difference is that we increased the number of input and output featuremaps to address the multi-layer nature of our model. More detailed description of the backbone is provided in Supplementary (Appendix A). We consider the following three schemes to StereoLayers-2 (GC+RSBg)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StereoLayers-4 (BI+RSBg)</head><p>StereoMag-P4 StereoMag-32 <ref type="figure">Figure 3</ref>. For the two stereo pairs (only reference views are shown), we visualize horizontal slices along the blue line. Mesh vertices are shown as dots with the predicted opacity. Colors encode the layer number. The horizontal axis corresponds to the pixel coordinate, while the vertical axis stands for the vertex depth w.r.t. the reference camera (only the most illustrative depth range is shown). StereoLayers method variants generate scene-adaptive geometry in a more efficient way than StereoMag <ref type="bibr" target="#b40">[40]</ref> resulting in more frugal geometry representation, while also obtaining better rendering quality. encode the layers. Group compositing (GC) scheme. In this scheme, F g returns the tensor of shape h ? w ? P with values in the range between 0 and 1. The P channels and the corresponding P planes of PSV are divided into L groups of equal size. Then L deformable layers are obtained as follows: within each group j, 1 ? j ? L, the depth valued j is computed by over-composing <ref type="bibr" target="#b25">[25]</ref> the planes' depths d 1 &lt; . . . &lt; d P with 'opacities' {? k } P k=1 predicted by F g network.</p><formula xml:id="formula_0">d j = I + j k=I ? j d k ? k k?1 i=I ? j (1 ? ? i ) , j = 1, . . . , L,<label>(1)</label></formula><p>where I ? j and I + j are the indices of the bounding planes for each group: I ? j = 1 + (j ? 1) P/L, and I + j = jP/L. For simplicity of notation, layers and planes in Eq. (1) are enumerated in front-to-back order. The 'opacities' ? I + j (corresponding to farmost planes of each group) are manually set to 1 for 1 ? j ? L. As a result, the depth of the j-th layer is bounded by design:</p><formula xml:id="formula_1">d I ? j ?d j ? d I + j . The compositing Eq. (1) is evaluated independently for each of h ? w ? L positions.</formula><p>The group compositing scheme is inspired by the merge procedure from <ref type="bibr" target="#b4">[4]</ref>, but moves this procedure inside the learnable scene representation and before texture and transparency estimation. The main benefit of the GC procedure is the guarantee that the L layers do not intersect and have explicit ordering. Soft-aggregation (SA) scheme. The main drawback of the GC depth aggregation is non-adaptive partition of the depth interval into L layers. Such a non-adaptive partition tends to waste representation capacity for parts that do not contain scene surfaces and to underfit parts where multiple layers are beneficial for scene representation. To overcome this, we make F g to predict the tensor of size h ? w ? L ? P , that is further reshaped to h ? w ? L ? P . After that, softmax is applied along the last axis, and the values obtained are used as weights for the planes' depths {d k } P k=1 (where the P depths span the whole depth range). These depths are averaged with the predicted weights, and the resulting tensor with the shape h ? w ? L is obtained, which contains the depths of the layers. It is worth noting that, unlike the GC approach, this scheme neither provides any ordering of layers, nor guarantees the absence of intersections. Therefore, a special loss promoting non-intersection should be applied during training.</p><p>Bounds interpolation (BI) scheme. We also consider a simplified version of SA scheme that predicts only weights to blend the minimum depth value d 1 and the maximum depth value d P (effectively predicting depths by direct regression). In this scheme, F g network returns the tensor of shape h ? w ? L with values in the range between 0 and 1. The depthd j of j-th layer is computed asd j = ? j d 1 + (1 ? ? j ) d P , where ? j is the output of the geometry network. In our experiments, this scheme achieves the best results; thus, we select the BI method as our default one.</p><p>Meshing. Irrespective of the layer depth prediction scheme, we treat each predicted layer as a mesh. We use the simplest mesh connectivity pattern, connecting each vertex with the six nearby nodes with edges so that each quad defined by four adjacent vertices is meshed with two triangles. Hereinafter, the whole set of resulting L meshes is referred to as the layered mesh. The examples of estimated geometry are showcased in <ref type="figure">Fig. 3</ref>. Now we explain the explored approaches to depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mesh texturing</head><p>The second stage of the inference process completes the reconstruction of the scene by inferring the color and opacity textures of the layered mesh. The process is similar to <ref type="bibr" target="#b40">[40]</ref> and follow-up works with some important modifications. Most importantly, we consider non-planar/nonspherical layers predicted by the previous stage. We thus 'unproject' the side view onto each of the L layers, and then sample each of those reprojections at the H ? W resolution. We employ the nvdiffrast differentiable renderer <ref type="bibr" target="#b16">[16]</ref> to make the rasterization process differentiable w.r.t. the layered mesh geometry. The reference view sampled at the same resolution is concatenated, resulting in a</p><formula xml:id="formula_2">H ? W ? (3L + 3)-sized tensor.</formula><p>This tensor is then processed by coloring network F c which aims to infer the color and opacity values for the mesh layers. Ultimately, our goal is predict the RGB and alpha values for each pixel in each layer. Previously, the authors of <ref type="bibr" target="#b40">[40]</ref> observed that predicting the RGB color indirectly produces better results. That is, they predicted a single "background" RGB image of size H ? W ? 3 and a layer-specific tensor of size H ? W ? L ? 2 that provides blending weights for the linear combination of the reference view with the background. We confirm their findings. We have further observed that in our case even better results can be obtained by predicting an additional mixture weight tensor of size H ? W ? L that contains blending weights for the side view unprojected to each layer.</p><p>Summarizing, within our texture prediction scheme, the network F c thus produces a tensor of size H?W ?(3L + 3) with the last three channels corresponding to the background image, and the remaining channels contain the mixture weights for the Reference view, the unprojected Side view, and the Background image. We refer to this scheme as RSBg, and it is default in our experiments. In the ablation study, we further compare it with the scheme employed in <ref type="bibr" target="#b40">[40]</ref>, where only reference and background images are blended into the texture (denoted RBg), and with the scheme that predicts RGB colors directly (denoted RAW).</p><p>In all cases, in addition to the RGB values, the network F c also predicts a tensor of shape H ?W ?L containing the opacity (alpha) values for each layer. Note that the texturing scheme is able to eliminate redundant layers by setting their opacity values to zero. <ref type="figure">Fig. 3</ref> provides such examples, where some layers were made transparent by the texturing network.</p><p>The architecture of F c is borrowed from <ref type="bibr" target="#b40">[40]</ref> (except for different shapes of the output tensors). Thus, it is a 2Dconvolutional UNet-like net with dilated convolutions in the bottleneck. For completeness, we detail this architecture in the Supplementary material (Appendix A). Rendering. To render a novel view, we project the mesh layers according to the desired pose of the camera while composing them using the compose-over operator <ref type="bibr" target="#b25">[25]</ref>. <ref type="figure">Fig. 2</ref> demonstrates novel views synthesized with the proposed pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning</head><p>We learn the parameters of the geometry network F g and the coloring network F c from datasets of short videos of static scenes, for which camera pose sequences have been estimated using structure-from-motion <ref type="bibr" target="#b27">[27]</ref>. Overall, the training is performed by minimizing the weighted combination of losses discussed below. Image-based losses. Similarly to previous work, e.g. <ref type="bibr" target="#b40">[40]</ref>, the main training loss comes from image supervision. For example, at each training step, we sample the image triplet (I s , I r , I n ) containing the side view I s , the reference view I r and the novel (hold-out) view I n from a training video. Given the current network parameters, we estimate the scene geometry and textures from (I s , I r ) and then project the resulting representation to the I n resulting in the predicted image? n . We then compute the perceptual <ref type="bibr" target="#b14">[14]</ref> and the L 1 losses between I n and? n and backpropagate them through the networks F g and F c . Regularization losses. As was explained above, BI and SA schemes of depth prediction cannot guarantee the ordering of layers. Therefore, we apply a simple hinge loss with zero margin to layers with neighbor indices to ensure that they are predicted in front-to-back order: L ord = L?1 j=0 max 0,d j ?d j+1 . Additionally, we regularize the geometry of the layers by imposing the total variation (TV) loss on the depths of each layer (the total variation is computed for each of the L maps encoding the depths). Adversarial loss. While image-based and geometric losses suffice to obtain the plausible quality of novel view generation for RSBg and RBg coloring schemes (see Sec. 4 for metrics), we did not manage to obtain satisfactory results with RAW scheme without adversarial learning. Specifically, we impose adversarial loss <ref type="bibr" target="#b9">[9]</ref> only for the RAW scheme on the predicted images? n . The main goal of adversarial loss is to reduce unnatural artefacts such as ghosting and duplications. To regularize the discriminator, R 1 penalty <ref type="bibr" target="#b20">[20]</ref> is applied. We stress that adversarial loss is only needed for RAW prediction and is not used in our default configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We consider the RealEstate10k dataset and the Local Lightfield Fusion (LLFF) dataset introduced in previous  <ref type="table">Table 1</ref>. Results of the evaluation on SWORD, RealEstate10K <ref type="bibr" target="#b40">[40]</ref>, and LLFF datasets <ref type="bibr" target="#b21">[21]</ref>. For the latter dataset, models were trained on SWORD. All metrics are computed on central crops of synthesized novel views. Our approach outperforms all baselines on these datasets, although it contains fewer layers in the scene proxy. In particular, the StereoLayers method surpasses IBRNet despite the fact that the latter was trained on 80% of LLFF scenes in a multiview setting. The digit after the type of the model denotes the number of layers in the estimated geometry. Suffix P stands for the model after the applied postprocessing. works, while also proposing a new dataset. The details of the three datasets are provided below.</p><formula xml:id="formula_3">SWORD RealEstate10K LLFF PSNR ? SSIM ? LPIPS ? FLIP ? PSNR ? SSIM ? LPIPS ? FLIP ? PSNR ? SSIM ? LPIPS ? FLIP ?</formula><p>RealEstate10k dataset.</p><p>Following prior works <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b40">40]</ref>, we evaluate our approach on the subset of RealEstate10k <ref type="bibr" target="#b40">[40]</ref> dataset containing consecutive frames from real estate videos with camera parameters. The subset used in our experiments consists of 10, 000 scenes for training and 7, 700 scenes for test purposes. The RealEstate10k dataset serves as the most popular benchmark for novel view synthesis pipelines. Despite the relatively large size, the diversity of scenes in the dataset is limited. The dataset is predominantly indoor and also does not contain enough scenes with central objects. Consequently, models trained on RealEstate10k generalize poorly to outdoor scenes or scenes with large close-by objects <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b40">40]</ref>.  <ref type="table">Table 4</ref>. Cross-dataset generalization. We evaluate models on RealEstate10k (R), SWORD (S) and LLFF (L) datasets. Notaion (X) ? (Y) stands for a model, trained on dataset X and being evaluated on Y. Generally, our approach is on par or more robust to the dataset shift, while having a more compact representation. Evaluation on hold-out LLFF dataset also shows the benefit of training on the proposed SWORD dataset (compared to RealEstate10k).</p><p>SWORD dataset. To evaluate both our and prior methods on more diverse data, we have collected a new dataset, which we call 'Scenes With Occluded Regions' Dataset (SWORD). The new dataset contains around 1, 500 train video and 290 test videos, with 50 frames per video on average. The dataset was obtained after processing the manually captured video sequences of static real-life urban scenes. The processing pipeline was the same as described in <ref type="bibr" target="#b40">[40]</ref>. The main property of the dataset is the abundance of close objects and, consequently, the larger prevalence of occlusions. To prove this quantitatively, we calculate the occlusion areas, that is, areas of those regions of the novel frames that are occluded in the reference frames. To obtain masks for such regions, the off-the-shelf optical flow estimator <ref type="bibr" target="#b32">[32]</ref> is employed. The complete procedure for getting the occlusion masks and the examples of those masks are provided in Supplementary (Appendix E). According to this heuristic, the mean area of occluded image parts for SWORD is approximately five times larger than for RealEstate10k data (14% vs 3% respectively). This rationalizes the collection and usage of SWORD and explains that SWORD allows training more powerful models despite being of smaller size. LLFF dataset. LLFF dataset is another popular dataset with central objects that was released by the authors of the paper on Local Light Field Fusion <ref type="bibr" target="#b21">[21]</ref>. It is too small to train a network on it (40 scenes), and we use these data for evaluation goals only to test the models trained on the other two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation details</head><p>Compared approaches. We use the system described in <ref type="bibr" target="#b40">[40]</ref> as our main baseline and refer to it as StereoMag. By default, StereoMag uses 32 regularly spaced fronto-parallel planes (with uniformly spaced inverse depth), for which color and transparency textures are estimated by a deep network operating on a plane sweep volume. The obtained representation is known as "multi-plane images" (MPI). The original system uses this plane-based geometry for final renderings. We refer to this baseline as StereoMag-32 or omit the number of planes for brevity if it is equal to the default.</p><p>Additionally, we have evaluated variants of the Stere-oMag (denoted as StereoMag-P8 and -P4) that coalesce the 32 planes into 8 or 4 non-planar meshes respectively. The coalescence procedure is detailed in the Supplementary (Appendix D) and is very similar to the one proposed in <ref type="bibr" target="#b4">[4]</ref>. Finally, we trained a variant of StereoMag with eight planes (StereoMag-8). We stress that while StereoMag system was proposed some time ago, based on the comparison in the recent work [28, Appendix A], it remains state-of-the-art for two image inputs.</p><p>We also consdider the more recent IBRNet <ref type="bibr" target="#b37">[37]</ref> system trained to model the radiance field of the scene by blending features of the source images. Unlike StereoMag, this approach has no restrictions on the number of input frames, although it requires a very significant amount of computation to generate each view. Moreover, as the authors have shown, IBRNet shows its best quality after fine-tuning to the new scene under consideration. For evaluation, we used the implementation and checkpoints of the network, provided by the authors, who used 80% of LLFF dataset for training among other data. Despite this, we compared our approach with this method on all data (including LLFF). We also tried to retrain IBRNet on the SWORD dataset (in the setting of two input images). This, however, led to considerably worse performance, so we stick with the authors' provided variant. We also note that test-time fine-tuning of IBRNet is not possible with two view inputs.</p><p>We trained different variations of our model with L ? {2, 4, 8} layers obtained from P = 32 planes of PSV, unless another number is specified. All models were trained for 500, 000 iterations with batch size 4 on a single NVIDIA P40 GPU. The training time is not particularly different for the listed variants of the model. For our approach, we set the following weights for the losses described above: 1 for L 1 loss, 10 for perceptual loss, 5 for TV regularization, and 2 for ordering loss. The RAW scheme for RGB prediction requires a careful tuning of parameters, and we report its results for the configuration with adversarial and feature matching losses with weights set to 5, while the discriminator gradient was penalized every 16-th step with the weight of R 1 penalty equal to 0.0001. Most experiments were conducted at the resolution of 256 on the smallest side.</p><p>Metrics. We follow the standard evaluation process for the novel view task and measure how similar the synthesized view is to the ground-true image. Therefore, we compute the peak signal-to-noise ratio (PSNR), structural (SSIM), and perceptual (LPIPS <ref type="bibr" target="#b39">[39]</ref>) similarity, as well as the recently introduced FLIP metric [2] between the obtained rendering and the ground truth. Artifacts in areas near the image boundary are similar both for planes and layers, and we exclude those regions from consideration by computing metrics over the central crops.</p><p>Finally, to measure the plausibility of rendered images, we perform the study of human preference on a crowdsourcing platform. The evaluation protocol was as follows:</p><p>The assessors were shown two short videos with the virtual camera moving along the predefined trajectory in the same scene from SWORD (validation subset) or LLFF: one video was obtained using the baseline model, and another one was produced with our approach. We asked the users which of the two videos looked more realistic to them. In total, we generated 280 pairs of videos (120 from LLFF and 160 from SWORD scenes), and twenty different workers assessed each pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main results</head><p>Ablation results. In Tab. 2 we present the relative performance of several schemes of depth estimation (denoted GC, SA and BI) and mesh texturing (RSBg, RBg, RAW). For this ablation, all systems were trained and evaluated on the SWORD dataset. As the results show, the best metrics are obtained with a combination of BI + RBSBg methods. Therefore, we choose this model as our default one and refer to it as just StereoLayers model. This pipeline is used in further experiments, unless another configuration is explicitly specified.</p><p>Comparison with prior art. The main results are reported in Tab. 1. Here, due to the relatively small size of the validation part of the both of SWORD and LLFF datasets, we sampled multiple triplets (the reference, side, and novel cameras) for each scene to get a more accurate estimation of the score. We selected the model with L = 4 layers as our main variant because it performs better on the holdout LLFF data. It consistently outperforms the baseline StereoMag-32 model according to the considered metrics, while containing significantly less layers.  <ref type="bibr" target="#b21">[21]</ref>. The leftmost column shows the ground truth, two other columns demonstrate patches of a novel view obtained with StereoMag-32 and our system respectively. In the cutout StereoMag results, small translations of a camera from the reference pose reveal discontinuities in the approximate scene geometry leading to ghosting artefacts. In our case, thanks to the scene-adapted geometry, ghosting is not so apparent.</p><p>Post hoc coalescing of StereoMag representations into non-planar layers worsened the results (this finding is consistent with one reported in <ref type="bibr" target="#b4">[4]</ref>). Finally, the eight-planar structure is consistently worse than the 32-plane one.</p><p>At the same time, the results of our model with four layers are even better than when using more (eight) layers. Furthermore, a configuration with just two layers remains competitive (better than eight-layer configuration in some metrics and better than StereoMag-32 in most metrics).</p><p>Notably, there is a large gap between our configuration with four layers and the StereoMag method with geometry merged into four layers (StereoMag-P4). This emphasizes the benefit of the end-to-end training used by our method. As showcased in <ref type="figure">Fig. 3</ref>, the novel approach approximates the scene geometry in a reasonable way even with just two layers and, vice versa, is able to 'zero out' the redundant layers. We attribute the superiority of our method over StereoMag-P to the proposed end-to-end training procedure.</p><p>We show the percentage of times users prefer each method in Tab. 3. One of our qualitative improvements is illustrated in <ref type="figure" target="#fig_0">Fig. 4</ref>: the deformable layers successfully overcome the "ghost" edge artifacts, occasionally observed in the case of rigid planes.</p><p>Also, we conducted a separate study of the model's sen-sitivity to the group size when estimating the scene geometry (i.e. the ratio of the number of planes in PSV P to the number of layers L). In summary, quality does not change dramatically under variation in group size; see Supplementary (Appendix B) for numerical details.</p><p>In the supplementary video, we show the results on a wide range of photos from different datasets. The video contains a comparison with StereoMag and IBRNet that demonstrates that our approach produces less blurry details while having a similar quality of estimated geometry. We encourage the reader to watch the supplementary video. Cross-dataset evaluation. As mentioned above, SWORD contains mostly outdoor scenes with a central object, which is similar in nature to the LLFF dataset. It is the main reason why we observed a pretty good quality of the model trained on SWORD and evaluated on LLFF (the rightmost part of Tab. 1). We have also investigated a more challenging setting: the performance of methods in the cross-dataset setting is reported in Tab. 4: we cross-evaluate our and baseline models on RealEstate10k and SWORD datasets that are rather different. Timings. The representations produced by our method are well suited for rendering within mobile photography applications. Thus, on Samsung Galaxy S20 (Mali-G77 GPU), rendering our representations at 512?256 resolution runs at about 180 frames per second. Furthermore, our representations can be quickly created from new stereo pairs (our current unoptimized inference takes 0.19 seconds on an NVidia P40 GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and discussion</head><p>In this work, we proposed an end-to-end pipeline that recovers the geometry of the scene from an input stereo pair using a fixed number of semitransparent layers. Despite using fewer layers (4 layers vs. 32 planes for the baseline StereoMag model), our approach demonstrated superior quality in terms of commonly used metrics for the novel view synthesis problem, as well as human evaluation. Unlike the StereoMag system, the quality of which heavily depends on the number of planes, our method has reached better scores while being robust to reducing the number of layers. We have verified that the proposed method can be trained on multiple datasets and generalizes well to unseen data. The resulting mesh geometry can be effectively rendered using standard graphics engines, making the approach attractive for mobile 3D photography.</p><p>Additionally, we presented a new challenging SWORD dataset, which contains cluttered scenes with heavily occluded regions. Even though SWORD consists of fewer scenes than the popular RealEstate10K dataset, systems trained on SWORD are likely to generalize better to other datasets, e.g. the LLFF dataset.  <ref type="table">Table S1</ref>. Architecture of the geometry network Fg for BI parameterization. K is the kernel size, S -stride, D -dilation, Ppadding, C -the number of output channels for each layer, and input denotes the input source of each layer. Up-arrow ? denotes the 2x bilinear upscaling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architectures</head><p>Geometry network F g . The architecture of our depth estimator resembles the network from SynSin <ref type="bibr" target="#b38">[38]</ref>. It takes the plane sweep volume (PSV) as its input and returns 'opacities' for each of the P regular planes, that are used to construct deformable layers. Each block sequentially applies a convolution, layer normalization and LeakyReLU to the input tensor. We apply spectral normalization <ref type="bibr" target="#b23">[23]</ref> to the convolution kernel weights. Other details are given in Tab. S1.</p><p>Coloring network F c . The architecture of the coloring network is inspired by the one described in StereoMag paper <ref type="bibr" target="#b40">[40]</ref>. Each block consists of a convolution, layer normalization, and ReLU unit (except for the final block). Detailed parameters for RSBg scheme are provided in Tab. S2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional results</head><p>Scaling to hi-res.  <ref type="table">Table S2</ref>. Architecture of the coloring network Fc for the RSBg parameterization. K is the kernel size, S -stride, D -dilation, P -padding, C -the number of output channels for each layer, and input denotes the input source of each layer. <ref type="figure">Figure S1</ref>. Performance of our system as a function of the number of layers. The plot confirms the ability of our approach to represent complex scenes with just a few layers. Scene slices. <ref type="figure">Fig. S2</ref> provides additional examples of the estimated geometry for different scenes.</p><p>Number of layers in BI scheme. For MPI-based approaches, the number of planes was shown to be critical for constructing a plausible representation of the scene <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30]</ref>.</p><p>To demonstrate the properties of our deformable layers, we consider the influence of the number of layers in the estimated geometry on common quality metrics. <ref type="figure">Fig. S1</ref> shows that the resulting performance falls as the number of layers decreases to one, proving that multi-layer structure is crucial. Perhaps surprisingly, the measured quality does not always grow as this number increases. We suggest that the model cannot handle the redundant geometry properly. It is worth noting that the authors of the Worldsheet paper reported a similar effect in the single-image case <ref type="bibr" target="#b13">[13]</ref>.</p><p>Number of layers per group in GC scheme. In addition to our main bounds interpolation (BI) scheme of depth parameterization, we study the properties of the group compositing (GC) model. Namely, we investigate the performance of this system as a function of the number of planes in plane sweep volume during the geometry estimation step. As Tab. S5 shows, the resulting quality of the model does not depend on the size of the group. However, we see that if both the number of layers and the size of the group are reduced simultaneously, the quality deteriorates. And with an increase in the size of the group, there is no increase in metrics. In general, robustness to these parameters is provided by two points: the nature of the semitransparent proxy geometry, in which the alpha channel takes the main responsibility for the object structure, and the adaptive layered proxy geometry, which can bend itself under objects to depend less on the number of planes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Failure cases</head><p>To demonstrate the limitations of our approach, we show typical artifacts of the method in <ref type="figure">Fig. S3</ref>. Note that most of the drawbacks are visible only when the camera moves around the scene and are not distinguishable in randomly selected frames without temporal context.</p><p>When the baseline is magnified by a great factor, one can observe "stretching" faces of our layered mesh near the depth discontinuities. We believe that this type of artifact is caused by the mesh structure of our geometry. The "ghost" semitransparent textures is another common issue of the synthesized views. One of the problems could also be attributed to inconsistent depth prediction when some pixels have minor errors in depth values, which leads to small ghostings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MPI postprocessing</head><p>In this section we briefly describe the postprocessing procedure that aims to merge the predicted rigid planes of StereoMag-32 <ref type="bibr" target="#b40">[40]</ref> to the fewer number of deformable layers. In our experiments, the final number of such layers equals 8, that coincides with the basic configuration of our approach.</p><p>The pipeline partially follows the one described in <ref type="bibr" target="#b4">[4]</ref>. Firstly, we divide 32 planes into eight groups and composeover the depth within each group on top of the furthest plane in the group. This operation results in 8 deformable layers. To infer the textures of those layers, we perform the second The distribution of rays r ? V (t) is constructed as follows: the line passing through the pinhole camera and texel t intersects the reference image plane at the pixel coordinate p. The coordinate q is normally distributed around p, and the ray r passes from q through t. The weighing function w (r) is equal to the Gaussian density value at q. Color c r and transmittance? r values are computed with the composeover operation along the ray r over the planes that belong to the same group as texel t does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Occlusion masks</head><p>In this section, we describe the heuristic to create masks of occluded regions. Examples of such masks are provided in <ref type="figure" target="#fig_0">Fig. S4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Cycle consistency of optical flows</head><p>Consider two images A and B, without loss of generality, they are assumed to be grayscale. For the coordinates of the pixel p we denote the color of this pixel in the image A as A [p] . The coordinate grid G is such a "image" (twodimensional matrix) that ?p G [p] = p. We define the back- Proof. We assume that the pixel p of the image A corresponds to the pixel q of the image B under the warping operation. This implies the following equations: </p><formula xml:id="formula_4">B [q] = A [p] ,<label>(S3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Estimation of occlusion masks</head><p>We employ the pretrained optical flow estimator <ref type="bibr" target="#b32">[32]</ref> and compute optical flowsF rn andF nr between the reference view I r and ground-true novel view I n . According to the lemma E.1, these flows should be cycle-consistent. However, the views do not completely correspond to each other because of the presence of occluded regions. Therefore, the result? of warping of one flow with another G = backward F rn ,F nr does not result in the "ideal" coordinate grid.</p><p>Based on this, we treat a pixel p that |? [p] ? p| &lt; as non-occluded because the optical flow estimator can find the corresponding pixel in another image. Otherwise, we include the pixel in the occlusion mask. The threshold is set to the size of one pixel. As a downside, the flow estimator is very sensitive to the image borders. To overcome this issue, we use central crops that finally contain reasonable masks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Comparison on challenging scenes from the LLFF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure S2 .Figure S3 . 1 V 1 V</head><label>S2S311</label><figDesc>Additional horizontal slices (along the blue line) on scenes from LLFF dataset. Mesh vertices are shown as dots with the predicted opacity. Colors encode the layer number. The horizontal axis corresponds to the pixel coordinate, while the vertical axis stands for the vertex depth w.r.t. the reference camera (only the most illustrative depth range is shown). Configurations of StereoLayers method generate scene-adaptive geometry in a more efficient way than StereoMag, resulting in more frugal geometry representation, while also obtaining better rendering quality. Examples of most common failures of StereoLayers outputs. In most cases they can be attributed to a combination of photometric scene complexity, and an unfortunate choice of the input pair.step, averaging the color c and transmittance? of RGBA planes within each group over the set V (t) of rays passing through the texel t. Namely, we run the Monte Carlo ray tracing defined by the equations below,log (? t ) = ? ?(t) w (r) [log (? r )] 2 dr, c t = ? ?(t) w (r) c r log (? r ) dr,where ? is a normalizing constant ? = V (t) w (r) log (? r ) dr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFFFFF</head><label></label><figDesc>AB of images A and B and the backward warping backward operation as follows B = backward A, ? AB ?? ?q B [q] = A ? AB [q] . (S1) Similarly, forward flow matrix ? AB and forward warping are defined as B = forward A, ? AB ?? ?p A [p] = B ? AB [p] . (S2) Lemma E.1. For two optical flows of the same kind F AB and F BA the following cycle-consistency property holds backward (F BA , F AB ) = G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?F?FF</head><label></label><figDesc>BA [q] (S5) = p.(S7)Let X be the result of warping one backward flow with another,therefore, X = G.The case of forward flow may be considered in the same way. Denote the result of warping with Y ,Y = backward ? F BA , ? F AB .The value in the pixel p gives us the followingY [p] BA [q] (S7) = p,which leads to Y = G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Table 3</head><label>23</label><figDesc></figDesc><table><row><cell>Depth estimation</cell><cell cols="2">Texturing scheme</cell><cell cols="4">PSNR ? SSIM ? LPIPS ? FLIP ?</cell></row><row><cell>BI</cell><cell cols="2">RSBg</cell><cell>25.95</cell><cell>0.81</cell><cell>0.096</cell><cell>0.14</cell></row><row><cell>BI</cell><cell cols="2">RBg</cell><cell>24.96</cell><cell>0.77</cell><cell>0.111</cell><cell>0.15</cell></row><row><cell>BI</cell><cell cols="2">RAW</cell><cell>24.90</cell><cell>0.77</cell><cell>0.099</cell><cell>0.15</cell></row><row><cell>SA</cell><cell cols="2">RSBg</cell><cell>24.66</cell><cell>0.76</cell><cell>0.121</cell><cell>0.16</cell></row><row><cell>SA</cell><cell cols="2">RAW</cell><cell>24.30</cell><cell>0.75</cell><cell>0.099</cell><cell>0.15</cell></row><row><cell>GC</cell><cell cols="2">RSBg</cell><cell>25.24</cell><cell>0.77</cell><cell>0.115</cell><cell>0.15</cell></row><row><cell>GC</cell><cell cols="2">RAW</cell><cell>24.90</cell><cell>0.77</cell><cell>0.107</cell><cell>0.15</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">Baseline</cell><cell cols="3">Our score, % p-value</cell></row><row><cell cols="2">SWORD</cell><cell cols="2">StereoMag-32 IBRNet</cell><cell>61 81</cell><cell cols="2">&lt; 0.001 &lt; 0.001</cell></row><row><cell>LLFF</cell><cell></cell><cell cols="2">StereoMag-32 IBRNet</cell><cell>54 70</cell><cell cols="2">&lt; 0.001 &lt; 0.001</cell></row></table><note>. Evaluation of StereoLayers configs. Top row represents a model chosen as a default one. Details of the predicting schemes are discussed in Sec. 3. All the configurations were trained with P = 32 planes and L = 4 layers.. User study results. The 3rd column contains the ratio of users who selected the output of our model (StereoLayers-4) as more realistic in side-by-side comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S3 .</head><label>S3</label><figDesc>Scaling to higher resolution on SWORD dataset. We examine our model and StereoMag in a fully-convolutional regime: both were trained at resolution of 256 ? 512 and applied for 512 ? 1024. As in previous experiments, we used the checkpoint of IBRNet provided by the authors of the corresponding paper.</figDesc><table><row><cell>Dataset</cell><cell>Baseline</cell><cell cols="2">Our score, % p-value</cell><cell></cell></row><row><cell>SWORD</cell><cell>StereoMag-32 IBRNet</cell><cell>55.62 75.69</cell><cell>&lt; 0.001 &lt; 0.001</cell><cell></cell></row><row><cell>LLFF</cell><cell>StereoMag-32 IBRNet</cell><cell>54.42 50.27</cell><cell>&lt; 0.001 &lt; 0.001</cell><cell></cell></row><row><cell>RealEstate10k</cell><cell>StereoMag-32 IBRNet</cell><cell>63.91 60.74</cell><cell>&lt; 0.001 &lt; 0.001</cell><cell></cell></row><row><cell cols="4">Table S4. Additional user study on high-resolution images. The</cell><cell></cell></row><row><cell cols="4">3rd column contains the ratio of users who selected the output</cell><cell></cell></row><row><cell cols="4">of our model as more realistic under the two-alternative forced</cell><cell></cell></row><row><cell>choice.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Tab. 2 of the main text, our model trained with the RBg</cell><cell></cell></row><row><cell cols="4">texturing scheme (which is the default for StereoMag) per-</cell><cell></cell></row><row><cell cols="4">forms significantly worse than with RSBg: LPIPS of 0.111</cell><cell></cell></row><row><cell cols="4">vs 0.096. To demonstrate that the texturing scheme is</cell><cell></cell></row><row><cell cols="4">not the most crucial part of our pipeline, we retrained</cell><cell></cell></row><row><cell cols="4">StereoMag-32 model with RSBg scheme. In particular, this</cell><cell></cell></row><row><cell cols="4">modification did not improve the quality of the baseline on</cell><cell></cell></row><row><cell cols="4">SWORD: SSIM of 0.77 vs 0.76, LPIPS of 0.107 vs 0.107.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell cols="2">PSNR ? SSIM ? LPIPS ? FLIP ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IBRNet</cell><cell>27.4</cell><cell>0.67</cell><cell>0.219</cell><cell>0.27</cell></row><row><cell></cell><cell></cell><cell></cell><cell>StereoMag (256?512)</cell><cell>23.3</cell><cell>0.65</cell><cell>0.178</cell><cell>0.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours (256?512)</cell><cell>24.2</cell><cell>0.69</cell><cell>0.155</cell><cell>0.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">from SWORD (with resolution of 512 ? 1024), 60 scenes</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">from RealEstate10k (576?1024) and 80 scenes (40 unique)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">from LLFF data (512 ? 512). All scenes and input views</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">are randomly sampled from the test sets. The results of this</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">experiment are reported in Tab. S4.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">StereoMag with RSBg scheme. As was shown in the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S5 .</head><label>S5</label><figDesc>Performance dependence on the number of layers and the size of the plane group for group compositing (GC) configuration. The quality in terms of SSIM and LPIPS is slightly dependent on the size of the group and the number of layers for 256 ? 256 images.</figDesc><table><row><cell>Group size P/L</cell><cell>Number of planes P</cell><cell>Number of layers L</cell><cell cols="2">LPIPS? SSIM?</cell></row><row><cell>4</cell><cell>16</cell><cell>4</cell><cell>0.129</cell><cell>0.67</cell></row><row><cell>4</cell><cell>24</cell><cell>6</cell><cell>0.120</cell><cell>0.70</cell></row><row><cell>4</cell><cell>32</cell><cell>8</cell><cell>0.119</cell><cell>0.70</cell></row><row><cell>4</cell><cell>40</cell><cell>10</cell><cell>0.124</cell><cell>0.70</cell></row><row><cell>16</cell><cell>64</cell><cell>4</cell><cell>0.122</cell><cell>0.72</cell></row><row><cell>32</cell><cell>64</cell><cell>2</cell><cell>0.121</cell><cell>0.71</cell></row><row><cell>15</cell><cell>120</cell><cell>8</cell><cell>0.119</cell><cell>0.70</cell></row><row><cell>20</cell><cell>120</cell><cell>6</cell><cell>0.122</cell><cell>0.70</cell></row><row><cell>30</cell><cell>120</cell><cell>4</cell><cell>0.120</cell><cell>0.70</cell></row><row><cell>60</cell><cell>120</cell><cell>2</cell><cell>0.119</cell><cell>0.70</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural point-based graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kara-Ali</forename><surname>Aliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sevastopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Kolos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FLIP: A Difference Evaluator for Alternating Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Akenine-M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalle?str?m</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matryodshka: Real-time 6DoF video view synthesis using multi-sphere images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Attal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selena</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Left: reference images; middle: generated novel views; right: magenta masks indicate the parts of novel views that were occluded from the reference point of view. The area of such regions in SWORD is much greater than for RealEstate10k</title>
	</analytic>
	<monogr>
		<title level="m">Figure S4. Occlusion masks</title>
		<imprint/>
	</monogr>
	<note>justifying its usage</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Immersive light field video with a layered mesh representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Broxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Overbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Duvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dourgarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View interpolation for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shenchang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multi-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepview: High-quality view synthesis by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Broxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Du-Vall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Styles</forename><surname>Overbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael F</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep blending for free-viewpoint image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">True</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable inside-out image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Worldsheet: Wrapping the world in a 3d sheet for view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of point-based techniques in computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Botsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modular primitives for high-performance differentiable rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongho</forename><surname>Seol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep multi depth panoramas for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-En</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hold-Geoffroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020. 1</title>
		<meeting>ECCV, 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural sparse voxel fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Kyaw Zaw Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural volumes: Learning dynamic renderable volumes from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Which Training Methods for GANs do actually Converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local light field fusion: Practical view synthesis with prescriptive sampling guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><forename type="middle">Khademi</forename><surname>Ortiz-Cayon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NeRF: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d ken burns effect from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositing digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Free view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d photography using context-aware layered depth inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepvoxels: Learning persistent 3d feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of view extrapolation with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stereo matching with transparency and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">State of the art on neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Single-view view synthesis with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Layer-structured 3d scene inference via view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ibrnet: Learning multi-view image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Synsin: End-to-end view synthesis from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High-quality video view interpolation using a layered representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
