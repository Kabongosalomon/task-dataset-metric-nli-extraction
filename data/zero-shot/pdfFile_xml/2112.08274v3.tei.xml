<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Putting People in their Place: Monocular Regression of 3D People in Depth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>yusun@stu.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
							<email>liuwu1@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Qian Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Explore Academy of JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Putting People in their Place: Monocular Regression of 3D People in Depth</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 https://github.com/Arthur151/ROMP 2 https://github.com/Arthur151/Relative_Human arXiv:2112.08274v3 [cs.CV] 20 Apr 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Monocular reconstruction of multiple 3D people with coherent depth reasoning. We introduce BEV, a monocular one-stage method with an efficient new "bird's-eye-view" representation that enables the network to explicitly reason about people in 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional imaginary Bird's-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body * This work was done when Yu Sun was an intern at Explore Academy of JD.com. ? Corresponding author.</p><p>centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a "Relative Human" (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code 1 and dataset 2 are released for research purposes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this article, we focus on simultaneously estimating the 3D pose and shape of all people in an RGB image along with their relative depth. There has been rapid progress <ref type="bibr">[22]</ref> on regressing the 3D pose and shape of individual (cropped) people <ref type="bibr">[4,</ref><ref type="bibr">15,</ref><ref type="bibr">16,</ref><ref type="bibr">18,</ref><ref type="bibr">19,</ref><ref type="bibr">26,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref> as well as the direct regression of groups <ref type="bibr">[11,</ref><ref type="bibr" target="#b33">34]</ref>. Neither class of methods explicitly reasons about the depth of people in the scene. Such depth reasoning is critical to enable a deeper understanding of the scene and the multi-person interactions within it. To address this, we propose a unified method that jointly regresses multiple people and their relative depth relations in one shot from an RGB image.</p><p>While previous multi-person methods perform well in constrained experimental settings, they struggle with severe occlusion, diverse body size and appearance, the ambiguity of monocular depth, and in-the-wild cases <ref type="bibr">[11,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>. These challenges lead to unsatisfactory performance in crowded scenes, including detection misses, similar predictions for overlapping people, and all predictions having a similar height. We observe two inter-related limitations that result in these failures. First, the architecture of the regression networks is closely tied to the 2D image, while the people actually inhabit 3D space. We address this with a new architecture that reasons in 3D. Second, depth estimation is fundamentally ambiguous due to the unknown height of the people in the image and it is difficult to obtain training data of images with ground-truth height and depth. To address this, we present a new dataset and novel losses that allow training without having metric depth.</p><p>We observe that crowded scenes contain rich information about the relative relationships between people, which can be exploited for both training and validation of depth reasoning. However, we still lack a powerful representations to learn from these cases. A few learning-based methods have been proposed for reasoning about the depth of predicted body meshes <ref type="bibr">[11]</ref> or 3D poses <ref type="bibr">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>. Unfortunately, they all reason about depth via 2D representations, such as RoI-aligned features <ref type="bibr">[11,</ref><ref type="bibr">25]</ref> or a 2D depth map <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref>. These regression-based 2D representations have inherent drawbacks for representing the 3D world. The lack of an explicit 3D representation in the networks makes it challenging for these methods to deal with crowded scenes in which people overlap at different depths. Therefore, we argue that an explicit 3D representation is needed.</p><p>To achieve this, we develop BEV (for Bird's Eye View), a unified one-stage method for monocular reconstruction and depth reasoning of multiple 3D people. We take inspiration from ROMP <ref type="bibr" target="#b33">[34]</ref>, a one-stage, multi-person, regression method that directly estimates multiple 2D front-view maps for 2D human detection, positioning, and mesh parameter regression without depth reasoning. With ROMP, the network can only reason about the 2D location of people in the image plane. To go beyond this, we need to enable the network to efficiently reason about depth as well. To that end, we introduce a new imaginary 2D "bird's-eye-view" map that represents the likely centers of bodies in depth. To be clear, BEV takes only a single 2D image; the overhead view is inferred, not observed. BEV uses a powerful and efficient localization pipeline, performing bird's-eye-viewbased coarse detection and fine localization in parallel. We employ the 2D heatmaps for coarse detection from both the front (image) and bird's eye views. BEV combines these heatmaps to obtain a 3D heatmap, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. By learning the front and the bird's-eye view together, BEV explicitly models how people appear in images and in depth. This enables BEV to learn from available 2D and 3D annotations. BEV also uses a novel 3D Offset map to refine the initial coarse detections. From these coarse and fine maps, we obtain the 3D translation of all people in the scene. BEV transforms these predictions from the latent 3D Center-map space to an explicit camera-centric 3D space. Given these 3D translation predictions, BEV samples the features of all the people from a predicted mesh feature map and regresses the final SMPL <ref type="bibr">[23]</ref> parameters. Distinguishing people at different depths enables BEV to estimate multiple people even with severe occlusion as illustrated in <ref type="figure" target="#fig_4">Fig. 1</ref>.</p><p>Even with a powerful 3D representation, we need an appropriate training scheme to ensure generalization. The main reason is that without knowing subject height, we lack effective constraints to alleviate the depth/height ambiguity under perspective projection. In particular, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. The ambiguity causes incorrect depth estimates for children and infants, limiting the generalization of existing methods. Unfortunately, existing 3D datasets with multiple people have limited diversity in height and age, so they cannot be used to improve or evaluate generalization.</p><p>Since collecting ground-truth 3D data in the wild is difficult, we instead train BEV using cost-effective weak labels of in-the-wild images. Specifically, we collect a dataset, named "Relative Human" (RH), that contains weak annotations of depth layers and human ages categorized into the groups adult, teenager, child, and infant. Moreover, we propose a weakly supervised training scheme (WST) to effectively learn from these weak supervision signals. For instance, we use a piece-wise loss function that exploits the depth layers to penalize incorrect relative depth orders. Exploiting age information to constrain height is tricky. While age and height are correlated, heights can vary significantly within the same age group. Consequently, we develop an ambiguity-compatible mixed loss function that encourages body shapes with heights that lie within an appropriate range for each age group.</p><p>We evaluate BEV on three multi-person datasets: in-the-wild using the 2D RH dataset and in 3D using the real CMU Panoptic <ref type="bibr">[13]</ref> and the synthetic AGORA <ref type="bibr">[28]</ref> datasets. On RH, compared with previous methods <ref type="bibr">[11,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, BEV is more accurate in relative depth reasoning and pose estimation. On CMU Panoptic, BEV outperforms previous methods <ref type="bibr">[6,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> in 3D pose estimation. On AGORA, BEV significantly improves detection and achieves state-of-the-art results on "AGORA kids" in terms of the mesh reconstruction error. Also, fine-tunning on RH in a weakly supervised manner significantly improves the results for all age groups, especially for young people. In summary, the main contributions are: (1) We construct a 3D representation to alleviate the monocular depth ambiguity via combining a front-view representation with an imaginary bird's eye view. <ref type="bibr">(2)</ref> We collect the Relative Human dataset with weak annotations of in-the-wild images, which facilitates the training and evaluation on monocular depth reasoning in multi-person scenes. (3) We develop a weakly supervised training scheme to learn from weak depth annotations and to exploit age information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D mesh regression from natural scenes. Here, we focus on regressing a 3D body mesh using a parametric model like SMPL from a single RGB image. Most methods can be divided into multi-stage or single-stage approaches. For general multi-person cases, most existing methods <ref type="bibr">[4,</ref><ref type="bibr">15,</ref><ref type="bibr">19,</ref><ref type="bibr">26,</ref><ref type="bibr">29]</ref> are based on a typical two-stage framework, which first detects people and then estimates the parameters of each person separately. Recent methods focus on exploring various supervision <ref type="bibr">[33]</ref> signals, such as temporal coherence <ref type="bibr">[16]</ref>, contour alignment <ref type="bibr">[7,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b38">39]</ref>, selfcontact <ref type="bibr">[27]</ref>, ground constraints <ref type="bibr" target="#b80">[32,</ref><ref type="bibr" target="#b39">40]</ref>, or global human trajectory <ref type="bibr" target="#b40">[41]</ref> to enhance the geometric/dynamic consistency. However, for depth reasoning about all people in the scene, these multi-stage methods are not ideal. The processing of individual cropped people cannot exploit the scene context or reason about depth ordering.</p><p>A few one-stage methods <ref type="bibr">[24,</ref><ref type="bibr" target="#b33">34]</ref> estimate multiple 3D people simultaneously. Given a single image, ROMP <ref type="bibr" target="#b33">[34]</ref> outputs a 2D Body Center Heatmap, Camera Map, and Parameter Map for 2D human detection, positioning, and mesh parameter regression, respectively. At the position parsed from the 2D Body Center heatmap, ROMP samples the final mesh parameters from the Camera and Parameter maps. These one-stage methods enjoy a holistic view of the image, which is more suitable for depth reasoning. However, they are based on 2D representations that do not represent depth. Like most methods, they model adults (with SMPL), train on images of adults, and therefore only predict adults. To tackle the limitations of their 2D representation and age bias, we propose BEV and its training scheme of learning age priors that constrain body height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular depth reasoning.</head><p>Most previous methods place bodies in depth via post-processing. Due to their 2D-based pipeline and lack of height prior for different age groups, their results are unsatisfying. A few learning-based methods, like 3DMPPE <ref type="bibr">[25]</ref> and CRMH <ref type="bibr">[11]</ref>, address multi-stage depth reasoning. 3DMPPE uses image features to refine the bounding-box-based depth predictions. CRMH learns from instance segmentation to distinguish the relative depth between overlapping people. However, instance segmentation is expensive and unable to promote the learning of depth relations in cases without overlapping. SMAP <ref type="bibr" target="#b47">[48]</ref> and HMOR <ref type="bibr" target="#b37">[38]</ref> employ a 2D depth map to represent the root depth of 3D pose at each pixel. However, in crowded scenes, these 2D representations are ambiguous. In contrast, BEV adopts a novel bird's-eye-view-based 3D representation to distinguish people at different depths, therefore, it is more robust to the overlapping cases. Most recently, Ugrinovic et al. <ref type="bibr" target="#b35">[36]</ref> propose an optimization-based method to refine the 3D translation of estimated body meshes. They fit the 3D body mesh to the detected 2D poses and force the feet to touch the ground. In contrast, our learning-based, one-stage, framework is more efficient and flexible, and can adapt to more scenarios, such as jumping. Albiero et al. <ref type="bibr">[2]</ref> estimate the depth of all faces in a crowd in one shot by regressing their 6DoF pose; they do not deal with shape variation or articulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The overall framework is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. BEV adopts a multi-head architecture. Given a single RGB image as input, BEV outputs 5 maps. For coarse-to-fine localization, we use the first 4 maps, which are the Body Center heatmaps and the Localization Offset maps in the front view and bird's-eye view. We first expand the front-/bird'seye-view maps in depth/height and then combine them to generate the 3D Center/Offset maps. For coarse detection, we extract the rough 3D position of people from the 3D Center map. For fine localization, we sample the offset vectors from the 3D Offset map at the corresponding 3D center position. Adding these gives the 3D translation prediction. For 3D mesh parameter regression, we use the estimated 3D translation (x i , y i , d i ) and the Mesh Feature map. The depth value d i of 3D translation is mapped to a depth encoding. At (x i , y i ), we sample a feature vector from the Mesh Feature map and add it to the depth encoding for final parameter regression. Finally, we convert the estimated parameters to body meshes using the SMPL+A model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SMPL+A: Mesh Representation for All Ages</head><p>The SMPL <ref type="bibr">[23]</ref> and SMIL <ref type="bibr">[9]</ref> models are developed to parameterize 3D body meshes of adults and infants into low-dimensional parameters. Recently, AGORA <ref type="bibr">[28]</ref> further extends SMPL to support children by linearly blending the SMIL and SMPL template shapes with a weight ? ? [0, 1], which we refer to as an "age offset." While blending the templates to address scale and proportion differences between adults and children, AGORA uses the adult shape space regardless of age. Additionally, AGORA does not address the representation of infants. We make a small, but important, change to better support all ages.</p><p>Following the notation of SMPL <ref type="bibr">[23]</ref>, the SMPL+A model defines a piece-wise function B = M( ?, ?, ?) that maps 3D pose ?, shape ?, and age offset ? to a 3D body mesh B ? R 6890?3 . The pose parameters, ? ? R 6?22 , correspond to the 6D rotations <ref type="bibr" target="#b49">[50]</ref> of the first 22 body joints of SMPL. The shape parameter ? ? R 10 are the top-10 PCA coefficients of either the SMPL gender-neutral shape space or the SMIL shape space.</p><p>The adult shape space of AGORA produces shape deformations that are too large for an infant body, resulting in a distorted mesh when posed. Therefore, we use SMIL for infants when the age offset ? is above a threshold t ? . When ? &gt;t ? , M( ?, ?, ?) is the SMIL model M I ( ?, ?). When the age offset ? ? t ? , we use the AGORA formulation</p><formula xml:id="formula_0">M( ?, ?, ?) = W (T A ( ?, ?, ?; T , T I ), J( ?), ?, W), T A (?) = (1 ? ?)T + ?T I + B S ( ?) + B P ( ?),<label>(1)</label></formula><p>where W (?) performs linear blend-skinning with weights W to convert the T-posed mesh T A (?) to the target pose ? based on the skeleton joints J(?). The T-posed mesh T A (?) is the weighted sum of the templates (T , T I ), shapedependent deformation B S (?), and pose-dependent deformation B P (?). The age offset ? ? [0, 1] is used to interpolate between the adult SMPL template T and the infant SMIL template T I . The larger the ?, the lower the mesh template height. The 3D joints J of the output mesh are derived via J B, where J ? R K?6890 is a sparse weight matrix that linearly maps the vertices B to the K body joints. To supervise 3D joints J with 2D keypoints, regression methods <ref type="bibr">[15,</ref><ref type="bibr" target="#b33">34]</ref> typically adopt a weak-perspective camera model to project J into the image plane. For better depth reasoning, we employ a perspective camera model to perform projection; see Sup. Mat. for the details of our camera model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Relative Human dataset</head><p>Existing in-the-wild datasets lack groups of overlapping people with annotations. Since acquiring 3D annotations of large crowds is challenging, we exploit more cost-effective weak annotations. We collect a new dataset, named Relative Human (RH), to support in-the-wild monocular human depth reasoning.</p><p>The images are collected from multiple sources to ensure diversity in age, ethnicity, gender, and scene. Most images are collected from the existing 2D pose datasets <ref type="bibr">[20,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b45">46]</ref>. They contain few infants so we collect additional opensource family photos from Pexels <ref type="bibr" target="#b0">[1]</ref> and then annotate their 2D poses. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, we annotate the relative depth relationship between all people in the image. We treat subjects whose depth difference is less than one body-width (? = 0.3m) as people in the same layer. We then classify all people into different depth layers (DLs). Unlike prior work, which labels the ordinal relationships between pairs of joints of individuals <ref type="bibr">[5]</ref>, DLs capture the depth order of multiple people. Additionally, we label people with four age categories: adults, teenagers, children, and babies.</p><p>In total, we collect about 7.6K images with weak annotations of over 24.8K people. More than 21% of the subjects are young people (5.3K), including teenagers, children, and babies. For more analysis, please refer to Sup. Mat. Heatmaps: We build on the body-center heatmap representation from ROMP <ref type="bibr" target="#b33">[34]</ref>. The front-view heatmap of size R 1?H?W is aligned with the pixel space and represents the likelihood of a body being centered at a 2D location using Gaussian kernels. We go beyond ROMP to add a second 2D heatmap of size R 1?D?W that represents an unseen bird'seye-view. This heatmap represents the likelihood of a person being at some point in depth; this map, however, does not represent metric depth. BEV composes and refines these two maps into a 3D heatmap, M 3D C ? R 1?D?H?W , which represents the 3D position of the detected human body centers with 3D Gaussian kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Representations</head><p>Offset maps: The discretized Center Heatmaps coarsely localize the body but we want the network to produce more precise estimates. To improve the granularity of 3D localization, we use additional maps that, at each position, add an estimated offset vector to refine the coarse detection. The front-view Offset map of size R 3?H?W contains 3D offset vectors. The bird's-eye-view Offset map of size R 1?D?W contains 1D offset vectors for depth correction. M 3D O ? R 3?D?H?W corresponds to the 3D Center map and contains a 3D offset vector at each 3D position.</p><p>3D camera anchor maps: Each discretized coordinate in the 3D Center map corresponds to a set of camera param-eters, representing its 3D position in the world. The anchor map serves as a mapping function to transform the coordinates of the 3D Center map to the 3D position in a predefined perspective camera space. To establish a one-to-one mapping from the square Center map to a pyramidal camera space, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we voxelize camera space. Each voxel center corresponds to a discretized 3D coordinate in the Center map. The 3D position vector (x, y, d) of voxel center is the anchor value of 3D camera anchor map. Voxels of equal depth form a depth plane, corresponding to a 2D (x-y) slice of the 3D camera anchor map. During inference, the 3D camera anchor map is sampled at the same coordinate of 3D Center map to obtain the coarse 3D translation of the corresponding detection.</p><p>Mesh feature map: M F ? R 128?H?W contains a 128-D mesh feature vector at each 2D position. These features are aligned with the input 2D image at the pixel level. After a 3D-center-based sampling process, the relevant features are used for the regression of SMPL+A parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">BEV</head><p>To effectively establish the 3D representation, the frontview and the bird's-eye-view must work together to estimate the image position and depth of corresponding subjects. Independently estimating the map of two views in parallel would inevitably cause misalignment, leading to the failure of 3D heatmap-based detection. To connect the two views, we estimate the bird's-eye-view maps conditioned on the front-view maps (i.e. Center and Offset maps). Specifically, to estimate the bird's-eye-view maps, we take the concatenation of the front-view maps and the backbone feature maps as input. The front-view 2D body-centered heatmap is used as a form of robust attention to people in the image, which helps the model focus on exploring depth during bird's-eye view estimation. Then we expand and composite the 2D maps from the front and BEV views to generate the 3D maps. To integrate 2D features from two views and enhance 3D consistency, we further perform 3D convolution on the composited 3D maps for refinement.</p><p>Next, we extract the 3D translation from the estimated 3D maps, M 3D C , M 3D O . High-confidence 3D positions of the 3D Center map are where we sample 3D offset vectors from the 3D Offset map. From the same 3D position in the 3D camera anchor maps ( <ref type="figure" target="#fig_2">Fig. 4)</ref>, we obtain the 3D anchor values, which are positions in camera space of the corresponding 3D center voxel. Adding the 3D offset vectors to the 3D anchor values gives the 3D translation as output.</p><p>Finally, we take the estimated 3D translation (x i , y i , d i ) and Mesh Feature maps M F for parameter regression. We sample the pixel-level mesh feature vectors at (x i , y i ) of M F . Inspired by positional embeddings <ref type="bibr" target="#b36">[37]</ref>, we learn an embedding space to differentiate people at different depths, especially for the overlapping cases. The predicted depth value d i is mapped to a 128-dim encoding vector via an embedding layer. We sum up the depth encodings and the mesh feature vectors to differentiate the features of people at different depths, enabling individual estimates for different subjects. Then we estimate the SMPL+A parameters ( ?, ?, ?) via a fully-connected block. The output body meshes are obtained via M( ?, ?, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss Functions</head><p>Our loss functions are divided into two groups illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>: relative losses (in gold) and the standard mesh losses (in black). BEV is supervised by the weighted sum of all loss items. First, we introduce two relative loss functions for weakly supervised training (WST).</p><p>Piece-wise depth layer loss L depth . L depth is designed to supervise the predicted depth d i , d j of subject i, j by their depth layers r i , r j via</p><formula xml:id="formula_1">? ? ? ? ? (di ? dj) 2 , ri = rj log(1 + e d i ?d j ) ((di ? dj) ? ?(ri ? rj)), ri &lt; rj log(1 + e d j ?d i ) (?(ri ? rj) ? (di ? dj)), ri &gt; rj,<label>(2)</label></formula><p>where is a binarization function that maps positive values to 1 and negative values to 0.</p><p>is used to judge whether the BEV prediction is consistent with the depth relationship of the ground truth DLs. L depth is 0, if the predicted depth difference is within an acceptable range; that is, greater than the product of the DL difference and body-width ?. Otherwise, L depth will encourage the model to achieve it.</p><p>Previous ordinal depth losses <ref type="bibr">[5,</ref><ref type="bibr">30]</ref> encourage the model to enlarge the depth difference between people at different depth layers as much as possible. In contrast, the penalty in L depth is controlled within a range. This helps avoid pushing remote subjects too far away.</p><p>Ambiguity-compatible age loss L age . The classification of age categories (infant, child, teenager, adult) is inherently ambiguous, especially for teenagers and children. Also, while height is correlated with age, one can easily find children who are taller than some adults. Consequently, we formulate an ambiguity-compatible mixed loss L age .</p><p>Rather than supervise height directly, we supervise the ? parameter that controls the blending between the SMIL infant body and the SMPL adult body. To do so, we define ranges of ? values for each age group; i.e. (lowerbound, middle, upper-bound). We do this using the statistical data of heights for each age category that we then relate these to ranges of ? values. Formally, the ranges are (? k l , ? k m , ? k u ), k = 1 ? ? ? 4 where k is the annotated age class number; see Sec. 4 for details.</p><p>BEV is then trained to predict the body shape as well as an ? value for each person. Given the predicted ? and ground truth age class k g , the loss L age is defined as</p><formula xml:id="formula_2">Lage(?) = 0, ? kg l &lt; ? ? ? kg u (? ? ? kg m ) 2 , otherwise.<label>(3)</label></formula><p>Other losses. Following the previous methods <ref type="bibr">[15,</ref><ref type="bibr" target="#b33">34]</ref>, we employ the standard mesh losses to supervise the output maps and regressed SMPL+A parameters. L cm is the focal loss <ref type="bibr" target="#b33">[34]</ref> of the front-view Body Center heatmap. In the same pattern, we further use a 3D focal loss L cm3D to supervise the 3D Center map via converting L cm 's 2D operation to 3D. L pm consists of three parts, L ? , L ? , and L prior . L ? and L ? are L 2 losses of SMPL+A pose ? and shape ? parameters respectively. L prior is the Mixture of Gaussian pose prior <ref type="bibr">[4,</ref><ref type="bibr">23]</ref> on ?. To supervise the 3D body joints J, we use L j3d , which is composed of L mpj and L pmpj . L mpj is the L 2 loss of 3D joints J. To alleviate the domain gap between training datasets, we follow <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> to calculate the L 2 loss L pmpj of the predicted 3D joints after Procrustes alignment with the ground truth. L pj2d is the L 2 loss of the 2D projection of 3D joints J. Lastly, w (.) denotes the corresponding weight of these losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Training details. For basic training, we use two 3D pose datasets (Human3.6M <ref type="bibr">[10]</ref> and MuCo-3DHP <ref type="bibr">[24]</ref>) and four 2D pose datasets (COCO <ref type="bibr">[21]</ref>, MPII <ref type="bibr">[3]</ref>, LSP <ref type="bibr">[12]</ref>, and CrowdPose <ref type="bibr">[20]</ref>). We also use the pseudo SMPL annotations from <ref type="bibr">[14]</ref> and WST on RH. Most samples in RH are collected from 2D pose datasets <ref type="bibr">[20,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b45">46]</ref>. For a fair comparison, we only use the samples that are also used for training in compared methods <ref type="bibr">[11,</ref><ref type="bibr">18,</ref><ref type="bibr">19,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>. To compare with <ref type="bibr">[18,</ref><ref type="bibr">28]</ref>, we further fine-tune our model and ROMP on AGORA. The threshold for the age offset is set to t ? = 0.8. The age offset ranges (? k l , ? k m , ? k u ) are: adults (?0.05, 0, 0.15), teenagers (0.15, 0.3, 0.45), children (0.45, 0.6, 0.75), and infants (0.75, 0.9, 1). See Sup. Mat. for more details.</p><p>Evaluation benchmarks. We evaluate BEV on three multi-person datasets, RH, CMU Panoptic, <ref type="bibr">[13]</ref> and AGORA <ref type="bibr">[28]</ref>, containing 257 child scans and significant person-person occlusion.</p><p>Evaluation matrix. To evaluate the accuracy of depth reasoning, we employ the Percentage of Correct Depth Relations (PCDR 0.2 ), and set the threshold for equal depth to 0.2m. To evaluate the accuracy of projected 2D poses on RH, we also report the mean Percentage of Correct Keypoints (mPCK 0.6 h ), setting the matching threshold to 0.6 times the head length.</p><p>Also, following AGORA <ref type="bibr">[28]</ref>, we evaluate the accuracy of 3D pose/mesh estimation while considering missing detections. To evaluate the detection accuracy, we re-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons to the state-of-the-art methods</head><p>Monocular depth reasoning. We first evaluate BEV on monocular depth reasoning in Tab. 1 using the RH dataset. Results in Tab. 1 are obtained using the official implementations of compared methods. BEV uses the same training samples as <ref type="bibr" target="#b33">[34]</ref> to perform WST. We first compare with the most competitive methods <ref type="bibr">[11,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b47">48]</ref>, which solve depth relations in monocular images. We also compare with ROMP <ref type="bibr" target="#b33">[34]</ref>, for one-stage multi-person mesh recovery. Their 3D translation results are obtained by solving the PnP algorithm (RANSAC <ref type="bibr">[8]</ref>) between their 3D pose and projected 2D pose predictions. As shown in Tab. 1, BEV outperforms all these methods in the accuracy of both depth reasoning and projected 2D poses by a large margin.</p><p>Monocular detection and mesh regression. We also run BEV on AGORA and CMU Panpotic to evaluate the detection and 3D mesh accuracy. We compare with the stateof-the-art (SOTA) multi-stage methods <ref type="bibr">[6,</ref><ref type="bibr">11,</ref><ref type="bibr">17,</ref><ref type="bibr">18,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> and the one-stage ROMP <ref type="bibr" target="#b33">[34]</ref>. Benefiting from the superiority in recall, in Tab. 3, BEV outperforms SOTA methods on detection by 5.2% and 2.2% in terms of F1 score on the kid and full subset, respectively. This is evidence that the 3D representation helps alleviate depth ambiguity in crowded scenes. On the kid subset, BEV significantly outperforms previous methods in terms of mesh reconstruction. Especially, compared with ROMP <ref type="bibr" target="#b33">[34]</ref>, BEV reduces errors over 19.6% and 26.9% in terms of matched MVE and all NMVE on AGORA kids, indicating that BEV effectively reduces the age bias using WST. Also, as shown in Tab. 2, on CMU Panpotic, BEV significantly reduces 3D pose errors by 13.9% compared to multi-person SOTA methods. For qualitative results, see <ref type="figure" target="#fig_4">Fig. 1</ref> and <ref type="figure" target="#fig_8">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Bird's-eye-view representation &amp; BEV w/o WST. To further test the effectiveness of BEV's 3D representation, we train it without performing WST on RH and compare it with SOTA methods on AGORA and RH. On RH in Tab. 1, compared with CRMH <ref type="bibr">[11]</ref>, the depth reasoning accuracy of BEV w/o WST is 4.1% higher (PCDR 0.2 of all). BEV w/o WST outperforms the 2D representation-based network ROMP <ref type="bibr" target="#b33">[34]</ref>. These results point to the effectiveness of our 3D representation for dealing with monocular depth ambiguity. On AGORA, as shown in Tab. 3, BEV w/o WST significantly outperforms ROMP in all detection metrics. Additionally, the strong detection ability of the 3D representation makes BEV w/o WST outperform the SOTA methods <ref type="bibr">[18,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b33">34]</ref> in terms of NMVE and NMJE.</p><p>Weakly supervised training (WST) losses, L depth and L age . Results in Tab. 1 show that performing WST significantly improves the accuracy of depth reasoning, es- <ref type="figure" target="#fig_8">Figure 5</ref>. Qualitative results on AGORA, RH, and Internet images <ref type="bibr" target="#b0">[1]</ref>. Note how children and adults are properly placed in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Relative Human AGORA PCDR   pecially for the young groups. Also, Tab. 1 shows that separately using L depth or L age make BEV produce better depth reasoning than BEV w/o WST, and, when using both terms, BEV performs best. 3D Offset map (OM) and Front-view condition (FVC) for 3D localization. FVC is taking the front-view 2D bodycentered heatmap as a robust attention signal to explore the depth of detected persons during bird's-eye view estimation. Results in Tab. 4 verify that OM and FVC significantly improve the granularity of 3D localization.</p><p>Piece-wise depth layer loss L depth v.s. ordinal depth loss <ref type="bibr" target="#b37">[38]</ref>. Unlike an ordinal depth loss, L depth keeps the penalty within a reasonable range (see Sec. 3.6). As shown in Tab. 5, on AGORA validation set, training with L depth reduces the 3D translation error, especially in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion, Limitations, Ethics, Risks</head><p>In this paper, we introduce BEV, a unified one-stage method for monocular regression and depth reasoning of multiple 3D people. By introducing a novel bird's eye view representation, we enable powerful 3D reasoning that reduces the monocular depth ambiguity. Exploiting the correlation between body height and depth, BEV learns depth reasoning from complex in-the-wild scenes by exploiting relative depth relations and age group classification. We make available an in-the-wild dataset to promote the training and evaluation of monocular depth reasoning in the wild. The ablation studies point to the value of the 3D representation and the fine-grained localization in the network, the importance of our training scheme, and the value of the collected dataset. BEV is a preliminary attempt to explore complex multi-person relationships in the 3D world, and we hope the framework will serve as a simple yet effective foundation for future progress.</p><p>Limitations. While BEV goes beyond current methods to cover more diverse ages, it is not trained to capture diverse weights, gender, ethnicity, etc. BEV also assumes a constant focal length. Our labeling approach, however, suggests that weak labels can produce strong results; i.e. improved metric accuracy. Note that BEV is not trained or designed to deal with large "crowds" (e.g. 100's of people).</p><p>Ethics and data. We collected RH images from a free photo website <ref type="bibr" target="#b0">[1]</ref> under a Creative Commons license that enables sharing. We strove to have a dataset that is diverse in age, ethnicity, and gender. Also, our weak annotations do not contain any personal information and the annotators, themselves, are anonymous and were not studied.</p><p>Potential Negative Societal Impacts. Methods for monocular 3D pose and shape estimation might be used for automated surveillance, tracking, and behavior analysis, which may violate people's privacy. To help prevent this, BEV is released for research only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this material, we provide more implementation details, analysis of the "Relative Human" (RH) dataset, and quantitative/qualitative comparisons to the state-of-the-art methods. Additionally, we present more visual results, like <ref type="figure" target="#fig_4">Fig. 1, to</ref> show the performance of BEV under different situations and to explore its failure modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Implementation Details</head><p>In this section, we introduce the details of our camera representation, network architecture, and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Normalized Camera Representation</head><p>To supervise 3D joints J with 2D poses, existing methods <ref type="bibr">[13,</ref><ref type="bibr">30]</ref> widely adopt a weak-perspective camera model to project J onto the image plane. For better depth reasoning, we employ a perspective camera model to perform this 2D projection.</p><p>In most cases, accurate camera parameters for in-thewild images are unavailable. In this situation, to avoid reliance on the camera parameters of 2D projection, we assume that the input image is captured with a standard camera without radial distortion. Then we can assign static values for the field of view (FOV) and image size W of this standard camera. The focal length f = (f x , f y ) can be defined as W /(2tan(F OV /2)). Given the 3D translation (x i , y i , d i ) of i-th subject and the focal length, the 2D pro-</p><formula xml:id="formula_3">jection ( u i , v i ) of 3D joints ( J x i , J y i , J d i ) is defined as u i = f x ( J x i + x i ) J d i + d i , v i = f y ( J y i + y i ) J d i + d i .<label>(1)</label></formula><p>In cases where the camera parameters are provided, we can convert the 3D translation estimated in our standard camera space to the given one. With K pairs of estimated 3D joints J and their 2D projection (obtained via Eq. 1), we can solve the 3D translation at a specific camera space via a PnP algorithm (e.g. RANSAC <ref type="bibr">[7]</ref>).</p><p>However, in the image, 3D translation is not as intuitive as the person's scale used by weak-perspective methods. For instance, a small 2D scale change in an image may correspond to a large difference in 3D translation in camera space, especially for people who are far away in depth. Therefore, to alleviate this difference, we convert the 3D translation (x i , y i , d i ) to a normalized scale-based format (s i , t y i , t x i ) via a scale factor s i = (d i tan(F OV /2)) ?1 , where t y i = y i s i , t x i = x i s i . The normalized representation is proportional to the person's scale. When FOV=60 ? , the sensitive part s i ? (0, 2) corresponds to d i ? (0.86, +?) in meters, which is more suitable for the network to estimate.</p><p>Additionally, we observe that people in the depth range (1m,10m) show more abundant and stable information in pose, shape, and depth, which deserve more attention. Additionally, most of our training samples are within this depth range. As we introduced in the main paper, 3D camera anchor maps define the way we voxelize the 3D camera space. Therefore, we adjust the occupancy ratio of different depths in the channel number of 3D camera anchor maps. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we first split the camera space into 4 regions in depth and then evenly put the different number (shown in the table) of 3D camera anchor maps inside each region. For instance, we put 25/32 3D camera anchor maps inside the depth range (1m,10m); this gives more attention to this critical depth range. Each anchor map contains the normalized camera values (s i , t y i , t x i ) at the corresponding position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network Architecture</head><p>We develop a bird's-eye-view-based coarse-to-fine localization pipeline to estimate the 3D translation of all people in the scene in one shot. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we present the network architecture of estimating five 2D maps and two 3D maps, which are used to generate the final results as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> of the main paper. The input size W is (512, 512). Following ROMP <ref type="bibr">[30]</ref>, we adopt a multi-head architecture and use HRNet-32 <ref type="bibr">[3]</ref> as backbone. With backbone feature maps of size R 32?H?W , we employ three head branches to estimate four front-/bird's-eye-view 2D maps and a Mesh Feature map.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, our key design is to convert the front-view features to a bird's eye view via explicit operations including height-wise suppression and depth-wise exploration. As shown in the middle branch of <ref type="figure" target="#fig_1">Fig. 3</ref>, we first explore the depth information of backbone features via a Bottleneck block. And then we concatenate the explored depth features and front-view 2D maps as input to the BVH branch. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we compress the 2D feature maps in height to obtain 1D feature vectors. In the BVH branch ( <ref type="figure" target="#fig_1">Fig. 3)</ref>, we employ six 1D convolution blocks to explicitly explore features in depth. Two bird's-eye-view maps are of size R 1?D?W .</p><p>Next, we compose the front-view and bird's-eye-view maps to generate 3D maps. We extend the front-view maps with an additional depth dimension and repeat D times. We also extend the bird's-eye-view maps with an additional height dimension and repeat H times. To obtain the 3D Center map, we multiply the bird's-eye-view Body Center heatmap to the front-view one and refine it with a 3D refiner ( <ref type="figure" target="#fig_1">Fig. 3)</ref>. Then we add the bird's-eye-view offset map to the last channel of the front-view one to refine the depth. To obtain the 3D Offset map, we further use a 3D refiner to refine the composed 3D maps, which improves the consistency between features of two views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Datasets</head><p>In this section, we introduce the datasets we used during training and evaluation.</p><p>AGORA <ref type="bibr">[27]</ref> is a synthetic dataset with accurate annota-  tions of body meshes and 3D translations, with 4,240 highrealism textured scans in diverse poses and clothes. Importantly, it contains 257 child scans. It contains 14K training and 3K test images. Each image has 5-15 people with frequent occlusions. AGORA-PC <ref type="bibr">[27]</ref> is a high occlusion subset of the AGORA validation set. Each image has over 70% occlusion. We use it to evaluate the performance under severe occlusion. Note that there are no child samples in the validation set. Human3.6M <ref type="bibr">[8]</ref> is a single-person 3D pose dataset. It contains videos of 9 professional actors performing activities in 17 scenarios. It provides 3D pose annotations for each frame. We sample every 5 frames to reduce redundancy. We use its training set for training.</p><p>MuCo-3DHP <ref type="bibr">[23]</ref> is a synthetic multi-person 3D pose dataset. It is built on the single-person 3D pose dataset, MPI-INF-3DHP <ref type="bibr">[23]</ref>. They use segmentation annotations to blend multiple single-person images into one. For a fair comparison with 3DMPPE <ref type="bibr">[25]</ref>, we use the same synthetic version for training.</p><p>Other 2D pose datasets. For better generalization, we also use four 2D pose datasets for training, including COCO <ref type="bibr">[21]</ref>, MPII <ref type="bibr">[2]</ref>, LSP <ref type="bibr">[10]</ref>, and CrowdPose <ref type="bibr">[18]</ref>. Besides, we also use the pseudo-3D annotations <ref type="bibr">[12]</ref> for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training Details</head><p>The size of output maps are H = W = 128, and D = 64. The threshold for the age offset is set to t ? = 0.8. The FOV is set to 60 ? . The loss weights are w mpj = 200, w pmpj = 360, w pj2d = 400, w ? = 80, w ? = 60, w prior = 1.6, w cm = 100, w cm3d = 1000, w age = 4000, and w depth = 400. We train BEV on a server with four Tesla V100 GPUs. The batch size is 64. The learning rate is 5e ?5 . The confidence threshold of the Body Center heatmap is 0.12.</p><p>Additionally, although we strive to alleviate the age bias in training samples, the age bias in existing 3D pose datasets is severe, and we have to use them to obtain good 3D pose estimation. To handle the imbalanced distribution of the training sample space, we balance the sampling ratio of different datasets and evenly select the training samples from different age groups on RH. The sampling ratios of different datasets are 16% AGORA, 16% MuCo-3DHP, 16% RH, 18% Human3.6M, 14% COCO, 8% CrowdPose, 6% MPII, and 6% LSP.</p><p>Also, we adopt a two-step training strategy. We first learn monocular 3D pose and shape estimation for 120 epochs on basic training datasets. Then we add the weak annotations of RH to training samples and train for 120 epochs. If we need to fine-tune on AGORA, we add AGORA to the training sequence and train for 80 epochs. In this process, the validation set of the RH is used to select checkpoints with good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Processing High-resolution Images</head><p>As a one-stage method, BEV takes an image of constant size as input. However, to process the high-resolution images, directly resizing them to a constant size would sacrifice the performance. Therefore, we develop a sliding window-based pipeline to achieve promising results on high-resolution images, as shown in <ref type="figure" target="#fig_4">Fig. 1</ref> of the main paper. In detail, we evenly split the image into multiple grids and then apply BEV on each grid. This process is similar to the sliding window operation of 2D convolution. At each grid, we only take the result whose body center falls in the center area of the grid. Then we perform non-maximum suppression on the edge between grids to get rid of redundant predictions. In this process, overlapping predictions with lower center confidence values will be deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Relative Human Dataset</head><p>In this section, we provide more detailed analyses of our Relative Human dataset.</p><p>In total, we collect about 7,689 images with weak annotations of 24,814 people. We split them into three groups (5218, 635, 1836) for training, validation, and test respectively. Among these images, about 1,000 images are collected from a free photo website <ref type="bibr" target="#b0">[1]</ref> and we annotate the 2D poses defined as <ref type="figure" target="#fig_8">Fig. 5</ref>. Note that compared with LSP's 14 keypoints, we add keypoints on the face and feet to represent their orientations. The remaining images are selected from existing 2D pose datasets <ref type="bibr">[18,</ref><ref type="bibr">21,</ref><ref type="bibr">33]</ref>. We correct some erroneous 2D poses from the existing 2D pose dataset and add the missing detections. Note that a large number of images in CrodPose <ref type="bibr">[18]</ref> and OCHuman <ref type="bibr">[33]</ref> are selected from COCO <ref type="bibr">[21]</ref> and MPII <ref type="bibr">[2]</ref>, which are also used as training samples by our compared methods <ref type="bibr">[9,</ref><ref type="bibr">15,</ref><ref type="bibr">16,</ref><ref type="bibr">30]</ref>. Therefore, we use these common images for training.</p><p>We classify all people in the image into four age groups, baby, child, teenager, and adult according to the following age ranges: baby (0-3), kid <ref type="bibr">(3)</ref><ref type="bibr">(4)</ref><ref type="bibr">(5)</ref><ref type="bibr">(6)</ref><ref type="bibr">(7)</ref><ref type="bibr">(8)</ref>, teenager <ref type="bibr">(8)</ref><ref type="bibr">(9)</ref><ref type="bibr">(10)</ref><ref type="bibr">(11)</ref><ref type="bibr">(12)</ref><ref type="bibr">(13)</ref><ref type="bibr">(14)</ref><ref type="bibr">(15)</ref><ref type="bibr">(16)</ref>, and adult (16+). As shown in Tab. 1, we provide the number of subjects in the four age groups and their proportions. Compared with the existing multi-person 3D pose datasets <ref type="bibr">[23,</ref><ref type="bibr">27,</ref><ref type="bibr">31]</ref>, RH contains richer subjects and more occlusion cases. Therefore, RH is more general and suitable for evaluating depth reasoning in the wild.</p><p>The consistency of weak annotations. During the collection of weak annotations, we observe that people's judgments for such weak labels vary greatly. It is hard to obtain consistent weak labels through online platforms (e.g. AMT). Therefore, offline, we organized a group of labelers and trained them with unified standards. To test how well they learn the standards, we prepare some pre-labeled data as test samples. Ones who pass the test after training were employed for official labeling. In addition, the annotations  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Why not estimate the 3D heatmap directly? The main challenge is the lack of sufficient multi-person data with accurate 3D translation annotations for supervision, especially for in-the-wild cases. Due to the data lack problem, directly learning 3D heatmap performs poorly. It is hard to effectively supervise multi-person 3D heatmaps with 2D annotations. In contrast, our separable representation disentangles the 3D heatmap into the front-view and the bird's-eye-view maps. In this way, our model can learn robust front-view localization from abundant 2D in-the-wild datasets. With robust front-view attention, the model can focus on learning depth reasoning from weak annotations in RH with the proposed WST.</p><p>Heatmap refinement and decomposition. Following previous methods <ref type="bibr">[30]</ref>, we also adopt the powerful heatmap representation for detection. While its rough granularity limits its effectiveness in fine localization. Some previous methods explore the refinement and decomposition of the heatmap to alleviate this problem. PifPaf <ref type="bibr">[17]</ref> estimates offset maps to refine the coarse 2D pose coordinates parsed from the heatmap. VNect <ref type="bibr">[24]</ref>    <ref type="table" target="#tab_1">Table 3</ref>. Comparison to existing SOTA methods on AGORA full test set. Results are obtained from the AGORA leaderboard. is finetuning on the AGORA training set or synthetic data <ref type="bibr">[15]</ref> generated in the same way as AGORA. ? means the optimization-based method while the rest are learning-based methods. ? means the paper is under review. containing x/y/z coordinates of the 3D pose at each position. Luvizon et al. <ref type="bibr">[22]</ref> employ soft-argmax to decompose 2D/3D heatmap into 1D for separate supervision, while it does not deal with multiple overlapping people. Different from previous solutions, we propose a novel bird's-eyeview-based representation for multi-person 3D localization. As we introduced above, it disentangles the depth-wise information into an individual map for easier learning. We also estimate a 3D offset map to improve the granularity of 3D localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Quantitative and Qualitative Results</head><p>In this section, we first show more comparisons to SOTA methods on AGORA and then provide more qualitative results on Internet images, CMU Panpotic <ref type="bibr">[11]</ref>, AGORA <ref type="bibr">[27]</ref>, and RH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Comparisons</head><p>In Tab. 2 and 3, we show the results of existing SOTA methods on "AGORA kids" and the full test set respectively. Results in Tab. 2 show that BEV outperforms all previous methods by a large margin in terms of child mesh <ref type="figure">Figure 6</ref>. Qualitative results on CMU Panoptic <ref type="bibr">[11]</ref> and AGORA <ref type="bibr">[27]</ref> datasets. <ref type="figure">Figure 7</ref>. Qualitative comparisons to SOTA methods, ROMP <ref type="bibr">[30]</ref> and CRMH <ref type="bibr">[9]</ref>, on RH test set.</p><p>reconstruction. It demonstrates that learning weak annotations via the proposed weakly-supervised training (WST) helps to alleviate the age bias. Multi-stage methods, like Pose2Pose <ref type="bibr">[26]</ref>, benefit from taking high-resolution person crops as input, which helps process the small-scale subjects in AGORA. Besides, as a sanity check, we also compare with SOTAs on 3DPW and MuPoTS datasets. While not tuned for uncrowded scenes, BEV is on par with the previous methods on MuPoTs (Tab. 4) and 3DPW (Tab. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>All? Matched?  <ref type="table">Table 6</ref>. 3D mesh/pose error on AGORA-PC, the high occlusion (over 70%) subset of the AGORA validation set (no kids).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>To analyse the performance gain of different designs, we perform more ablation studies on AGORA?PC, a high occlusion (over 70%) subset of the AGORA validation set (no kids). This has ground truth 3D annotations for detailed evaluation while the test set does not. BEV uses the same training samples as <ref type="bibr">[30]</ref>. Comparing BEV and BEV w/o WST in Tab. 6 also shows that our gains in high occlusion situations come from the 3D representation.</p><p>Besides, we also evaluate the effectiveness of depth encoding (DC) for 3D mesh parameter regression. Depth encoding is developed to transfer people at different depths to individual feature spaces. Tab. 6 shows that adding the depth encoding reduces mesh reconstruction error under high occlusion (over 70%). It demonstrates that achieving depth-aware mesh regression via adding depth encoding is beneficial to alleviating depth ambiguity and improving the stability under occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results</head><p>In <ref type="figure">Fig. 6</ref>, we present more qualitative results on CMU Panoptic and AGORA. In <ref type="figure" target="#fig_4">Fig. 1, 7, 8</ref>, we show the results under various crowded scenarios, including queuing, standing side by side, and mixed scenarios. Compared with ROMP <ref type="bibr">[30]</ref> and CRMH <ref type="bibr">[9]</ref>, BEV performs much better in detection, depth reasoning, and robustness to occlusion, especially in cases containing children. These results demonstrate the superiority of our 3D representation, WST, and perspective camera model. However, we also observe some limitations of BEV from failure cases in <ref type="figure">Fig. 9</ref>. Without modeling the contact between multiple people, BEV may miss obvious contact and cannot avoid mesh intersections. Besides, BEV is unable to handle occlusions with few visible parts and dense small-scale subjects in crowds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview. Given an RGB image, BEV first estimates the 3D translation of all people in the scene via compositing the frontview and the bird's-eye-view predictions. Then guided by the 3D translation, we sample the mesh feature of each person to regress their age-aware SMPL+A parameters. See Sec. 3.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example images from the Relative Human (RH) dataset with weak annotations: depth layers (DLs) and age group classification. Examples are a) adults at different DLs, and b) people of different age groups at the same DL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Pre-defined 3D camera anchor maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>gives an overview of BEV's representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 .</head><label>1</label><figDesc>More qualitative results on Internet images<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Pre-defined 3D camera anchor maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Operations to convert the front-view features to a bird's eye view (shown in 3D camera space represented by 3D camera anchor maps).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>The 2D skeleton definition. are double-checked by professional testers and the author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>[25] 39.33 51.42 60.91 57.95 57.47 -CRMH [11] 34.74 48.37 59.11 55.47 54.83 0.781 SMAP [48] 31.58 40.29 47.35 41.65 41.55 -ROMP [34] 30.08 48.41 51.12 55.34 54.81 0.866 BEV w/o WST 34.27 50.81 54.34 57.43 57.17 0.850 BEV w/o L depth 43.61 51.55 50.88 57.27 55.97 Accuracy of relative depth relations (PCDR 0.2 ) and projected 2D poses (mPCK 0.6 h ) on RH. ? uses the ground truth bounding boxes. Method Haggl. Mafia Ultim. Pizza Mean Zanfir et. al. [43] 141.4 152.3 145.0 162.5 150.3 MSC [42] 140.0 165.9 150.7 156.0 153.4 CRMH [11] 129.6 133.5 153.0 156.7 143.2 ROMP [34] 110.8 122.8 141.6 137.6 128.2 3DCrowdNet [6] 109.6 135.9 129.8 135.6 127.3 Comparisons to the state-of-the-art methods on CMU Panoptic in MPJPE. Results are obtained from the original papers.</figDesc><table><row><cell>Method</cell><cell>Baby</cell><cell cols="2">PCDR 0.2 (%)? Kid Teen Adult</cell><cell>All</cell><cell>mPCK 0.6 h ?</cell><cell></cell></row><row><cell cols="6">3DMPPE  ? 0.794 BEV w/o Lage 49.09 56.55 60.92 62.47 61.47 BEV 60.77 67.09 66.07 69.71 68.27 0.884 0.810</cell><cell>BEV</cell><cell>90.7</cell><cell>103.7 113.1 125.2 109.5</cell></row><row><cell>Method</cell><cell cols="7">Kid subset Matched? F1 score Precision Recall MVE MPJPE NMVE NMJE F1 score Precision Recall MVE MPJPE NMVE NMJE Full set Detection? All? Detection? Matched? All?</cell></row><row><cell>PARE [17] SPIN [28] SPEC [18] ROMP [34] BEV w/o WST BEV</cell><cell>0.55 0.31 0.52 0.50 0.58 0.55</cell><cell>0.44 0.21 0.40 0.37 0.44 0.41</cell><cell cols="3">0.74 186.4 193.9 338.9 352.5 0.60 186.7 191.7 602.3 618.4 0.73 163.2 171.0 313.8 328.8 0.80 156.6 159.8 313.2 319.6 0.86 146.0 148.3 251.7 255.7 0.85 125.9 129.1 228.9 234.7</cell><cell>0.84 0.77 0.84 0.91 0.93 0.93</cell><cell>0.96 0.91 0.96 0.95 0.96 0.96</cell><cell>0.75 140.9 146.2 167.7 174.0 0.67 148.9 153.4 193.4 199.2 0.74 106.5 112.3 126.8 133.7 0.88 103.4 108.1 113.6 118.8 0.90 105.6 109.7 113.5 118.0 0.90 100.7 105.3 108.3 113.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Comparison of SOTA methods on AGORA test set. All methods are fine-tuned on the AGORA training set or synthetic data [18] generated in the same way as AGORA. We fine-tune ROMP [34] using the public implementation; results from the AGORA leaderboard.port Precision, Recall, and F1 score. For matched detec- tions, we report the Mean Per Joint Position Error (MPJPE) and Mean Vertex Error (MVE). To punish misses and false alarms in detection, we normalize the MPJPE and MVE by F1 score to get Normalized Mean Joint Error (NMJE) and Normalized Mean Vertex Error (NMVE).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Dist.?</cell><cell>X?</cell><cell>Y?</cell><cell>Depth?</cell></row><row><cell>Ordinal loss [38] Piece-wise L depth (ours)</cell><cell>0.608 0.518</cell><cell>0.153 0.128</cell><cell>0.184 0.166</cell><cell>0.509 0.423</cell></row></table><note>Ablation study of front-view condition (FVC) and 3D Offset map (OM) on RH and AGORA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>3D translation error on AGORA validation set with dif- ferent depth losses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>Comparison to existing SOTA methods on the "AGORA kids" test set. Results are obtained from the AGORA leaderboard. is fine-tuning on the AGORA training set or synthetic data[15]  generated in the same way as AGORA. ? means the optimization-based method while the rest are learning-based methods. ? means the paper is under review.</figDesc><table><row><cell cols="2">Input</cell><cell>Method</cell><cell>F1 score?</cell><cell cols="2">Precision? Recall?</cell><cell cols="4">MVE? MPJPE? NMVE? NMJE?</cell></row><row><cell>Multi-stage</cell><cell>Person crops from 3840x2160</cell><cell>HMR [13] SMPLify-X  ? [28] EFT [12] SPIN [16] ExPose [5] Frankmocap [29] PyMAF [32] PIXIE [6] SPIN [27] SPEC [15] PARE [14] Pose2Pose  ? [26]</cell><cell>0.80 0.71 0.69 0.78 0.82 0.80 0.84 0.82 0.77 0.84 0.84 0.94</cell><cell>0.93 0.86 0.97 0.91 0.96 0.93 0.86 0.95 0.91 0.96 0.96 0.94</cell><cell>0.70 0.60 0.54 0.69 0.71 0.71 0.82 0.73 0.67 0.74 0.75 0.93</cell><cell>173.6 187.0 159.0 168.7 151.5 204.2 192.0 142.2 168.7 106.5 140.9 84.8</cell><cell>180.5 182.1 165.4 175.1 150.4 203.7 203.2 140.3 175.1 112.3 146.2 89.8</cell><cell>217.0 263.3 196.3 216.3 184.8 510.5 711.1 173.4 216.3 126.8 167.7 90.2</cell><cell>226.0 256.5 203.6 223.1 183.4 509.2 752.6 171.1 223.1 133.7 174.0 95.5</cell></row><row><cell>One-stage</cell><cell>512x512</cell><cell>ROMP [30] BEV w/o WST ROMP [30] BEV w/o WST BEV</cell><cell>0.69 0.75 0.91 0.93 0.93</cell><cell>0.97 0.97 0.95 0.96 0.96</cell><cell>0.54 0.61 0.88 0.90 0.90</cell><cell>161.4 164.2 103.4 105.6 100.7</cell><cell>168.1 169.1 108.1 109.7 105.3</cell><cell>233.9 218.9 113.6 113.5 108.3</cell><cell>242.3 225.5 118.8 118.0 113.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Comparisons to the SOTAs on MuPoTS. Comparisons to the SOTAs on 3DPW test set.</figDesc><table><row><cell>CRMH [9] ROMP [30] 3DCrowdNet [4] 72.7 69.1 69.9 BEV 70.2</cell><cell>72.2 74.6 73.3 75.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pexels</surname></persName>
		</author>
		<ptr target="https://www.pexels.com" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guan Pang, and Tal Hassner. img2pose: Face alignment and detection via 6dof, face pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Vitor Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="7617" to="7627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to estimate robust 3d human mesh from in-the-wild crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonkyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to regress bodies from images using differentiable semantic rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Sai Kumar Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="11250" to="11259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning an infant body model from rgb-d data for accurate full body motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolas</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bodensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mijna</forename><surname>Tacke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hadders-Algra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="792" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Panoptic Studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PARE: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11127" to="11137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SPEC: Seeing people in the wild with an estimated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multiperson pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pose2Pose: 3d positional pose-guided 3d rotational pose prediction for expressive 3d human pose and mesh estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On self-contact and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="9990" to="9999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TexturePose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HuMoR: 3d human motion model for robust pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11488" to="11499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3d human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5340" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Body size and depth disambiguation in multi-person reconstruction from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ugrinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HMOR: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="242" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ICON: Implicit Clothed humans Obtained from Normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human-aware object placement for visual environment reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GLAMR: Global occlusion-aware human mesh recovery with dynamic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7054" to="7063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyMAF: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11446" to="11456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pose2Seg: Detection free human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lightweight multi-person total motion capture using sparse multi-view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5560" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SMAP: Single-shot multiperson absolute 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="550" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for 3d keypoint estimation via view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jingwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jimei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to estimate robust 3d human mesh from in-the-wild crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonkyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="20" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Collaborative regression of expressive bodies using moderation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="792" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Panoptic Studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PARE: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11127" to="11137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">SPEC: Seeing people in the wild with an estimated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11035" to="11045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3383" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TOG</publisher>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multiperson pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Pose2Pose: 3d positional pose-guided 3d rotational pose prediction for expressive 3d human pose and mesh estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">AGORA: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13468" to="13478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1749" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11179" to="11188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">PyMAF: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11446" to="11456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Pose2Seg: Detection free human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
