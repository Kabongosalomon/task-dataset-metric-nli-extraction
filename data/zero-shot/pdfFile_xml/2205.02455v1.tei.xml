<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COGMEN: COntextualized GNN based Multimodal Emotion recognitioN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Joshi</surname></persName>
							<email>ajoshi@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur (IIT-K)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwani</forename><surname>Bhat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur (IIT-K)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Jain</surname></persName>
							<email>aayushj@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur (IIT-K)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atin</forename><forename type="middle">Vikram</forename><surname>Singh</surname></persName>
							<email>atinvs@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur (IIT-K)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
							<email>ashutoshm@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kanpur (IIT-K)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COGMEN: COntextualized GNN based Multimodal Emotion recognitioN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multimodal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-theart (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotions are intrinsic to humans and guide their behavior and are indicative of the underlying thought process <ref type="bibr">(Minsky, 2007)</ref>. Consequently, understanding and recognizing emotions is vital for developing AI technologies (e.g., personal digital assistants) that interact directly with humans. During a conversation between a number of people, there is a constant ebb and flow of emotions experienced and expressed by each person. The task of multimodal emotion recognition addresses the problem of monitoring the emotions expressed (via various modalities, e.g., video (face), audio (speech)) by individuals in different settings such as conversations.</p><p>Emotions are physiological, behavioral, and communicative reactions to cognitively processed stimuli <ref type="bibr">(Planalp et al., 2018)</ref>. Emotions are often a result of internal physiological changes, and  <ref type="figure">Figure 1</ref>: An example conversation between two speakers, with corresponding emotions evoked for each utterance.</p><p>these physiological reactions may not be noticeable by others and are therefore intra-personal. For example, in a conversational setting, an emotion may be a communicative reaction that has its origin in a sentence spoken by another person, acting as a stimulus. The emotional states expressed in utterances correlate with the context directly; for example, if the underlying context is about a happy topic like celebrating a festival or description of a vacation, there will be more positive emotions like joy and surprise. Consider the example shown in <ref type="figure">Figure 1</ref>, where the context depicts an exciting conversation. Speaker-1 being excited about his admission affects the flow of emotions in the entire context. The emotion states of Speaker-2 show the dependency on Speaker-1 in U 2 , U 4 and U 6 , and maintains intra-personal state depicted in U 8 and U 10 by being curious about the responses of Speaker-1. The example conversation portrays the effect of global information as well as inter and intra dependency of speakers on the emotional states of the utterances. Moreover, emotions are a multimodal phenomenon; a person takes cues from different modalities (e.g., audio, video) to infer the emotions of others, since, very often, the in-formation in different modalities complement each other. In this paper, we leverage these intuitions and propose COGMEN: COntextualized Graph neural network based Multimodal Emotion recog-nitioN architecture that addresses both, the effects of context on the utterances and inter and intra dependency for predicting the per-utterance emotion of each speaker during the conversation. There has been a lot of work on unimodal (using text only) prediction, but our focus is on multimodal emotion prediction. As is done in literature on multimodal emotion prediction, we do not focus on comparison with unimodal models. As shown via experiments and ablation studies, our model leverages both the sources (i.e., local and global) of information to give state-of-the-art (SOTA) results on the multimodal emotion recognition datasets IEMOCAP and MOSEI. In a nutshell, we make the following contributions in this paper:</p><p>? We propose a Contextualized Graph Neural Network (GNN) based Multimodal Emotion Recognition architecture for predicting per utterance per speaker emotion in a conversation. Our model leverages both local and global information in a conversation. We use GraphTransformers <ref type="bibr">(Shi et al., 2021)</ref> for modeling speaker relations in multimodal emotion recognition systems. ? Our model gives SOTA results on the multimodal Emotion recognition datasets of IEMO-CAP and MOSEI. ? We perform a thorough analysis of the model and its different components to show the importance of local and global information along with the importance of the GNN component. We release the code for models and experiments: https://github. com/Exploration-Lab/COGMEN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Emotion recognition is an actively researched problem in NLP <ref type="bibr">(Sharma and Dhall, 2021;</ref><ref type="bibr">Sebe et al., 2005)</ref>. The broad applications ranging from emotion understanding systems, opinion mining from a corpus to emotion generation have attracted active research interest in recent years <ref type="bibr">(Dhuheir et al., 2021;</ref><ref type="bibr">Franzen et al., 2021;</ref><ref type="bibr">Vinola and Vimaladevi, 2015;</ref><ref type="bibr">Ko?akowska et al., 2014;</ref><ref type="bibr">Colombo et al., 2019;</ref><ref type="bibr">Janghorbani et al., 2019;</ref><ref type="bibr">Goswamy et al., 2020;</ref><ref type="bibr">Singh et al., 2021a;</ref><ref type="bibr">Agarwal et al., 2021;</ref><ref type="bibr">Singh et al., 2021b)</ref>. Availability of bench-mark multimodal datasets, such as CMU-MOSEI <ref type="bibr" target="#b6">(Zadeh et al., 2018b), and</ref><ref type="bibr">IEMOCAP (Busso et al., 2008)</ref>, have accelerated the progress in the area. Broadly speaking, most of the existing work in this area can be categorized mainly into two areas: unimodal approaches and multimodal approaches. Unimodal approaches tend to consider the text as a prominent mode of communication and solve the emotion recognition task using only text modality. In contrast, multimodal approaches are more naturalistic and consider multiple modalities (au-dio+video+text) and fuse them to recognize emotions. In this paper, we propose a multimodal approach to emotion recognition. Nevertheless, we briefly outline some of the prominent unimodal approaches as some of the techniques are applicable to our setting.</p><p>Unimodal Approaches: COSMIC <ref type="bibr" target="#b3">(Yu et al., 2019)</ref> performs text only emotion classification problem by leveraging commonsense knowledge. DialogXL <ref type="bibr">(Shen et al., 2021a)</ref> uses XLnet <ref type="bibr" target="#b2">(Yang et al., 2019)</ref> as architecture in dialogue feature extraction. CESTa <ref type="bibr" target="#b0">(Wang et al., 2020)</ref> captures the emotional consistency in the utterances using Conditional Random Fields <ref type="bibr">(Lafferty et al., 2001)</ref> for boosting the performance of emotion classification. Other popular approaches parallel to our work use graph-based neural networks as their baseline and solve the context propagation issues in <ref type="bibr">RNN-based architectures, including DialogueGCN (Ghosal et al., 2019)</ref>, <ref type="bibr">RGAT (Ishiwatari et al., 2020)</ref>, ConGCN <ref type="bibr" target="#b7">(Zhang et al., 2019)</ref>, and SumAgg-Gin <ref type="bibr">(Sheng et al., 2020)</ref>. Some of the recent approaches like DAG-ERC (Shen et al., 2021b) combine the strengths of conventional graph-based neural models and recurrence-based neural models.</p><p>Multimodal Approaches: Due to the high correlation between emotion and facial cues <ref type="bibr">(Ekman, 1993)</ref>, fusing modalities to improve emotion recognition has drawn considerable interest <ref type="bibr">(Sebe et al., 2005)</ref>. Some of the initial approaches include <ref type="bibr">Datcu and Rothkrantz (2014)</ref>, who fused acoustic information with visual cues for emotion recognition. <ref type="bibr" target="#b1">Wollmer et al. (2010)</ref> use contextual information for emotion recognition in a multimodal setting. In the past decade, the growth of deep learning has motivated a wide range of approaches in multimodal settings. The Memory Fusion network (MFN) <ref type="bibr" target="#b5">(Zadeh et al., 2018a)</ref> proposes synchronizing multimodal sequences using multi-view gated memory storing intra-view and cross-view interac-tions through time. <ref type="bibr">Graph-MFN (Bagher Zadeh et al., 2018)</ref> extends the idea of MFN and introduces Dynamic Fusion Graph <ref type="bibr">(DFG)</ref>, which learns to model the n-modal interactions and alter its structure dynamically to choose a fusion graph based on the importance of each n-modal dynamics during inference. Conversational memory network (CMN) <ref type="bibr">(Hazarika et al., 2018b</ref>) leverages contextual information from the conversation history and uses gated recurrent units to model past utterances of each speaker into memories. Tensor fusion Network (TFN) <ref type="bibr" target="#b4">(Zadeh et al., 2017)</ref> uses an outer product of the modalities. Other popular approaches include <ref type="bibr">DialogueRNN (Majumder et al., 2019</ref>) that proposes an attention mechanism over the different utterances and models emotional dynamics by its party GRU and global GRU. B2+B4 (Kumar and Vepa, 2020), use a conditional gating mechanism to learn cross-modal information. bc-LSTM <ref type="bibr" target="#b4">(Poria et al., 2017)</ref> proposes an LSTM-based model that captures contextual information from the surrounding utterances. Multilogue-Net (Shenoy and Sardana, 2020) proposes a solution based on a context-aware RNN and uses pairwise attention as a fusion mechanism for all three modalities <ref type="bibr">(audio, video, and text)</ref>. <ref type="bibr">Recently, Delbrouck et al. (2020)</ref> proposed TBJE, a transformer-based architecture with modular co-attention <ref type="bibr" target="#b3">(Yu et al., 2019)</ref> to encode multiple modalities jointly. <ref type="bibr">CONSK-GCN (Fu et al., 2021)</ref> uses graph convolutional network (GCN) with knowledge graphs. <ref type="bibr">Lian et al. (2020)</ref> use GNN based architecture for Emotion Recognition using text and speech modalities. Af-CAN <ref type="bibr">(Wang et al., 2021a)</ref> proposes RNN based on contextual attention for modeling the transaction and dependence between speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In a conversation involving different speakers, there is a continuous ebb and flow in the emotions of each of the speakers, usually triggered by the context and reactions of other speakers. Inspired by this intuition, we propose a multimodal emotion prediction model that leverages contextual information, inter-speaker and intra-speaker relations in a conversation.</p><p>In our model, we leverage both the context of dialogue and the effect of nearby utterances. We model these two sources of information via two means: 1) Global Information: How to capture the impact of underlying context on the emotional state of an utterance? 2) Local information: How to establish relations between the nearby utterances that preserve both inter-speaker and intra-speaker dependence on utterances in a dialogue? Global Information: We want to have a unified model that can capture the underlying context and handle its effect on each utterance present in the dialogue. A transformer encoder <ref type="bibr">(Vaswani et al., 2017)</ref> architecture is a suitable choice for this goal. Instead of following the conventional sequential encoding by adding positional encodings to the input, in our approach, a simple transformer encoder without any positional encodings leverages the entire context to generate distributed representations (features) efficiently corresponding to each utterance. The transformer facilitates the flow of information from all utterances when predicting emotion for a particular utterance. Local Information: The emotion expressed in an utterance is often triggered by the information in neighboring utterances. We establish relations between the nearby utterances in a way that is capable of capturing both inter-speaker and intra-speaker effects of stimulus over the emotion state of an utterance. Our approach comes close to <ref type="bibr">DialogueGCN (Ghosal et al., 2019)</ref>, and we define a graph where each utterance is a node, and directed edges represent various relations. We define relations (directed edges) between nodes R ij = u i ? u j , where the direction of the arrow represents the spoken order of utterances. We categorize the directed relations into two types, for self-dependent relations between the utterances spoken by the same speaker R intra , and interrelations between the utterances spoken by different speakers R inter . We propose to use Relational <ref type="bibr">GCN (Schlichtkrull et al., 2018)</ref> followed by a GraphTransformer <ref type="bibr">(Shi et al., 2021)</ref> to capture dependency defined by the relations. <ref type="figure" target="#fig_0">Figure 2</ref> shows the detailed architecture. The input utterances go as input to the Context Extractor module, which is responsible for capturing the global context. The features extracted for each utterance by the context extractor form a graph based on interactions between the speakers. The graph goes as input to a Relational GCN, followed by GraphTransformer, which uses the formed graph to capture the inter and intra-relations between the utterances. Finally, two linear layers acting as an emotion classifier use the features obtained for all  the utterances to predict the corresponding emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>Context Extractor: Context Extractor takes concatenated features of multiple modalities (audio, video, text) as input for each dialogue utterance (u i ; i = 1, . . . , n) and captures the context using a transformer encoder. The feature vector for an utterance u i with the input features corresponding to available modalities, audio (u</p><formula xml:id="formula_0">(a) i ? R da ), text (u (t) i ? R dt ) and video (u (v) i ? R dv ) is: x (atv) i = [u (a) i ? u (t) i ? u (v) i ] ? R d where d = d a + d t + d v .</formula><p>The combined features matrix for all utterances in a dialogue is given by:</p><formula xml:id="formula_1">X = x (atv) = [x (atv) 1 , x (atv) 2 . . . , x (atv) n ] T</formula><p>We define a Query, a Key, and a Value vector for encoding the input features X ? R n?d as follows:</p><formula xml:id="formula_2">Q (h) = XW h,q , K (h) = XW h,k , V (h) = XW h,v , where, W h,q , W h,k , W h,v ? R d?k</formula><p>The attention mechanism captures the interaction between the Key and Query vectors to output an attention map ? (h) , where ? j denotes the softmax function over the row vectors indexed by j:</p><formula xml:id="formula_3">? (h) = ? j Q (h) (K (h) ) T ? k</formula><p>where ? (h) ? R n?n represents the attention weights for a single attention head (h). The obtained attention map is used to compute a weighted sum of the values for each utterance:</p><formula xml:id="formula_4">head (h) = ? (h) (V (h) ) ? R n?k U = [head (1) ? head (2) ? . . . head (H) ]W o</formula><p>where, W o ? R kH?d and H represents the total number of heads in multi-head attention. Note U ? R n?d . We add residual connection X and apply LayerNorm, followed by a feed forward and Add &amp; Norm layer:</p><formula xml:id="formula_5">U = LayerNorm X + U ; ? 1 , ? 1 ; Z = ReLU (UW 1 ) W 2 ; Z = LayerNorm U + Z ; ? 2 , ? 2 ; where, ? 1 , ? 1 ? R d , W 1 ? R d?m , W 2 ? R m?d , and ? 2 , ? 2 ? R d .</formula><p>The transformer encoder provides features corresponding to every utterance in a dialogue ([z 1 , z 2 , . . . , z n ] T = Z ? R n?d ). Graph Formation: A graph captures inter and intra-speaker dependency between utterances. Every utterance acts as a node of a graph that is connected using directed relations (past and future relations). We define relation types as speaker to speaker. Formally, consider a conversation between M speakers defined as a dia-</p><formula xml:id="formula_6">logue D = {U S 1 , U S 2 , . . . , U S M }, where U S 1 = {u (S 1 ) 1 , u (S 1 ) 2 , . . . , u (S 1 )</formula><p>n } represent the set of utterances spoken by speaker-1. We define intra relations between the utterances spoken by the same speaker, R intra ? {U S i ? U S i }, and inter relations between the utterances spoken by different speakers, R inter ? {U S i ? U S j } i =j . We further consider a window size and use P and F as hyperparameters to form relations between the past P utterances and future F utterances for every utterance in a dialogue. For instance, R intra and R inter for utterance u (S 1 ) i (spoken by speaker-1) are defined as:</p><formula xml:id="formula_7">Rintra(u (S 1 ) i ) = { u (S 1 ) i ? u (S 1 ) i?P . . . u (S 1 ) i ? u (S 1 ) i?1 , u (S 1 ) i ? u (S 1 ) i , u (S 1 ) i ? u (S 1 ) i+1 . . . u (S 1 ) i ? u (S 1 ) i+F } Rinter(u (S 1 ) i ) = { u (S 1 ) i ? u (S 2 ) i?P , . . . , u (S 1 ) i ? u (S 2 ) i?1 , u (S 1 ) i ? u (S 2 ) i+1 , . . . , u (S 1 ) i ? u (S 2 ) i+F }</formula><p>where ? and ? represent the past and future relation type respectively (example in Appendix F). Relational Graph Convolutional Network (RGCN): The vanilla RGCN (Schlichtkrull et al., 2018) helps accumulate relation-specific transformations of neighboring nodes depending on the type and direction of edges present in the graph through a normalized sum. In our case, it captures the inter-speaker and intra-speaker dependency on the connected utterances.</p><formula xml:id="formula_8">x i = ? root ? z i + r?R j?Nr(i) 1 |N r (i)| ? r ? z j</formula><p>where N r (i) denotes the set of neighbor indices of node i under relation r ? R, ? root and ? r denote the learnable parameters of RGCN, |N r (i)| is the normalization constant and z j is the utterance level feature coming from the transformer. GraphTransformer: For extracting rich representation from the node features, we use a GraphTransformer <ref type="bibr">(Shi et al., 2021)</ref>. GraphTransformer adopts the vanilla multi-head attention into graph learning by taking into account nodes connected via edges. Given node features H = x 1 , x 2 , . . . , x n obtained from RGCN,</p><formula xml:id="formula_9">h i = W 1 x i + j?N (i) ? i,j W 2 x j</formula><p>where the attention coefficients ? i,j are computed via multi-head dot product attention: Emotion Classifier: A linear layer over the features extracted by GraphTransformer (h i ) predicts the emotion corresponding to the utterance.</p><formula xml:id="formula_10">? i,j = softmax ? ? (W 3 x i ) W 4 x j ? d ? ?</formula><formula xml:id="formula_11">h i = ReLU(W 1 h i + b 1 ) P i = softmax(W 2 h i + b 2 ) y i = arg max(P i )</formula><p>where? i is the emotion label predicted for the utterance u i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experiment for the Emotion Recognition task on the two widely used datasets: IEMOCAP (Busso et al., 2008) and MOSEI <ref type="bibr" target="#b6">(Zadeh et al., 2018b)</ref>. The dataset statistics are given in <ref type="table">Table 1</ref>. IEMOCAP is a dyadic multimodal emotion recognition dataset where each utterance in a dialogue is labeled with one of the six emotion categories: anger, excited, sadness, happiness, frustrated, and neutral. In literature, two IEMOCAP settings are used for testing, one with 4 emotions (anger, sadness, happiness, neutral) and one with 6 emotions. We experiment with both of these settings. MOSEI is a multimodal emotion recognition dataset annotated with 7 sentiments (-3 (highly negative) to +3 (highly positive)) and 6 emotion labels (happiness, sadness, disgust, fear, surprise, and anger). Note that the emotion labels differ across the datasets. We use weighted F1-score and Accuracy as evaluation metrics (details in Appendix C).   5 Results and Analysis IEMOCAP: <ref type="table" target="#tab_4">Table 2</ref> shows the results for IEMO-CAP (6-way) multimodal setting. Overall, COG-MEN performs better than all the previous baselines as measured using accuracy and F1-score. We also see an improvement in the class-wise F1 for happy, sad, neutral, and excited emotions. This improvement is possibly due to the GNN architecture (described in analysis later) that we are using in our model, and none of the previous multimodal baselines uses GNN in their architecture. Results for IEMOCAP (4-way) setting are in    <ref type="table">Table 6</ref>: Ablation study on IEMOCAP dataset. All values are F1-score (%). The results shows the importance of GCN layer. adding visual modality, possibly because of noise present in the visual modality and lack of alignment with respect to other modalities. In contrast, our model can capture rich relations across the modalities and show a performance boost while adding visual modality. We conducted further analysis on our model. Although due to space limitations, the results below mainly describe experiments over IEMOCAP, similar trends were observed for MOSEI as well. Effect of Local and Global Info.: We test our architecture in two information utilization settings: global and local. To test the importance of context in our architecture, we create a sub-dataset using the IEMOCAP (4-way) setting by splitting each dialogue into n utterances and training our architecture.  with number of utterances present in a dialogue (more details on effect of window size in Appendix G.2). This experiment helps understand the importance of context in a dialogue. Moreover, it points towards challenges in developing a real-time system (details in ?6). We test the local information hypothesis by removing the GNN module and directly passing the context extracted features to the emotion classifier. <ref type="table">Table 6</ref> shows the drop in performance across modalities when the GNN component is removed from the architecture, making our local information hypothesis more concrete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Relation Types:</head><p>We also test the effect of inter and intra-relations in the dialogue graph by making all relations of the same type and training the architecture. We observe a drop in performance <ref type="table">(Table 6</ref>) when the relations are kept the same in the graph formation step. The explicit relation formation helps capture the local dependencies present in the dialogue. Effect of Modalities: The focus of this work is multimodal emotion recognition. However, just for the purpose of comparison, we also compare with unimodal (text only) approaches. We compare <ref type="table" target="#tab_10">(Table 7)</ref>  Text-based models are specifically optimized for text modalities and incorporate changes to architectures to cater to text. It is not fair to compare with our multimodal approach from that perspective. As shown in results, COGMEN, being a fairly generic architecture, still gives better (for IEMOCAP (4-way)) or comparable performance with respect to the SOTA unimodal architectures.</p><p>In the case of our model, adding more information via other modalities helps to improve the performance. Results on different modality combinations are in Appendix D. Error Analysis: After analysing the predictions made across the datasets, we find that our model falls short in distinguishing between similar emotions, such as happiness vs excited and anger vs frustration <ref type="figure" target="#fig_2">(Figure 3</ref>). This issue also exists in previous methods as reported in <ref type="bibr">Shen et al. (2021b), and</ref><ref type="bibr">Ghosal et al. (2019)</ref>. We also find that our model misclassifies the other emotion labels as neutral because of a more significant proportion of neutral labeled examples. Moreover, we observe the accuracy of our model when classifying examples having emotion shift is 53.6% compared to 74.2% when the emotion remains the same (more details in Appendix B).</p><p>Efficacy of the GNN Layer: For observing the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before After</head><p>Happiness Sadness Neutral Anger effect of the GNN component in our architecture, we also visualize the features before and after the GNN component. <ref type="figure" target="#fig_3">Figure 4</ref> clearly shows the better formation of emotion clusters depicting the importance of capturing local dependency in utterances for better performance in emotion recognition (more in Appendix E and Appendix <ref type="figure" target="#fig_7">Figure-9</ref>). Importance of utterances: To verify the effect of utterances and their importance in a prediction for a dialogue, we infer the trained model on dialogues by masking one utterance at a time and calculating the F1-score for prediction. <ref type="figure" target="#fig_4">Figure 5</ref> shows the obtained results for a dialogue (Appendix Table 10) instance taken randomly from IEMOCAP (4-way) (more in Appendix E). For the first 4 utterances, emotions state being neutral, the effect of masking the utterances is significantly less. In contrast, masking the utterances with emotion shift (9, 10, 11) completely drops the dialogue's F1-score, showing that our architecture captures the effects of emotions present in the utterances.  <ref type="formula">2017)</ref>) uses LSTM to capture the contextual information and maintain long relations between the utterances from the past and future. Another contemporary architecture Af-CAN <ref type="bibr">(Wang et al., 2021a)</ref> utilizes recurrent neural networks based on contextual attention to model the interaction and dependence between speakers and uses bi-directional GRU units to capture the global features from past and future. We propose to address these issues using a unified architecture that captures the effect of context on utterances while maintaining the states for self and interpersonal dependencies. We make use of transformers for encoding the global context and make use of GraphTransformers to capture the self and interpersonal dependencies. Our way of forming relational graphs between the utterances comes close to DialogueGCN (unimodal architecture). We further use a shared Emotion classifier for predicting emotions from all the obtained utterance level features. Moreover, our unified architecture handles multiple modalities effectively and shows an increase in performance after adding information from other modalities. Limitations (Offline Setting): A noteworthy limitation of all the proposed Emotion Recognition approaches (including the current one) is that they use global context from past and future utterances to predict emotions. However, baseline systems compared in this paper are also offline systems. For example, bc-LSTM (bi-directional contextual LSTM) and Af-CAN use utterances from the past and future to predict emotions. Other popular baselines like DialogueGCN and DialogueRNN (BiDi-alogueRNN) also peek into the future, assuming the presence of all the utterances during inference (offline setting). All such systems that depend on future information can only be used in an offline setting to process and tag the dialogue. An Emotion Recognition system that could work in an online setting exhibits another line of future work worth exploring due to its vast use cases in live telecasting and telecommunication. A possible approach to maintain the context in an online setting would be to take a buffer of smaller context size, where the model can predict emotions taking not the complete dialogue but a smaller subset of it as input in real-time. We tried exploring this setting for our architecture with an online buffer of maintaining a smaller context window. For experimenting with it, we created a sub-dataset using the IEMOCAP (4-way) setting by splitting each dialogue into n utterances and training our architecture. Our results in <ref type="table" target="#tab_8">Table 5</ref> show the decrease in performance with the number of utterances present in a dialogue depicting the importance of context in a conversation. Performance improvements in these settings where the system can work in real-time are worth exploring and are an interesting direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We present a novel approach of using GNNs for multimodal emotion recognition and propose COGMEN: COntextualized GNN based Multimodal Emotion recognitioN. We test COGMEN on two widely known multimodal emotion recognition datasets, IEMOCAP and MOSEI. COGMEN outperforms the existing state-of-the-art methods in multimodal emotion recognition by a significant margin (i.e., 7.7% F1-score increase for IEMO-CAP (4-way)). By comprehensive analysis and ablation studies over COGMEN, we show the importance of different modules. COGMEN fuses information effectively from multiple modalities to improve the performance of emotion prediction tasks. We perform a detailed error analysis and observe that the misclassifications are mainly between the similar classes and emotion shift cases. We plan to address this in future work, where the focus will be to incorporate a component for capturing the emotional shifts for fine-grained emotion prediction.   We use PyTorch (Paszke et al., 2019) for training our architecture and PyG (PyTorch Geometric) (Fey and Lenssen, 2019) for the GNN component in our architecture. We use comet <ref type="bibr">(Com, 2021)</ref> for logging all our experiments and its Bayesian optimizer for hyperparameter tuning. Our architecture trained on the IEMOCAP dataset has 55,932,052 parameters and takes around 7 minutes to train for 50 epochs on the NVIDIA Tesla K80 GPU. Comparison of the model with baselines in terms of the number of parameters is challenging, as the baselines parameters vary depending on the hyperparameter setting. Moreover, many baselines do not provide information about the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Analysis</head><p>We study IEMOCAP dataset in detail for error analysis of our model. We observe the emotion transition at Utterance level ( <ref type="figure">Figure 6</ref>) and Speaker level <ref type="figure">(Figure 7)</ref>. We find a high percentage of transitions between similar emotions, causing the models to confuse between the similar classes of emotion. Considering the emotion transition between states that are opposite, like from happy to sad, we deduce the poor performance of emotion recognition architectures for such cases. We plan to address this issue in future work where we target a model which performs better in fine-grained emotion recognition and is robust towards the shifts in emotions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation Metrics</head><p>Weighted F1 Score: The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</p><formula xml:id="formula_12">F 1 = 2 * (precision * recall) (precision + recall)</formula><p>For weighted F1 score, we calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). Accuracy: It is defined as the percentage of correct predictions in the test set. <ref type="table" target="#tab_16">Table 11</ref> shows results on the IEMOCAP dataset for all the modality combinations for our architectures. <ref type="figure" target="#fig_6">Figure 8</ref> shows the confusion matrix for prediction on IEMOCAP 4-way dataset.      obtained results for a dialogue instance taken randomly from IEMOCAP 4-way. For the first 15 utterances, emotions state being sadness, the effect of masking the utterances is more negligible for the first 5 utterances. This drop depicts the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Results on Modality Combinations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relations with Past Utterances</head><p>Relations with Future Utterances <ref type="figure">Figure 11</ref>: Graph formation process in (COGMEN) architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure Central Node</head><p>Rintra Rinter <ref type="table" target="#tab_4">Table 12</ref>: Relations for each instance of <ref type="figure">Figure 11</ref>, where relations with past utterances are denoted by (?) and relations with future utterances are denoted by (?)</p><formula xml:id="formula_13">(a) u (S1) i?3 u (S1) i?3 ? u (S1) i?3 , u (S1) i?3 ? u (S1) i?1 , u (S1) i?3 ? u (S1) i+1 , u (S1) i?3 ? u (S1) i+3 u (S1) i?3 ? u (S2) i?2 , u (S1) i?3 ? u (S2) i , u (S1) i?3 ? u (S2) i+2 (b) u (S2) i?2 u (S2) i?2 ? u (S2) i?2 , u (S2) i?2 ? u (S2) i , u (S2) i?2 ? u (S2) i+2 u (S2) i?2 ? u (S1) i?3 , u (S2) i?2 ? u (S1) i?1 , u (S2) i?2 ? u (S1) i+1 , u (S2) i?2 ? u (S1) i+3 (c) u (S1) i?1 u (S1) i?1 ? u (S1) i?3 , u (S1) i?1 ? u (S1) i?1 , u (S1) i?1 ? u (S1) i+1 , u (S1) i?1 ? u (S1) i+3 u (S1) i?1 ? u (S2) i?2 , u (S1) i?1 ? u (S2) i , u (S1) i?1 ? u (S2) i+2 (d) u (S2) i u (S2) i ? u (S2) i?2 , u (S2) i ? u (S2) i , u (S2) i ? u (S2) i+2 u (S2) i ? u (S1) i?3 , u (S2) i ? u (S1) i?1 , u (S2) i ? u (S1) i+1 , u (S2) i ? u (S1) i+3 (e) u (S1) i+1 u (S1) i+1 ? u (S1) i?3 , u (S1) i+1 ? u (S1) i?1 , u (S1) i+1 ? u (S1) i+1 , u (S1) i+1 ? u (S1) i+3 u (S1) i+1 ? u (S2) i?2 , u (S1) i+1 ? u (S2) i , u (S1) i+1 ? u (S2) i+2 (f) u (S2) i+2 u (S2) i+2 ? u (S2) i?2 , u (S2) i+2 ? u (S2) i , u (S2) i+2 ? u (S2) i+2 u (S2) i+2 ? u (S1) i?3 , u (S2) i+2 ? u (S1) i?1 , u (S2) i+2 ? u (S1) i+1 , u (S2) i+2 ? u (S1) i+3 (g) u (S1) i+3 u (S1) i+3 ? u (S1) i?3 , u (S1) i+3 ? u (S1) i?1 , u (S1) i+3 ? u (S1) i+1 , u (S1) i+3 ? u (S1) i+3 u (S1) i+3 ? u (S2) i?2 , u (S1) i+3 ? u (S2) i , u (S1) i+3 ? u (S2) i+2</formula><p>Relation Type Node A Node B Relation Causality Relation 1 u (S1) u (S1) Past u (S1) ? u (S1) 2 u (S1) u (S2) Past u (S1) ? u (S2) 3 u (S2) u (S1) Past u (S2) ? u (S1) 4 u (S2) u (S2) Past u (S2) ? u (S2) 5 u (S1) u (S1) Future u (S1) ? u (S1) 6 u (S1) u (S2) Future u (S1) ? u (S2) 7 u (S2) u (S1) Future u (S2) ? u (S1) 8 u (S2) u (S2) Future u (S2) ? u (S2) <ref type="table" target="#tab_5">Table 13</ref>: Unique Relation types for a conversation between two speakers importance of utterances 5-15 that affect future utterances. Further, masking the utterances with high emotion shift (15 to 30) drops the F1 score of the dialogue, showing the importance of fluctuations for predicting the emotion states for other utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Graph Formation</head><p>To give a clear picture of the graph formation procedure, we describe the process for utterances spoken in a dialogue. As an illustration, let's consider two speakers, S 1 and S 2 , present in a conversation of 7 utterances. Features corresponding to each utterance is shown as a node in <ref type="figure">Figure 11</ref>. Speaker 1 speaks utterances u i?3 , u i?1 , u i+1 , u i+3 and Speaker 2 speaks u i?2 , u i , u i+2 . After creating the graphs with relations, the constructed graph would look like shown in <ref type="figure">Figure 11</ref>, and the corresponding relations for each instance would be as shown in <ref type="table" target="#tab_4">Table 12</ref>. Since there are two speakers in the conversation (S N = 2), the total number of unique relations would be:</p><p>number of relations = 2 ? (S N ) 2 = 2 ? (2) 2 = 8 <ref type="table" target="#tab_5">Table 13</ref> shows the number of possible unique relations for a conversation between two speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Modality Fusing Mechanisms</head><p>While experimenting with the model architecture, we explored various mechanisms for mixing information from multiple modalities. Some of the mechanisms include pairwise attention inspired from <ref type="bibr">Ghosal et al. (2018)</ref>, bimodal attention present in Multilogue-Net (Shenoy and Sardana, 2020), and crossAttention layer proposed in <ref type="bibr">HKT (Hasan et al., 2021)</ref>. However, in our case, none of these fusing mechanisms shows significant performance improvement over simple concatenation. Moreover, all these fusing mechanisms require extra computation steps for fusing information. In contrast, a simple concatenation of modality features works well with no additional computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Effect of window size in Graph Formation</head><p>To explore the effect of window size in the Graph Formation module of our architecture, we conduct experiments with multiple window sizes. The obtained results are present in <ref type="table" target="#tab_7">Table 14</ref>. The window size can be treated as a hyperparameter that could be adjusted while training our architecture. Moreover, the freedom of setting the window size makes our architecture more flexible in terms of usage. A larger window size would result in better performance for cases where the inter and intra speaker dependencies are maintained for longer sequences.</p><p>In contrast, setting a lower window size would be better in a use case where the topic frequently changes in dialogues and speakers are less affected by another speaker. In the future, we plan to explore a dynamic and automatic selection of window size depending on the dialogue instance.  <ref type="table" target="#tab_7">Table 14</ref>: Results for various window sizes for graph formation on the IEMOCAP (4-way) dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed model (COGMEN) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>with EmoBERTa (Kim and Vossen, 2021), DAG-ERC (Shen et al., 2021b), CESTa (Wang et al., 2020), SumAggGIN (Sheng et al., 2020), DialogueCRN (Hu et al., 2021), DialogXL (Shen et al., 2021a) and DialogueGCN (Ghosal et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Confusion Matrix for IEMOCAP (6-way)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>UMAP (Becht et al., 2019)  representation of IEMOCAP (4-way) features before and after GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Importance of utterances in IEMOCAP (4way). Performance drop is observed while masking 6 Discussion Comparison with Baselines: Emotion recognition in a multimodal conversation setting comes with two broadly portrayed research challenges (Poria et al., 2019), first, the ability of a model to capture global and local context present in the dialogues, and second, the ability to maintain self and interpersonal dependencies among the speakers. All the popular baselines like Dialogue-GCN (Ghosal et al., 2019), DialogueRNN (Majumder et al., 2019), bc-LSTM (Poria et al., 2017) Af-CAN (Wang et al., 2021a), etc., try to address these challenges by proposing various architectures. bc-LSTM (bi-directional contextual LSTM (Poria et al.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Utterance-level Emotion transition for IEMO-CAP. These are emotions transitions in consecutive utterances across speakers. h a p p in e s s s a d n e s s n e u t r a l a n g e r e x c it e Speaker-level Emotion transition for IEMO-CAP. These are emotions transitions in the consecutive utterances of the same speaker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Confusion Matrix for IEMOCAP 4-Way classificationE Additional AnalysisEfficacy of the GNN Layer: We observe the efficacy of the GNN component in our architecture and visualize the features before GNN and after the GNN component(Figure 9) explained in section 5. Importance of utterances:Figure 10shows the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>UMAP (Becht et al., 2019)  representation of IEMOCAP 6-way features before and after GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Importance of utterances in IEMOCAP classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>It's a relief too. I mean I was thinking I was going to have to pack up all of my stuff but yeah. Hooray for U.S.C. Yeah. Big time. Mmhmm. Mmhmm. Yay. So what major? What are you doing? Yeah. I'm not sure. I know. We should throw a party. A rapper party ho. Yeah. Okay. I am just-Yeah. I'm sticking around, I'm just doing my thing. I'm living here. I'm so glad you're going to stay, I'm so glad you're going to be here. Yeah. Woo. Umm...P.h.D. [LAUGHTER] That's good. Don't you have to like teach a class, too, when you do that? Masters or a P.h.D.? Or can you...? Nice</figDesc><table><row><cell>Speaker-1</cell><cell>Speaker-2</cell></row><row><cell>Conversation Instance taken from</cell><cell></cell></row><row><cell>IEMOCAP Dataset</cell><cell></cell></row><row><cell>Excited Excited Excited Is this a Excited</cell><cell>Excited Happy Excited Neutral</cell></row><row><cell>Excited</cell><cell></cell></row><row><cell></cell><cell>Neutral</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on IEMOCAP (6-way) multimodal (A+T+V) setting. Avg. denotes weighted average.</figDesc><table><row><cell></cell><cell>concatena-</cell></row><row><cell cols="2">tion. We also explored other fusion mechanisms</cell></row><row><cell cols="2">(Appendix G.1). However, concatenation gave the</cell></row><row><cell cols="2">best performance. We conduct a hyper-parameter</cell></row><row><cell cols="2">search for our proposed model using Bayesian op-</cell></row><row><cell cols="2">timization techniques (details in Appendix A).</cell></row><row><cell cols="2">Baselines: We do a comprehensive evaluation of</cell></row><row><cell cols="2">COGMEN by comparing it with a number of</cell></row><row><cell cols="2">baseline models. For IEMOCAP, we compare</cell></row><row><cell cols="2">our model with the existing multimodal frame-</cell></row><row><cell cols="2">works (Table 2), which includes DialogueRNN</cell></row><row><cell cols="2">(Majumder et al., 2019), bc-LSTM (Poria et al.,</cell></row><row><cell cols="2">2017), CHFusion (Majumder et al., 2018), mem-</cell></row><row><cell cols="2">net (Sukhbaatar et al., 2015), TFN (Zadeh et al.,</cell></row><row><cell cols="2">2017), MFN (Zadeh et al., 2018a), CMN (Haz-</cell></row><row><cell cols="2">arika et al., 2018b), ICON (Hazarika et al., 2018a),</cell></row><row><cell cols="2">and Af-CAN (Wang et al., 2021b). For MOSEI,</cell></row><row><cell cols="2">COGMEN is compared (Table 4) with multimodal</cell></row><row><cell cols="2">models, including Multilogue-Net (Shenoy and Sar-</cell></row><row><cell cols="2">dana, 2020) and TBJE (Delbrouck et al., 2020)</cell></row><row><cell cols="2">(details and analysis of baselines in  ?6).</cell></row><row><cell>Model</cell><cell>F1-score (%)</cell></row><row><cell>bc-LSTM</cell><cell>75.13</cell></row><row><cell>CHFusion</cell><cell>76.80</cell></row><row><cell>COGMEN</cell><cell>84.50</cell></row></table><note>sentence-level static features. For Audio and Vi- sual modalities, we use sentence/utterance level features by averaging all the token level features. We fuse the features of all the available modalities (A(audio)+T(text)+V(video): ATV) via</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on IEMOCAP dataset for 4 emotion classes in multimodal setting (weighted F1-score).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>In this setting, COGMEN achieves 7.7% improvement over the previous SOTA model. MOSEI: For emotion classification across 6 emotion classes, we used two settings (as done in previous works): Binary Classification across each emotion label where a separate model is trained for every emotion class, and Multi-label Classification in which the sentence is tagged with more than 1 emotion and single model predicts multiple classes. The reason for doing this was that Multilogue-Net provides results on binary classification setting and TBJE provides results on Multi-label setting. We ran both models on these settings. For a fair comparison, we use the same utterance level textual features similar to our setting (extracted from sBERT) and train Multilogue-Net architecture on both the settings. Originally, Multilogue-Net used GloVe embeddings (Pennington et al., 2014) for textual features, and actual results in the paper are different than reported here. For TBJE, we use the features provided by the paper as it uses token-level features. COGMEN outperforms (Table 4) the baseline models in most of the cases. For 2 class sentiment classification, COGMEN outperforms the previous baselines with the highest accuracy score of 85% for A+T. For 7 class, our model shows comparable performance. All the multimodal approaches tend to perform poorly when Class Happiness Sadness Angry Fear Disgust Surprise Happiness Sadness Angry Fear Disgust Surprise Multilogue-Net T + A + V 82.88 44.83 67.84 65.34 67.03 87.79 74.91</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Sentiment Class</cell><cell></cell><cell></cell><cell cols="2">Emotion Class</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Multi-label Emotion Class</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy(%)</cell><cell></cell><cell cols="4">(weighted) F1-score (%)</cell><cell></cell><cell></cell><cell cols="2">(weighted) F1-score (%)</cell></row><row><cell>Model</cell><cell></cell><cell cols="8">2 Class 7 86.05</cell><cell>70.6</cell><cell>70.7</cell><cell>74.4 86.0 83.4</cell><cell>87.8</cell></row><row><cell></cell><cell>T</cell><cell>81.9</cell><cell>44.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.4</cell><cell>65.8</cell><cell>75.3 84.0 84.5</cell><cell>81.4</cell></row><row><cell>TBJE</cell><cell>A + T</cell><cell>82.4</cell><cell>43.91</cell><cell>65.91</cell><cell cols="4">70.78 70.86 87.79 82.57</cell><cell>86.04</cell><cell>65.5</cell><cell>67.9</cell><cell>76.0 87.2 84.5</cell><cell>86.1</cell></row><row><cell cols="3">T + A + V 81.5</cell><cell>44.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.0</cell><cell>67.9</cell><cell>74.7 84.0 83.6</cell><cell>86.1</cell></row><row><cell></cell><cell>T</cell><cell cols="2">84.42 43.50</cell><cell>69.28</cell><cell cols="4">70.49 73.04 87.80 83.69</cell><cell>85.83</cell><cell>69.92</cell><cell cols="2">72.16 77.34 86.39 86.00</cell><cell>88.27</cell></row><row><cell>COGMEN</cell><cell>A + T</cell><cell cols="2">85.00 44.31</cell><cell>68.39</cell><cell cols="4">73.28 74.98 88.08 83.90</cell><cell>85.35</cell><cell>69.62</cell><cell cols="2">72.67 76.93 86.39 85.35</cell><cell>88.21</cell></row><row><cell cols="4">T + A + V 84.34 43.90</cell><cell>70.42</cell><cell cols="4">72.31 76.20 88.17 83.69</cell><cell>85.28</cell><cell>72.74</cell><cell cols="2">73.90 78.04 86.71 85.48</cell><cell>88.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on MOSEI dataset. For emotion classification, a weighted F1-score is used. For Sentiment Classification, the results are reported using accuracy. 2 class sentiment consists of only positive and negative sentiment. 7 class sentiment consists of sentiments from highly negative (-3) to highly positive (+3). For the cells showing '-', the results were not provided in the paper, and we were not able to reproduce the results since TBJE used token level features, and we are using sentence-level features.</figDesc><table><row><cell># Utterances in Context</cell><cell>F1-score (%)</cell></row><row><cell>All Utterances in a dialogue</cell><cell>84.50</cell></row><row><cell>10 Utterances in a dialogue</cell><cell>77.43 (?7.07)</cell></row><row><cell>3 Utterances in a dialogue</cell><cell>75.39 (?9.11)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Importance of Context in a dialogue. Experiment performed on IEMOCAP (4-way). Relations 76.76 (?4.79) 80.27 (?1.32) 79.61 (?4.88)</figDesc><table><row><cell></cell><cell>Modalities</cell><cell>T</cell><cell>A+T</cell><cell>A+T+V</cell></row><row><cell></cell><cell>Actual</cell><cell>66.00</cell><cell>65.42</cell><cell>67.63</cell></row><row><cell>(6 way)</cell><cell>w/o GNN</cell><cell cols="3">64.34 (?1.66) 61.69 (?3.73) 62.96 (?4.14)</cell></row><row><cell></cell><cell cols="4">w/o Relations 60.49 (?5.51) 65.32 (?0.10) 62.13 (?5.50)</cell></row><row><cell></cell><cell>Actual</cell><cell>81.55</cell><cell>81.59</cell><cell>84.50</cell></row><row><cell>(4 way)</cell><cell>w/o GNN</cell><cell cols="3">81.18 (?0.37) 80.16 (?1.43) 80.28 (?4.22)</cell></row><row><cell></cell><cell>w/o</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>shows the decrease in performance</figDesc><table><row><cell>Model</cell><cell>Modality</cell><cell>F1-score (%)</cell></row><row><cell></cell><cell>4-way</cell><cell></cell></row><row><cell>DialogueGCN</cell><cell>T</cell><cell>71.58</cell></row><row><cell>DialogXL</cell><cell>T</cell><cell>73.02</cell></row><row><cell>DAG-ERC</cell><cell>T</cell><cell>78.08</cell></row><row><cell>COGMEN</cell><cell>T A+T+V</cell><cell>81.55 84.50</cell></row><row><cell></cell><cell>6-way</cell><cell></cell></row><row><cell>EmoBERTa</cell><cell>T</cell><cell>68.57</cell></row><row><cell>DAG-ERC</cell><cell>T</cell><cell>68.03</cell></row><row><cell>CESTa</cell><cell>T</cell><cell>67.10</cell></row><row><cell>SumAggGIN</cell><cell>T</cell><cell>66.61</cell></row><row><cell>DialogueCRN</cell><cell>T</cell><cell>66.20</cell></row><row><cell>DialogXL</cell><cell>T</cell><cell>65.94</cell></row><row><cell>DialogueGCN</cell><cell>T</cell><cell>64.18</cell></row><row><cell>COGMEN</cell><cell>T</cell><cell>66.00</cell></row><row><cell></cell><cell>A+T+V</cell><cell>67.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Comparison with unimodal architectures on IEMOCAP dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Harsh Agarwal, Keshav Bansal, Abhinav Joshi, and Ashutosh Modi. 2021. Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts. CoRR, abs/2112.01938.</figDesc><table><row><cell>Devamanyu Hazarika, Soujanya Poria, Rada Mihal-Nils Reimers and Iryna Gurevych. 2019. Sentence-</cell><cell>surveillance systems using neural networks: A sur-Zheng Lian, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei emotion-cause pair extraction. In Proceedings of the</cell></row><row><cell>cea, Erik Cambria, and Roger Zimmermann. 2018a. BERT: Sentence embeddings using Siamese BERT-</cell><cell>vey. Yang, and Rongjun Li. 2020. Conversational emo-Eleventh Workshop on Computational Approaches</cell></row><row><cell>ICON: Interactive conversational memory network networks. In Proceedings of the 2019 Conference on</cell><cell>tion recognition using self-attention mechanisms to Subjectivity, Sentiment and Social Media Analy-</cell></row><row><cell>for multimodal emotion detection. In Proceedings Empirical Methods in Natural Language Processing</cell><cell>Paul Ekman. 1993. Facial expression and emotion. and graph neural networks. In INTERSPEECH, sis, pages 84-91, Online. Association for Computa-</cell></row><row><cell>of the 2018 Conference on Empirical Methods in and the 9th International Joint Conference on Natu-</cell><cell>American psychologist, 48(4):384. pages 2347-2351. tional Linguistics.</cell></row><row><cell>AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 2236-2246, Melbourne, Australia. Association for Computational Linguis-tics. Tadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency. 2018. Openface 2.0: Fa-cial behavior analysis toolkit. In 2018 13th IEEE In-ternational Conference on Automatic Face Gesture Recognition (FG 2018), pages 59-66. Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. 2019. Dimensionality reduction for visualizing single-cell data using umap. Nature biotechnology, 37(1):38-44. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jean-nette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008. Iemocap: Interactive emotional dyadic motion capture database. Language re-sources and evaluation, 42(4):335-359. Pierre Colombo, Wojciech Witon, Ashutosh Modi, James Kennedy, and Mubbasir Kapadia. 2019. Affect-driven dialog generation. In Proceedings of the 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3734-3743, Minneapolis, Minnesota. Association for Computational Linguis-tics. Dragos Datcu and Leon JM Rothkrantz. 2014. Seman-tic audio-visual data fusion for automatic emotion recognition. Emotion recognition: a pattern analy-sis approach, pages 411-435. Jean-Benoit Delbrouck, No? Tits, Mathilde Brous-miche, and St?phane Dupont. 2020. A transformer-based joint-encoding for emotion recognition and sentiment analysis. In Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML), pages 1-7, Seattle, USA. Association for Computational Linguistics. Natural Language Processing, pages 2594-2604, Brussels, Belgium. Association for Computational Linguistics. ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics. Devamanyu Hazarika, Soujanya Poria, Amir Zadeh, Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Erik Cambria, Louis-Philippe Morency, and Roger Rianne van den Berg, Ivan Titov, and Max Welling. Zimmermann. 2018b. Conversational memory net-work for emotion recognition in dyadic dialogue videos. In Proceedings of the 2018 Conference 2018. Modeling relational data with graph convolu-tional networks. In The Semantic Web, pages 593-607, Cham. Springer International Publishing. of the North American Chapter of the Associa-tion for Computational Linguistics: Human Lan-guage Technologies, Volume 1 (Long Papers), pages 2122-2132, New Orleans, Louisiana. Association Nicu Sebe, Ira Cohen, Theo Gevers, and Thomas S Huang. 2005. Multimodal approaches for emo-tion recognition: A survey. Proceedings of SPIE -label prediction: Unified massage passing model for Probabilistic models for segmenting and labeling se-Zhong, Shikun Feng, and Yu Sun. 2021. Masked C. N. Pereira. 2001. Conditional random fields: Yunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui John D. Lafferty, Andrew McCallum, and Fernando (ICASSP), pages 4477-4481. IEEE. Linguistics. ference on Acoustics, Speech and Signal Processing 19-28, Seattle, USA. Association for Computational ysis. In ICASSP 2020-2020 IEEE International Con-on Multimodal Language (Challenge-HML), pages nism for attention based multi modal sentiment anal-tion. In Second Grand-Challenge and Workshop In Human-Computer Systems Interaction: Back-grounds and Applications 3, pages 51-62. Springer. Ayush Kumar and Jithendra Vepa. 2020. Gated mecha-tion detection and sentiment analysis in conversa-net: A context-aware RNN for multi-modal emo-Aman Shenoy and Ashish Sardana. 2020. Multilogue-2014. Emotion recognition and its applications. pages 4153-4163. Speaker-Aware Emotion Recognition in Conver-sation with RoBERTa. arXiv e-prints, page arXiv:2108.12009. Agata Ko?akowska, Agnieszka Landowska, Mariusz Szwoch, Wioleta Szwoch, and Michal R Wrobel. national Conference on Computational Linguistics, tion recognition. In Proceedings of the 28th Inter-graph inference network for conversational emo-before aggregate: A global-to-local heterogeneous Zheng, and Haozhuang Liu. 2020. Summarize Dongming Sheng, Dong Wang, Ying Shen, Haitao Taewoon Kim and Piek Vossen. 2021. EmoBERTa: for Computational Linguistics. Dou Hu, Lingwei Wei, and Xiaoyong Huai. 2021. Di-aloguecrn: Contextual reasoning networks for emo-tion recognition in conversations. In ACL/IJCNLP (1), pages 7042-7052. Association for Computa-tional Linguistics. Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, and Jun Goto. 2020. Relation-aware graph attention net-works with relational position encodings for emo-tion recognition in conversations. In Proceedings of the 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 7360-7370. Sepehr Janghorbani, Ashutosh Modi, Jakob Buhmann, and Mubbasir Kapadia. 2019. Domain authoring as-sistant for intelligent virtual agent. AAMAS '19, page 104-112, Richland, SC. International Founda-tion for Autonomous Agents and Multiagent Sys-tems. The International Society for Optical Engineering, 5670:56-67. Proceedings of SPIE-IS and T Elec-tronic Imaging -Internet Imaging VI ; Conference date: 18-01-2005 Through 20-01-2005. Garima Sharma and Abhinav Dhall. 2021. A Survey on Automatic Multimodal Emotion Recognition in the Wild, pages 35-64. Weizhou Shen, Junqing Chen, Xiaojun Quan, and Zhix-ian Xie. 2021a. Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition. Proceed-ings of the AAAI Conference on Artificial Intelli-gence, 35(15):13789-13797. Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Online. Association for Computational Linguistics. Quan. 2021b. Directed acyclic graph network for conversational emotion recognition. In Proceed-ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 1551-1560,</cell><cell>Florian Eyben, Martin W?llmer, and Bj?rn Schuller. 2010. Opensmile: The munich versatile and fast Navonil Majumder, Devamanyu Hazarika, Alexander Gelbukh, Erik Cambria, and Soujanya Poria. 2018. Multimodal sentiment analysis using hierarchical fu-Gargi Singh, Dhanajit Brahma, Piyush Rai, and Ashutosh Modi. 2021b. Fine-Grained Emotion Pre-diction by Modeling Emotion Definitions. In 2021 open-source audio feature extractor. In Proceedings sion with context modeling. Knowledge-based sys-9th International Conference on Affective Comput-of the 18th ACM International Conference on Mul-tems, 161:124-133. ing and Intelligent Interaction (ACII), pages 1-8. timedia, MM '10, page 1459-1462, New York, NY, USA. Association for Computing Machinery. Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with pytorch geomet-ric. CoRR, abs/1903.02428. Navonil Majumder, Soujanya Poria, Devamanyu Haz-arika, Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. 2019. Dialoguernn: An attentive rnn for Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory net-works. In Proceedings of the 28th International emotion detection in conversations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6818-6825. Conference on Neural Information Processing Sys-tems -Volume 2, NIPS'15, page 2440-2448, Cam-bridge, MA, USA. MIT Press. Marc Franzen, Michael Stephan Gresser, Tobias M?ller, and Prof. Dr. Sebastian Mauser. 2021. De-veloping emotion recognition for video conference software to support people with autism. Yahui Fu, Shogo Okada, Longbiao Wang, Lili Guo, Yaodong Song, Jiaxing Liu, and Jianwu Dang. 2021. Consk-gcn: Conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition. In 2021 IEEE International Conference on Multimedia and Expo (ICME), pages 1-6. Deepanway Ghosal, Md Shad Akhtar, Dushyant Chauhan, Soujanya Poria, Asif Ekbal, and Pushpak Bhattacharyya. 2018. Contextual inter-modal atten-tion for multi-modal sentiment analysis. In Proceed-ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3454-3466, Brussels, Belgium. Association for Computational Linguistics. ria, Niyati Chhaya, and Alexander Gelbukh. 2019. DialogueGCN: A graph convolutional neural net-work for emotion recognition in conversation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-tional Linguistics. Tushar Goswamy, Ishika Singh, Ahsan Barkati, and tational Linguistics. 873-883. tational linguistics (volume 1: Long papers), pages 55th annual meeting of the association for compu-Spain (Online). International Committee on Compu-ysis in user-generated videos. In Proceedings of the putational Linguistics, pages 2787-2801, Barcelona, Morency. 2017. Context-dependent sentiment anal-ings of the 28th International Conference on Com-Navonil Majumder, Amir Zadeh, and Louis-Philippe for controlled affective text generation. In Proceed-Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Ashutosh Modi. 2020. Adapting a language model 256-268. Cambridge University Press. edition, Cambridge Handbooks in Psychology, page 2018. The Roles of Emotion in Relationships, 2 164, Hong Kong, China. Association for Computa-Sally Planalp, Julie Fitness, and Beverley A. Fehr. guage Processing (EMNLP-IJCNLP), pages 154-Qatar. Association for Computational Linguistics. Processing (EMNLP), pages 1532-1543, Doha, ence on Empirical Methods in Natural Language representation. In Proceedings of the 2014 Confer-Manning. 2014. GloVe: Global vectors for word Jeffrey Pennington, Richard Socher, and Christopher Deepanway Ghosal, Navonil Majumder, Soujanya Po-Brian McFee, Colin Raffel, Dawen Liang, Daniel P. W. Ellis, Matt McVicar, Eric Battenberg, and Oriol Ni-eto. 2015. librosa: Audio and music signal analysis Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro-in python. cessing Systems, volume 30. Curran Associates, Inc. Marvin Minsky. 2007. The Emotion Machine: Com-C Vinola and K Vimaladevi. 2015. A survey on human monsense Thinking, Artificial Intelligence, and the emotion recognition approaches, databases and ap-Future of the Human Mind. SIMON &amp; SCHUSTER. Adam Paszke, Sam Gross, Francisco Massa, Adam plications. ELCVIA Electronic Letters on Computer Vision and Image Analysis, 14(2):24-44. Lerer, James Bradbury, Gregory Chanan, Trevor Tana Wang, Yaqing Hou, Dongsheng Zhou, and Qiang Killeen, Zeming Lin, Natalia Gimelshein, Luca Zhang. 2021a. A contextual attention network for Antiga, Alban Desmaison, Andreas Kopf, Edward multimodal emotion recognition in conversation. In Yang, Zachary DeVito, Martin Raison, Alykhan Te-2021 International Joint Conference on Neural Net-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. works (IJCNN), pages 1-7. Py-torch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch?-Buc, E. Fox, and R. Gar-nett, editors, Advances in Neural Information Pro-cessing Systems 32, pages 8024-8035. Curran Asso-ciates, Inc. Tana Wang, Yaqing Hou, Dongsheng Zhou, and Qiang Zhang. 2021b. A contextual attention network for multimodal emotion recognition in conversation. In 2021 International Joint Conference on Neural Net-works (IJCNN), pages 1-7. IEEE.</cell></row><row><cell>quence data. In Proceedings of the Eighteenth Inter-semi-supervised classification. In IJCAI.</cell><cell>Soujanya Poria, Navonil Majumder, Rada Mihalcea,</cell></row><row><cell>Marwan Dhuheir, Abdullatif Albaseer, Emna Baccour, national Conference on Machine Learning, ICML</cell><cell>and Eduard Hovy. 2019. Emotion recognition in</cell></row><row><cell>Aiman Erbad, Mohamed Abdallah, and Mounir '01, page 282-289, San Francisco, CA, USA. Mor-Aaditya Singh, Shreeshail Hingane, Saim Wani, and</cell><cell>conversation: Research challenges, datasets, and re-</cell></row><row><cell>Hamdi. 2021. Emotion recognition for healthcare gan Kaufmann Publishers Inc. Ashutosh Modi. 2021a. An end-to-end network for</cell><cell>cent advances. IEEE Access, 7:100943-100953.</cell></row></table><note>M. Hasan, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan Hoque. 2021. Humor knowledge enriched trans- former for understanding multimodal humor. In AAAI.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameter values for our model on IEMOCAP dataset. ILR: Initial learning rate.</figDesc><table><row><cell>Modalities</cell><cell>Dropout</cell><cell>GNNHead</cell><cell>SeqContext</cell><cell>ILR</cell></row><row><cell>T</cell><cell>0.399</cell><cell>3</cell><cell>5</cell><cell>3.3e-3</cell></row><row><cell>A+T</cell><cell>0.103</cell><cell>1</cell><cell>2</cell><cell>6.9e-3</cell></row><row><cell>A+T+V</cell><cell>0.337</cell><cell>2</cell><cell>1</cell><cell>1.1e-3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameter value on MOSEI dataset.</figDesc><table /><note>ILR: Initial learning rate.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>She's been in New York three and a half years. Why all of the sudden?" neutral M 'Well maybe. Maybe she just wanted to see her again.' neutral F "What did you mean? He lived next door to the girl all of his life, why wouldn't he want to see her again? Don't look at me like that, he didn't tell me any more than he told you."</figDesc><table><row><cell>Speaker</cell><cell>Utterance Text</cell><cell>Emotion</cell></row><row><cell>M</cell><cell>'Why does that bother you?'</cell><cell>neutral</cell></row><row><cell>F</cell><cell cols="2">"neutral</cell></row><row><cell>M</cell><cell>"She's not his girl. She knows she's not."</cell><cell>angry</cell></row><row><cell>F</cell><cell>"I want you to pretend like he's coming back!"</cell><cell>angry</cell></row><row><cell>M</cell><cell>"Because if he's not coming back, then I'll kill myself."</cell><cell>angry</cell></row><row><cell>F</cell><cell>'Laugh. Laugh at me, but what happens the night that she goes to sleep in his bed, and his memorial breaks in pieces?"</cell><cell>angry</cell></row><row><cell></cell><cell>'Only last week, another boy turned up in Detroit,</cell><cell></cell></row><row><cell>M</cell><cell>been missing longer than Larry,</cell><cell>angry</cell></row><row><cell></cell><cell>you read it yourself, '</cell><cell></cell></row><row><cell>F</cell><cell>"You've got to believe. You've got to-"</cell><cell>sad</cell></row><row><cell>M</cell><cell>"What do you mean me above all? Look at you. You're shaking!"</cell><cell>angry</cell></row><row><cell>F</cell><cell>"I can't help it!"</cell><cell>angry</cell></row><row><cell>M</cell><cell>'What have I got to hide? What the hell is the matter with you, Kate?'</cell><cell>angry</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Dialogue utterances corresponding to plot shown inFigure 5.</figDesc><table><row><cell>Modalities</cell><cell>IEMOCAP-4way</cell><cell>IEMOCAP-6way</cell></row><row><cell></cell><cell>F1 Score (%)</cell><cell>F1 Score (%)</cell></row><row><cell>a</cell><cell>63.58</cell><cell>47.57</cell></row><row><cell>t</cell><cell>81.55</cell><cell>66.00</cell></row><row><cell>v</cell><cell>43.85</cell><cell>37.58</cell></row><row><cell>at</cell><cell>81.59</cell><cell>65.42</cell></row><row><cell>av</cell><cell>64.48</cell><cell>52.20</cell></row><row><cell>tv</cell><cell>81.52</cell><cell>62.19</cell></row><row><cell>atv</cell><cell>84.50</cell><cell>67.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">: Results on IEMOCAP-4way and IEMOCAP-</cell></row><row><cell cols="2">6way datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>happiness</cell><cell>112</cell><cell>5</cell><cell>27</cell><cell>0</cell><cell>300</cell></row><row><cell>True Label</cell><cell>sadness neutral</cell><cell>22 3</cell><cell>22 217</cell><cell>320 19</cell><cell>20 6</cell><cell>100 200</cell></row><row><cell></cell><cell>anger</cell><cell>0</cell><cell>13</cell><cell>13</cell><cell>144</cell><cell>0</cell></row><row><cell></cell><cell cols="2">h a p p i n e s s</cell><cell cols="2">s a d n e s s Predicted Label n e u t r a l</cell><cell>a n g e r</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">th , 10 th and 11 th utterances during inference.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We would like to thank reviewers for their insightful comments. This research is supported by SERB India (Science and Engineering Board) Research Grant number SRG/2021/000768.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Hyperparameter Setting</head><p>Hyperparameters used to train our model are described in <ref type="table">Table 8</ref> for IEMOCAP (4-way and 6way) and <ref type="table">Table 9</ref> for MOSEI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout</head><p>GNNHead SeqContext ILR 0.1 7 4 1e-4</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextualized emotion recognition in conversation as sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH 2010</title>
		<meeting>INTERSPEECH 2010<address><addrLine>Makuhari, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2362" to="2365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory fusion network for multiview sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI&apos;18/IAAI&apos;18/EAAI&apos;18</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI&apos;18/IAAI&apos;18/EAAI&apos;18</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/752</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="page" from="5415" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
