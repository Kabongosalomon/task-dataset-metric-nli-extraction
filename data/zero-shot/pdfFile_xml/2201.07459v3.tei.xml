<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PT4AL: Using Self-Supervised Pretext Tasks for Active Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Seon Keun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>1?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Seo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>4?</roleName><forename type="first">Jongchan</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Geol</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology 1</orgName>
								<address>
									<addrLine>SI Analytics 2, Lunit Inc 3</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hanbat National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PT4AL: Using Self-Supervised Pretext Tasks for Active Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active Learning</term>
					<term>Self-supervised Learning</term>
					<term>Pretext Task</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Labeling a large set of data is expensive. Active learning aims to tackle this problem by asking to annotate only the most informative data from the unlabeled set. We propose a novel active learning approach that utilizes self-supervised pretext tasks and a unique data sampler to select data that are both difficult and representative. We discover that the loss of a simple self-supervised pretext task, such as rotation prediction, is closely correlated to the downstream task loss. Before the active learning iterations, the pretext task learner is trained on the unlabeled set, and the unlabeled data are sorted and split into batches by their pretext task losses. In each active learning iteration, the main task model is used to sample the most uncertain data in a batch to be annotated. We evaluate our method on various image classification and segmentation benchmarks and achieve compelling performances on CIFAR10, Caltech-101, ImageNet, and Cityscapes. We further show that our method performs well on imbalanced datasets, and can be an effective solution to the cold-start problem where active learning performance is affected by the randomly sampled initial labeled set. Code is available at https://github.com/johnsk95/PT4AL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent success in deep learning has shown remarkable advancements in computer vision tasks such as classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref> and semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>. This has been possible due to the advent of deep convolutional neural networks (CNNs) and large annotated datasets such as ImageNet <ref type="bibr" target="#b11">[12]</ref> and COCO <ref type="bibr" target="#b26">[27]</ref>.</p><p>As deep learning models are trained in a data-driven manner, having a large enough training set is crucial to achieve high performance. However, building a large labeled dataset is prohibitively time-consuming and expensive. Labeling costs increase with the size of data and complexity of the tasks. Instead of labeling the entire data, active learning (AL) <ref type="bibr" target="#b38">[39]</ref> aims to select informative subsets to label that achieve the highest performance within a fixed labeling budget. Main Task Learner <ref type="figure">Fig. 1</ref>: The overall framework of the proposed method. Unlabeled data are sorted by pretext task losses, split into batches, and sampled for training</p><p>Existing AL approaches can be divided into two main groups: distributionbased and uncertainty-based methods. Distribution-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref> aim to sample data that well covers the distribution of the feature space. The advantage of such methods is that they can sample representative points: data points from high density regions that well represent the overall feature distribution. However, distribution-based sampling fails to select data that are placed near the decision boundary (i.e. high uncertainty data points). Uncertainty-based approaches <ref type="bibr" target="#b25">[26]</ref> resolve this problem by sampling the most uncertain points. Simple approaches that utilize class posterior probabilities <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>, entropy <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b20">21]</ref>, and loss prediction <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b22">23]</ref> were revealed to perform well on various settings. While these approaches effectively sample uncertain or difficult data near decision boundaries in the feature space, they do not capture the overall distribution of the data, according to our qualitative analysis in <ref type="figure">Fig. 4</ref>. Our method aims to capture the best of both worlds by sampling both representative and difficult data. This paper proposes Pretext Tasks for Active Learning (PT4AL), a novel active learning framework that utilizes self-supervised pretext tasks combined with an uncertainty-based sampler. We train a pretext task model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref> with unlabeled data, and the pretext task loss is highly correlated to the main task loss. In order to sample diversely from both representative and difficult data, the unlabeled data are sorted in descending order by their pretext task loss, and split into batches to be used for each AL iteration. Starting from the batch containing data with the highest losses, the most uncertain K data points are sampled from each batch, based on the posterior class probability of the previous main task learner. The uncertainty-based sampler enables PT4AL to sample difficult data, while the batch split allows balanced sampling across the entire data distribution.</p><p>PT4AL also resolves the innate problem in active learning: the cold start problem. Existing approaches start from a randomly sampled set of labeled data, rendering the overall performance highly dependent on the distribution of the initial set. Since our method learns the representation of the unlabeled set in advance, we can sample informative data from the first iteration. This approach avoids the issue of high variance and decrease in performance that can stem from randomly sampling the initial labeled set.</p><p>We validate our proposed method on various image classification and semantic segmentation datasets and achieve state-of-the-art or compelling results across different datasets and tasks. Additionally, we demonstrate the robustness of PT4AL on a class imbalanced setting by evaluating on an artificially created class-imbalanced CIFAR10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Active Learning Various AL approaches has been proposed, such as information theoretical approaches <ref type="bibr" target="#b31">[32]</ref>, ensemble approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15]</ref>, uncertainty based methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b20">21]</ref> and Bayesian AL methods <ref type="bibr" target="#b21">[22]</ref>. However, these traditional methods have not been verified in large-scale datasets for large-scale models, such as in the field of CNN-based deep learning, which has achieved state-ofthe-art in various computer vision tasks.</p><p>Recent AL methods have been centered on large-scale settings for CNNbased deep learning models. Sener &amp; Savarese <ref type="bibr" target="#b37">[38]</ref> proposed a core-set selection method, which chooses data points that cover all data with high diversity based on the feature distribution. This method targets two problems of the previous uncertainty-based methods. First, uncertainty-based methods select only hard samples, resulting in redundant, overlapping data points. Second, the existing methods are not suitable for batch processing on CNNs. The core-set algorithm aims to sample diverse data points in a batch manner. Yoo &amp; Kweon <ref type="bibr" target="#b45">[46]</ref> proposed a sub-task module to predict the main task loss of unlabeled data, and sample the high-loss samples from the unlabeled pool. This method samples from a subset of the unlabeled pool to avoid selecting redundant data points when sampling consecutively from the most uncertain data <ref type="bibr" target="#b3">[4]</ref>. However, in our qualitative analysis in <ref type="figure">Fig. 4</ref>, uncertainty-based methods like Yoo &amp; Kweon sample data points from decision boundaries with less diversity in distribution. Recently, using a variational autoencoder architecture <ref type="bibr" target="#b40">[41]</ref>, the discriminator adversarially trains the input data to be unlabeled or labeled. In the data sampling phase, a method that first labels the sample predicted as unlabeled with the lowest confidence was proposed.</p><p>Our active learning method uses a self-supervised pretext task to supplement the flaws of the data distribution-based method and the uncertainty-based method. As described above, AL is largely divided into data distribution-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref> and uncertainty-based methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b9">10]</ref>. The data distribution-based method has the disadvantage that it cannot extract hard samples, and the uncertainty-based method has the possibility to sample overlapping data points and it is difficult to extract the representation of the entire data distribution. Other works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b0">1]</ref> sample from both representative and difficult data by utilizing variance maximization between labeled and unlabeled data or using separate sampling criteria for data in each category. Our method uses pretext task-based batch split which allows us to select representative samples across the semantic distribution, and an uncertainty-based in-batch sampler which allows us to select difficult samples.</p><p>Representation Learning with Pretext Tasks Representation learning aims to learn good pre-trained weights by learning self-supervised pretext tasks with unlabeled data. The pre-trained weights are fine-tuned with a small amount of labeled data to achieve high performance on downstream tasks. The key assumption and the findings in representation learning is that pretext tasks provide enough learning signals without any labels (i.e. direct supervision) provided. Using these assumptions, Liu et al. <ref type="bibr" target="#b27">[28]</ref> proposed unsupervised neural architecture search (NAS) using self-supervised pretext tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16]</ref> and achieved similar performance to supervised NAS baselines. Zhang et al. <ref type="bibr" target="#b47">[48]</ref> proposed a pretext task to restore the color of the original image through a network after transforming the input image to gray scale. Noroozi &amp; Favaro <ref type="bibr" target="#b33">[34]</ref> improved the performance of representation learning in image classification through the task of dividing input images into grids, mixing them with each other, and inputting each grid into the network. Gidaris et al. <ref type="bibr" target="#b15">[16]</ref> proposed a pretext task that rotates the input image by 0 ? , 90 ? , 180 ? , and 270 ? and training the network to match the rotated angle of the transformed input image. This method achieved the highest performance among representation learning methods utilizing data structures. Recently proposed representation learning methods use contrastive learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> to minimize the distance between different pairwise augmentations of the same image, and repel from augmentations of different images. Contrastive learning is proved to be robust on different downstream tasks and provide state-of-the-art results by far.</p><p>There have been several efforts to use self-supervised pretext tasks in active learning. Zhu et al. <ref type="bibr" target="#b48">[49]</ref> uses graph contrastive learning <ref type="bibr" target="#b46">[47]</ref> for active learning on graph neural networks. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17]</ref> utilizes self-supervised learning to pre-train the main task model, which is then fine-tuned on labeled data. Bhatnagar et al. <ref type="bibr" target="#b2">[3]</ref> presents a multi-task active learner trained for both pretext task and main task, while being robust to mislabeled samples. Although these methods help justify the use of pretext tasks in active learning, they are limited to specific domains <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17]</ref>, fail to sample both difficult and representative data <ref type="bibr" target="#b1">[2]</ref>, and does not solve the cold start problem <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>As pretext tasks provide good initializations for downstream tasks, we assume that the information learned through these tasks is highly correlated to the semantic data distribution. We analyze and identify the correlation between the pretext task loss and the supervised loss in downstream tasks in Section 3. Finally, we propose an active learning method using pretext tasks in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Using Pretext Tasks for Active Learning</head><p>The success of representation learning with self-supervised pretext tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>, leads us to believe that there is a high correlation between self-supervised pretext tasks and downstream tasks, and thus pretext tasks can be utilized for active learning. Rather than utilizing the feature distribution after the pretext task training, we resort to a simpler metric for active learning -the pretext task loss. In this section, we propose and validate a hypothesis, and use these evidences to formulate our AL algorithm. Our hypothesis is that:</p><p>H1: Pretext task loss is correlated with the main task loss. We think that if a pretext task is correlated or representative of the main task, images that are hard (i.e. having high loss values) for the pretext task will also be hard for the main task. <ref type="figure" target="#fig_1">Fig. 2</ref> presents scatter plots of the pretext task loss and the main task loss in three benchmark datasets. The x-axis is the normalized rank of the main task loss, and the y-axis is the normalized rank of the pretext task loss. The pretext task and the main task are independently trained with the training set, and the losses are computed in the test set. For ease of interpretation, we visualized 1,000 random samples on the plots. Spearman's rank correlation <ref type="bibr" target="#b41">[42]</ref> denoted as ? is calculated on the full test set.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the pretext and main task losses have a strong positive correlation. That is, if a data sample has high loss for a pretext task, it is likely for it to have high loss for the main task, and vice versa. We observe high ? values for all three datasets: CIFAR10 (? = 0.79), Caltech-101 (? = 0.78), and ImageNet (? = 0.88). Note that these datasets vary in image size, number of classes, and class balance. The strong correlation between the pretext task loss and the main task loss across diverse datasets validates our hypothesis, and thus is a strong evidence for using pretext task losses for active learning. However, there is one caveat to the hypotheses: methods that use contrastive loss as the pretext task <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> do not have a strong loss correlation. (? = ?0.001 for SimSiam) We contribute this result to two main reasons: class bias of the contrastive loss and strong reliance to augmentations. Details are explained in the supplementary material. Even if we could find a way to achieve close correspondence with the main loss, we decide not to use contrastive methods since the large batch size and long training time generally required for these methods beat our purpose of a simple and quick AL model. Details are explained in the supplementary material.</p><p>Throughout this work, we validate the efficacy of PT4AL with 4 different pretext tasks: Rotation prediction <ref type="bibr" target="#b15">[16]</ref>, colorization <ref type="bibr" target="#b47">[48]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b33">[34]</ref>, and SimSiam <ref type="bibr" target="#b8">[9]</ref>. We compare and analyze the efficacy of different pretext tasks on classification and semantic segmentation in Section 6.2. Since rotation pre-diction <ref type="bibr" target="#b15">[16]</ref> performs best in CIFAR10 and colorization <ref type="bibr" target="#b47">[48]</ref> performs the best in Cityscapes, we use rotation prediction for image classification main tasks, and colorization for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we introduce the specifics of PT4AL. First, we provide a brief overview of our active learning algorithm. Then we provide details of the pretext task learning for batch split and in-batch sampling in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>In a typical active learning scenario, we are initially provided with a pool of unlabeled data x U ? X U . The objective of AL is to achieve the best performance in the main task model F m (?) with a limited amount of labeled data. In specific, we follow the batch mode active learning scheme: in the i-th AL iteration, we select K samples from</p><formula xml:id="formula_0">X i U , add them into labeled pool (X i L , Y i L ) with oracle, train and evaluate F i m (?) with (X i L , Y i L ).</formula><p>The iterations are repeated until the specified labeling budget is reached.</p><p>The overall framework of PT4AL is illustrated in <ref type="figure">Fig. 1</ref>. PT4AL is split into two parts: pretext task learning for batch split and in-batch sampling. Pretext task learning is done prior to the AL iterations. We train a pretext task learner with X U . The unlabeled samples are sorted in descending order of their pretext task losses, and split into batches. The in-batch sampling is done at each AL iteration. At the i-th iteration, the sampling module selects K samples from the i-th batch, according to the uncertainty of the main task learner in these samples. The main task learner F i m (?) is trained with (X i L , Y i L ) and evaluated on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pretext Task Learning for Batch Split</head><p>In this section, we explain how a pretext task is used for active learning batch split. The term batch refers to a pool of unlabeled data to be sampled in an AL iteration. While any pretext task can be used in our method, we use the widely used rotation prediction task <ref type="bibr" target="#b15">[16]</ref> for the explanation. For the rotation prediction task, the backbone neural network <ref type="bibr" target="#b18">[19]</ref> is trained on all four orientations (0 ? , 90 ? , 180 ? , 270 ? degrees) of the input image. The loss function is defined as the average of the losses for each orientation:</p><formula xml:id="formula_1">loss(x i , ? p ) = 1 k k y=1 L CE (F p (g(x i | y) | ? p ), y)<label>(1)</label></formula><p>Where L CE is the cross-entropy loss. The rotation operator g(? | y) yields the rotated input image according to the orientation label y. We define k = 4 since we predict four different rotations. F p represents the probability distribution of the input image rotated by label y. Note that the rotation label y is unknown to F p . In inference, four orientations of each image is fed into the trained network F p and the extracted loss is the same averaged loss loss(x i , ? p ) used in training.</p><p>F p is trained and tested on the same unlabeled set X U . The model weights ? p with the best test accuracy is used for loss extraction. After training the pretext task learner, we extract pretext task loss values from X U and split them into batches. Given the pretext task loss values of the unlabeled data loss X U in the pretext task learning phase, we first sort the losses in descending order. The sorted data X U is then divided into I batches of equal size. The number of I is equal to the number of AL iterations: if there are ten iterations(I = 10), there will be ten batches</p><formula xml:id="formula_2">B = {b i } I=10 i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">In-batch Sampling</head><p>The in-batch sampler selects K samples at each AL iteration. At the i-th iteration, the in-batch sampler ?(?) selects K samples from the i-th batch to be annotated by the oracle. The sampler computes the top-1 posterior probability in the given batch using the previous main task learner F i?1 m , and K data points with the lowest confidence scores are selected. In the first iteration, K points are sampled from the first batch b 0 at even intervals. Equation 2 summarizes the sampler ?(?). The sampling makes use of the main task model from the previous iteration,</p><formula xml:id="formula_3">F i?1 m . ?(b i , F i?1 m ) = min K {max(F i?1 m (b i | ? m ))}<label>(2)</label></formula><p>Algorithm 1 illustrates our overall sampling algorithm including batch splitting and in-batch sampling. In the first iteration when we do not have F 0 m , we uniformly select samples in the first batch, based on our empirical observation that visually similar samples have similar pretext task loss values. Sampled data have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sampling Strategy</head><p>Input: Unlabeled XU , labeled XL, pretext task losses lossX U , main task model Fm XU = sort(lossX U ) ? Sort losses in descending order Split XU into batches B for bi in B do if i == 1, XK = unif orm(bi, loss i X U ) ? For the first batch, uniformly sample else, XK = ?(bi, F i?1 m ) ? For other batches, sample top-K uncertain data XU ? XU ? X k ? Remove from unlabeled pool XL ? XL ? X k ? Add to labeled pool train F i m with XL end for two main traits: difficult and representative. Difficult or uncertain data refers to data that the main task model cannot easily distinguish because it is near a decision boundary. Conversely, representative data well defines the distribution in the feature space. Our intuition is that if we can sample data from both categories, we can form a labeled pool with the most informative data. This is empirically verified through query analysis in Section 5. Our batch split method combined with the sampler samples both representative and difficult data. Our method is much simpler and well performing compared to previous works that sample data from both traits <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the efficacy of our method on two commonly used visual recognition tasks: image classification and semantic segmentation. We choose CI-FAR10 <ref type="bibr" target="#b23">[24]</ref>, Caltech-101 <ref type="bibr" target="#b13">[14]</ref>, ImageNet <ref type="bibr" target="#b11">[12]</ref> benchmarks for image classification, and Cityscapes <ref type="bibr" target="#b10">[11]</ref> for semantic segmentation. To further demonstrate our method's efficacy in a more challenging class-imbalanced setting, we additionally use a class-imbalanced version of CIFAR10. Finally, we show the use of PT4AL as an effective solution to the cold start problem. Unless otherwise specified, all the experiment results are reproduced by ourselves, averaged over multiple runs with different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Classification</head><p>Dataset We perform experiments on three image classification datasets with varying size and number of classes. CIFAR10 contains 50,000 training and 10,000 testing images of size 32 ? 32 with 10 object categories. We start with 1,000 labeled images, and 1,000 images are added for each iteration. Caltech-101 has 9,144 images of size around 300 ? 200 distributed around 101 classes. We divide the data into 8,046 for training and 1,098 for testing. Similar to CIFAR10 we also start with 1,000 labeled images with increments of 1,000 per iteration. Ima-geNet consists of over 1.3M images of 1,000 classes. 1,279,867 and 49,950 images are used for the training and testing set. For ease of experimentation and to avoid noise from similar class labels, ImageNet classes are reduced to 67 based on the WordNet <ref type="bibr" target="#b35">[36]</ref> superclasses. ImageNet starts with K ? 128, 000 labeled samples, and the same K samples are selected for each iteration. Due to heavy computation, each ImageNet performance is the average of 3 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and implementation details</head><p>We compare PT4AL with random sampling, Core-Set <ref type="bibr" target="#b37">[38]</ref>, Variational Adversarial Active Learning (VAAL) <ref type="bibr" target="#b40">[41]</ref>, Learning Loss <ref type="bibr" target="#b45">[46]</ref>, CoreGCN <ref type="bibr" target="#b4">[5]</ref>, and PAL <ref type="bibr" target="#b2">[3]</ref>. For CIFAR10 we add "Learning loss(detached)", where the loss prediction task is detached during supervised learning to avoid influences from multi-task learning. ResNet-18 <ref type="bibr" target="#b18">[19]</ref> is used as the backbone network for the pretext task and the main task learner. The final linear layer of the pretext task learner is converted to (512,4) to account for the four orientations of the rotation task. For Caltech-101 and ImageNet, input images are resized into 224 ? 224. No data augmentation is applied in the pretext task learning phase. Random resized crop and horizontal flip is applied in Results <ref type="figure" target="#fig_2">Fig. 3a</ref> demonstrates the results for CIFAR10. PT4AL clearly outperforms other methods across all AL iterations by a noticeable margin. The accuracy of PT4AL in the final iteration of 10,000 labeled points is 95.13% (? 8.91%), while the second-best performing learning loss scores 89.93% (? 3.71%). Note that detached learning loss <ref type="bibr" target="#b45">[46]</ref> performs significantly worse than the original multi-task learning approach, where the main task model is simultaneously trained with auxiliary tasks. The significant drop in performance due to the detachment indicates that the multi-task approaches <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5]</ref> may benefit from multi-task learning. To strictly measure the benefit of AL to select informative samples, we need to compare the detached setting across all methods. Our method also has a significant advantage from the first iteration, achieving an accuracy of 55.83% (? 9.81%) compared to the other methods' 46.02%. This emphasizes the advantage of PT4AL sampling informative points in the first iteration, instead of random sampling in other AL frameworks. Further details of PT4AL solving the cold-start problem is described in Section 5.4. Similar results can also be observed in Caltech-101 and ImageNet, in <ref type="figure" target="#fig_2">Fig. 3b</ref> and <ref type="figure" target="#fig_2">Fig. 3c</ref>. Our method outperforms other methods across most of the iterations with a considerable advantage from the start. <ref type="figure">Fig. 4</ref> illustrates t-SNE <ref type="bibr" target="#b30">[31]</ref> embeddings of the CIFAR10 data points sampled by random, learning loss <ref type="bibr" target="#b45">[46]</ref> and ours. For a fair comparison, we use embeddings extracted from a ResNet-18 model trained with fully labeled CIFAR10. To visualize the sampled data in different methods across the AL iterations, each of the 1,000 samples from the first iteration are marked in circle, fifth iteration in triangle, and tenth (last) iteration as square. <ref type="figure">Fig. 4a</ref> shows that random sampling queries evenly from the embedding space, but fails to sample difficult data points along the decision boundaries. As shown in <ref type="figure">Fig. 4b</ref>, learning loss <ref type="bibr" target="#b45">[46]</ref> has most of its queries concentrated on the border regions. While this may be effective for the labeled classifier to learn difficult points, it does not query points that represent the classes well. <ref type="figure">Fig. 4c</ref> shows that PT4AL queries from both difficult and representative regions. The sampled points are either concentrated on the class boundaries or evenly located in the class distributions. Since PT4AL initially samples from batches with higher pretext loss values, selected points from the first iteration are concentrated on the decision boundaries of the embedding space. As the sampler progresses to batches with lower loss values, we can see that the sampled points propagate to the remaining regions of the class clusters. Such sampling behavior is a mix of both distribution and uncertainty-based methods, mitigating their flaws while sampling both difficult and representative data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semantic Segmentation</head><p>Dataset We choose Cityscapes <ref type="bibr" target="#b10">[11]</ref>, a public benchmark dataset widely used in semantic segmentation. The dataset consists of 2,975 training and 500 validation images. At each AL iteration, 100 images are sampled for the labeled training set. The original training set is set as the unlabeled set.</p><p>Baselines and Implementation Details We choose the state-of-the-art active learning methods for this experiment: Core-Set <ref type="bibr" target="#b37">[38]</ref>, Learning loss <ref type="bibr" target="#b45">[46]</ref>, VAAL <ref type="bibr" target="#b40">[41]</ref>, TA-VAAL <ref type="bibr" target="#b22">[23]</ref>, and PAL <ref type="bibr" target="#b2">[3]</ref>. We choose a widely-used semantic segmentation architecture, DeepLab <ref type="bibr" target="#b6">[7]</ref> with a ResNet-101 <ref type="bibr" target="#b18">[19]</ref> backbone. The model is initialized with ImageNet <ref type="bibr" target="#b12">[13]</ref> pre-trained weights. The input images are resized to (1024,512) and no data augmentation is applied. The training batch size is 1, and all hyper-parameters follow the original paper <ref type="bibr" target="#b6">[7]</ref>, unless otherwise specified.</p><p>Results <ref type="figure">Fig. 5a</ref> demonstrates that PT4AL outperforms all other methods by a noticeable margin across all iterations. The performance improvement at the first iteration is also significant, showing that PT4AL is an effective solution for the cold start problem. Note that learning loss <ref type="bibr" target="#b45">[46]</ref>, VAAL <ref type="bibr" target="#b40">[41]</ref> and Core-Set <ref type="bibr" target="#b37">[38]</ref> are not as effective as in CIFAR10, sometimes being worse or on-par with the random selection baseline. One possible reason for the universal effectiveness of PT4AL across tasks is that the nature of PT4AL can dynamically change with respect to the pretext task being used. A more detailed analysis among pretext tasks is described in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Image Classification on an Imbalanced Dataset</head><p>Dataset &amp; Experiment details To evaluate the efficacy of PT4AL on a more challenging class-imbalanced setting, we recompose the CIFAR10 dataset. The number of images for each class is as follows: airplane-500, automobile-1,000, bird-1,500, cat-2,000, deer-2,500, dog-3,000, frog-3,500, horse-4,000, ship-4,500 and truck-5,000. All implementation details are identical to the balanced CI-FAR10 described in 5.1, except for dataset composition.</p><p>Results <ref type="figure">Fig. 5b</ref> demonstrates the performances of PT4AL and other baselines on imbalanced CIFAR10. PT4AL outperforms other baselines across all iterations by a large margin. In the pretext task, data from classes with little training data generally have high loss, and classes that have abundant training data generally have low loss values. Since PT4AL samples from data batches with high to low loss, it can sample in a class-balanced way even in imbalanced settings. Also, unlike other methods using only the main task model related metrics, PT4AL utilizes the pretext task loss which is completely independent from the main task model. Interestingly, unlike the experiment results in balanced CIFAR10, data distribution-based AL methods (Core-Set, CoreGCN) obtains higher performance than the uncertainty-based methods (VAAL, learning loss). These results empirically show that uncertainty-based methods are more negatively affected by the class-imbalanced setting than the distribution-based methods. PT4AL outperforms other methods by a margin, showing robustness on a more challenging class-imbalanced setting. Furthermore, we observe that PT4AL samples data in a more class-balanced way. Details on the class distribution of the sampled data are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cold Start Problem in Active Learning</head><p>Since most AL approaches require a trained main task model, the first AL iteration starts with randomly selected labeled data. This is what we call the cold start problem in active learning. To thoroughly validate the efficacy of our method as a solution to the cold start problem, we take a closer look into the first AL iteration in the CIFAR10 benchmark. Note that all other methods use random selection for the first iteration. For PT4AL, after training the pretext task learner, the unlabeled data are sorted by pretext task loss in a descending order, split into 10 batches, and 1,000 data points are uniformly selected from the first batch. The experiment is repeated 20 times with different random seeds. All implementation details are identical to Section 5.1. <ref type="table" target="#tab_0">Table 1</ref> summarizes the experiment results. PT4AL displays more stable performance compared with random sampling in the first iteration, as the standard deviation is smaller, and the gap between the max/min accuracy is smaller than that of the random baseline. PT4AL significantly outperforms random in the average accuracy, indicating that more informative data points are sampled for the main task model. These results indicate that PT4AL is a good solution to the cold start problem, and can be used as a good starting point for existing AL methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5]</ref>. More details are in the supplement material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Computational Overheads</head><p>As described in Section 4, the extra computation for PT4AL, apart from the main task model training, is the pretext task learning for batch split, and the unlabeled data inference for uncertainty measurement in in-batch sampling. To fairly compare the computational overheads of different approaches, we measure the wall-clock time of the methods compared in CIFAR10 experiment under the same environment. In <ref type="figure" target="#fig_2">Fig. 3a</ref> and <ref type="table" target="#tab_1">Table 2</ref>, we can observe that PT4AL achieves the best performance while having on-par computational overheads with others. Core-Set <ref type="bibr" target="#b37">[38]</ref> has similar computations as the random selection baseline.    <ref type="figure" target="#fig_4">6</ref>.a shows the ablations results of the two core components of PT4AL. Instead of the pretext task loss, "Sampling only" uses the main task model's entropy. Batches are made by randomly segmenting the unlabeled data. Data in the first iteration are randomly sampled as there is no main task model to begin with. "Pretext only" replaces our proposed sampling method with a naive sampling of high-loss samples or low-loss samples. Compared with PT4AL which uses both heuristics, the two variations display inferior performance. Using both components is imperative to a well-performing model.  <ref type="figure" target="#fig_4">Fig. 6</ref>.c presents active learning performance of PT4AL using different pretext tasks. Rotation prediction <ref type="bibr" target="#b15">[16]</ref>, colorization <ref type="bibr" target="#b47">[48]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b33">[34]</ref>, and SimSiam <ref type="bibr" target="#b8">[9]</ref> are compared in CIFAR10 and Cityscapes benchmarks. The experiment settings are identical to Section 5.1 and Section 5.2. The inferior performance of SimSiam is analyzed in detail in the supplement material. The rotation prediction task shows the best performance in CIFAR10, and the colorization task performs best in Cityscapes. The best performing pretext task differs by the main task. Since rotation prediction is an image-level task and colorization is a pixel-level task, it is intuitive to match rotation prediction with image classification and colorization with segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Pretext Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sampling Strategy</head><p>Sampling in the first iteration In the first iteration, we do not have access to the main task model for uncertainty measurement. Thus, sampling within the first batch resorts to sampling with the pretext task losses. We compare three simple sampling methods in CIFAR10: top-K, random, and uniform. The performances for top-K loss, random, and uniform sampling are 44.65%, 51.88%, and 55.20%, respectively. As the uniform sampling outperforms the other two sampling methods, we choose it as our in-batch sampling method for the first iteration. We observe that the samples with similar loss values are visually similar, indicating overlapping semantic information in the top-K sampling. This observation is also coherent with the best performance of uniform sampling, as it avoids selecting data points with visually too similar data points. More details are in the supplementary material.</p><p>High loss first vs Low loss first batch split We examine two different strategies for batch split: high loss batch first or low loss batch first. High loss batch first method starts the first iteration with the batch containing the highest pretext task losses, then moves to batches with lower losses for consecutive iterations.</p><p>Low loss batch first is in reverse. On the CIFAR10 experiment, the high loss first strategy displays slightly better results with 55.20% accuracy in the first iteration and 95.13% from the last iteration. Low loss first strategy scores 53.47% and 94.59% in the first and last iterations. We attribute the small performance difference between the two batch split methods to a finding in curriculum learning <ref type="bibr" target="#b43">[44]</ref>. Low loss batch first and high loss batch first are analogous to curriculum learning and anti-curriculum learning, respectively. Wu et al. <ref type="bibr" target="#b43">[44]</ref> concludes that curriculum or anti-curriculum is not effective in standard settings, which explains the small performance gap. As the high loss first method performs better across all iterations, we choose it as our batch split method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we introduce PT4AL, a novel active learning method based on pretext tasks. We demonstrate the correlation between pretext tasks and semantic recognition tasks, and utilize the pretext task losses to split unlabeled samples into batches. In the query analysis in Section 5, we show that the batches are scattered across the whole semantic distribution. Combined with the uncertaintybased in-batch sampler, PT4AL samples both difficult and representative data from the unlabeled pool. We thoroughly examine our method on two widely used vision tasks across various datasets. Our method demonstrates compelling results on datasets with varying resolution, scale and class distribution. We also show that PT4AL is an effective solution for the cold start problem. Although our proposed method performs well on different tasks and datasets, performance varies by the pretext task being used and some tasks such as SimSiam <ref type="bibr" target="#b8">[9]</ref> perform poorly. Future research directions may include designing a pretext task that is universal across various recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(From left to right) The loss rank correlation plots for the main task loss and the pretext task loss in CIFAR10, Caltech-101 and ImageNet. The x and y axes represent the normalized rank of the two losses, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of image classification performance on CIFAR10, Caltech-101, ImageNet-67. Best viewed in color the main task phase. The main task is trained for 200 epochs in CIFAR10 and Caltech-101, and 100 epochs in ImageNet. SGD with a multi-stage learning rate is applied. Detailed hyper-parameters are described in the supplement material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>t-SNE visualization of the CIFAR10 dataset for random, learning loss<ref type="bibr" target="#b45">[46]</ref> and PT4AL. Vivid points are sampled for labeling. Best viewed in color(a) Cityscapes (b) Imbalanced CIFAR10 Comparison of cityscapes semantic segmentation and imbalanced cifar10 classification. Best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>(a) Ablation on two components of PT4AL. (b),(c) PT4AL with different pretext tasks on CIFAR10 and Cityscapes 6 Ablation Study 6.1 Ablation on sampling strategy and pretext task loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><label></label><figDesc>Fig. 6.a shows the ablations results of the two core components of PT4AL. Instead of the pretext task loss, "Sampling only" uses the main task model's entropy. Batches are made by randomly segmenting the unlabeled data. Data in the first iteration are randomly sampled as there is no main task model to begin with. "Pretext only" replaces our proposed sampling method with a naive sampling of high-loss samples or low-loss samples. Compared with PT4AL which uses both heuristics, the two variations display inferior performance. Using both components is imperative to a well-performing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>b and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of the first active learning iteration in CIFAR10</figDesc><table><row><cell>Method</cell><cell>Mean accuracy Min / Max</cell></row><row><cell>Random</cell><cell>47.49 ? 3.15% 43.06 / 53.74 %</cell></row><row><cell cols="2">PT4AL(Rotation) 55.20 ? 1.95 % 52.00 / 57.71 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The wall-clock time of each algorithm in the CIFAR10 experiment</figDesc><table><row><cell cols="5">Method Random Selection PT4AL Learning Loss [46] VAAL [41] CoreGCN [5]</cell></row><row><cell>Time</cell><cell>2hr 16min</cell><cell>3hr 36min</cell><cell>2hr 37min</cell><cell>14hr 9min 3hr 48min</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning for probabilistic structured prediction of cuts and matchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reducing label effort: Self-supervised meets active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Bengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1631" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15947</idno>
		<title level="m">Pal: Pretext-based active learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11409</idno>
		<title level="m">Semantic redundancies in image-classification datasets: The 10% you don&apos;t need</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequential graph convolutional network for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caramalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9583" to="9592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mcdal: Maximum classifier discrepancy for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="248" to="255" />
			<date type="published" when="2009" />
			<publisher>Ieee</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selective sampling using the query by committee algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="168" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02794</idno>
		<title level="m">Active learning on a budget: Opposite strategies suit high and low budgets</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active learning by querying informative and representative examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Active learning with gaussian processes for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Task-aware variational adversarial active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8166" to="8175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;94</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Are labels necessary for neural architecture search? In: European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="798" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Influence selection for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9274" to="9283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Information-based objective functions for active data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="590" to="604" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Employing em and pool-based active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallumzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michelizzi</surname></persName>
		</author>
		<title level="m">Wordnet:: Similaritymeasuring the relatedness of concepts. In: AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A simple baseline for lowbudget active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pourahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nooralinejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.12033</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00489</idno>
		<title level="m">Active learning for convolutional neural networks: A core-set approach</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational adversarial active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5972" to="5981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03107</idno>
		<title level="m">When do curricula work</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A variance maximization criterion for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="358" to="370" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning loss for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">When contrastive learning meets active learning: A novel graph active learning paradigm with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16091</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
