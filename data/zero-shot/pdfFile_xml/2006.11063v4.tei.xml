<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dataset for Automatic Summarization of Russian News</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-05">5 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Moscow Institute of Physics and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dataset for Automatic Summarization of Russian News</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-05">5 Oct 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text summarization ? Russian language ? Dataset ? mBART</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic text summarization has been studied in a variety of domains and languages. However, this does not hold for the Russian language. To overcome this issue, we present Gazeta, the first dataset for summarization of Russian news. We describe the properties of this dataset and benchmark several extractive and abstractive models. We demonstrate that the dataset is a valid task for methods of text summarization for Russian. Additionally, we prove the pretrained mBART model to be useful for Russian text summarization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization is the task of creating a shorter version of a document that captures essential information. Methods of automatic text summarization can be extractive or abstractive.</p><p>Extractive methods copy chunks of original documents to form a summary. In this case, the task usually reduces to tagging words or sentences. The resulting summary will be grammatically coherent, especially in the case of sentence copying. However, this is not enough for high-quality summarization as a good summary should paraphrase and generalize an original text.</p><p>Recent advances in the field are usually utilizing abstractive models to get better summaries. These models can generate new words that do not exist in original texts. It allows them to compress text in a better way via sentence fusion and paraphrasing.</p><p>Before the dominance of sequence-to-sequence models <ref type="bibr" target="#b0">[1]</ref>, the most common approach was extractive.</p><p>The approach's design allows us to use classic machine learning methods <ref type="bibr" target="#b1">[2]</ref>, various neural network architectures such as RNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> or Transformers <ref type="bibr" target="#b4">[5]</ref>, and pretrained models such as BERT <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. The approach can still be useful on some datasets, but modern abstractive methods outperform extractive ones on CNN/DailyMail dataset since Pointer-Generators <ref type="bibr" target="#b6">[7]</ref>. Various pretraining tasks such as MLM (masked language model) and NSP (next sentence prediction) used in BERT <ref type="bibr" target="#b5">[6]</ref> or denoising autoencoding used in BART <ref type="bibr" target="#b8">[9]</ref> allow models to incorporate rich language knowledge to understand original documents and generate grammatically correct and reasonable summaries.</p><p>In recent years, many novel text summarization datasets have been revealed. XSum <ref type="bibr" target="#b10">[11]</ref> focuses on very abstractive summaries; Newsroom <ref type="bibr" target="#b11">[12]</ref> has more than a million pairs; Multi-News <ref type="bibr" target="#b12">[13]</ref> reintroduces multi-document summarization. However, datasets for any language other than English are still scarce. For Russian, there are only headline generation datasets such as RIA corpus <ref type="bibr" target="#b13">[14]</ref>. The main aim of this paper is to fix this situation by presenting a Russian summarization dataset and evaluating some of the existing methods on it.</p><p>Moreover, we adapted the mBART [10] model initially used for machine translation to the summarization task. The BART <ref type="bibr" target="#b8">[9]</ref> model was successfully used for text summarization on English datasets, so it is natural for mBART to handle the same task for all trained languages.</p><p>We believe that text summarization is a vital task for many news agencies and news aggregators. It is hard for humans to compose a good summary, so automation in this area will be useful for news editors and readers. Furthermore, text summarization is one of the benchmarks for general natural language understanding models.</p><p>Our contributions are as follows: we introduce the first Russian summarization dataset in the news domain 1 . We benchmark extractive and abstractive methods on this dataset to inspire further work in the area. Finally, we adopt the mBART model to summarize Russian texts, and it achieves the best results of all benchmarked models 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Source</head><p>There are several requirements for a data source. First, we wanted news summaries as most of the datasets in English are in this domain. Second, these summaries should be human-generated. Third, no legal issues should exist with data and its publishing. The last requirement was hard to fulfill as many news agencies have explicit restrictions for publishing their data and tend not to reply to any letters.</p><p>Gazeta.ru was one of the agencies with explicit permission on their website to use their data for non-commercial purposes. Moreover, they have summaries for many of their articles.</p><p>There are also requirements for content of summaries. We do not want summaries to be fully extractive, as it would be a much easier task, and consequently, it would not be a good benchmark for abstractive models.</p><p>We collected texts, dates, URLs, titles, and summaries of all articles from the website's foundation to March 2020. We parsed summaries as the content of a "meta" tag with "description" property. A small percentage of all articles had a summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cleaning</head><p>After the scraping, we did cleaning. We removed summaries with more than 85 words and less than 15 words, texts with more than 1500 words, pairs with less than 30% unigram intersection, and more than 92% unigram intersection. The examples outside these borders contained either fully extractive summaries or not summaries at all. Moreover, we removed all data earlier than the 1st of June 2010 because the meta tag texts were not news summaries. The complete code of a cleaning phase is available online with a raw version of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Statistics</head><p>The resulting dataset consists of 63435 text-summary pairs. To form training, validation, and test datasets, these pairs were sorted by time. We define the first 52400 pairs as the training dataset, the proceeding 5265 pairs as the validation dataset, and the remaining 5770 pairs as the test dataset. It is still essential to randomly shuffle the training dataset before training any models to reduce time bias even more.</p><p>Statistics of the dataset can be seen in <ref type="table" target="#tab_0">Table 1</ref>. Summaries of the training part of the dataset are shorter on average than summaries of validation and test parts. We also provide statistics on lemmatized texts and summaries. We compute normal forms of words using the pymorphy2 [28] 3 package. Numbers in the "Common UL" row show size of an intersection between lemmas' vocabularies of texts and summaries. These numbers are almost similar to numbers in the "Unique lemmas" row of summaries' columns. It means that almost all lemmas of the summaries are presented in original texts. We depict the distribution of tokens counts in texts in <ref type="figure">Figure 1</ref>, and the distribution of tokens counts in summaries is in <ref type="figure">Figure 2</ref>. The training dataset has a smoother distribution of text lengths in comparison with validation and test datasets. It also has an almost symmetrical distribution of summaries' lengths, while validation and test distributions are skewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Documents distribution by count of tokens in a text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Documents distribution by count of tokens in a summary</head><p>To evaluate the dataset's bias towards extractive or abstractive methods, we measured the percentage of novel n-grams in summaries. Results are presented in <ref type="table" target="#tab_1">Table 2</ref> and show that more than 65% of summaries' bi-grams do not exist in original texts. This number decreases to 58% if we consider different word forms and calculate it on lemmatized bi-grams. Although we can not directly compare these numbers with CNN/DailyMail or any other English dataset as this statistic is heavily language-dependent, we should state that it is 53% for CNN/DailyMail and 83% for XSum. From this, we can conclude that the bias towards extractive methods can exist.</p><p>Another way to evaluate the abstractiveness is by calculating metrics of oracle summaries (the term is defined in 3.2). To evaluate all benchmark models, we used ROUGE <ref type="bibr" target="#b21">[22]</ref> metrics. For CNN/DailyMail oracle summaries score 31.2 ROUGE-2-F <ref type="bibr" target="#b7">[8]</ref>, and for our dataset, it is 22.7 ROUGE-2-F. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">BPE</head><p>We extensively utilized byte-pair encoding (BPE) tokenization in most of the described models. For Russian, the models that use BPE tokenization performs better than those that use word tokenization as it enables the use of rich morphology and decreases the number of unknown tokens. The encoding was trained on the training dataset using the sentencepiece <ref type="bibr" target="#b24">[25]</ref> library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Lowercasing</head><p>We lower-cased all texts and summaries in most of our experiments. It is a controversial decision. On the one hand, we reduced vocabulary size and focused on the essential properties of models, but on the other hand, we lost important information for a model to receive. Moreover, if we speak about our summarization system's possible end-users, it is better to generate summaries in the original case.</p><p>We provide a non-lower-cased version of the dataset as the main version for possible future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmark methods</head><p>We used several groups of methods. TextRank <ref type="bibr" target="#b14">[15]</ref> and LexRank <ref type="bibr" target="#b15">[16]</ref> are fully unsupervised extractive summarization methods. Summarunner <ref type="bibr" target="#b3">[4]</ref> is a supervised extractive method. PG <ref type="bibr" target="#b6">[7]</ref>, CopyNet <ref type="bibr" target="#b19">[20]</ref>, mBART <ref type="bibr" target="#b9">[10]</ref> are abstractive summarization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised methods</head><p>This group of methods does not have any access to reference summaries and utilizes only original texts. All of the considered methods in this group extract whole sentences from a text, not separated words.</p><p>TextRank TextRank <ref type="bibr" target="#b14">[15]</ref> is a classic graph-based method for unsupervised text summarization. It splits a text into sentences, calculates a similarity matrix for every distinct pair of them, and applies the PageRank algorithm to obtain final scores for every sentence. After that, it takes the best sentences by the score as a predicted summary. We used TextRank implementation from the summa [17] 4 library. It defines sentence similarity as a function of a count of common words between sentences and lengths of both sentences.</p><p>LexRank Continuous LexRank <ref type="bibr" target="#b15">[16]</ref> can be seen as a modification of the Tex-tRank that utilizes TF-IDF of words to compute sentence similarity as IDF modified cosine similarity. A continuous version uses an original similarity matrix, and a base version performs binary discretization of this matrix by the threshold. We used LexRank implementation from lexrank Python package 5 .</p><p>LSA Latent semantic analysis can be used for text summarization <ref type="bibr" target="#b20">[21]</ref>. It constructs a matrix of terms by sentences with term frequencies, applies singular value decomposition to it, and searches right singular vectors' maximum values. The search represents finding the best sentence describing the k'th topic. We evaluated this method with sumy library 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised extractive methods</head><p>Methods in this group have access to reference summaries, and the task for them is seen as sentences' binary classification. For every sentence in an original text, the algorithm must decide whether to include it in the predicted summary.</p><p>To perform the reduction to this task, we first need to find subsets of original sentences that are most similar to reference summaries. To find these so-called "oracle" summaries, we used a greedy algorithm similar to SummaRunNNer paper <ref type="bibr" target="#b3">[4]</ref> and BertSumExt paper <ref type="bibr" target="#b7">[8]</ref>. The algorithm generates a summary consisting of multiple sentences which maximize the ROUGE-2 score against a reference summary.</p><p>SummaRuNNer SummaRuNNer <ref type="bibr" target="#b3">[4]</ref> is one of the simplest and yet effective neural approaches to extractive summarization. It uses 2-layer hierarchical RNN and positional embeddings to choose a binary label for every sentence. We used our implementation on top of the AllenNLP [19] 7 framework along with Pointer-Generator <ref type="bibr" target="#b6">[7]</ref> implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Abstractive methods</head><p>All of the tested models in this group are based on a sequence-to-sequence framework. Pointer-generator and CopyNet were trained only on our training dataset, and mBART was pretrained on texts of 25 languages extracted from the Common Crawl. We performed no additional pretraining, though it is possible to utilize Russian headline generation datasets here.</p><p>Pointer-generator Pointer-generator <ref type="bibr" target="#b6">[7]</ref> is a modification of a sequence-tosequence RNN model with attention <ref type="bibr" target="#b17">[18]</ref>. The generation phase samples words not only from the vocabulary but from the source text based on attention distribution. Furthermore, the second modification, the coverage mechanism, prevents the model from attending to the same places many times to handle repetition in summaries.</p><p>CopyNet CopyNet <ref type="bibr" target="#b19">[20]</ref> is another variation of sequence-to-sequence RNN model with attention with slightly different copying mechanism. We used the stock implementation from AllenNLP <ref type="bibr" target="#b18">[19]</ref>.</p><p>mBART for summarization BART <ref type="bibr" target="#b8">[9]</ref> and mBART <ref type="bibr" target="#b9">[10]</ref> are sequence-tosequence Transformer models with autoregressive decoder trained on the denoising autoencoding task. Unlike the preceding pretrained models like BERT, they focus on text generation even in the pretraining phase.</p><p>mBART was pretrained on the monolingual corpora for 25 languages, including Russian. In the original paper, it was successfully used for machine translation. BART was used for text summarization, so it is natural to try a pretrained mBART model for Russian summarization.</p><p>We used training and prediction scripts from fairseq <ref type="bibr" target="#b26">[27]</ref> 8 . However, it is possible to convert the model for using it within HuggingFace's Transformers 9 . We had to truncate input for every text to 600 tokens to fit the model in GPU memory. We also used &lt;unk&gt; token instead of language codes to condition mBART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatic evaluation</head><p>We measured the quality of summarization with three sets of automatic metrics: ROUGE <ref type="bibr" target="#b21">[22]</ref>, BLEU <ref type="bibr" target="#b22">[23]</ref>, METEOR <ref type="bibr" target="#b23">[24]</ref>. All of them are used in various text generation tasks and are based on the overlaps of N-grams. ROUGE and ME-TEOR are prevalent in text summarization research, and BLEU is a primary automatic metric in machine translation. BLUE is a precision-based metric and does not take recall into account, while ROUGE uses both recall and precisionbased metrics in a balanced way, and METEOR weight for the recall part is higher than weight for the precision part.</p><p>All three sets of metrics are not perfect as we only have only one version of a reference summary for each text, while it is possible to generate many correct summaries for a given text. Some of these summaries can even have zero n-gram overlap with reference ones.</p><p>We lower-cased and tokenized reference and predicted summaries with Razdel tokenizer to unify the methodology across all models. We suggest to all further researchers to use the same evaluation script. We provide all the results in <ref type="table" target="#tab_2">Table 3</ref>. Lead-1, lead-2, and lead-3 are the most basic baselines, where we choose the first, the first two, or the first three sentences of every text as our summary. Lead-3 is a strong baseline, as it was in CNN/DailyMail dataset <ref type="bibr" target="#b6">[7]</ref>. The oracle summarization is an upper bound for extractive methods.</p><p>Unsupervised methods give summaries that are very dissimilar to the original ones. LexRank is the best of unsupervised methods in our experiments.</p><p>The SummRuNNer model has the best METEOR score and high BLEU and ROUGE scores. In <ref type="figure" target="#fig_0">Figure 3</ref>, SummaRuNNer has a bias towards the sentences at the beginning of the text compared to the oracle summaries. In contrast, LexRank sentence positions are almost uniformly distributed except for the first sentence.</p><p>It seems that more complex extractive models should perform better on this dataset, but unfortunately, we did not have time to prove it.</p><p>To evaluate an abstractiveness of the model, we used extraction and plagiarism scores <ref type="bibr" target="#b25">[26]</ref>. The plagiarism score is a normalized length of the longest common sequence between a text and a summary. The extraction score is a more sophisticated metric. It computes normalized lengths of all long non-overlapping common sequences between a text and a summary and ensures that the sum of these normalized lengths is between 0 and 1.</p><p>As for abstractive models, mBART has the best result among all the models in terms of ROUGE and BLEU. However, <ref type="figure" target="#fig_1">Figure 4</ref> shows that it has fewer novel n-grams than Pointer-Generator with coverage. Consequently, it has worser extraction and plagiarism scores <ref type="bibr" target="#b25">[26]</ref>  <ref type="table" target="#tab_3">(Table 4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human evaluation</head><p>We also did side-by-side annotation of mBART and human summaries with Yandex.Toloka 11 , a Russian crowdsourcing platform. We sampled 1000 text and summary pairs from the test dataset and generated a new summary for every text. We showed a title, a text, and two possible summaries for every example. Nine people annotated every example. We asked them which summary is better and provided them three options: left summary wins, draw, right summary wins. The side of the human summary was random. Annotators were required to pass training, exam, and their work was continuously evaluated through the control pairs ("honeypots").  <ref type="table" target="#tab_4">Table 5</ref> shows the results of the annotation. There were no full draws, so we exclude them from the table. mBART wins in more than 73% cases. We cannot just conclude that it performs on a superhuman level from these results. We did not ask our annotators to evaluate the abstractiveness of the summaries in any way. Reference summaries are usually too provocative and subjective, while mBART generates highly extractive summaries without any errors and with many essential details, and annotators tend to like it. The annotation task should be changed to evaluate the abstractiveness of the model. Even so, that is an excellent result for mBART. <ref type="table">Table 6</ref> shows examples of mBART losses against reference summaries. In the first example, there is an unnamed entity in the first sentence, "by them" ("???"). In the second example, the factual error and repetition exist. In the last example, the last sentence is not cohesive. <ref type="table">Table 6</ref>. mBART summaries that lost 9/9 ????????????? ??? ????? ????????????? ???????? ???????? ????????????? ??? ??????????? ???????? ????? ?? ????? ????? ?????? ????? ?????????? . ??? ???????? ? ??????? ???????? ???????? ???????????????? ????? ? ??? ????????? ??? . ????????? ?????? ???????? ????? ?? ??????? ? ?????????? ? ??????????????? ???????????? ?????????????? ??????????? ?????? , ??? ?????? ??? ?????????????? ??????? ?? ??? ?????? ?????????? ?? 10 ??? . ?????? ????????? ? ?? 5 ??? . ?????? ?????????? ???? . ?? 50 ??? . ?????? ????? ????? ????????? ?????????? ???? . ????? ? ???????? ???????????? ???????? ????????? ????? ?????????? ???????? ???????????? ?????? ??? . ????? ???????? ????? ????????? , ??????? , ?????????? ????? ???? . ????? ???? , ??????? ?????? ???? ??????????? ?? ???????????????? ? ????? ?????? ????? ? ????? ?????? ???????? ????? ?? ????? ????? , ?????? ?? ???????? ??????????? ? 46-?????? ?????????? ?? ???-????????? ??????????? ????? , ??? ?????? ??????? ? ??????????? .', '?? ???-?????? lacma art + film gala , ????????? ??? ????????? gucci , ????? ????? ???? ????? ??????? ??????? ?? ????????? 20 ??? . ?? ?????? ??????? , ? ???? ????? ????? ????????? ? ???? , ????????? ??? ?????????? ?????????? ????????????? ???????? ???????????? ? ???????????? ?????? ??? ???????? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present the first corpus for text summarization in the Russian language. We demonstrate that most of the text summarization methods work well for Russian without any special modifications. Moreover, mBART performs exceptionally well even if it was not initially designed for text summarization in the Russian language.</p><p>We wanted to extend the dataset using data from other sources, but there were significant legal issues in most cases, as most of the sources explicitly forbid any publishing of their data even in non-commercial purposes.</p><p>In future work, we will pre-train BART ourselves on standard Russian text collections and open news datasets. Furthermore, we will try the headline generation as a pretraining task for this dataset. We believe it will increase the performance of the models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Proportion of extracted sentences according to their position in the original document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Proportion of novel n-grams in model generated summaries on the test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics after lowercasing</figDesc><table><row><cell></cell><cell cols="2">Train</cell><cell cols="2">Validation</cell><cell cols="2">Test</cell></row><row><cell></cell><cell>Text</cell><cell>Summary</cell><cell>Text</cell><cell>Summary</cell><cell>Text</cell><cell>Summary</cell></row><row><cell>Dates</cell><cell cols="2">01.06.10 -31.05.19</cell><cell cols="4">01.06.19 -30.09.19 01.10.19 -23.03.20</cell></row><row><cell>Pairs</cell><cell cols="2">52 400</cell><cell cols="2">5265</cell><cell cols="2">5770</cell></row><row><cell cols="2">Unique words: UW 611 829</cell><cell>148 073</cell><cell>167 612</cell><cell>42 104</cell><cell>175 369</cell><cell>44 169</cell></row><row><cell cols="2">Unique lemmas: UL 282 867</cell><cell>63 351</cell><cell>70 210</cell><cell>19 698</cell><cell>75 214</cell><cell>20 637</cell></row><row><cell>Common UL</cell><cell cols="2">60 992</cell><cell cols="2">19 138</cell><cell cols="2">20 098</cell></row><row><cell>Min words</cell><cell>28</cell><cell>15</cell><cell>191</cell><cell>18</cell><cell>357</cell><cell>18</cell></row><row><cell>Max words</cell><cell>1500</cell><cell>85</cell><cell>1500</cell><cell>85</cell><cell>1498</cell><cell>85</cell></row><row><cell>Avg words</cell><cell>766.5</cell><cell>48.8</cell><cell>772.4</cell><cell>54.5</cell><cell>750.3</cell><cell>53.2</cell></row><row><cell>Avg sentences</cell><cell>37.2</cell><cell>2.7</cell><cell>38.5</cell><cell>3.0</cell><cell>37.0</cell><cell>2.9</cell></row><row><cell>Avg UW</cell><cell>419.1</cell><cell>41.3</cell><cell>424.2</cell><cell>46.0</cell><cell>415.7</cell><cell>45.1</cell></row><row><cell>Avg UL</cell><cell>350.0</cell><cell>40.2</cell><cell>352.5</cell><cell>44.6</cell><cell>345.4</cell><cell>43.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average % of novel n-grams</figDesc><table><row><cell></cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>Uni-grams</cell><cell>34.2</cell><cell>30.5</cell><cell>30.6</cell></row><row><cell>Lemmatized uni-grams</cell><cell>21.4</cell><cell>17.8</cell><cell>17.6</cell></row><row><cell>Bi-grams</cell><cell>68.6</cell><cell>65.0</cell><cell>65.5</cell></row><row><cell>Lemmatized bi-grams</cell><cell>61.4</cell><cell>58.0</cell><cell>58.5</cell></row><row><cell>Tri-grams</cell><cell>84.5</cell><cell>81.5</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Automatic scores for all models on the test set</figDesc><table><row><cell></cell><cell>1</cell><cell>ROUGE 2</cell><cell>L</cell><cell cols="2">BLEU 10 Meteor</cell></row><row><cell>Lead-1</cell><cell>27.6</cell><cell>12.9</cell><cell>20.2</cell><cell>5.3</cell><cell>18.6</cell></row><row><cell>Lead-2</cell><cell>30.6</cell><cell>13.7</cell><cell>25.6</cell><cell>10.9</cell><cell>23.7</cell></row><row><cell>Lead-3</cell><cell>31.0</cell><cell>13.4</cell><cell>26.3</cell><cell>10.8</cell><cell>26.0</cell></row><row><cell>Greedy Oracle</cell><cell>44.3</cell><cell>22.7</cell><cell>39.4</cell><cell>17.7</cell><cell>35.5</cell></row><row><cell>TextRank</cell><cell>21.4</cell><cell>6.3</cell><cell>16.4</cell><cell>3.9</cell><cell>17.5</cell></row><row><cell>LexRank</cell><cell>23.7</cell><cell>7.8</cell><cell>19.9</cell><cell>6.2</cell><cell>18.1</cell></row><row><cell>LSA</cell><cell>19.3</cell><cell>5.0</cell><cell>15.0</cell><cell>3.6</cell><cell>15.2</cell></row><row><cell>SummaRuNNer</cell><cell>31.6</cell><cell>13.7</cell><cell>27.1</cell><cell>11.5</cell><cell>26.0</cell></row><row><cell>CopyNet</cell><cell>28.7</cell><cell>12.3</cell><cell>23.6</cell><cell>8.5</cell><cell>21.0</cell></row><row><cell>PG small</cell><cell>29.4</cell><cell>12.7</cell><cell>24.6</cell><cell>9.0</cell><cell>21.2</cell></row><row><cell>PG words</cell><cell>29.4</cell><cell>12.6</cell><cell>24.4</cell><cell>8.9</cell><cell>20.9</cell></row><row><cell>PG big</cell><cell>29.6</cell><cell>12.8</cell><cell>24.6</cell><cell>9.3</cell><cell>21.5</cell></row><row><cell cols="2">PG small +coverage 30.2</cell><cell>12.9</cell><cell>26.0</cell><cell>10.1</cell><cell>22.7</cell></row><row><cell cols="4">Finetuned mBART 32.1 14.2 27.9</cell><cell>12.4</cell><cell>25.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Extraction scores on the test set</figDesc><table><row><cell></cell><cell cols="2">Extraction score Plagiarism score</cell></row><row><cell>Reference</cell><cell>0.031</cell><cell>0.124</cell></row><row><cell>PG small +coverage</cell><cell>0.325</cell><cell>0.501</cell></row><row><cell>Finetuned mBART</cell><cell>0.332</cell><cell>0.502</cell></row><row><cell>SummaRuNNer</cell><cell>0.513</cell><cell>0.662</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Human side-by-side evaluation</figDesc><table><row><cell cols="3">Votes for winner Reference wins mBART wins</cell></row><row><cell>Majority</cell><cell>265</cell><cell>735</cell></row><row><cell>9/9</cell><cell>7</cell><cell>47</cell></row><row><cell>8/9</cell><cell>18</cell><cell>106</cell></row><row><cell>7/9</cell><cell>30</cell><cell>185</cell></row><row><cell>6/9</cell><cell>54</cell><cell>200</cell></row><row><cell>5/9</cell><cell>123</cell><cell>180</cell></row><row><cell>4/9</cell><cell>32</cell><cell>17</cell></row><row><cell>3/9</cell><cell>1</cell><cell>0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/IlyaGusev/gazeta 2 https://github.com/IlyaGusev/summarus</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/kmike/pymorphy2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/summanlp/textrank 5 https://github.com/crabcamp/lexrank 6 https://github.com/miso-belica/sumy 7 https://github.com/allenai/allennlp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/pytorch/fairseq 9 https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">BLEU scores in earlier versions of the paper were incorrect. They were describing character-level n-grams instead of word-level n-grams. We apologize for the inconvenience.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://toloka.yandex.ru/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extractive Summarization Using Supervised and Semisupervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text Summarization with Pretrained Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4003" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<title level="m">Multilingual Denoising Pre-training for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Don&apos;t Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">NEWSROOM: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Mode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-attentive Model for Headline Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gavrilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kalaidin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Malykh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval. ECIR 2019</title>
		<editor>Azzopardi L., Stein B., Fuhr N., Mayr P., Hauff C., Hiemstra D.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11438</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based Lexical Centrality as Salience in Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Argerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wachenchauzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03606</idno>
		<title level="m">Variations of the Similarity Function of TextRank for Automated Summarization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07640</idno>
		<title level="m">AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generic text summarization using relevance measure and latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diverse beam search for increased novelty in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cibils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hossmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baeriswyl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01457</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Morphological Analyzer and Generator for Russian and Ukrainian Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korobov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Analysis of Images, Social Networks and Texts</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="320" to="332" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
