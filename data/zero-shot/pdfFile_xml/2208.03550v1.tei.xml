<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frozen CLIP Models are Efficient Video Learners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
							<email>zylin@link.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<email>gaopeng@pjlab.org.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Hasso Plattner Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Centre for Perceptual and Interactive Intelligence Limited</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Frozen CLIP Models are Efficient Video Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video recognition</term>
					<term>Efficient learning</term>
					<term>Vision-language model</term>
					<term>Spatiotemporal Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video recognition has been dominated by the end-to-end learning paradigm -first initializing a video recognition model with weights of a pretrained image model and then conducting end-to-end training on videos. This enables the video network to benefit from the pretrained image model. However, this requires substantial computation and memory resources for finetuning on videos and the alternative of directly using pretrained image features without finetuning the image backbone leads to subpar results. Fortunately, recent advances in Contrastive Vision-Language Pre-training (CLIP) pave the way for a new route for visual recognition tasks. Pretrained on large open-vocabulary image-text pair data, these models learn powerful visual representations with rich semantics. In this paper, we present Efficient Video Learning (EVL) -an efficient framework for directly training high-quality video recognition models with frozen CLIP features. Specifically, we employ a lightweight Transformer decoder and learn a query token to dynamically collect frame-level spatial features from the CLIP image encoder. Furthermore, we adopt a local temporal module in each decoder layer to discover temporal clues from adjacent frames and their attention maps. We show that despite being efficient to train with a frozen backbone, our models learn high quality video representations on a variety of video recognition datasets. Code is available at https://github.com/OpenGVLab/efficient-video-recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a fundamental component of video understanding, learning spatiotemporal representations remains an active research area in recent years. Since the beginning of the deep learning era, numerous architectures have been proposed to learn spatiotemporal semantics, such as traditional two-stream networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59]</ref>, 3D convolutional neural networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>, and spatiotemporal Transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b51">52]</ref>. As videos are high-dimensional and exhibit substantial spatiotemporal redundancy, training video recognition models from Pre-trained by Image-text Pairs A cute shiba inu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP Others</head><p>Pre-trained by Images only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Learner</head><p>Initialize for Fine-tuning scratch is highly inefficient and may lead to inferior performance. Intuitively, the semantic meaning of a video snippet is highly correlated with each of its individual frames. Previous studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52]</ref> have shown that the datasets and methodologies for image recognition can benefit video recognition as well. Owing to the close relationship between image and video recognition, as a routine practice, most existing video recognition models take advantage of pretrained image models by using them for initialization and then re-training all parameters for video understanding in an end-to-end manner.</p><p>However, the end-to-end finetuning regime has two major drawbacks. The first is efficiency. Video recognition models are required to process multiple frames simultaneously and are several times larger than their image counterparts in terms of model size. Finetuning the entire image backbone inevitably incurs an enormous computation and memory consumption cost. As a result, this issue limits the adoption and scalability of some of the largest image architectures for video recognition under restricted computational resources. The second issue is known as catastrophic forgetting <ref type="bibr" target="#b33">[34]</ref> in the context of transfer learning. When conducting end-to-end finetuning on downstream video tasks, we risk destroying the powerful visual features learned from image pretraining and obtaining subpar results if the downstream videos are insufficiently informative. Both concerns suggest that end-to-end finetuning from pre-trained image models is not always an ideal choice, which calls for a more efficient learning strategy to transfer knowledge from images to videos.</p><p>Considerable efforts have been made on learning high-quality and general visual representations through contrastive learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref>, masked vision modeling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b1">2]</ref>, and traditional supervised learning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b36">37]</ref>. Masked vision modeling approaches such as MAE <ref type="bibr" target="#b20">[21]</ref> train an encoder-decoder architecture to reconstruct the original image from the latent representation and mask tokens. Supervised learning-based methods train image backbones with a fixed set of predefined category labels. Since they are usually trained uni-modally, they both lack the ability to represent rich semantics. In contrast, contrastive vision-language models such as CLIP <ref type="bibr" target="#b35">[36]</ref> are pretrained with large-scale open-vocabulary image-text pairs. They can learn more powerful visual representations aligned with much richer language semantics. Another advantage of CLIP is its promising feature transferability, which forms a strong foundation for a series of transfer learning methods on various downstream tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The above reasons inspire us to rethink the relationship between image and video features and devise efficient transfer learning methods to make use of frozen CLIP image features for video recognition. To this end, we propose an Efficient Video Learning (EVL) framework based on a lightweight Transformer decoder <ref type="bibr" target="#b44">[45]</ref>. The difference between EVL and other video recognition models is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> Left. Specifically, EVL learns a query token to dynamically gather frame-level spatial features from each layer of the CLIP image encoder. On top of that, we introduce a local temporal module to collect temporal cues with the help of temporal convolution, temporal positional embeddings, and cross-frame attention. Finally, a fully-connected layer is used to predict scores of video categories. We conduct extensive experiments to show the effectiveness of our method and find EVL to be a simple and effective pipeline with higher accuracy but lower training and inference costs, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> Right. Our contributions are as follows:</p><p>-We point out the shortcomings of the current end-to-end learning paradigm for video understanding and propose to leverage frozen CLIP image features to facilitate video recognition tasks. It incurs much shorter training time than end-to-end finetuning, yet achieves competitive performance. This makes video recognition accessible to a broader community with average computation resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Recognition. Recent advances in video recognition can be divided into two major directions -improving model architectures and proposing new training strategies. Following the success of Transformers in image recognition, video recognition has as well seen a transition from 3D-CNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref> to Transformer-based architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref>. Uniformer <ref type="bibr" target="#b27">[28]</ref> is a custom fused CNN-Transformer architecture achieving good speed-accuracy trade-off. Yan et al. <ref type="bibr" target="#b51">[52]</ref> propose a multi-stream Transformer operating on different resolutions with lateral connections. Prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52]</ref> has shown the benefit of image pretraining for Class Score</p><formula xml:id="formula_0">FC Q K K Q V V K V (b) X 1 X 2 Temporal Convolution + Position Embed Multi-head Attention MLP X T-1 X T Q K, V [CLS]</formula><p>Cross-frame Attention video recognition tasks. However, the end-to-end finetuning remains expensive, especially due to the large memory footprint. In terms of new training strategies, pretext task design for self-supervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref> and multi-task co-training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref>   <ref type="bibr" target="#b18">[19]</ref> or low rank decomposition <ref type="bibr" target="#b22">[23]</ref>. A collection of approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57]</ref> train adapters, which are additional fully-connected layers with residual connections, keeping the original weights in the pretrained model fixed. Another line of methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b59">60]</ref> learn prompts, which are additional learnable tokens appended to the input or intermediate feature sequence for task-specific adaption. While these category of methods share the same motivation as ours, we employ Transformer decoders, which is more flexible and also efficient to train, as we will analyze in the Methods section.</p><p>In terms of video recognition, the exploration in efficient transfer learning is still limited. Ju at el. <ref type="bibr" target="#b24">[25]</ref> transfer CLIP models to video recognition by learning prompts and temporal modeling. Wang et al. <ref type="bibr" target="#b46">[47]</ref> utilize CLIP models for video recognition by traditional end-to-end finetuning. We will compare with them in the Experiments section. There are also several works utilizing transferable image features for video-text tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, but these works focus more on cross-modality modeling. In contrast, our work aims to improve the single-modal video representations, which should be complementary to most of the video-text learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>The three primary goals of our image to video transfer learning pipeline are (1) capability to summarize multi-frame features and infer video-level predictions; (2) capability to capture motion information across multiple frames; and (3) efficiency. We thus propose the Efficient Video Learning (EVL) framework, which we detail in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Structure</head><p>The overall structure of EVL, as illustrated in <ref type="figure" target="#fig_3">Fig. 2</ref>, is a multi-layer spatiotemporal Transformer decoder on top of a fixed CLIP backbone. The CLIP backbone extracts features from each frame independently. The frame features are then stacked to form a spatiotemporal feature volume, modulated with temporal information, and fed into the Transformer decoder. The Transformer decoder performs global aggregation of multi-layer features: a video-level classification token [CLS] is learned to act as query, and multiple feature volumes from different backbone blocks are fed to the decoder blocks as key and value. A linear layer projects the output of the last decoder block to class predictions. Formally, the operations of the Transformer decoder can be expressed as follows:</p><formula xml:id="formula_1">Y i = Temp i ([X N ?M +i,1 , X N ?M +i,2 , . . . , X N ?M +i,T ]) ,<label>(1)</label></formula><formula xml:id="formula_2">q i = q i?1 + MHA i q i?1 , Y i , Y i ,<label>(2)</label></formula><formula xml:id="formula_3">q i =q i + MLP i (q i ) , (3) p = FC (q M ) ,<label>(4)</label></formula><p>where X n,t denotes the frame features of the t-th frame extracted from the n-th layer of the CLIP backbone, Y i denotes the temporal modulated feature volume fed into the i-th layer of the Transformer decoder, q i is the progressively refined query token with q 0 as learnable parameters and p is the final prediction.</p><p>N , M denote the number of blocks in the backbone image encoder and the spatiotemporal decoder, respectively. MHA stands for multi-head attention, and the three arguments are the query, key, and value, respectively. Temp is the temporal modelling, which produces feature tokens modulated by more finegrained temporal information, as is elaborated in the next section. The network is optimized as a standard classification model by cross-entropy loss with ground-truth labels, except that the back-propagation stops at image features X and no weight in the image encoder is updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Temporal Cues from Spatial Features</head><p>While CLIP models generate powerful spatial features, they entirely lack temporal information. Despite the Transformer decoder being capable of weighted feature aggregation, which is a form of global temporal information, fine-grained and local temporal signals may also be valuable for video recognition. Hence, we introduce the following temporal modules to encode such information before features are fed into the Transformer decoder. Temporal Convolution. Temporal depthwise convolutions are capable of capturing local feature variations along the temporal dimension, and in known to be efficient and effective <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12]</ref>. Formally the feature encoded by this convolution is written as Y conv , and</p><formula xml:id="formula_4">Y conv (t, h, w, c) = ?t?{?1,0,1} W conv (?t, c) X (t + ?t, h, w, c) + b conv (c) . (5)</formula><p>Temporal Positional Embeddings. We learn a set of T vectors of dimension C, denoted as P ? R T ?C , to serve as temporal positional embedding. Image features are added with one of the vectors according to their temporal position t, or formally</p><formula xml:id="formula_5">Y pos (t, h, w, c) = P (t, c) .<label>(6)</label></formula><p>While temporal convolutions may also capture temporal position information implicitly, positional embeddings are more explicit by making similar features at different time distinguishable. Positional embeddings are also more powerful for long-range temporal modelling, for which multiple convolutional blocks have to be stacked to achieve a large receptive field. Temporal Cross Attention. Another interesting but often overlooked source of temporal information lies in the attention maps. As attention maps reflect feature correspondence, calculating attention maps between two frames naturally reveals object movement information. More specifically, we first construct attention maps between adjacent frames using the original query and key projections in CLIP:</p><formula xml:id="formula_6">A prev (t) = Softmax (QX (t)) T (KX (t ? 1)) , A next (t) = Softmax (QX (t)) T (KX (t + 1)) .<label>(7)</label></formula><p>We omitted the attention heads for simplicity, and average across all heads in our implementation. Then we linearly project it into the feature dimension:</p><formula xml:id="formula_7">Y attn (t, h, w, c) = H h ? =1 W w ? =1 W prev (h ? h ? , w ? w ? , c) A prev (t, h ? , w ? ) + W next (h ? h ? , w ? w ? , c) A next (t, h ? , w ? ) .<label>(8)</label></formula><p>Experiments have shown that, despite the query, key, and input features all being learned from pure 2D image data, such attention maps still provide useful signals.</p><p>The final modulated features are obtained by blending the temporal features with the original spatial features in a residual manner, i.e.</p><formula xml:id="formula_8">Y = X + Y conv + Y pos + Y attn .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity Analysis</head><p>Inference The additional Transformer decoder introduces only a negligible amount of computational overhead given that only one query token is used. To show this, we consider ViT-B/16 as our image backbone, and write out the FLOPS for a Transformer block as follows:</p><formula xml:id="formula_9">FLOPS = 2qC 2 + 2kC 2 + 2qkC + 2?qC 2<label>(9)</label></formula><p>Here, q, k, C, ? stand for the number of query tokens, number of key (value) tokens, number of embedding dimensions, and MLP expansion factor. With this formula, we can roughly compare the FLOPS of an encoder block and decoder block (h, w, t is the feature size along the height, width, temporal dimensions, and we adopt a common choices ? = 4, h = w = 14, C = 768 for estimation):</p><formula xml:id="formula_10">FLOPS dec FLOPS enc ? 2hwtC 2 t(12hwC 2 + 2h 2 w 2 C) ? 1 6<label>(10)</label></formula><p>From this, we can see that a decoder block is much more lightweight compared to an encoder block. Even with a full configuration (one decoder block on every encoder output, no channel reduction and all temporal modules enabled), the FLOPS increase is within 20% of the backbone.</p><p>Training As we use a fixed backbone and a non-intrusive Transformer decoder head (i.e., our inserted module does not change the input of any backbone layer), we can completely avoid back-propagation through the backbone. This vastly reduces both the memory consumption and the time per training iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We benchmark our method on 2 datasets: Kinetics-400 and Something-Something-v2. Extra implementation details are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>In this section we provide a comparison with important baselines from recent work. Comparison with State-of-the-art. Comparisons with recent state-of-the-art video recognition models are provided in <ref type="table" target="#tab_1">Table 1</ref>. While we aim to build a fast transfer learning pipeline, we find our models achieve competitive accuracy among regular video recognition methods. The models listed in <ref type="table" target="#tab_1">Table 1</ref> achieve similar accuracy as ours but require substantially more computation than our method.</p><p>Comparison with CLIP-based Methods. To the best of our knowledge, there are two previous studies that utilize CLIP models for video recognition. As shown in <ref type="table" target="#tab_2">Table 2</ref>, we achieve higher accuracy with fewer frames and a smaller number of new parameters, showing a more efficient use of CLIP.</p><p>Training Time and Reduced Memory. One of the major advantages of our efficient transfer pipeline is the vastly reduced training time. We cite the training time reported in several previous studies in <ref type="table" target="#tab_4">Table 4</ref> for comparison. <ref type="bibr" target="#b0">1</ref> In this case,  powerful pretraining leads to a roughly 10? training time reduction, and our efficient transfer learning scheme leads to a further reduction of about 8?. We also compare training times in an idealized setting in <ref type="table" target="#tab_5">Table 5</ref>: We report single step time (forward + backward + update) using fake data on a single GPU. This bypasses the data loading and distributed communication overhead, which are confounding factors that may be unoptimized and difficult to control.</p><p>Inference Latency and Throughput. Despite our method not being specially optimized for inference speed, we show an important advantage of utilizing largescale pretrained models. Training on small datasets requires injecting hand-crafted inductive biases, which are not necessarily friendly to modern accelerators. On the contrary, ViT models consist almost entirely of standard linear algebra operations. The simplicity of ViT typically enables a higher utilization of hardware resources.</p><p>reproduction, which we find to be a few times smaller than the reported number in their paper (reported value is around 400 hours). Training time of ActionCLIP is estimated by doubling the value for 8-frame variant reported in their paper.  As shown in <ref type="table" target="#tab_3">Table 3</ref>, the latency and throughput are even better than the theoretical FLOPS improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>We provide detailed ablation studies to clarify the effects of each part of our design. Unless otherwise specified, results are obtained using ViT-B/16 backbone, 8 input frames and 3 testing views on Kinetics-400.</p><p>Intermediate Features. We vary the number of features and Transformer decoder layers and present the results in <ref type="table" target="#tab_6">Table 6a and Table 6b</ref>. Utilizing multiple decoder blocks improves the accuracy by 1.0%. Feeding each decoder block with multi-layer intermediate features further improves by 0.8%. Another observation is that features in deeper layers provide more effective features for video recognition.</p><p>Spatiotemporal Features. We find a crucial design to achieve high transfer performance is to use high-resolution, unpooled feature maps. The results are shown in <ref type="table" target="#tab_6">Table 6c</ref>, from which we can see that summarizing along either the temporal or spatial dimension leads to a significant drop in accuracy. We conjecture that this shows the importance of task-specific re-attention, e.g., for human action recognition datasets like Kinetics-400, features relating to the human body are very important, which could be different in the pretraining stage.</p><p>Pretraining Quality. One major factor driving the paradigm shift from finetuned to frozen backbone is the improvement in quality of pretrained models. We show that our method outperforms previous methods that fully finetune the backbone weights given the high quality CLIP backbones in <ref type="table">Table 7</ref>. All models in the table use the same backbone architecture. While on ImageNet-21k pretrained backbones our method lags behind full-finetuning, on CLIP backbones our method outperforms the competitive full-finetuning baselines. Numbers in the marker are numbers of frames per view. Frozen backbone is more efficient when pretraining quality is higher.</p><p>We also find that, despite being designed for a frozen backbone, our model architecture with a finetuned backbone turns out to be a strong fullfinetuning baseline. However, the tendency of higher training efficiency of frozen backbones given high-quality pretrained models remains the same, as shown in <ref type="figure">Fig. 3</ref>. Full-finetuning with our model architecture yields similar efficiency curve on ViT-B/16, but with the larger ViT-L/14, the gap of the training time to reach the same accuracy becomes clear. We point out that even ViT-L/14 is a relatively small pretrained model by modern standards, with about 300M parameters (for comparison, GPT-3 <ref type="bibr" target="#b3">[4]</ref> for natural language processing has 175B parameters, and ViT-G <ref type="bibr" target="#b54">[55]</ref> for computer vision has 1.8B parameters). We believe freezing the backbone may potentially bring further benefits if even larger pretrained models are released in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Temporal Information</head><p>An interesting property of our method is to provide a decomposed approach for video recognition: the spatial information is encoded almost entirely in the fixed, <ref type="table">Table 7</ref>: Results of different pretrained image features. A ViT-B/16 backbone and 8 frames are used unless otherwise specified. We compare with TimeSformer <ref type="bibr" target="#b2">[3]</ref> and ActionCLIP <ref type="bibr" target="#b46">[47]</ref>. Both of them conduct extensive experiments to determine competitive settings for end-to-end training on video datasets. high quality CLIP backbone, while the temporal information is encoded only in the Transformer decoder head. As shown in <ref type="table">Table 8</ref>, temporal modelling exhibits vastly different behaviors on the two datasets: On Kinetics-400,temporal modules bring accuracy gains of less than 0.5%, while on Something-Something-v2, adding the temporal module yields a dramatic +13.8% accuracy gain. This shows a clear difference between temporal information required for the two benchmarks. For Kinetics-400, temporal information is primarily captured in the form of global weighted feature aggregation, as shown in <ref type="table" target="#tab_6">Table 6</ref>. For Something-Something-v2, local temporal features (e.g., object motion, feature variations) are also an important source of signals to achieve strong results. Something-Something-v2 also tend to benefit from deep decoders more than Kinetics-400. As shown in <ref type="table">Table 8b</ref>, Something-Something-v2 benefit from using all 12 decoder blocks, while for Kinetics-400 only around 4 blocks are required (see <ref type="table" target="#tab_6">Table 6a</ref>).</p><p>Finally we provide our main results on Something-Something-v2 dataset in <ref type="table" target="#tab_9">Table 9</ref>. While Something-Something-v2 is a motion-heavy dataset, our lightweight temporal learning module still learns meaningful motion information and reaches mainstream performance (for comparison, a linear probe of CLIP ViT-B/16 achieves only around 20% accuracy). We are also the first CLIP-based method to report results on Something-Something-v2, and we hope this is useful for future reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CLIP-based Models Learn Complementary Knowledge</head><p>Another finding is that knowledge learned by our CLIP-based model is highly complementary to that of regular supervised learning. To show this, we consider an ensemble of our model with supervised models and observe the performance <ref type="table">Table 8</ref>: Effects of temporal information for video recognition.    gain. Ensemble is done by weighted averaging the video-level prediction scores and the average weight ? ? [0, 1] is searched with a coarse granularity of 0.1 on the validation set. As shown in <ref type="table" target="#tab_1">Table 10</ref> and <ref type="table" target="#tab_1">Table 11</ref>, On both Kinetics-400 and Something-Something-v2, we consistently observe more performance gain if CLIP-based models are in the ensemble.</p><p>The implications of these ensemble experiments are two-fold. First, they show that, practically, our CLIP-based models can be used in a two-stream fashion <ref type="bibr" target="#b39">[40]</ref>.Compared to the optical-flow-based second stream in <ref type="bibr" target="#b39">[40]</ref>, a CLIP-based second stream avoids the expensive optical-flow calculation and is much faster to train. Second, the results suggest that there remains knowledge in the dataset that is not captured by our CLIP-based learning paradigm. This shows the potential of CLIP-based models to further improve once more knowledge from the datasets can be utilized.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a new form of pipeline for video action recognition: learning an efficient transfer learning head on top of fixed transferable image features. By freezing the image backbone, the training time is vastly reduced. Moreover, the accuracy loss due to the frozen backbone can be largely compensated by leveraging multi-layer high-resolution intermediate feature maps from the backbone. Thus, our method effectively leverage powerful image features for video recognition, while avoiding the heavy or prohibitive full-finetuning of very large image models. We further show that transferable image features learned in an open-world setting harbor knowledge that is highly complementary to that of labeled datasets, which may inspire more efficient ways to build state-of-the-art video models. We believe our observations have the potential to make video recognition accessible to a broader community, and push video models to a new state-of-the-art in a more efficient manner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>Kinetics-400. Our Kinetics-400 dataset contains 240,436 training videos and 19,787 validation videos. We use 224 spatial input size in all experiments. We sample evenly strided frames for Kinetics-400, and use a stride of <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8</ref> for the 8-, 16-, 32-frame model variants, respectively. We use RandomResized-Crop, RandomHorizontalFlip and RandAugment (as implemented in https: //github.com/facebookresearch/SlowFast) for data augmentation and apply a 0.5 dropout rate in each trainable MLP block and before the final classification head. All models are trained using a batch size of 256 for 50,000 steps with AdamW optimizer. We use a half-period cosine learning rate schedule with initial value of 4 ? 10 ?4 and constant weight decay of 0.05. For testing, we resize the short size of videos to 224 and use 3 temporal crops and the center spatial crop.</p><p>Something-Something-v2. Training on Something-Something-v2 is similar to Kinetics-400, except for the following differences. We use TSN-style sampling for Something-Something-v2, i.e., we divide the video evenly into n segments and select one frame from each -A random frame from each segment is sampled during training and the center frame is used during evaluation. 3 spatial crops are used for testing. We also train for a shorter 30,000 steps on Something-Something-v2. We do not use Kinetics-400 pretraining to initialize models for Something-Something-v2, as we have found the accuracy difference negligible.</p><p>Model Details. By default we use ViT-B/16 with CLIP pretraining as the image backbone. We use decoder blocks with the same configuration as backbone encoder blocks. Unless otherwise specified, for Kinetics-400, we use 4 Transformer decoder blocks taking information from the last 4 blocks of the backbone as key and value. For Something-something v2, as we have found using deeper decoders helps model motion information, we use 12 (for ViT-B) or 24 (for ViT-L) Transformer decoder blocks, taking information from all Transformer encoder blocks in the CLIP backbone.</p><p>Full-finetuning Details. For TimeSformer experiments, we use a training configuration similar to their original implementation, except that training epochs are set to 25 and a 100x learning rate reduction on backbone weights is applied for CLIP-related experiments, as we found these changes lead to higher accuracy. For full-finetuning with our own architecture, we also use a 100x learning rate reduction on backbone weights, and all other training configuration remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on UCF-101 and HMDB-51</head><p>We benchmark our method on two additional datasets: UCF-101 <ref type="bibr" target="#b40">[41]</ref> and HMDB-51 <ref type="bibr" target="#b25">[26]</ref>. We report competitive results even among methods utilizing additional modalities, as shown in <ref type="table" target="#tab_1">Table 12</ref>.</p><p>Implementation Details on UCF-101 and HMDB-51. We finetune from the Kinetics-400 checkpoints and use a 10x smaller learning rate and weight decay on pretrained weights. We use 32 frames with a temporal stride of 2 for each view and we use 2 temporal views ? 3 spatial views during testing. For ViT-L/14 and ViT-L/14@336px, we train for 600 and 1,000 steps respectively. All other configurations are identical to what we use for Kinetics-400. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Model Ensemble Results</head><p>As shown in the main text, EVL video features learned on top of CLIP models are highly complementary to supervised features. We thus provide the complete ensemble result of our models in <ref type="table" target="#tab_1">Table 13</ref> (Kinetics-400) and <ref type="table" target="#tab_1">Table 14</ref> (Something-Something-v2). All model ensembles are performed between one of our model and Uniformer-B (32 frames) <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Results</head><p>Visualization of Video-level Decoder Attention Maps In <ref type="figure">Figure ?</ref>?    human-action-specific regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Cross-frame Attention Maps</head><p>We show the attention maps of some human-identifiable motion patterns in <ref type="figure" target="#fig_9">Figure 6</ref>. From the visualization, we observe that, even if our backbone is pretrained without consecutive frames, it can spontaneously capture the motion information by only adding some nonparametric modules.  . Three frames are shown in each example (previous, current and next frames), and the query token is the middle patch in the current frame, marked as a red square. We also mark the same position in the other two frames for reference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Left: illustration of the difference between our EVL training pipeline and other video recognition methods. Right: despite that EVL targets efficient training, our models set new accuracy vs. inference FLOPS Pareto frontiers. On Kinetics-400, the 8-frame ViT-B/16 model achieves 82.9% top-1 accuracy with only 60 V100 GPU-hours of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc>We develop EVL -an efficient transfer learning pipeline from image to video recognition, in which we train a lightweight Transformer decoder module on top of fixed transferable image features to perform spatiotemporal fusion. -Extensive experiments demonstrate the effectiveness and efficiency of EVL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?Fig. 2 :</head><label>2</label><figDesc>Model architecture overview. (a) Top-level architecture: multiple intermediate feature maps from a massively pretrained image backbone are fed into a Transformer decoder to gather information from them. (b) Motionenhanced Transformer decoder block: temporal modeling is added on top of raw frame features X i to retain structural information of the spatiotemporal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>16 Fig. 3 :</head><label>163</label><figDesc>EVL, ViT-B/16 tuned-backbone EVL, ViT-B/16 frozen-backbone EVL, ViT-L/14 tuned-backbone EVL, ViT-L/14 TimeSformer ViT-B/16 ActionCLIP ViT-B/Training time vs. accuracy with frozen or finetuned backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Local temporal information for both datasets. T-Conv : temporal convolution. T-PE : temporal positional embedding. T-CA: temporal cross attention. (b) Something-Something-v2 needs deeper decoder blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>T-PE T-CA K-400 Acc. (%) SSv2 Acc. (%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of video-level decoder attention maps. Visualization of the 2D CLIP [CLS] token and the 3D video-level [CLS] token are provided in the top and bottom rows, respectively. Human-action-specific contents are attended more (e.g., human body, facial parts, objects in hands, moving objects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>accuracy (%) on Kinetics-400 Ours -EVL (ViT-B/16) Ours -EVL (ViT-L/14) Ours -EVL (ViT-B/16) Ens Ours -EVL (ViT-L/14) Ens Model ensemble and single model accuracy vs. GFLOPS on Kinetics-400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of cross-frame attention maps. We select a few representative attention maps from Something-something v2 reflecting human understandable motion information. The motion information in the examples include position change (a, d, e, h), shape change (b, f, g) and object appear or disappear (c, i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-arts on Kinetics-400. We cite a series of models within similar range of accuracy as ours and compare the FLOPS. Frame counts are reported as frames per view ? number of views.</figDesc><table><row><cell>Method</cell><cell cols="4">Pretraining Acc. (%) #Frames GFLOPS</cell></row><row><cell>Uniformer-B [28]</cell><cell>ImageNet-1k</cell><cell>82.9</cell><cell>32 ? 4</cell><cell>1,036</cell></row><row><cell>Swin-B [32]</cell><cell>ImageNet-21k</cell><cell>82.7</cell><cell>32 ? 12</cell><cell>3,384</cell></row><row><cell>irCSN-152 [43]</cell><cell>IG-65M</cell><cell>82.6</cell><cell>32 ? 30</cell><cell>2,901</cell></row><row><cell>MViT-S [49]</cell><cell>ImageNet-21k</cell><cell>82.6</cell><cell>16 ? 10</cell><cell>710</cell></row><row><cell>Omnivore-B [17]</cell><cell>IN1k + SUN</cell><cell>83.3</cell><cell>32 ? 12</cell><cell>3,384</cell></row><row><cell>ViViT-L FE [1]</cell><cell>JFT</cell><cell>83.5</cell><cell>32 ? 3</cell><cell>11,940</cell></row><row><cell>TokenLearner 8at18 (L/16) [38]</cell><cell>JFT</cell><cell>83.2</cell><cell>32 ? 6</cell><cell>6,630</cell></row><row><cell>MViT-L [49]</cell><cell cols="2">MaskFeat, K600 85.1</cell><cell>16 ? 10</cell><cell>3,770</cell></row><row><cell>MTV-L [52]</cell><cell>JFT</cell><cell>84.3</cell><cell>32 ? 12</cell><cell>18,050</cell></row><row><cell></cell><cell></cell><cell>82.9</cell><cell>8 ? 3</cell><cell>444</cell></row><row><cell>EVL ViT-B/16 (Ours)</cell><cell>CLIP</cell><cell>83.6</cell><cell>16 ? 3</cell><cell>888</cell></row><row><cell></cell><cell></cell><cell>84.2</cell><cell>32 ? 3</cell><cell>1,777</cell></row><row><cell></cell><cell></cell><cell>86.3</cell><cell>8 ? 3</cell><cell>2,022</cell></row><row><cell>EVL ViT-L/14 (Ours)</cell><cell>CLIP</cell><cell>87.0 87.3</cell><cell>16 ? 3 32 ? 3</cell><cell>4,044 8,088</cell></row><row><cell>EVL ViT-L/14 (336px, ours)</cell><cell></cell><cell>87.7</cell><cell>32 ? 3</cell><cell>18,196</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with CLIP-based methods on Kinetics-400. All models use ViT-B/16 as backbone. As the paper<ref type="bibr" target="#b24">[25]</ref> is vague about the details, we estimate their new parameters to be 3 Transformer blocks with feature size 512 and MLP expansion factor 4. For ActionCLIP<ref type="bibr" target="#b46">[47]</ref>, we do not count parameters in the text branch.</figDesc><table><row><cell>Method</cell><cell cols="3">New Params (M) #Frames?#Views Acc. (%)</cell></row><row><cell>Efficient-Prompting [25] (A5)</cell><cell>9.43*</cell><cell>16 ? 5</cell><cell>76.9</cell></row><row><cell></cell><cell></cell><cell>8 ? 1</cell><cell>81.1</cell></row><row><cell></cell><cell></cell><cell>16 ? 1</cell><cell>81.7</cell></row><row><cell>ActionCLIP [47]</cell><cell>105.15</cell><cell>32 ? 1</cell><cell>82.3</cell></row><row><cell></cell><cell></cell><cell>16 ? 3</cell><cell>82.6</cell></row><row><cell></cell><cell></cell><cell>32 ? 3</cell><cell>83.8</cell></row><row><cell>EVL ViT-B/16 (Ours, 1 Layer)</cell><cell>7.41</cell><cell>8 ? 3</cell><cell>81.1</cell></row><row><cell>EVL ViT-B/16 (Ours, 4 Layers)</cell><cell>28.70</cell><cell>8 ? 3</cell><cell>82.9</cell></row><row><cell>EVL ViT-B/16 (Ours, 4 Layers)</cell><cell>28.78</cell><cell>32 ? 3</cell><cell>84.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Model (# frames)</cell><cell cols="2">Acc. (%) GFLOPS</cell><cell cols="2">Latency (ms) Throughput (V/s)</cell></row><row><cell>Uniformer-B (32) [28]</cell><cell>82.9</cell><cell cols="2">1036 (1.00?) 314.58 (1.00?)</cell><cell>3.42 (1.00?)</cell></row><row><cell>EVL ViT-B/16 (Ours, 8)</cell><cell>82.9</cell><cell cols="2">454 (0.44?) 102.88 (0.33?)</cell><cell>25.53 (7.47?)</cell></row></table><note>Inference latency and throughput measured on actual hard- ware. Both models achieve 82.9% accuracy on Kinetics-400. Results are obtained using V100-32G with PyTorch-builtin mixed precision. Latency is measured using a batch size of 1 and throughput is measured using the largest possible batch size before running out of memory.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Training time comparison.</figDesc><table><row><cell cols="4">Method (#Frames per View) Acc. (#Views) Pretraining Training GPU Hours</cell></row><row><cell>Uniformer-B [28] (32)</cell><cell>82.9 (4)</cell><cell>ImageNet-1k</cell><cell>5000 ? V100</cell></row><row><cell>TimeSformer [3] (8)</cell><cell>82.0 (3)</cell><cell>CLIP</cell><cell>100 ? V100</cell></row><row><cell>ActionCLIP [47] (16)</cell><cell>82.6 (3)</cell><cell>CLIP</cell><cell>480 ? RTX3090</cell></row><row><cell>EVL ViT-B/16 (8)</cell><cell>82.9 (3)</cell><cell>CLIP</cell><cell>60 ? V100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Idealized training step time. 4 decoder layers are used. All data are measured on a single V100-16G GPU. The step time is measured with 64 training samples.</figDesc><table><row><cell>Backbone</cell><cell>Head</cell><cell cols="2">Max Batch Size Step Time (s)</cell></row><row><cell cols="2">CLIP (Frozen) global average pool</cell><cell>inf.</cell><cell>0.57</cell></row><row><cell>CLIP (Open)</cell><cell>global average pool</cell><cell>8</cell><cell>3.39</cell></row><row><cell>CLIP (Frozen)</cell><cell>EVL</cell><cell>64</cell><cell>1.03</cell></row><row><cell>CLIP (Open)</cell><cell>EVL</cell><cell>8</cell><cell>4.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Effects of multi-layer high-resolution feature maps.</figDesc><table><row><cell>(a) Varying</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Main results on Something-Something-v2. Ens experiments combine EVL with Uniformer-B (32) pretrained on Kinetics-600.</figDesc><table><row><cell>Method</cell><cell>SSv2 Acc. (%)</cell><cell>#Frames</cell><cell>GFLOPS</cell></row><row><cell>EVL ViT-B/16</cell><cell>61.0</cell><cell>8 ? 3</cell><cell>512</cell></row><row><cell>EVL ViT-B/16</cell><cell>61.7</cell><cell>16 ? 3</cell><cell>1,023</cell></row><row><cell>EVL ViT-B/16</cell><cell>62.4</cell><cell>32 ? 3</cell><cell>2,047</cell></row><row><cell>EVL ViT-L/14</cell><cell>65.1</cell><cell>8 ? 3</cell><cell>2,411</cell></row><row><cell>EVL ViT-L/14</cell><cell>66.7</cell><cell>32 ? 3</cell><cell>9,641</cell></row><row><cell>EVL ViT-L/14 (336px)</cell><cell>68.0</cell><cell>32 ? 3</cell><cell>24,259</cell></row><row><cell>EVL ViT-B/16 Ens</cell><cell>72.1</cell><cell>32 ? 3 + 32 ? 3</cell><cell>2,824</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ensemble results of different combinations. We combine different models with similar accuracy with the same model and measure the accuracy gain.</figDesc><table><row><cell>Model 1</cell><cell>Acc. 1</cell><cell>Model 2</cell><cell cols="2">Acc. 2 Model 1 + 2 Acc. (?)</cell></row><row><cell></cell><cell></cell><cell cols="2">Uniformer-B [28] (32) 82.9</cell><cell>83.6 (+1.6)</cell></row><row><cell cols="2">Uniformer-B [28] (16) 82.0</cell><cell>Swin-B [32]</cell><cell>82.7</cell><cell>83.7 (+1.7)</cell></row><row><cell></cell><cell></cell><cell>EVL ViT-B/16 (8)</cell><cell>82.9</cell><cell>84.5 (+2.5)</cell></row><row><cell>Swin-B [32]</cell><cell>82.7</cell><cell cols="2">Uniformer-B [28] (32) 82.9 EVL ViT-B/16 (8) 82.9</cell><cell>84.7 (+2.0) 85.0 (+2.3)</cell></row><row><cell cols="2">Uniformer-B [28] (32) 82.9</cell><cell>Swin-B [32] EVL ViT-B/16 (8)</cell><cell>82.7 82.9</cell><cell>84.7 (+1.8) 85.2 (+2.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Ensemble results on Something-Something-v2.</figDesc><table><row><cell>Although EVL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Acknowledgements. This work is supported in part by Centre for Perceptual and Interactive Intelligence Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants (Nos. 14204021, 14207319), in part by CUHK Strategic Fund. This work is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Main results on UCF-101 and HMDB-51. Methods using additional modalities (e.g. optical flow, pose) are grayed out. We report the last-step validation accuracy averaged over 3 official splits.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>Modalities</cell><cell cols="2">UCF-101 HMDB-51</cell></row><row><cell>STC [8]</cell><cell>K400</cell><cell>RGB</cell><cell>95.8</cell><cell>72.6</cell></row><row><cell>ECO [62]</cell><cell>K400</cell><cell>RGB</cell><cell>93.6</cell><cell>68.4</cell></row><row><cell>R(2+1)D-34 [44]</cell><cell>K400</cell><cell>RGB</cell><cell>96.8</cell><cell>74.5</cell></row><row><cell>I3D [5]</cell><cell>ImageNet+K400</cell><cell>RGB</cell><cell>95.6</cell><cell>74.8</cell></row><row><cell>S3D [50]</cell><cell>ImageNet+K400</cell><cell>RGB</cell><cell>96.8</cell><cell>75.9</cell></row><row><cell>FASTER32 [61]</cell><cell>K400</cell><cell>RGB</cell><cell>96.9</cell><cell>75.7</cell></row><row><cell>VideoPrompt [25]</cell><cell>CLIP</cell><cell>RGB</cell><cell>93.6</cell><cell>66.4</cell></row><row><cell>LGD-3D [35]</cell><cell>ImageNet+K600</cell><cell>RGB</cell><cell>97.0</cell><cell>75.7</cell></row><row><cell>SlowOnly-R101 [9]</cell><cell>OmniSource[9]</cell><cell>RGB</cell><cell>97.3</cell><cell>79.0</cell></row><row><cell>Two-Stream I3D [5]</cell><cell>ImageNet+K400</cell><cell>RGB+Flow</cell><cell>98.0</cell><cell>80.7</cell></row><row><cell>Two-Stream LGD-3D [35]</cell><cell>ImageNet+K600</cell><cell>RGB+Flow</cell><cell>98.2</cell><cell>80.5</cell></row><row><cell>PERF-Net [31]</cell><cell cols="2">ImageNet+K700 RGB+Flow+Pose</cell><cell>98.6</cell><cell>83.2</cell></row><row><cell cols="2">SlowOnly-R101-RGB + I3D-Flow [9] OmniSource[9]</cell><cell>RGB+Flow</cell><cell>98.6</cell><cell>83.8</cell></row><row><cell>SMART [18]</cell><cell>ImageNet+K400</cell><cell>RGB+Flow</cell><cell>98.6</cell><cell>84.3</cell></row><row><cell>EVL ViT-L/14 (ours)</cell><cell>CLIP+K400</cell><cell>RGB</cell><cell>98.5</cell><cell>83.6</cell></row><row><cell>EVL ViT-L/14@336px (ours)</cell><cell>CLIP+K400</cell><cell>RGB</cell><cell>98.6</cell><cell>83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>we show the difference in attention maps between the CLIP [CLS] token and the video-level [CLS] token. Compared to the original pretrained [CLS] token, [CLS] token learned from videos generate attention maps that concentrate more on</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth: javelin throw</cell><cell></cell><cell></cell></row><row><cell>clip</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>decoder</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth: flipping pancake</cell><cell></cell><cell></cell></row><row><cell>clip</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>decoder</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth: playing saxophone</cell><cell></cell><cell></cell></row><row><cell>clip</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>decoder</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth: counting money</cell><cell></cell><cell></cell></row><row><cell>clip</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>decoder</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth: climbing tree</cell><cell></cell><cell></cell></row><row><cell>clip</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>decoder</cell><cell>attention map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Model ensemble results on Kinetics-400. Accuracy of Uniformer-B (32 frames) used in the ensemble is 82.9%.</figDesc><table><row><cell>Model</cell><cell>Single Model Acc.</cell><cell>Ensemble Acc.</cell><cell>Ensemble GFLOPS</cell></row><row><cell>EVL ViT-B/16 (8 frames)</cell><cell>82.9</cell><cell>85.2</cell><cell>1,480</cell></row><row><cell>EVL ViT-B/16 (16 frames)</cell><cell>83.6</cell><cell>85.5</cell><cell>1,924</cell></row><row><cell>EVL ViT-B/16 (32 frames)</cell><cell>84.2</cell><cell>85.8</cell><cell>2,813</cell></row><row><cell>EVL ViT-L/14 (8 frames)</cell><cell>86.3</cell><cell>87.1</cell><cell>3,058</cell></row><row><cell>EVL ViT-L/14 (16 frames)</cell><cell>87.0</cell><cell>87.7</cell><cell>5,080</cell></row><row><cell>EVL ViT-L/14 (32 frames)</cell><cell>87.3</cell><cell>88.0</cell><cell>9,124</cell></row><row><cell>EVL ViT-L/14 (32 frames, 336px)</cell><cell>87.7</cell><cell>88.2</cell><cell>19,232</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Model ensemble results on Something-Something-v2. Accuracy of Uniformer-B (32 frames) used in the ensemble is 71.2%</figDesc><table><row><cell>Model</cell><cell>Single Model Acc.</cell><cell>Ensemble Acc.</cell><cell>Ensemble GFLOPS</cell></row><row><cell>EVL ViT-B/16 (32 frames)</cell><cell>62.4</cell><cell>72.1</cell><cell>2,824</cell></row><row><cell>EVL ViT-L/14 (8 frames)</cell><cell>65.1</cell><cell>72.5</cell><cell>3,188</cell></row><row><cell>EVL ViT-L/14 (32 frames)</cell><cell>66.7</cell><cell>72.8</cell><cell>10,418</cell></row><row><cell>EVL ViT-L/14 (32 frames, 336px)</cell><cell>68.0</cell><cell>72.9</cell><cell>25,036</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Training time of Uniformer-B is estimated by halving the value for Kinetics-600 provided in their GitHub repo. Training time of TimeSformer is from our own</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Omni-sourced webly-supervised learning for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="670" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: Better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Clip2tv: An empirical study on transformer-based methods for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08377</idno>
		<title level="m">Omnivore: A Single Model for Many Visual Modalities</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smart frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1451" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07463</idno>
		<title level="m">Parameter-efficient transfer learning with diff pruning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Prompting visual-language models for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04478</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Uniformer: Unified transformer for efficient spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<title level="m">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01526</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perf-net: Pose empowered rgb-flow net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="513" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<title level="m">Adapterfusion: Nondestructive task composition for transfer learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05974</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Tokenlearner: What can 8 learned tokens do for images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11297</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cliport: What and where pathways for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Conference on Robot Learning (CoRL)</title>
		<meeting>the 5th Conference on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2199</idno>
		<title level="m">Two-stream convolutional networks for action recognition in videos</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<title level="m">Actionclip: A new paradigm for video action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">Simmim: A simple framework for masked image modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04288</idno>
		<title level="m">Multiview transformers for video recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">Florence: A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<title level="m">Scaling vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12104" to="12113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Co-training transformer with videos and images improves action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03930</idno>
		<title level="m">Tipadapter: Training-free clip-adapter for better vision-language modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02413</idno>
		<title level="m">Pointclip: Point cloud understanding by clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Faster recurrent networks for efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13098" to="13105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
