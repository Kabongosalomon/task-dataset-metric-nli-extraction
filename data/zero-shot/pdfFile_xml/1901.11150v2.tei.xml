<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory-Efficient Adaptive Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-13">September 13, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
						</author>
						<title level="a" type="main">Memory-Efficient Adaptive Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-13">September 13, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of perparameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-ofthe-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Adaptive gradient-based optimizers such as Adagrad <ref type="bibr" target="#b8">[9]</ref> and Adam <ref type="bibr" target="#b12">[13]</ref> are among the de facto methods of choice in modern machine learning. These methods adaptively tune the learning rate for each parameter during the optimization process using cumulative second-order statistics. Often offering superior convergence properties, these methods are very attractive in large scale applications due to their moderate time and space requirements, which are linear in the number of parameters. However, when training extremely large models even the modest memory overhead imposes grave limitations on the quality of the trained model. For example, recent advances in natural language processing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15]</ref> show that models with hundreds of millions to billions of parameters, trained with adaptive optimization methods, achieve state-of-the-art results. In such cases, the memory overhead of the optimizer severely restricts the size of the model that can be used as well as the number of examples in each mini-batch, both of which have a dramatic effect on the accuracy of the model.</p><p>Motivated by these challenges, we describe an adaptive optimization method that retains the benefits of standard per-parameter adaptivity while significantly reducing memory overhead. Our construction is general and flexible, and very simple to implement. We give convergence guarantees for our method in the convex (online or stochastic) optimization setting, and demonstrate experimentally that it is particularly effective when the gradients exhibit natural activation patterns; namely, when the parameters can be subdivided into (not necessarily disjoint) sets where gradient entries within sets are correlated and of a similar order of magnitude. For example, we often observe in deep networks that the incoming (outgoing) edges into (from) a neuron are jointly activated and, loosely speaking, their associated gradients exhibit similar statistical characteristics. That said, the analysis of our optimization algorithm makes no statistical assumptions on the gradients and is applicable in general stochastic convex optimization settings. Further, we do not assume that the activation pattern is fully prescribed a-priori to the algorithm.</p><p>Large scale experiments show that our algorithm achieves comparable, and at times superior, rates of convergence compared to standard linear-space adaptive methods. Focusing primarily on language modeling tasks where state-of-the-art models are extremely large, we further demonstrate that the reduction in memory footprint can be utilized for a substantial increase in the batch size, which greatly speeds up convergence in a distributed environment. For a fixed budget of computational resource our method is able to shorten the end to end wall-time for convergence by up to 50%. Our method exhibits slightly improved per-step time. The latter could be attributed to reduction in the frequency of memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Adaptive learning rates in online and stochastic optimization date back at least to <ref type="bibr" target="#b3">[4]</ref> and were popularized in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, the former of which introduced the well-known Adagrad algorithm. Several variants of Adagrad have now been proposed in the optimization and machine learning literature (see <ref type="bibr" target="#b16">[17]</ref> and the references therein), the most notable of which is Adam <ref type="bibr" target="#b12">[13]</ref>. All of these methods require (at least) linear space for maintaining various per-parameter statistics during their execution. One notable exception, which is directly related to our work, is the Adafactor algorithm <ref type="bibr" target="#b20">[21]</ref> that was proposed as a way to reduce the memory costs of Adam, primarily for training large language models. While the memory requirements of our construction are similar to Adafactor's, the application scope and the convergence properties of the two algorithms are quite different. We discuss the relationship in more detail in Section 4 and give an empirical comparison between the algorithms in Section 5.</p><p>Another closely related method is the Shampoo <ref type="bibr" target="#b9">[10]</ref> algorithm for optimization over tensor structures. Seemingly, the goal of Shampoo is very different from ours: going beyond entrywise learning rates and employing full-matrix regularization in a computationally efficient way. Nonetheless, Shampoo can also be seen as a method to substantially reduce the memory footprint of full-matrix preconditioned algorithms (specifically, full-matrix Adagrad). In a sense, our algorithms are analogous to a diagonalized version of the Shampoo algorithm. Yet another recent adaptive optimization method is the GGT algorithm <ref type="bibr" target="#b1">[2]</ref>. Similarly to Shampoo, the goal of the latter is to reduce the computation cost of full-matrix preconditioning in order to make it practical in large scale settings. However, GGT stores multiple copies of the gradient over the course of its execution, and as a result, its space requirements restricts it from being applied at large scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Online optimization</head><p>We henceforth assume the general online optimization setting (see <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>). Online optimization consists of rounds t = 1, . . . , T , where in each round the algorithm chooses a parameter vector w t ? R d . After making a choice on round t, the algorithm receives a loss function t : R d ? R which is used to form an update of the parameters. In our analysis, we focus on online convex optimization in which 1 , . . . , T are convex. Often, as is the case in this paper, the update is determined by the gradient g t = ? t (w t ) of the instantaneous loss t at the current iterate w t . The algorithm is measured by its T -round regret with respect to a given comparator w ? R d , defined as the quantity T t=1 t (w t ) ? T t=1 t (w ). An online optimization algorithm is convergent if its regret is o(T ), i.e., its average regret approaches zero as T grows.</p><p>The above setting includes stochastic (possibly mini-batched) optimization as a special case. In stochastic optimization the underlying goal is to minimize a population loss L(w) = E z?D [ (w, z)] based on samples of z. Here (w, z) defines the loss of parameters w w.r.t a batch z. The online loss function t (w) = (w, z t ) is the average loss over a mini-batch z t received on iteration t. The stochastic gradient g t is a conditionally unbiased estimate of the gradient of L at the current parameter vector w t . Under convexity assumptions, an online algorithm with vanishing average regret can be converted to a stochastic optimization algorithm for minimizing the population loss L [5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive methods</head><p>For the sake of self-containment, we give a brief description of adaptive gradient methods, focusing on Adagrad <ref type="bibr" target="#b8">[9]</ref>. Adagrad maintains at each step t parameter-wise accumulated statistics which are computed from the previously obtained gradients g 1 , . . . , g t :</p><formula xml:id="formula_0">? t (i) = t s=1 g 2 s (i) , ? i ? [d] .<label>(1)</label></formula><p>Based on these statistics, the update rule of the algorithm on step t takes the form:</p><formula xml:id="formula_1">w t+1 (i) = w t (i) ? ? g t (i) ? t (i) , ? i ? [d] ,</formula><p>where ? &gt; 0 is an external learning rate parameter. Duchi et al. <ref type="bibr" target="#b8">[9]</ref> proved the following regret bound for Adagrad with respect to a given w (with properly tuned ?):</p><formula xml:id="formula_2">T t=1 t (w t ) ? T t=1 t (w ) = O ? ? D d i=1 T t=1 g 2 t (j) ? ? ,<label>(2)</label></formula><p>where D ? max t w t ? w ? . Adagrad has proved to be particularly useful in training sparse models, where the effective learning rates ? ? t (i) decay in a moderate way for rare, yet potentially informative, features. In these settings, Adagrad can potentially lead to substantial improvements in convergence time; see for instance the discussion in <ref type="bibr" target="#b8">[9]</ref>. Crucially, however, Adagrad must maintain auxiliary sequence of accumulators ? t and thus needs ?(d) additional space. The goal of this paper is to provide memory-efficient methods with comparable convergence characteristics that refrain from maintaining the full vectors ? t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The SMAlgorithm</head><p>We now present our memory-efficient adaptive optimization algorithm. As an abstraction, the algorithm employs a cover of the parameters: a collection of k nonempty sets {S r } k r=1 , such that S r ? [d] and ? r S r = [d]. In particular, each index i ? [d] may be contained in multiple sets S r . The algorithm maintains a single variable for each set S r in the cover. Thus, the additional space it requires is O(k) rather than the O(d) required by standard adaptive methods. In large scale applications, k will be chosen to be negligible in comparison to d, which would translates to substantial savings in memory; see Section 4 for a discussion on the covers used in practice.</p><p>Concretely, for each set S r in the cover, the algorithm maintains a running sum, ? t (r), of the maximal variance over all gradient entries j ? S r . Next, for each parameter i, we take the minimum over all variables ? t (r) associated with sets which cover i, denoted S r i. Thereafter, the learning rate corresponding to the i'th gradient entry is determined by taking the square-root of this minimum, denoted by ? t (i). Accordingly, we name our algorithm the S quare-root of M inima of S ums of M axima of S quared-gradients M ethod, or in short, SM3. See Algorithm SM3-I for its pseudocode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM3 -I</head><p>1: parameters: learning rate ? 2: initialize w 1 = 0 ; ?r ? [k] : ? 0 (r) = 0 3: for t = 1, . . . , T do 4:</p><formula xml:id="formula_3">receive gradient g t = ? t (w t ) 5:</formula><p>for r = 1, . . . , k do 6:</p><formula xml:id="formula_4">set ? t (r) ? ? t?1 (r) + max j?Sr g 2 t (j) 7: for i = 1, . . . , d do 8: set ? t (i) ? min r:Sr i ? t (r) 9: update w t+1 (i) ? w t (i) ? ? g t (i) ? t (i)</formula><p>with the convention that 0/0 = 0</p><p>As noted above, SM3-I requires only O(k) space in addition to the space required for storing the parameters w t themselves. The time per iteration of SM3-I is O( k r=1 |S r |). To see this, consider a bipartite graph defined over d + k vertices. Nodes on one side of the graph correspond to indices i ? [d], while nodes on the other side correspond to indices j ? [k]. The edges of the graphs are all pairs (i, j) such that i ? S j . The complexity of each inner for-loop of the algorithm scales with the number of edges in this graph, which is equal to O( k r=1 |S r |). Note that updating the weights w t takes O(d) time, which is always dominated by the former quantity.</p><p>The following provides convergence guarantees for SM3-I.</p><p>Proposition 1. Assume that the loss functions 1 , 2 , . . . are convex, and let w 1 , w 2 , . . . be the iterates generated by SM3-I. Then, for any</p><formula xml:id="formula_5">w ? R d , T t=1 t (w t ) ? t (w ) ? 2D d i=1 min r:Sr i T t=1 max j?Sr g 2 t (j) , where max t w t ? w ? ? D and choosing ? = D. 1</formula><p>For stochastic optimization, i.e., when the functions t correspond to i.i.d. samples with E[ t (w)] = L(w), the above bound translates via standard arguments to a O(1/ ? T )-type convergence guarantee for the average iterate</p><formula xml:id="formula_6">w T = 1 T T t=1 w t of the form E[L(w T )] ? L(w ) = O ? ? 1 T d i=1 E min r:Sr i T t=1 max j?Sr g 2 t (j) ? ? .</formula><p>Note that adding more sets S r to the cover used by SM3 always improves its convergence bound, but results in a worse space complexity and a higher runtime per step. When k = d and S i = {i} for all i ? [d], SM3-I reduces to the Adagrad algorithm, and the regret bound in Proposition 1 then precisely recovers the bound attained by Adagrad (recall Eq. <ref type="formula" target="#formula_2">(2)</ref>). In general, the right-hand side of Proposition 1 is never smaller than Adagrad's regret bound, as expected from a space-restricted scheme (this is a consequence of Claim 2 below). Nevertheless, the two bounds can be of similar order of magnitude in practical scenarios; see Section 4 below for a detailed discussion.</p><p>We now give a proof of Proposition 1. First, we state two elementary properties of the step sizes the algorithm computes. For a proof, see Appendix A.</p><p>Claim 2. For any i, the sequence ? 1 (i), ? 2 (i), . . . is monotonically increasing, and ? t (i) ? Proof of Proposition 1. Let us first assume that g 1 (i) &gt; 0 for all i, so that ? t (i) &gt; 0 for all i and t ? 1 due to Claim 2. We start by observing that SM3-I performs Online Mirror Descent updates, where the step on round t uses the positive definite diagonal matrix H t = diag(? 1/2 t ) for regularization. Then, employing a standard regret bound for the Online Mirror Descent algorithm with time-dependent regularization (see for instance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Proposition 3]</ref>), the regret of the algorithm is bounded by</p><formula xml:id="formula_7">1 2? T t=1 w t ? w 2 Ht ? w t+1 ? w 2 Ht + ? 2 T t=1 g t * Ht 2 .</formula><p>Here, x H = ? x T Hx and ? * is the corresponding dual norm,</p><formula xml:id="formula_8">x * H = ? x T H ?1 x.</formula><p>Henceforth, for notational convenience we set ? 0 = 0. Simplifying the first sum above using the fact that H t are diagonal matrices, we have</p><formula xml:id="formula_9">T t=1 w t ? w 2 Ht ? w t+1 ? w 2 Ht ? T t=1 (? 1/2 t ? ? 1/2 t?1 ) ? (w t ? w ) 2 ? T t=1 (? 1/2 t ? ? 1/2 t?1 ) ? w t ? w 2 ? 1 d ? D 2 ? 1/2 T ? 1 d = D 2 Tr(H T ) . Now, let ? t (i) = t s=1 g 2 s (i) and consider the positive definite diagonal matrix G t = diag(? 1/2 t ). From [10, Lemma 2] with ?(G) = Tr(G), we have T t=1 g t * Gt 2 ? T t=1 g t * G T 2 + Tr(G T ) = ? ?1/2 T ? ? T + Tr(G T ) = 2 Tr(G T ) .</formula><p>Also, from Claim 2 we know that for all t, H t G t , thus</p><formula xml:id="formula_10">T t=1 g t * Ht 2 ? T t=1 g t * Gt 2 ? 2 Tr(G T ) ? 2 Tr(H T ) .</formula><p>In summary, we have established that</p><formula xml:id="formula_11">T t=1 t (w t ) ? t (w ) ? D 2 2? + ? Tr(H T ) .</formula><p>Plugging in ? = D and the expression for the diagonal elements of H T , we obtain the claim. For the degenerate case where the matrices H t may not be strictly positive definite, a careful yet technical inspection of the proof above reveals that our arguments apply to this case as well by replacing inverses with pseudo-inverses. The rest of the proof remains intact as the algorithm does not update parameter i on step t if the corresponding diagonal entry in H t is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SM3-II</head><p>We now discuss a slightly more efficient variant of SM3, which we describe in SM3-II. It is similar to SM3-I, and improves on the latter in the following sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM3 -II</head><p>1: parameters: learning rate ? 2: initialize w 1 = 0 ; ?r ? [k] : ? 0 (r) = 0 3: for t = 1, . . . , T do 4:</p><formula xml:id="formula_12">receive gradient g t = ? t (w t ) 5: initialize ? t (r) = 0 for all r ? [k] 6: for i = 1, . . . , d do 7: ? t (i) ? min r:Sr i ? t?1 (r) + g 2 t (i) 8: w t+1 (i) ? w t (i) ? ? g t (i) ? t (i)</formula><p>with the convention that 0/0 = 0 9:</p><p>for all r : S r i do 10:</p><formula xml:id="formula_13">? t (r) ? max{? t (r), ? t (i)} Proposition 3. For any i ? [d]</formula><p>, the sequence ? 1 (i), . . . , ? T (i) is monotonically increasing. Further, fixing a sequence of gradients g 1 , . . . , g T , we have for all t, i that t s=1 g 2</p><formula xml:id="formula_14">s (i) ? ? t (i) ? ? t (i), where ? 1 (i), . . . , ? T (i)</formula><p>is the sequence SM3-I emits upon receiving the gradients g 1 , . . . , g T .</p><p>(See Appendix A for a proof.) In other words, SM3-II provides a tighter upper bound on the cumulative gradient squares than SM3-I. Consequently, we can show, along similar lines to the proof of Proposition 1, a slightly better bound for SM3-II that scales with the quantity d i=1 ? t (i), which is always smaller than the one appearing in the bound of SM3-I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Thus far, we gave an analysis of SM3 in a worst-case (convex) setting without placing any further assumptions on the statistical characteristics of the underlying stochastic gradients. Further, we did not attempt to relate the cover used by SM3 to properties of the underlying stochastic optimization problem. It should not come as a surprise that in this general setting, the convergence of SM3 might be much worse, at least in theory, than its linear-memory counterpart Adagrad.</p><p>Activation patterns. Often in our experiments, we observe common statistical attributes that could be exploited by SM3. Specifically, we see that certain entries of the stochastic gradients have (on average) similar values, and exhibit what we refer to as an activation pattern. For example, in gradients of embedding layers of deep networks, an entire row (or column) is either zero or non-zero. Similarly, in intermediate layers we often observe that gradients associated with the same unit are of similar order of magnitude. In these cases, a similar phenomenon is observed in the second-order statistics maintained by adaptive methods. In <ref type="figure" target="#fig_0">Figure 1</ref> we visualize this phenomenon for different layers of a Transformer network. In Appendix B.3 we give additional illustrations of similar phenomena in convolutional layers of image classification models.</p><p>Choice of covers. The intuitive notion of an activation pattern motivates a natural and generic choice for the cover used by SM3 in practice. For the parameters of deep networks, that are organized as a collection of tensors, we form a cover consisting of slices of co-dimension 1 for each tensor. Thus, for an m ? n parameter matrix, the cover consists of rows and columns of the matrix. The memory requirements therefore drop from ?(mn) to merely ?(m + n). For a parameter tensor of dimension n 1 ? ? ? ? ? n p , the reduction in memory consumption is even more pronounced, dropping from ?( p i=1 n i ) to ?( p i=1 n i ). This virtually eliminates the memory overhead associated with maintaining the adaptive learning rates.</p><p>We argue, though only informally, that when choice of cover used by SM3 is compatible with the observed activation patterns, we expect the convergence of SM3 to be significantly better, and closely match Adagrad. Quantitatively, if each parameter i ? [d] is covered by a set S r such that g s (j) ? g s (i) for all j ? S r , then max j?Sr g 2 s (j) ? g 2 s (i), and thus min r:Sr i s max j?Sr g 2 s (j) ? s g 2 s (i) . Thus, the bounds in Proposition 1 and Eq. (2) are of similar order of magnitude. In other words, in such scenarios we inherit the convergence properties of Adagrad while using a negligible amount of memory. We remark that the activation pattern need not be fully specified in advance; in particular, SM3 is robust to whether a certain parameter is "row tied" or "column tied", as long as both rows and columns are included in the cover.</p><p>Comparison with Adafactor. Adafactor <ref type="bibr" target="#b20">[21]</ref> is a very effective method for space-efficient adaptive optimization. SM3 and Adafactor differ in a number of important ways. First, Adafactor is only defined for matrix-shaped parameters while SM3 applies to tensors of arbitrary dimensions, and even more generally, to any predefined cover of the parameters. Second, Adafactor is in essence a fixed learning-rate algorithm, being a memory-constrained variation of Adam, and often requires a manually devised learning-rate schedule to ensure convergence. In contrast, SM3 adapts its learning rates in an adaptive, data-driven manner similar to Adagrad. Finally, SM3 comes with rigorous convergence guarantees in stochastic convex optimization settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We demonstrate the practical efficacy of SM3 on several machine learning tasks using published state-of-the-art architectures. We focus on three domains: machine translation, language modeling, and image classification. We implemented SM3 as an optimizer in TensorFlow <ref type="bibr" target="#b0">[1]</ref>; source code is publicly available at <ref type="bibr" target="#b2">[3]</ref>. Our implementation follows the pseudocode of SM3-II, as it performed slightly yet consistently better than SM3-I in our experiments (as predicted by our bounds). We use covers induced by rows and columns of matrices, and more generally, by slices of higher-order tensors (e.g., in convolutional layers represented by 4-dimensional tensors), as described in Section 4. In addition to being compatible with the natural activation patterns, these covers facilitates efficient tensor operations available on GPUs and TPUs for computing max and min over the sets. In all experiments, we used the Cloud TPU-v2 device <ref type="bibr" target="#b11">[12]</ref> where each core has 8GiB of memory. For more details on all of our experiments, including the values of hyperparameters used in each of them, see Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Machine translation</head><p>We experimented with machine translation tasks on two standard datasets from WMT'14: English to French (en?fr) with 36.3M sentence pairs, and English to German (en?de) with 4.5M sentence pairs. We used the state-of-the-art Transformer architecture Vaswani et al. <ref type="bibr" target="#b22">[23]</ref>. The basic version of this model has 93.3M parameters and consumes 0.36GiB memory. The larger variant (coined Transformer-Big) has 375.4M parameters (1.432GiB) and consists of 6 layers for its encoder and decoder, where each layer is composed of 1024 model dimensions, 8192 hidden dimensions, and 16 attention heads.</p><p>Here we report our results on the larger Transformer-Big, and defer results on the basic Transformer to Appendix B. We trained Transformer-Big on the en?fr dataset with batches of size 384, and compared SM3 with several standard optimizers in each of the tasks. In all cases, we used momentum (including for Adagrad) and extensively tuned all hyperparameters. We also ran SGD with momentum (with various exponential decay schedules), but it performed poorly and hence it is omitted from the figures. The results are provided in <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="table" target="#tab_1">Table 1</ref>, and demonstrate that SM3 performed substantially better and provided a large improvement in BLEU score compared to Adam and Adafactor. In addition, the small memory requirements of SM3 and Adafactor allowed us to double the number of examples in a batch to a total of 768, with minimal additional computation resources. In this setting, we found that SM3 outperformed Adafactor in terms of the number of steps as well as the wall-time to convergence by roughly a factor of 2. We further observed that SM3 approximated the Adagrad second-order statistics tightly. More details are provided in Appendix B.</p><p>Both models were trained on a 4?4 Cloud TPU-v2 using the Lingvo <ref type="bibr" target="#b21">[22]</ref> sequence modeling framework, with 32K word-pieces <ref type="bibr" target="#b18">[19]</ref> for each language pair. BLEU scores were computed on the Newstest 2014 for evaluation, on tokenized, true-case outputs, and without manual postprocessing of the text, similar to <ref type="bibr" target="#b24">[25]</ref>. Our BLEU scores are not directly comparable to those of <ref type="bibr" target="#b22">[23]</ref>. We instead followed the experimental protocol described in a later work <ref type="bibr" target="#b5">[6]</ref>.  </p><formula xml:id="formula_15">0.2M 0.4M 0.6M 0.8M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language modeling</head><p>Next, we considered a language modeling task on the concatenation of Wikipedia and BooksCorpus <ref type="bibr" target="#b25">[26]</ref>, with 2.5B and 800M words respectively. We used the recent Bidrectional Encoder Representation (BERT) architecture of Devlin et al. <ref type="bibr">[</ref> GiB), and is set up to jointly optimize two objectives: (a) masked language model (Masked-LM) loss where the task is to predict masked tokens based on surrounding context, and (b) next sentence prediction (NSP) loss where the task is to predict whether two given sentences are consecutive in the text. As before, we compared SM3 with Adagrad, Adam and Adafactor. Our results are presented in <ref type="figure" target="#fig_2">Figure 3</ref>. We see that SM3 worked as well as Adam and Adagrad for a fixed batch size. However, the savings in memory allowed us to train SM3 with double the batch size, resulting in a substantial increase in accuracy. The experiments were run using the open sourced code from <ref type="bibr" target="#b7">[8]</ref> on a 8?8 Cloud TPU-V2 configuration.</p><p>To underscore the importance of our memory savings in the context of very large models, we report additional results on the number of steps required for reaching a given solution quality for various batch sizes. We chose a solution quality of 70% Masked-LM accuracy on the holdout set, which Adam and AdaGrad reached at 500k steps. We use Cloud TPU-v3 device which has 16Gib per core for this experiment. We measured the number of steps SM3 needed to reach this accuracy as a function of the batch size. Our results are presented in <ref type="figure" target="#fig_2">Figure 3</ref>. SM3 scaled almost linearly with the batch size, up to a size of 2 <ref type="bibr" target="#b15">16</ref> , at which point the training program reached the limits of memory available on hardware. We also found that SM3 came out ahead in terms of wall-time: with the same batch size, a step of SM3 was faster than Adam's by 3%, and doubling the batch size allowed it to reach the same solution quality in almost 35% less wall-time for the same computational budget.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">AmoebaNet-D on ImageNet</head><p>Finally, we report results from a different domain: image classification on ImageNet <ref type="bibr" target="#b17">[18]</ref> with the state-of-the-art AmoebaNet-D architecture <ref type="bibr" target="#b15">[16]</ref>, that has recently won the Stanford DAWN-Bench competition <ref type="bibr" target="#b6">[7]</ref>. We compared SM3 with SGD with momentum (Adam performed poorly on this task). The results shown in <ref type="figure" target="#fig_3">Figure 4</ref> indicate that SM3 performed very well in this task and achieved improved convergence to state-of-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>Motivated by the large increase in models sizes and the huge amounts of memory required for training them, we have presented a new memory-efficient adaptive optimization algorithm for stochastic optimization called SM3. We demonstrated empirically that SM3 can be used effectively in training modern mammoth-sized models and dramatically decrease memory overhead. Utilizing the freed memory for increasing the batch size, our experiments indicate that this saving can also lead to significant improvements in performance. Our theoretical investigation focused on convex objectives. As with many other optimization scenarios, we believe the analysis of convex memory-efficient adaptive optimization could serve as a basis for understanding non-convex settings. Our memory savings virtually eliminate the overhead coming from the second-order statistics ? t with little and often no impact on convergence. Additional and potentially substantial improvements in memory consumption could come from compressing or sketching the momentum terms employed by virtually all first-order optimizers used in practice. We leave the exploration of this promising direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Tightness of SM3 approximation</head><p>We corroborate our discussion from Section 4 with an illustration for both variants of SM3, of the tightness of approximation of Adagrad's second-order statistics. <ref type="figure" target="#fig_5">Figure 5</ref> demonstrates that overall SM3 provides a tight approximation, with the SM3-II performing significantly better than SM3-I, especially for higher-magnitude values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Results for basic Transformer on WMT'14 en?de</head><p>In <ref type="figure" target="#fig_6">Figure 6</ref> we report results for the basic Transformer after training for 700,000 steps on en?de with a batch size of 1536. As in previously discussed experiments, SGD with momentum performs poorly compared to adaptive optimizers and hence is not included in the comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Activation patterns in convolutional networks</head><p>We give additional evidence of self-formation of row and column activation patterns which arise in convolutional image recognition models. See <ref type="figure" target="#fig_7">Figure 7</ref> for an illustration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Experiments</head><p>We report the settings of hyperparameters used in our experiments in <ref type="table">Table 3</ref>. We performed a grid search on the following hyper-parameters: ? ? [10 ?5 , 10 0 ], ? 1 ? {0.9, 0.95, 0.99}, and ? 2 ? [0.9, 0.999] for each of the optimizers when applicable. We were able to discard a large fraction of the search space for learning rates, as large values typically cause instability and lower values make the progress slow. We found we found ? 1 = 0.9 to work well for almost all experiments (where batch size &lt; 2048) except in the case of SM3 on BERT-Large where ? 1 = 0.95 worked best for 2 13 and 2 16 batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Optimizer Batch size</head><p>Hyperparameters Warmup (T 0 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer en?de</head><p>Adafactor 1536 ? = 0.0003, ? 1 = 0.9, ? 2 = 0.98 10k Adam 1536 ? = 0.0004, ? 1 = 0.9, ? 2 = 0.98 10k Adagrad 1536 ? = 0.1, ? 1 = 0.9 10k SM3 1536 ? = 0.225, ? 1 = 0.9 10k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer en?fr</head><p>Adafactor 384 ? = 0.00045, ? 1 = 0.9, ? 2 = 0.98 40k Adam 384 ? = 0.00015, ? 1 = 0.9, ? 2 = 0.98 40k Adagrad 384 ? = 0.075, ? 1 = 0.9 40k SM3 384 ? = 0.125, ? 1 = 0.9 40k Adafactor 768 ? = 0.00045, ? 1 = 0.9, ? 2 = 0.98 40k SM3 768 ? = 0.25, ? 1 = 0.9 40k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-Large</head><p>Adafactor 1024 ? = 0.005, ? 1 = 0.9, ? 2 = 0.999 10k Adam 1024 ? = 0.0001, ? 1 = 0.9, ? 2 = 0.999 10k Adagrad 1024 ? = 0.25, ? 1 = 0.9 10k SM3 1024 ? = 0.1, ? 1 = 0.9 10k SM3 2048 ? = 0.1, ? 1 = 0.9 10k SM3 8192 ? = 0.05, ? 1 = 0.95 2k SM3 65536 ? = 0.15, ? 1 = 0.95 2k AmoebaNet SGD 4096 ? = 6.15, ? 0 = 0.042, ? =4.5k, ? 1 = 0.9 1.2k SM3 4096 ? = 0.5, ? 1 = 0.9 1.2k <ref type="table">Table 3</ref>: Hyperparameter setup used in our experiments.</p><p>We also employed a short initial "warmup" stage for all optimizers. During warmup we gradually increased the learning rate ? from zero to its maximal value during the first few thousand updates. This is a common heuristic in training of deep models, where often a high learning rate setting in the early stages of optimization causes instabilities and results in failure to converge, colloquially called "blowup". The choice of the number of steps used for warmup does not affect the eventual performance of the trained models, and was chosen somewhat liberally before tuning the rest of the hyperparameters. We would like to stress that in each experiment, we used the same value for all optimizers. In the experiments with BERT-Large using large batch sizes, warmup was very short as experimentally it deemed almost unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>Optimizer LR Schedule (after warmup)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>Adam, Adafactor ? d/t [23] BERT Adam, Adafactor ?(1 ? t/T ) [8] AmoebaNet-D SGD (+momentum) max ? 0 , ?? t/? Folklore All Adagrad, SM3 ? <ref type="table">Table 4</ref>: Learning rate schedules used by the algorithms we experimented with. Here, t is the current time step, ? is the base learning rate, ? &lt; 1 is the decay constant, ? is the staircase step interval, ? 0 is the minimum learning rate for staircase schedule, T 0 is the number of warmup steps, T is the total number of training steps, and d is the size of the model.</p><p>We note that, compared to other optimizers, SM3 has a single hyper-parameter that requires tuning, the initial learning rate ?. Concretely, past the warmup phase, SM3 does not employ a schedule for learning-rate decay which is often difficult to tune. <ref type="table">Table 4</ref> we summarize the procedures for scheduling the learning rate of all optimizers.</p><p>For experimenting with Adafactor, we made use of the implementation in the Tensor2Tensor framework <ref type="bibr" target="#b23">[24]</ref> and tuned the parameters as described above. We found Adafactor to work quite well on translation tasks, for which it was designed and optimized. Alas, we could not get it to work on the BERT language models. Adafactor's implementation has numerous hyperparameters which makes it extremely difficult to set up for new domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of Adagrad's statistics (cf. Eq. (1)) for different weight matrices in Transformer-Big model trained with Adagrad on WMT'14 en?fr (color intensities are in log scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Test log-perplexity of a Transformer-Big model on WMT'14 en?fr, when training with batch sizes of 384 (left) and 768 (right). For batch size of 768, Adam and Adagrad were infeasible as they exceeded the available memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Masked LM test accuracy (left), and number of steps to get 70% test accuracy as a function of the batch size (right), of the BERT-Large language model trained on Wikipedia+BooksCorpus. SM3 with batch size 2048 uses about the same amount of memory as Adam/Adagrad with batch size 1024, and scales linearly up to a batch size of 2<ref type="bibr" target="#b15">16</ref> , at which point we hit the hardware memory limits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Top-1 (left) and Top-5 (right) test accuracy of AmoebaNet-D on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The magnitude of the 100 largest accumulators Eq. (1) of Adagrad for the embedding layer of a Transformer model trained on the WMT'14 en?fr dataset. The accumulators are sorted by magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Test log-perplexity (left) and BLUE scores (right) on of a Transformer model trained on the WMT'14 en?de dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of gradient square statistics Eq. (1) for different weight matrices using a AmoebaNet-D model. (Color intensities are in log scale.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>BLEU scores and memory usage for various batch sizes on the WMT'14 en?fr dataset.</figDesc><table><row><cell>Optimizer</cell><cell>Batch Size</cell><cell>Memory</cell><cell>BLEU</cell></row><row><cell></cell><cell>per core (total)</cell><cell>Usage per</cell><cell></cell></row><row><cell></cell><cell></cell><cell>core</cell><cell></cell></row><row><cell>Adam</cell><cell>12 (384)</cell><cell>6.88 GiB</cell><cell>38.96 ? 0.002</cell></row><row><cell>Adagrad</cell><cell>12 (384)</cell><cell>6.85 GiB</cell><cell>39.90 ? 0.003</cell></row><row><cell>Adafactor</cell><cell>12 (384)</cell><cell>5.43 GiB</cell><cell>37.89 ? 0.002</cell></row><row><cell>SM3</cell><cell>12 (384)</cell><cell>5.36 GiB</cell><cell>39.81 ? 0.002</cell></row><row><cell>Adafactor</cell><cell>24 (768)</cell><cell>7.04 GiB</cell><cell>39.65 ? 0.002</cell></row><row><cell>SM3</cell><cell>24 (768)</cell><cell>7.02 GiB</cell><cell>40.50 ? 0.001</cell></row></table><note>8], focusing on its larger variant, coined BERT-Large. BERT-Large is a large bidirectional transformer model containing 24 transformer blocks with 1024 hidden dimensions and 16 self attention heads. It has 340M parameters (1.297</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Training memory consumption at different batch sizes for BERT-Large on 8x8 TPUs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t s=1 g 2 s (i).<ref type="bibr" target="#b0">1</ref> Here we implicitly assume that the iterates of SM3-I remain bounded and D is a constant. This can be enforced by projecting the iterates to a bounded set of choice; we avoid introducing projections explicitly as they are rarely used in practice.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Omitted Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Claim 2</head><p>Proof. Monotonicity is immediate as for any r ? [k] the variable ? t (r) is increasing in t by definition. Therefore, ? t (i) = min r:Sr i ? t (r) is also increasing for all i ? [d].</p><p>Next, since g 2 s (i) ? max j?S g 2 s (j) for any set S that contains i, we have</p><p>Hence, The claim now follows since min r:Sr i ? t (r) = ? t (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Proposition 3</head><p>Proof. First, in order to establish monotonicity note that the algorithm maintains ? t (r) = max j?Sr ? t (j) for t ? 1 and r ? [k]. Hence, for t ? 1 and i ?</p><p>Let ? t (i) = t s=1 g 2 s (i). We next prove by induction that ? t (i) ? ? t (i) ? ? t (i) for all t and i ? [d]. For t = 1 this is true as ? 1 (i) = ? 1 (i) = g 2 1 (i) ? ? 1 (i) for all i by Claim 2. For the induction step, assume that ? t (i) ? ? t (i) ? ? t (i) for all i and write ? t+1 (i) = min r:Sr i max j?Sr ? t (j) + g 2 t+1 (i)</p><p>On the other hand, we have </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The case for full-matrix adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bullins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1806.02958</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/google-research/tree/master/sm3" />
		<title level="m">SM3 tensorflow optimizer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive and self-confident on-line learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="75" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the generalization ability of on-line learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2050" to="2057" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Analysis of dawnbench, a time-to-accuracy machine learning performance benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01427</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shampoo: Preconditioned stochastic tensor optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to online convex optimization. Foundations and Trends in Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Computer Architecture (ISCA</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adaptive bound optimization for online convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streeter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">244</biblScope>
		</imprint>
		<respStmt>
			<orgName>COLT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Online learning and online convex optimization. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="107" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4603" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/lingvo" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<ptr target="http://arxiv.org/abs/1803.07416" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
