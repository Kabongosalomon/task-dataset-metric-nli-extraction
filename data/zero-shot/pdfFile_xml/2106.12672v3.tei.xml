<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHARFORMER: FAST CHARACTER TRANSFORMERS VIA GRADIENT-BASED SUBWORD TOKENIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<email>yitay@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinh</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
							<email>vqtran@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baumgartner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepmind</forename></persName>
						</author>
						<title level="a" type="main">CHARFORMER: FAST CHARACTER TRANSFORMERS VIA GRADIENT-BASED SUBWORD TOKENIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce CHARFORMER, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that CHARFORMER outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, CHARFORMER is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural networks have achieved tremendous success in natural language processing (NLP) by replacing feature-engineered models with stacks of functions that are learned end-to-end from vast amounts of data <ref type="bibr" target="#b38">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b40">Peters et al., 2018;</ref><ref type="bibr" target="#b21">Howard and Ruder, 2018)</ref>. The single component of the traditional NLP pipeline <ref type="bibr" target="#b37">(Manning and Sch?tze, 1999</ref>) that has so far resisted gradient-based learning is tokenization, which is commonly applied as a pre-processing step. State-of-the-art pre-trained language models <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> generally rely on data-driven subword-based tokenization algorithms <ref type="bibr" target="#b46">(Schuster and Nakajima, 2012;</ref><ref type="bibr" target="#b47">Sennrich et al., 2016;</ref><ref type="bibr" target="#b30">Kudo and Richardson, 2018)</ref> while expert-crafted segmentation algorithms are still common for languages without whitespace separation such as Chinese, Thai, and Korean (cf. <ref type="bibr" target="#b31">Lample and Conneau, 2019)</ref>.</p><p>This reliance on rigid tokenization methods introduces a bottleneck into current NLP systems that limits their capabilities. Subword segmentation algorithms split tokens into subwords solely based on frequency, without taking into account lexical or semantic similarity. As a result, models are brittle to rare words <ref type="bibr" target="#b16">(Gong et al., 2018)</ref> and perturbations, both natural and adversarial <ref type="bibr" target="#b1">(Belinkov and Bisk, 2018;</ref><ref type="bibr" target="#b42">Pruthi et al., 2019;</ref><ref type="bibr" target="#b51">Sun et al., 2020)</ref>. In multilingual models, tokens in low-resource languages are split into many subwords, which impacts performance on those languages and deteriorates crosslingual transfer <ref type="bibr" target="#b58">Wang et al., 2021)</ref>. Finally, a separate tokenization algorithm leads to a mismatch between the pre-training and downstream distribution of words when adapting pre-trained language models to new settings, which requires significant engineering effort to overcome.</p><p>The direct application of character-level modelling into pre-trained language models in turn results in severely increased computational and memory complexity due to an increased sequence length and generally lower performance.</p><p>Published as a conference paper at ICLR 2022</p><p>To address this problem, we propose gradient-based subword tokenization (GBST), a new method that combines the compositionality of character-level representations with the efficiency of subword tokenization while enabling end-to-end learning. Our method learns latent subword representations from characters using large amounts of unlabeled data. Specifically, GBST learns a position-wise soft selection over candidate subword blocks by scoring them with a scoring network. In contrast to prior tokenization-free methods <ref type="bibr" target="#b9">(Clark et al., 2021)</ref>, GBST learns interpretable latent subwords, which enables easy inspection of lexical representations and is more efficient than other byte-based models <ref type="bibr" target="#b63">(Xue et al., 2021)</ref>. Given that simply applying a standard Transformer on a sequence of characters and bytes is computationally prohibitive, GBST paves the way for usable, practical and highly performant character-level models. A high level overview of how the GBST module is applied can be found at <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We furthermore introduce CHARFORMER, a Transformer encoder-decoder model that uses GBST to operate directly on the byte level. In addition, we experiment with a re-scaled variant of CHARFORMER, which allocates additional capacity to the encoder to make up for the lack of discrete subword embeddings. We evaluate our model on a range of standard and non-standard English, and multilingual downstream tasks. On English GLUE and long document classification tasks, CHARFORMER outperforms strong byte-level baselines and overall achieves performance on par with subword-based models such as BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and T5 <ref type="bibr" target="#b43">(Raffel et al., 2020)</ref>. On toxicity detection in social media datasets <ref type="bibr" target="#b2">(Borkan et al., 2019;</ref><ref type="bibr" target="#b61">Wulczyn et al., 2017)</ref>, CHARFORMER outperforms byte-level baselines as well as subword-based models, demonstrating robustness to spelling variation and non-standard language. Finally, a multilingually pre-trained CHARFORMER performs on par or outperforms strong subword-based multilingual baselines on standard cross-lingual datasets.</p><p>We additionally demonstrate CHARFORMER is more efficient compared to byte-level and subwordbased models with similar numbers of parameters. On a comparable setup, CHARFORMER outperforms a baseline similar to the recent state-of-the-art byte-level model ByT5 <ref type="bibr" target="#b63">(Xue et al., 2021)</ref> while being 2? more memory efficient and 10-93% faster. CHARFORMER also trains 28% faster than the subword-level mT5 model <ref type="bibr" target="#b62">(Xue et al., 2020)</ref>, has 3? fewer parameters and achieves comparable quality on well-established benchmarks. Finally, we demonstrate via visualization that the latent subwords learned by CHARFORMER are interpretable to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CHARFORMER</head><p>This section introduces our efficient character-level architecture, CHARFORMER. CHARFORMER is comprised of a Gradient-Based Subword Tokenization (GBST) module, followed by deep Transformer layers. The input to the GBST module is a sequence of characters or bytes 1 , which is then downsampled to construct latent subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRADIENT-BASED SUBWORD TOKENIZATION (GBST)</head><p>The input to GBST is a tensor of shape X ? R L?d where L is the number of input characters and d is the character embedding dimension. The key idea behind GBST is for the model to learn to perform a latent subword segmentation of the input by selecting the most suitable subword block at every character position. A block is a contiguous span of characters X i:i+b of length b for 1 ? i ? L ? b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS</head><p>We first enumerate all possible subword blocks of size b up to a maximum block size M . In order to learn subword block embeddings, we use a non-parameterized strided pooling function F : R b?d ? R d that projects a subword block consisting of a sequence of character embeddings X i:i+b ? R b?d to a single subword block representation X b,i ? R d for block size b at position i. We compute subword blocks X b,i with a stride s:</p><formula xml:id="formula_0">X b = [F (X i:i+b ); F (X (i+s):(i+s)+b ); . . .]<label>(1)</label></formula><p>In practice we set s = b, thus X b ? R L b ?d . The construction of latent subword blocks creates a shorter overall sequence length by downsampling. We construct X b for b ? 1, . . . , M , which can be seen in <ref type="figure">Figure 2</ref> for M = 4.</p><p>Considering Offsets A limitation of a strided implementation is that it is unable to model all possible subword windows. For instance, for the character sequence <ref type="bibr">[a, b, c, d]</ref> we would only be able to allocate <ref type="bibr">[a, b] and [c, d]</ref> as subword blocks of length b = 2 and would ignore the subword block <ref type="bibr">[b, c]</ref>. Offsets can be used to model sliding windows of all possible subword blocks. We consider enumerating all possible strided blocks by additionally shifting sequences up until the offset s. As this increases computation, we instead propose to first apply a 1D convolution to X, prior to enumerating subword blocks. This effectively "smoothes" over the subword blocks. We use the variant with 1D convolutions in our main experiments and provide additional ablations in ?4.4.</p><p>Considering Intra-block Positions It is important to preserve the ordering of the characters within the block X i , X i+1 , . . . , X i+b . E.g., the output of F should differ for the blocks abc and bca. For certain choices of F it may be valuable to add a positional embedding <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> to X i:i+b before applying F . Note that this positional embedding would only be for individual blocks, and is not global to the entire input sequence. That is, only positional embedding values for positions 1, . . . , b would be used. However, in practice we apply a 1D convolution before the GBST layer and use the mean-pooling function for F . We find this to be sufficient to distinguish between same sized blocks with different character orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">BLOCK SCORING NETWORK</head><p>In order to allow the model to learn which block to select for every character position, we introduce a block scoring network. The block scoring network is simply a parameterized function F R (.) that produces a score for each candidate block. Given a subword candidate block X b,i ? R d , we compute a score p b,i associated with the block using a simple linear transformation F R : R d ? R:</p><formula xml:id="formula_1">p b,i = F R (X b,i )<label>(2)</label></formula><p>We perform ranking of subword blocks with regard to each character position in the original sequence. At every position i, the model learns to select the most suitable subword block X b,i among all block sizes 1 ? b ? M . As each sequence of subword blocks X b is downsampled, we realign the representations of the subword blocks by upsampling each X b to its original sequence length L. Specifically, for a block size of b, we replicate each block representation X b,i b times. We then score each candidate block at each position i using the softmax function:</p><formula xml:id="formula_2">P i = softmax([p 1,i , p 1,i , ? ? ? , p M,i ]),<label>(3)</label></formula><p>which computes a relative score of each candidate block at each position and P i ? R M . We show the scoring of realigned blocks in <ref type="figure">Figure 2</ref>.</p><p>(a) Formation of subword blocks to be scored by FR.</p><p>Offsets and/or pre-GBST convolutions not shown.</p><p>(b) Block scores that have been expanded back to length L. Softmax is taken over block scores at each position i to form block weights for constructing latent subword representations. <ref type="figure">Figure 2</ref>: Illustration of subword block formation and scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">FORMING LATENT SUBWORDS</head><p>We then sum the representations of all subword blocks X b,i at each position i multiplied by their learned probability P b,i to form a latent subword representationX i ? R d :</p><formula xml:id="formula_3">X i = M b P b,i X b,i<label>(4)</label></formula><p>Intuitively, the model learns an ideal subword block for each position. In contrast to standard deterministic subword tokenization algorithms, this selection is soft and can thus consider different possible segmentations at every position i. In general, however, this formulation still assumes that subwords are contiguous sequences of characters. While additional context can be considered via the convolutions in ?2.1.1, non-concatenative morphology where morphemes are discontinuous may be harder for the method to model. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">POSITION-WISE SCORE CALIBRATION</head><p>In the above approach, the scoring of each position is independent of other positions. We hypothesize that it may be beneficial for block scores at each position to be aware of each other. To this end, we introduce an optional module that enables learning a consensus among block scores by calculating dot products across the scores P i across all positions i ? [1, L]. This can be viewed as a form of self-attention across block scores, albeit without any projections for computational efficiency. To learn the new scoresP ? R L?M , we computeP = softmax(P P )P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">DOWNSAMPLING</head><p>After learning a candidate block or mixture of blocks for each position, we use a downsampling function F D : R L?d ? R L ds ?d that downsamples the sequence of latent subwordsX = [X 1 , . . . ,X L ] t? X, reducing its sequence length by a factor of d s . We choose F D to be a non-parameterized mean pooling operation. Notably, such simple stride-based pooling removes potential redundancies caused by adjacent positions selecting similar blocks as the mean pool of two identical block embeddings produces the same outcome. Intuitively, as the downsampling operation is fixed, the parameterized components preceding it should learn an optimal subword tokenization given the downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TRANSFORMER STACK</head><p>The remainder of the CHARFORMER model remains identical to a regular Transformer encoderdecoder model. The Transformer stack operates on the downsampled latent subwordsX instead of subword embeddings.</p><p>Re-scaling of the Transformer Stack While subword-based models allocate much of their capacity to subword embeddings-up to 71% of all parameters for contemporary multilingual models (Chung et al., 2021)-, the character vocabulary of character-level models is much smaller and thus less expressive. Similar to <ref type="bibr" target="#b63">Xue et al. (2021)</ref>, we hypothesize that character-level models require deeper encoder stacks than subword-based models to make up for their smaller embedding capacity. Consequently, we explore a scaling variant of CHARFORMER that puts more parameters at the encoder at the expense of the decoder while preferring a deep narrow model over a larger wide model. Specifically, we re-configure the Base model size to be similar to the T5 Small model size, with an expanded 24 layers in the encoder. The resulting CHARFORMER SBase (Scaled Base) has 134M parameters, which is about 67% the parameter footprint of the standard base T5 model <ref type="bibr">(200M parameters;</ref><ref type="bibr" target="#b43">Raffel et al., 2020)</ref>. Moreover, this particular CHARFORMER model is approximately 50-100% faster than the T5 base model (see ?4.1). <ref type="bibr">3</ref> For the re-scaled variant, we also used the GLU variant described in <ref type="bibr" target="#b48">(Shazeer, 2020)</ref> which is commonly referred to as the V1.1 variant in the T5 library.</p><p>A Note on Comparing Character-level and Subword-based Methods Prior work on efficient methods generally compares models with the same number of parameters <ref type="bibr" target="#b7">(Chung et al., 2021)</ref>. However, whereas embedding look-up even with large vocabularies in subword-based methods is O(1), re-distributing the subword embedding parameters in character-level models such as ByT5 <ref type="bibr" target="#b63">(Xue et al., 2021)</ref> to dense layers incurs much higher computational costs-a 25% penalty in training speed. We believe that a fair re-scaling of character-level models should not only aim to match the number of parameters but also the compute and inference costs of subword-based models under the assumption that char/byte-level models will require longer sequences (see ?4.1 for a comparison).</p><p>Span-based Pre-training Our pre-training scheme follows T5 quite closely. We mask N contiguous characters and train to predict them in a sequence-to-sequence architecture following <ref type="bibr" target="#b63">Xue et al. (2021)</ref>. The model optimizes the cross-entropy loss and is trained with teacher forcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We evaluate our method both in English as well as in a multilingual setting on relevant benchmarks and compare against state-of-the-art character-level and subword-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS</head><p>Data To showcase the effectiveness of the proposed method, we evaluate on a diverse set of standard English tasks from GLUE covering sentiment classification (SST-2; <ref type="bibr" target="#b50">Socher et al., 2013)</ref>, natural language inference (MNLI, QNLI; <ref type="bibr" target="#b44">Rajpurkar et al., 2016)</ref>, paraphrase detection (Dolan and Brockett, 2005, MRPC, QQP) and sentence similarity <ref type="bibr" target="#b5">(Cer et al., 2017)</ref>. In addition, we evaluate on tasks that require dealing with long documents, both for sentiment analysis (IMDb; <ref type="bibr" target="#b36">Maas et al., 2011)</ref> and news classification (AGNews; <ref type="bibr" target="#b66">Zhang et al., 2015)</ref>.</p><p>Baselines We compare CHARFORMER against the following state-of-the-art subword-based models: BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref>, an encoder-only pre-trained masked language model; and T5 <ref type="bibr" target="#b43">(Raffel et al., 2020)</ref>, an encoder-decoder model. We also compare against Byte-level T5 <ref type="bibr" target="#b63">(Xue et al., 2021</ref>), a T5 model that is directly applied to bytes. We additionally evaluate the impact of the downsampling in CHARFORMER by comparing it to the downsampling used by the character-level CANINE <ref type="bibr" target="#b9">(Clark et al., 2021)</ref> model in our framework. CANINE downsamples a character sequence using local attention and pooling via strided convolutions. As the original CANINE uses an encoder-only model and was only trained on multilingual data, we integrate CANINE-style downsampling into Byte-level T5, which we refer to as Byte-level T5+LASC (local attention-strided convolution). 4 As an ablation for the GBST inductive bias, we compare against Byte-level T5+Conv Base a convolutional baseline of Byte-level T5 with a 1D convolution of filter size 5 placed before the encoder. Note that in all the baselines and for CHARFORMER base models, in the spirit of fair comparison, we compare them at an equal parameterization (size). Our scaling experiments are reserved for our SBase models, which is intended to only be compared with subword T5 models, and not to unscaled byte-level baselines. Finally, we include an SBase scaled version of Byte-level T5 for comparison. Setup We evaluate Base and SBase configurations of CHARFORMER with 203M and 134M parameters respectively. We compare to Base configurations of BERT and T5 that have a similar number of parameters. We pre-train all models on the C4 corpus for 1M steps using a batch size of 64 and sequence length of 1024. All non-subword models use a vocabulary of 256 bytes. 5 Our pre-training scheme corrupts spans with a mean length of 20 bytes. Each model is pre-trained on 16 TPU V3 chips. We pre-train our models with the Adafactor optimizer with an inverse square root learning rate. We then fine-tune on each individual task separately using a constant learning rate of 10 ?3 . More details can be found in the Appendix.  Results For all result tables, we divide the table into three sections: subword baseline(s), un-scaled byte-level baselines, and scaled CHARFORMER results. If a section and task combination has more than one model result, we underline the best result. We show result for GLUE in <ref type="table" target="#tab_0">Table 1</ref>. CHAR-FORMER outperforms other character-level baselines trained under the same conditions with the same number of parameters across all tasks, while being considerably faster and requiring less compute than T5-style models that are directly applied to bytes or characters (see ?4.1). CHARFORMER SBase performs even better despite having a smaller number of parameters compared to the Base configuration, demonstrating the usefulness of rescaling the transformer stack for character-level models. CHARFORMER SBase furthermore is the only model that performs on par or even outperforms the standard subword-based models on some tasks in standard English. In <ref type="table" target="#tab_2">Table 3</ref> we provide results for text classification of long documents. Here, CHARFORMER SBase is the only byte-level model to outperform T5 Base,Subword on the IMDb classification task, and both CHARFORMER models outperform byte and subword level baselines on AGNews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPERIMENTS ON NON-STANDARD ENGLISH DATASETS</head><p>The previous set of experiments demonstrated the ability of CHARFORMER to perform well on clean datasets consisting of standard English. However, character-level models are particularly suited to data that is noisy, containing spelling variations, typos, and other non-standard language.</p><p>Data To demonstrate CHARFORMER's ability to perform well on such data, we evaluate on toxicity detection using the Civil Comments <ref type="bibr" target="#b2">(Borkan et al., 2019)</ref> and the Wikipedia Comments (Wulczyn <ref type="table">Table 4</ref>: Multilingual comparison of CHARFORMER against subword and byte-level models on in-language multi-task, translate-train multi-task, and cross-lingual zero-shot (training on English) settings. Model sizes are the same as those in <ref type="table" target="#tab_0">Table 1</ref>. mBERT and mT5 baseline results are from <ref type="bibr" target="#b62">(Xue et al., 2020)</ref>.  <ref type="bibr">, 2017)</ref> datasets. Both are standard benchmarks that require estimating the toxicity of usergenerated content. We use the same setup as for the standard English datasets.</p><formula xml:id="formula_4">In-Language Translate-Train-All Zero-Shot Model |?| TyDiQA-GoldP XQuAD MLQA XNLI PAWS-X XNLI PAWS-X mBERT Base (Subword) 179M 77.6/68.0 -/- -/- - - 65.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We show results in <ref type="table" target="#tab_1">Table 2</ref>. Character-level models outperform the subword-based T5 model on both datasets, demonstrating their suitability to deal with such noisy, user-generated data. CHARFORMER achieves performs on par or outperforms other character-level methods on both datasets across the different model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MULTILINGUAL EXPERIMENTS</head><p>Data To evaluate the effectiveness of character-level models on multilingual data, we evaluate on standard cross-lingual question answering and classification tasks. In particular, we evaluate on the question answering tasks TyDiQA-GoldP <ref type="bibr" target="#b8">(Clark et al., 2020)</ref>, XQuAD <ref type="bibr" target="#b0">(Artetxe et al., 2020)</ref>, and MLQA <ref type="bibr" target="#b34">(Lewis et al., 2020)</ref> as well as the natural language inference task XNLI <ref type="bibr" target="#b10">(Conneau et al., 2018)</ref> and the paraphrase detection task PAWS-X <ref type="bibr" target="#b64">(Yang et al., 2019)</ref> from XTREME . We evaluate on the in-language multi-task setting for TyDiQA-GoldP <ref type="bibr" target="#b8">(Clark et al., 2020)</ref> where models are fine-tuned on the combined gold data in all target languages and the translate-train-all setting where models are fine-tuned on English training data plus translations in all target languages for the other datasets. Both are the best-performing settings for the respective tasks in . In addition, we evaluate on zero-shot cross-lingual transfer from English on XNLI and PAWS-X.</p><p>Baselines We compare to strong multilingual subword-based baselines including multilingual BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and multilingual T5 <ref type="bibr" target="#b62">(Xue et al., 2020)</ref>. In addition, we compare to the byte-level models from ?3.1, which we pre-train on multilingual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>We pre-train CHARFORMER as well as the Byte-level T5 and Byte-level T5+LASC baselines on multilingual mC4 Common Crawl <ref type="bibr" target="#b62">(Xue et al., 2020)</ref> in 101 languages. Base size models were trained for 1M steps using a batch size of 64 and sequence length of 2048, with the exception of Byte-level T5 Base , which was trained with a sequence length of 1024, as training speed was prohibitively slow (see <ref type="table" target="#tab_0">Table 10</ref>). CHARFORMER SBase and CHARFORMER SBase,LongP T (longer pre-training) are trained with larger batch sizes for fair comparison with mT5. In particular, CHAR-FORMER SBase pre-trains on the same amount of tokens after downsampling as mT5 Base , while CHARFORMER SBase,LongP T pre-trains on roughly the same amount of raw text as mT5 Base , given that a SentencePiece subword token is about 4.1 bytes on average <ref type="bibr" target="#b63">(Xue et al., 2021)</ref>; see <ref type="table" target="#tab_4">Table 5</ref> for further details. All models were fine-tuned with an input sequence length of 4096 for questionanswering tasks and 2048 for inference tasks. Score calibration was not used for these experiments, as it did not benefit the model in the multilingual setting. For XNLI and PAWS-X (both translate-train and zero-shot settings), we also observed that performance improved if the GBST layer was not updated during fine-tuning; the reported CHARFORMER numbers reflect this configuration. Otherwise, all other hyper-parameters and model sizes are unchanged from the English experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We show in-language multi-task, translate-train, and cross-lingual zero-shot results in <ref type="table">Table 4</ref>. CHARFORMER SBase is competitive with standard subword-based models and CHAR-FORMER SBase,LongP T outperforms subword-based models on TyDiQA-GoldP (in-language multitask). Additionally, in the translate-train setting CHARFORMER SBase,LongP T is on par with subword models on XQuAD and MLQA, and close to parity on PAWS-X. Furthermore, CHARFORMER  outperforms other character-level models in the zero-shot setting. However, we observe that this setting still remains a challenge for token-free models in general. We hypothesize that model size may be a major factor here. Finally, we provide additional comparison between GBST and LASC at a fixed down-sampling rate in Section 4.3, showing that GBST significantly outperforms LASC on TyDiQA. <ref type="table" target="#tab_5">Table 6</ref> reports the speed (global training steps per second), parameter sizes and number of floating point operations (FLOPS) for each forward pass of the models used in our experiments. All experiments were run on 16 TPU-v3 chips and speed is benchmarked on English C4 pre-training at the 1K input length (L). CHARFORMER models are generally more efficient both in terms of speed and FLOPS compared to other character-level models at different parameter sizes. With a low down-sampling rate d s for CHARFORMER, Byte-level T5+LASC is more efficient due to using a higher down-sampling rate. Directly consuming the character sequence with a Transformer model is slow and requires a large number of FLOPS, which is exacerbated with longer sequence lengths where Byte-level T5 is more than 2? slower than the fastest CHARFORMER. This difference is even larger at longer input sequence lengths, which we report in the Appendix. CHARFORMER SBase achieves better performance (see ?3) with fewer parameters but more FLOPS by using a deep thin encoder and is twice as fast as the subword-based model with similar performance, T5 Base .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SPEED, MEMORY AND PARAMETERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VISUALIZING LATENT SUBWORDS</head><p>One benefit of CHARFORMER compared to other character-level methods is that the subwords it learns are directly interpretable and may give some indications to the behaviour of the underlying model. We visualize the scores the multilingual CHARFORMER has learned to assign to subword blocks of different sizes for the string 'on subword tokenization' in <ref type="figure" target="#fig_1">Figure 3</ref>. We observe that the model learns to allocate single-character subword blocks predominantly to vowels and whitespace in English. Moreover, in English the model allocates larger subword blocks to the beginning and end  consonants of a subword. Together, we believe this suggests that the model has learned a meaningful segmentation of the input, and that it is able to dynamically mix between byte-level and subword-level features. Such behaviour could also parallel the relative importance attributed to consonants for word identification observed during reading in humans <ref type="bibr" target="#b32">(Lee et al., 2001;</ref><ref type="bibr" target="#b4">Carreiras et al., 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPARING DOWNSAMPLING APPROACHES</head><p>In <ref type="table" target="#tab_9">Table 9</ref>, we compare GBST downsampling with LASC downsampling <ref type="bibr" target="#b9">(Clark et al., 2021)</ref> on TyDiQA-GoldP. For this experiment we use the same hyperparameters as in Section 3.3, except the pre-training input length is 1024 instead of 2048. Note that this difference is negligible (0.1 F1) for CHARFORMER Base , d s = 2 which also appears in <ref type="table">Table 4</ref>. All hyperparameters are fixed between CHARFORMER and Byte-level T5+LASC. Following <ref type="bibr" target="#b9">(Clark et al., 2021)</ref> we set d s = 4 for LASC, and we compare CHARFORMER at the same downsampling rate. We additionally include d s = 2 and d s = 3 for CHARFORMER for comparison. With the same hyperparameters and downsampling rate, CHARFORMER outperforms Byte-level T5+LASC on TyDiQA-GoldP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDY</head><p>This section presents our ablation experiments for both English and multilingual tasks. We analyze the impact of various hyper-parameters and modeling choices such as using offsets vs 1D convolutions. Across experiments, we find that pre-GBST convolutions are preferred to enumerating offset blocks, as it results in similar (or better) quality but a more efficient implementation. For English tasks, block score calibration (BC) improves performance. We note that in the multilingual setting, block score calibration has little effect. The impact of different downsampling rates varies across tasks and model sizes. We also experimented with different convolution filter sizes in English and found that they did not significantly impact performance. Likewise, using a different character span corruption rate during pre-training did not significantly impact performance. Adding feed-forward layers to the CHARFORMER module in similar fashion to a Transformer block was also not obviously helpful.  , and SentencePiece <ref type="bibr" target="#b30">(Kudo and Richardson, 2018)</ref>. Prior work has highlighted issues with some of these algorithms <ref type="bibr" target="#b3">(Bostrom and Durrett, 2020)</ref> and has generally observed that models learned with such rigid tokenization do not cope well with variation in language <ref type="bibr" target="#b51">(Sun et al., 2020)</ref>. To make a model more robust to morphological and compositional generalization, probabilistic segmentation algorithms such as subword regularization <ref type="bibr" target="#b29">(Kudo, 2018)</ref> and BPE-dropout <ref type="bibr" target="#b41">(Provilkov et al., 2020)</ref> have been proposed, which sample different segmentations during training. Recent methods propose to make models more robust for downstream tasks by enforcing prediction consistency between deterministic and probabilistic segmentations <ref type="bibr" target="#b58">(Wang et al., 2021)</ref> and propose to update the tokenizer based on the downstream loss under different segmentations <ref type="bibr" target="#b19">(Hiraoka et al., 2020;</ref>. <ref type="bibr" target="#b18">He et al. (2020)</ref> proposed DPE (dynamic programming encoding), a segmentation-based tokenization algorithm based on dynamic programming. Such methods, however, incur large computation costs due multiple forward passes needing to be performed for each segmentation of an example or due to the expensive DP computation, which make them unsuitable for pre-training.</p><p>Character-level models For recurrent neural networks, pure character-level models that take a sequence of characters as input <ref type="bibr" target="#b17">(Graves, 2013;</ref><ref type="bibr" target="#b66">Zhang et al., 2015;</ref><ref type="bibr" target="#b23">Hwang and Sung, 2017)</ref> have mostly been superseded by character-aware methods that compute a token-level representation using a CNN over characters <ref type="bibr" target="#b27">(Kim et al., 2016;</ref><ref type="bibr" target="#b24">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b40">Peters et al., 2018)</ref> due to poor performance when learning directly from characters. Such character-aware representations have lately been applied to deep Transformer models <ref type="bibr" target="#b15">(El Boukkouri et al., 2020;</ref>. These methods, however, still require tokenization for pre-processing and cannot be directly applied to languages without whitespace separation. Prior work also learned segmentation as part of the model but did not scale very well <ref type="bibr" target="#b56">(Wang et al., 2017;</ref><ref type="bibr" target="#b28">Kreutzer and Sokolov, 2018;</ref><ref type="bibr" target="#b26">Kawakami et al., 2019)</ref>. One notable exception is <ref type="bibr" target="#b33">(Lee et al., 2017)</ref>, which enabled fully character-level neural machine translation, using stacked convolutions, max pooling, and highway networks. Building on this, recent tokenization-free approaches such as CANINE <ref type="bibr" target="#b9">(Clark et al., 2021)</ref> revisit the original character-level setting in the context of large pre-trained language models with a focus on multilingual models. Our method outperforms CANINE-style downsampling (local attention, strided convolutions) and also leads to improvements in the monolingual setting, while using less compute and parameters to down-sample than both <ref type="bibr" target="#b33">Lee et al. (2017)</ref> and <ref type="bibr" target="#b9">Clark et al. (2021)</ref>. Recently, ByT5 <ref type="bibr" target="#b63">(Xue et al., 2021)</ref> set new start-of-the-art results for tokenization-free models, by operating on the byte-level. This work performs on par with or outperforms ByT5, with significant gains in speed and compute efficiency.</p><p>Multilingual models Current multilingual models are generally analogues to successful monolingual Transformer models . Consequently, models such as multilingual BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> and XLM-R <ref type="bibr" target="#b11">(Conneau et al., 2020)</ref> employ the same subword tokenization algorithms as monolingual models, now applied to a massively multilingual corpus. In the multilingual setting, the problems of subword-based tokenization are exacerbated as tokens in languages with few data are over-segmented while high-frequency tokens are under-segmented, which limits cross-lingual transfer <ref type="bibr" target="#b58">(Wang et al., 2021)</ref>. This motivates our work as well as recent work on character-level models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Transformers</head><p>Moving from subwords to characters significantly increases the sequence length, which is an issue for Transformers due to the quadratic complexity of self-attention. Many efficient self-attention models have been proposed <ref type="bibr" target="#b6">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b65">Zaheer et al., 2020)</ref> to tackle this problem; see <ref type="bibr" target="#b53">(Tay et al., 2020b</ref>;a) for a comprehensive overview. Notably, the CANINE model uses local attention , which could also be swapped with another efficient Transformer variant. We note that the problem of efficiency is important but not the only challenge towards developing performant tokenization-free models. While applying an efficient attention mechanism might solve the fundamental computational costs of employing character-level models, there is no guarantee that these models will learn locally meaningful compositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have proposed CHARFORMER, a re-scaled Transformer architecture that integrates gradient-based subword tokenization, a novel lightweight tokenization method that enables efficient end-to-end learning of latent subwords directly from characters. We have demonstrated that English and multilingual variants of CHARFORMER outperform strong character-level baselines across various datasets while being more efficient. CHARFORMER achieves performance on par with subword-based models on standard English tasks and outperforms subword-based models on noisy social media data. On multilingual data, CHARFORMER generally performs on par with subword-based models, while being faster than both byte-level and subword-level baselines. Finally, we provide a method to inspect the inner workings of the GBST module. Overall, we believe that the strong results presented in this paper pave the way for highly effective and powerful token-free models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>Standard subword tokenization algorithms produce segmentations that do not equally represents words and phrases in different languages. Instead, they are biased towards languages that already have many resources available, which leads to multilingual models performing worse on underrepresented languages <ref type="bibr" target="#b58">(Wang et al., 2021)</ref>. Tokenization-free approaches such as the one proposed in this paper may help to ameliorate this to some extent. Another challenge to using large multilingual models in practice is their relative computational inefficiency, which makes them unsuitable in resource-constrained settings common in scenarios where under-represented languages are spoken. CHARFORMER trains 28% faster than mT5 and has 3? fewer parameters, so may be a more suitable choice in such settings compared to state-of-the-art multilingual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>All code to train the core byte-level Transformer encoder-decoder for CHARFORMER its variants is already open-sourced as a part of the Mesh Tensorflow 6 , T5 7 <ref type="bibr" target="#b43">(Raffel et al., 2020)</ref>, and ByT5 8 <ref type="bibr" target="#b63">(Xue et al., 2021)</ref>   <ref type="figure" target="#fig_0">3072 and 12</ref> heads. The SBase model has 24 encoder layers and 6 decoder layers, while the remainder of its hyperparameters remain identical to the small model. All Transformer stacks use relative attention over positional encodings as per <ref type="bibr" target="#b43">(Raffel et al., 2020)</ref>. For pre-training, we run our models for 1M steps on C4 with a batch size of 64. The maximum sequence length for all tasks is set to 1024. TPU packing is not activated for Charformer. For Charformer, the filter size of the pre-GBST convolution is set to 5 by default. For CHARFORMER, the downsampling rate is tuned in the range of {2, 3, 4}. For smaller models, the rate of 2 seems to work consistently the best. For base models, the best models used a downsampling rate of either 2 or 3. For the SBase models, the optimal downsampling rate was often 3.</p><p>Multilingual Datasets Hyperparameters are kept constant between English and multilingual tasks except for the following differences. For pre-training, we run our models for 1M steps with a batch size of 64, except for CHARFORMER SBase which uses a batch size of 1024 and CHAR-FORMER SBase,LongP T which usees a batch size of 2048. Models were pre-trained with a maximum sequence length of 2048 and fine-tuned with a maximum sequence length of 4096 for TyDiQA, XQuAD, and MLQA, and 2048 for XNLI and PAWS-X. Byte-level T5 Base was the only model to be pre-trained with a maximum sequence length of 1024, as it was prohibitively slow, see <ref type="table" target="#tab_0">Table 10</ref>. Fine-tuning and inference for this model, however still used 4096 and 2048 input lengths identical to other models. For all tasks, CHARFORMER models used a downsampling rate of 2, while Byte-level T5+LASC models used a downsampling rate of 4 <ref type="bibr" target="#b9">(Clark et al., 2021)</ref>. The downsampling rate of 2 was picked by ablating the downsampling rate on the TyDiQA-GoldP validation set. CHARFORMER models for XNLI and PAWS-X additionally did not back-propagate into the GBST layer during fine-tuning. Checkpoints were picked based on the dev set metrics, and then evaluated on test set. Reported metrics represent the macro-average of all languages in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">LARGE-SCALE EXPERIMENTS</head><p>In this section we report preliminary results for scaling Charformer using the same number of parameters as mT5 Large and ByT5 Large (1.23B). We follow a model scaling configuration identical to ByT5 in these experiments, and use the same hyperparameter settings as our main multilingual results.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>High-level differences between traditional subword Transformer models and Charformer which uses gradient-based subword tokenization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of block scores (softmax weights) for every byte position from multilingual CHARFORMER SBase on an example English input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of CHARFORMER against other subword and character-level models with different parameter sizes on diverse standard English datasets.</figDesc><table><row><cell>Model</cell><cell>|?|</cell><cell>SST-2</cell><cell>MNLI</cell><cell>QNLI</cell><cell>MRPC</cell><cell>QQP</cell><cell cols="3">STSB COLA AVG</cell></row><row><cell>BERT Base,Subword</cell><cell>110M</cell><cell>92.7</cell><cell>84.4/-</cell><cell>88.4</cell><cell>86.7/-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>T5 Base,Subword</cell><cell>220M</cell><cell>92.7</cell><cell>84.2/84.6</cell><cell>90.5</cell><cell cols="2">88.9/92.1 91.6/88.7</cell><cell>88.0</cell><cell>53.8</cell><cell>84.3</cell></row><row><cell>Byte-level T5 Base</cell><cell>200M</cell><cell>91.6</cell><cell>82.5/82.7</cell><cell>88.7</cell><cell cols="2">87.3/91.0 90.9/87.7</cell><cell>84.3</cell><cell>45.1</cell><cell>81.5</cell></row><row><cell>Byte-level T5+Conv Base</cell><cell>205M</cell><cell>89.8</cell><cell>81.1/82.5</cell><cell>89.2</cell><cell cols="2">83.6/89.2 90.7/87.7</cell><cell>85.0</cell><cell>47.1</cell><cell>81.2</cell></row><row><cell cols="2">Byte-level T5+LASC Base 205M</cell><cell>90.0</cell><cell>80.0/80.8</cell><cell>87.1</cell><cell cols="2">82.8/88.1 89.0/85.4</cell><cell>83.7</cell><cell>25.3</cell><cell>77.0</cell></row><row><cell>CHARFORMER Base</cell><cell>203M</cell><cell>91.6</cell><cell>82.6/82.7</cell><cell>89.0</cell><cell cols="2">87.3/91.1 91.2/88.1</cell><cell>85.3</cell><cell>42.6</cell><cell>81.4</cell></row><row><cell>Byte-level T5 SBase</cell><cell>133M</cell><cell>91.2</cell><cell>83.9/83.7</cell><cell>90.9</cell><cell cols="2">85.5/89.2 91.1/88.1</cell><cell>85.7</cell><cell>49.3</cell><cell>82.6</cell></row><row><cell>CHARFORMER SBase</cell><cell>134M</cell><cell>91.5</cell><cell>83.7/84.4</cell><cell>91.0</cell><cell cols="2">87.5/91.4 91.4/88.5</cell><cell>87.3</cell><cell>51.8</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on comment classification on Civil Comments and Wiki Comments. Metrics are accuracy and AUC-PR. T5 baseline results are from<ref type="bibr" target="#b54">(Tay et al., 2021)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">Civil Comments Wiki Comments</cell></row><row><cell>T5 Base,Subword</cell><cell>81.2 / -</cell><cell>91.5 / -</cell></row><row><cell>Byte-level T5 Base</cell><cell>82.8 / 78.7</cell><cell>93.2 / 75.4</cell></row><row><cell>Byte-level T5+LASC Base</cell><cell>82.9 / 78.2</cell><cell>93.0 / 75.0</cell></row><row><cell>CHARFORMER Base</cell><cell>83.0 / 78.8</cell><cell>92.7 / 79.7</cell></row><row><cell>CHARFORMER SBase</cell><cell>83.0 / 78.9</cell><cell>93.5 / 75.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on text classification on long documents.</figDesc><table><row><cell>Model</cell><cell cols="2">IMDb News</cell></row><row><cell>T5 Base,Subword</cell><cell>94.2</cell><cell>93.5</cell></row><row><cell>Byte-level T5 Base</cell><cell>91.5</cell><cell>93.6</cell></row><row><cell>Byte-level T5+LASC Base</cell><cell>91.1</cell><cell>93.5</cell></row><row><cell>CHARFORMER Base</cell><cell>91.5</cell><cell>94.0</cell></row><row><cell>CHARFORMER SBase</cell><cell>94.4</cell><cell>94.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of pre-training compute metrics for mT5 (Subword) versus comparable quality CHARFORMER models on the mC4 dataset. 64 TPUv3 chips were used for this experiment. CHARFORMER SBase sees the same number of tokens after downsampling as mT5 Base , while CHARFORMER SBase,LongP T roughly sees the same amount of raw text as mT5 Base , given that a SentencePiece subword token is about 4.1 bytes on average<ref type="bibr" target="#b63">(Xue et al., 2021)</ref>. CHARFORMER SBase is 28% faster than mT5 Base , while using 33% of the FLOPS.</figDesc><table><row><cell>Model</cell><cell>Batch Size</cell><cell>L</cell><cell>ds</cell><cell>|?|</cell><cell>Speed (steps/s)</cell><cell>FLOPS</cell></row><row><cell>mT5Base (Subword)</cell><cell>1024</cell><cell>1024</cell><cell>-</cell><cell>582M</cell><cell>1.54</cell><cell>1.3 ? 10 15</cell></row><row><cell>CHARFORMERSBase</cell><cell>1024</cell><cell>2048</cell><cell>2</cell><cell>134M</cell><cell>1.98</cell><cell>4.3 ? 10 14</cell></row><row><cell>CHARFORMERSBase,LongP T</cell><cell>2048</cell><cell>2048</cell><cell>2</cell><cell>134M</cell><cell>1.01</cell><cell>4.3 ? 10 14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Pre-training compute metrics of models at different input lengths, downsampling rates, and model sizes on the English C4 dataset. 16 TPUv3 chips were used for this experiment. These numbers reflect a batch size of 64. Memory refers to per-device peak memory usage on TPUv3 chips.</figDesc><table><row><cell>Model</cell><cell>L</cell><cell>ds</cell><cell>|?|</cell><cell>Speed (steps/s)</cell><cell>FLOPS</cell><cell>Peak Mem.</cell></row><row><cell>T5Base (Subword)</cell><cell>512</cell><cell>-</cell><cell>220M</cell><cell>9.3</cell><cell>1.1 ? 10 13</cell><cell>-</cell></row><row><cell>Byte-level T5Base</cell><cell>1024</cell><cell>1</cell><cell>200M</cell><cell>8.2</cell><cell>2.9 ? 10 13</cell><cell>3.09GB</cell></row><row><cell cols="2">Byte-level T5+LASCBase 1024</cell><cell>4</cell><cell>205M</cell><cell>15</cell><cell>9.9 ? 10 12</cell><cell>1.62GB</cell></row><row><cell>CHARFORMERBase</cell><cell>1024</cell><cell>2</cell><cell>206M</cell><cell>11</cell><cell>1.6 ? 10 13</cell><cell>1.95GB</cell></row><row><cell>CHARFORMERBase</cell><cell>1024</cell><cell>3</cell><cell>203M</cell><cell>15</cell><cell>1.1 ? 10 13</cell><cell>1.63GB</cell></row><row><cell>CHARFORMERSBase</cell><cell>1024</cell><cell>2</cell><cell>134M</cell><cell>14</cell><cell>1.3 ? 10 13</cell><cell>1.73GB</cell></row><row><cell>CHARFORMERSBase</cell><cell>1024</cell><cell>3</cell><cell>134M</cell><cell>20</cell><cell>8.7 ? 10 12</cell><cell>1.34GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effect of d s on TyDiQA-GoldP (in-language multi-task).</figDesc><table><row><cell>Model</cell><cell cols="2">ds TyDiQA-GoldP F1</cell></row><row><cell>CHARFORMER Small</cell><cell>2</cell><cell>69.6</cell></row><row><cell>CHARFORMER Small</cell><cell>3</cell><cell>68.1</cell></row><row><cell>CHARFORMER Small</cell><cell>4</cell><cell>66.6</cell></row><row><cell>Byte-level T5+LASC Small</cell><cell>4</cell><cell>64.9</cell></row><row><cell>CHARFORMERBase</cell><cell>2</cell><cell>75.8</cell></row><row><cell>CHARFORMERBase</cell><cell>3</cell><cell>74.3</cell></row><row><cell>CHARFORMERBase</cell><cell>4</cell><cell>73.2</cell></row><row><cell>Byte-level T5+LASCBase</cell><cell>4</cell><cell>70.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation studies with CHARFORMER Small on English tasks.</figDesc><table><row><cell>Ablation</cell><cell cols="5">ds Size SST-2 MNLImm IMDb</cell></row><row><cell>Offsets</cell><cell>2</cell><cell>S</cell><cell>89.11</cell><cell>79.50</cell><cell>90.49</cell></row><row><cell>Conv</cell><cell>2</cell><cell>S</cell><cell>89.11</cell><cell>79.65</cell><cell>90.63</cell></row><row><cell>Conv + BC</cell><cell>2</cell><cell>S</cell><cell>89.56</cell><cell>80.15</cell><cell>90.60</cell></row><row><cell>Conv + Offsets + BC</cell><cell>2</cell><cell>S</cell><cell>89.11</cell><cell>79.68</cell><cell>90.48</cell></row><row><cell>Conv</cell><cell>3</cell><cell>S</cell><cell>89.45</cell><cell>80.07</cell><cell>90.15</cell></row><row><cell>Conv</cell><cell>4</cell><cell>S</cell><cell>89.11</cell><cell>79.82</cell><cell>90.21</cell></row><row><cell>Conv</cell><cell>2</cell><cell>B</cell><cell>90.60</cell><cell>82.92</cell><cell>91.46</cell></row><row><cell>Conv</cell><cell>3</cell><cell>B</cell><cell>91.40</cell><cell>82.74</cell><cell>91.46</cell></row><row><cell>Conv</cell><cell>4</cell><cell>B</cell><cell>91.40</cell><cell>82.67</cell><cell>92.33</cell></row><row><cell>5 RELATED WORK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Subword tokenization Standard algorithms for deterministic subword tokenization are Byte Pair Encoding (BPE; Sennrich et al., 2016), Wordpiece</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>This section describes the hyperparameters that we use in our experiments.Monolingual English Datasets Our small model follows the T5 small model size with 6 encoder layers and 6 decoder layers, hidden size d model of 512, 8 heads, d kv of 32 and d f f of 2048. This corresponds to bi_v1_small.gin in the T5 codebase. The base model (corresponding to bi_v1.gin) has 12 encoder layers, 12 decoder layers, d model of 768, d f f of</figDesc><table><row><cell>7 APPENDIX</cell></row><row><cell>7.1 HYPERPARAMETERS</cell></row><row><cell>libraries. Additionally, an implementation of Charformer GBST</cell></row><row><cell>compatible with existing open-source models has been open-sourced 9 . All detailed experiment and</cell></row><row><cell>hyperparameter settings required to reproduce our experiments can be found in Section 7.1 of the</cell></row><row><cell>Appendix.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison on TyDiQA at 1.23B parameters. *Due to resource constraints, the Charformer result below uses ?100K less pretraining steps than ByT5 and mT5.ResultsThe CHARFORMER model under the same scaling as ByT5 Large was able to outperform mT5 Large , a very strong baseline. Our preliminary results at this scale shows that CHARFORMER is competitive with, but is 1.4 F1 behind ByT5 Large . However, we point out two important notes. First, the CHARFORMER result is undertrained compared to ByT5 Large since 10% of the pretraining has not finished. Second, the CHARFORMER model is also twice as fast as ByT5, as seen fromTable 10.</figDesc><table><row><cell>Model</cell><cell>TyDiQA-GoldP F1 / EM</cell></row><row><cell>mT5Large</cell><cell>85.3 / 75.3</cell></row><row><cell>ByT5Large</cell><cell>87.7 / 79.2</cell></row><row><cell>CHARFORMER*</cell><cell>86.3 / 77.3</cell></row><row><cell>7.3 MULTILINGUAL EXPERIMENTS</cell><cell></cell></row><row><cell cols="2">This section contains detailed results for our multilingual experiments.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Compute metrics of base models at longer (2K) input length on the mC4 pre-training corpus, using a batch size of 64 on 16 TPU-v3 chips.</figDesc><table><row><cell>Model</cell><cell>L</cell><cell>ds</cell><cell>|?|</cell><cell>Speed (steps/s)</cell><cell>FLOPS</cell></row><row><cell>Byte-level T5Base</cell><cell>2048</cell><cell>1</cell><cell>200M</cell><cell>2.7</cell><cell>2.0 ? 10 13</cell></row><row><cell cols="2">Byte-level T5+LASCBase 2048</cell><cell>4</cell><cell>205M</cell><cell>11</cell><cell>5.5 ? 10 12</cell></row><row><cell>CHARFORMERBase</cell><cell>2048</cell><cell>2</cell><cell>203M</cell><cell>6.1</cell><cell>9.5 ? 10 12</cell></row><row><cell>CHARFORMERBase</cell><cell>2048</cell><cell>3</cell><cell>203M</cell><cell>10</cell><cell>6.5 ? 10 12</cell></row><row><cell>CHARFORMERSBase</cell><cell>2048</cell><cell>2</cell><cell>134M</cell><cell>6.1</cell><cell>9.2 ? 10 12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Per-language breakdown of in-language multi-task TyDiQA-GoldP results.</figDesc><table><row><cell>Model</cell><cell>|?|</cell><cell>ar</cell><cell>bn</cell><cell>en</cell><cell>fi</cell><cell>id</cell><cell>ko</cell><cell>ru</cell><cell>sw</cell><cell>te</cell><cell>Avg.</cell></row><row><cell>mBERTBase (Subword)</cell><cell>179M</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>77.6/68.0</cell></row><row><cell>mT5Base (Subword)</cell><cell cols="11">582M 84.2/71.8 80.0/69.0 76.6/65.2 80.1/69.3 85.5/75.0 70.3/61.6 77.5/64.4 83.6/74.9 88.2/78.0 80.8 / 70.0</cell></row><row><cell>Byte-level T5Base</cell><cell cols="11">200M 81.4/67.0 66.8/56.6 69.8/59.5 75.6/63.0 81.6/72.4 64.6/58.7 74.1/60.8 81.8/74.3 85.0/76.1 75.6/65.4</cell></row><row><cell>Byte-level T5+LASCBase</cell><cell cols="11">205M 78.1/62.3 61.1/50.4 66.7/55.2 72.5/60.4 79.9/68.3 51.5/43.5 70.4/58.7 74.7/67.5 80.2/71.2 70.6/59.7</cell></row><row><cell>CHARFORMERBase</cell><cell cols="11">203M 81.8/67.9 69.1/60.2 71.4/60.5 76.3/64.2 83.0/73.1 62.7/54.3 74.7/61.7 80.2/73.3 83.6/75.0 75.9/65.6</cell></row><row><cell>CHARFORMERSBase</cell><cell cols="11">134M 82.4/68.1 78.1/67.3 75.4/64.3 79.5/68.2 85.0/75.9 66.6/58.0 77.0/64.3 81.5/74.1 86.5/78.6 79.1/68.8</cell></row><row><cell cols="12">CHARFORMERSBase,LongP T 134M 85.7/74.5 78.7/67.3 76.8/65.9 81.9/70.6 86.7/79.1 69.4/61.6 79.2/67.1 83.7/75.2 88.8/80.6 81.2/71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Per-language breakdown of translate-train-all XQuAD results.</figDesc><table><row><cell>Model</cell><cell>|?|</cell><cell>ar</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>hi</cell><cell>ru</cell><cell>th</cell><cell>tr</cell><cell>vi</cell><cell>zh</cell><cell>Avg.</cell></row><row><cell>mT5Base (Subword)</cell><cell cols="13">582M 72.4/55.2 76.9/59.7 76.8/58.8 83.1/70.3 79.0/61.2 71.4/53.4 76.1/58.5 67.9/62.0 72.5/51.4 75.9/56.3 76.9/69.7 75.3/59.7</cell></row><row><cell>Byte-level T5Base</cell><cell cols="13">200M 64.8/47.9 74.3/58.3 69.2/51.8 81.5/70.4 77.2/60.4 67.0/51.5 72.3/55.5 48.3/41.9 69.6/51.7 73.3/54.4 57.3/53.3 68.6/54.3</cell></row><row><cell>Byte-level T5+LASCBase</cell><cell cols="13">205M 62.9/45.5 70.6/54.2 68.3/52.3 80.1/68.4 74.8/57.9 63.1/46.2 68.2/52.2 50.0/43.4 67.1/48.2 71.7/51.8 57.7/52.7 66.8/52.1</cell></row><row><cell>CHARFORMERBase</cell><cell cols="13">203M 65.7/49.8 74.2/58.0 71.1/53.1 82.2/70.5 77.8/61.0 67.0/51.3 73.4/57.6 54.3/48.0 70.3/53.0 74.6/55.6 62.0/56.6 70.2/55.9</cell></row><row><cell>CHARFORMERSBase</cell><cell cols="13">134M 70.3/53.7 78.6/61.4 74.4/55.1 85.1/73.7 79.8/63.6 69.1/52.7 76.7/61.3 57.6/51.2 73.9/55.8 76.8/57.6 67.4/62.4 73.6/59.0</cell></row><row><cell cols="14">CHARFORMERSBase,LongP T 134M 72.6/55.0 79.0/62.3 74.9/56.1 85.4/74.5 80.4/63.4 70.6/56.1 77.8/62.2 56.1/49.2 76.1/58.2 77.7/59.4 66.0/61.8 74.2/59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Per-language breakdown of translate-train-all MLQA results.</figDesc><table><row><cell>Model</cell><cell>|?|</cell><cell>ar</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell><cell>Avg.</cell></row><row><cell>mT5 Base (Subword)</cell><cell cols="9">582M 61.1/40.7 65.5/49.2 80.7/66.3 70.7/52.1 63.6/44.3 68.0/47.6 63.5/39.4 67.6/48.5</cell></row><row><cell>Byte-level T5 Base</cell><cell cols="9">200M 52.6/34.2 60.5/46.1 77.7/64.8 67.1/49.2 52.9/36.5 63.6/43.8 58.3/36.4 61.8/44.4</cell></row><row><cell>Byte-level T5+LASC Base</cell><cell cols="9">205M 50.8/32.0 58.1/43.5 75.8/62.2 64.7/46.7 49.2/32.6 60.4/40.4 52.6/30.6 58.8/41.1</cell></row><row><cell>CHARFORMER Base</cell><cell cols="9">203M 53.5/34.5 61.3/46.8 78.5/65.4 67.2/49.3 54.5/37.6 64.3/43.9 58.8/36.6 62.6/44.9</cell></row><row><cell>CHARFORMER SBase</cell><cell cols="9">134M 58.3/39.1 65.7/50.5 81.8/68.7 71.0/53.1 57.7/40.8 67.3/46.8 62.7/40.8 66.3/48.5</cell></row><row><cell cols="10">CHARFORMER SBase,LongP T 134M 59.6/40.0 66.6/51.3 82.2/69.0 72.1/54.5 59.7/42.9 68.2/47.4 62.4/40.7 67.2/49.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Per-language breakdown of translate-train-all and cross-lingual zero-shot XNLI results.</figDesc><table><row><cell>Model</cell><cell>|?|</cell><cell>ar</cell><cell>bg</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>hi</cell><cell>ru</cell><cell>sw</cell><cell>th</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>zh</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Translate-Train-All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mT5Base (Subword)</cell><cell cols="17">582M 74.4 78.5 77.7 78.1 82.0 79.1 77.9 72.2 76.5 71.5 75.0 74.8 70.4 74.5 76.0 75.9</cell></row><row><cell>Byte-level T5Base</cell><cell cols="17">200M 67.1 72.0 71.0 70.6 76.9 74.0 73.4 63.7 69.2 66.2 65.7 69.4 62.8 69.6 69.0 69.4</cell></row><row><cell>Byte-level T5+LASCBase</cell><cell cols="17">205M 65.6 72.1 70.5 67.9 75.6 73.4 72.2 63.5 68.6 65.4 64.5 67.4 62.4 68.3 61.0 67.9</cell></row><row><cell>CHARFORMERBase</cell><cell cols="17">203M 69.5 72.9 72.7 72.6 78.2 74.5 73.6 67.0 71.7 67.9 68.1 70.8 65.0 70.7 71.5 71.1</cell></row><row><cell>CHARFORMERSBase</cell><cell cols="17">134M 70.8 75.7 75.9 73.1 80.9 76.9 76.8 65.6 74.7 65.7 67.7 72.0 63.1 72.9 71.5 72.2</cell></row><row><cell cols="18">CHARFORMERSBase,LongP T 134M 71.1 75.9 73.6 74.2 80.8 76.6 76.8 69.2 72.2 68.2 71.0 71.2 65.7 72.9 73.0 72.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Cross-Lingual Zero-Shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERTBase (Subword)</cell><cell cols="17">179M 64.3 68.0 70.0 65.3 80.8 73.5 73.4 58.9 67.8 49.7 54.1 60.9 57.2 69.3 67.8 65.4</cell></row><row><cell>mT5Base (Subword)</cell><cell cols="17">582M 73.3 78.6 77.4 77.1 84.7 80.3 79.1 70.8 77.1 69.4 73.2 72.8 68.3 74.2 74.1 75.4</cell></row><row><cell>Byte-level T5Base</cell><cell cols="17">200M 56.7 61.2 63.0 60.9 79.2 70.1 65.3 43.9 61.0 45.5 43.5 52.0 44.3 58.3 55.6 57.4</cell></row><row><cell>Byte-level T5+LASCBase</cell><cell cols="17">205M 53.3 58.8 62.2 54.9 77.1 68.6 65.4 44.7 58.4 46.1 43.6 50.4 42.8 55.9 46.1 55.2</cell></row><row><cell>CHARFORMERBase</cell><cell cols="17">203M 55.7 61.1 64.8 60.1 77.3 69.9 67.9 44.4 60.2 45.3 47.9 54.0 43.5 59.1 53.4 57.6</cell></row><row><cell>CHARFORMERSBase</cell><cell cols="17">134M 66.4 71.0 72.7 68.6 82.4 77.1 75.4 57.6 70.6 48.7 61.4 61.8 54.1 68.9 62.8 66.6</cell></row><row><cell cols="18">CHARFORMERSBase,LongP T 134M 68.4 70.9 74.3 70.2 82.4 77.0 76.6 59.9 71.0 42.6 64.0 65.5 56.5 71.2 66.0 67.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Per-language breakdown of translate-train-all and cross-lingual zero-shot PAWS-X results.</figDesc><table><row><cell>Model</cell><cell>|?|</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="3">Translate-Train-All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mT5 Base (Subword)</cell><cell cols="9">582M 90.9 95.5 91.4 92.5 83.6 84.8 86.4 89.3</cell></row><row><cell>Byte-level T5 Base</cell><cell cols="9">200M 89.3 94.6 90.1 90.3 81.4 81.1 82.3 87.0</cell></row><row><cell>Byte-level T5+LASC Base</cell><cell cols="9">205M 87.3 93.1 89.2 89.2 81.0 72.9 80.8 84.8</cell></row><row><cell>CHARFORMER Base</cell><cell cols="9">203M 89.9 94.6 89.8 91.4 82.7 78.4 83.3 87.2</cell></row><row><cell>CHARFORMER SBase</cell><cell cols="9">134M 89.9 95.9 91.8 92.2 83.9 78.9 84.4 88.2</cell></row><row><cell cols="10">CHARFORMER SBase,LongP T 134M 90.7 95.1 92.2 92.2 84.1 81.6 84.6 88.6</cell></row><row><cell></cell><cell cols="4">Cross-Lingual Zero-Shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT Base (Subword)</cell><cell cols="9">179M 85.7 94.0 87.4 87.0 73.0 69.6 77.0 81.9</cell></row><row><cell>mT5 Base (Subword)</cell><cell cols="9">582M 89.4 95.4 89.6 91.2 79.8 78.5 81.1 86.4</cell></row><row><cell>Byte-level T5 Base</cell><cell cols="9">200M 84.7 93.8 85.8 86.4 72.2 67.9 75.2 80.9</cell></row><row><cell>Byte-level T5+LASC Base</cell><cell cols="9">205M 83.2 93.2 84.1 85.0 67.9 66.4 73.4 79.0</cell></row><row><cell>CHARFORMER Base</cell><cell cols="9">203M 86.1 94.8 87.2 88.0 70.1 69.7 75.5 81.6</cell></row><row><cell>CHARFORMER SBase</cell><cell cols="9">134M 89.6 95.2 90.7 90.7 77.1 74.4 78.9 85.2</cell></row><row><cell cols="10">CHARFORMER SBase,LongP T 134M 89.8 95.3 88.7 89.7 74.5 68.9 78.9 83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Effect of freezing the GBST layer for XNLI and PAWS-X.For additional clarity, we include a simplified implementation of the GBST module in Tensorflow below. Default hyper-parameters here match those used in the paper. Shape [len, embedding_size] of input tensor in future calls, without batch dimension. downsample_rate: Integer of how much to downsample by. max_subword_block_width: Integer of max block size to use for enumeration. block_attention: Hhether to use block score calibration. block_scoring_network: module for parameterized block scoring. conv_kernel_size: Integer of the size of the pre-GBST convolution kernel.</figDesc><table><row><cell>Model</cell><cell cols="6">ds Freeze GBST XNLI (Zero) XNLI (Translate) PAWS-X (Zero) PAWS-X (Translate)</cell></row><row><cell>CHARFORMERSmall</cell><cell>2</cell><cell>No</cell><cell>44.5</cell><cell>62.7</cell><cell>27.9</cell><cell>37.5</cell></row><row><cell>CHARFORMERSmall</cell><cell>2</cell><cell>Yes</cell><cell>50.9</cell><cell>68.7</cell><cell>77.1</cell><cell>84.8</cell></row><row><cell>CHARFORMERSmall</cell><cell>3</cell><cell>No</cell><cell>47.9</cell><cell>67.9</cell><cell>29.5</cell><cell>36.8</cell></row><row><cell>CHARFORMERSmall</cell><cell>3</cell><cell>Yes</cell><cell>43.2</cell><cell>68.6</cell><cell>77.8</cell><cell>83.7</cell></row><row><cell>CHARFORMERSmall</cell><cell>4</cell><cell>No</cell><cell>47.5</cell><cell>47.5</cell><cell>30.9</cell><cell>36.9</cell></row><row><cell>CHARFORMERSmall</cell><cell>4</cell><cell>Yes</cell><cell>43.6</cell><cell>43.6</cell><cell>77.9</cell><cell>83.5</cell></row><row><cell cols="3">7.4 EXAMPLE IMPLEMENTATION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">from typing import Optional</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">import tensorflow as tf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">keras_layers = tf.keras.layers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">class GBSTLayer(keras_layers.Layer):</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">"""Performs Charformer GBST on a sequence.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Attributes:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>input_shape: """</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>def __init__(self,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">input_shape: tf.Tensor,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">downsample_rate: int = 2,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">max_subword_block_width: int = 4,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">block_attention: bool = False,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">conv_kernel_size: Optional[int] = 5):</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">super(GBSTLayer, self).__init__()</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">self.downsample_rate = downsample_rate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">self.max_subword_block_width = max_subword_block_width</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">self.conv_kernel_size = conv_kernel_size</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">self.conv_layer = keras_layers.Conv1D(</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">input_shape[-1], self.conv_kernel_size, input_shape=input_shape)</cell><cell></cell><cell></cell></row><row><cell cols="3">self.block_attention = block_attention</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">self.block_scoring_network = keras_layers.Dense(1, use_bias=False)</cell><cell></cell><cell></cell></row><row><cell cols="2">def call(self, inputs):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">"""Performs downsampling on the character-scale input representation.</cell><cell></cell><cell></cell></row><row><cell>Args:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">inputs: float Tensor of shape [batch_size, seq_length,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We choose bytes rather than characters (Unicode code points) as this allows us to use a vocabulary of 256 possible byte values for all settings. We note that for languages with a Latin alphabet, many characters correspond to a single byte. For other languages, each character corresponds to 2-3 bytes in general. For simplicity and to align with prior work, we will generally talk about characters unless stated otherwise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Future work could explicitly seek to model discontinuous morphological processes by considering skipgrams in addition to character n-grams, although this would increase computational costs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The benefits of such re-scaling have also been observed for subword-based encoder-decoder neural machine translation models<ref type="bibr" target="#b12">(Devlin, 2017;</ref> Kasai et al., 2021).4  Compared to CANINE, Byte-level T5+LASC does not operate on Unicode codepoints and has a decoder. It thus forgoes character hash embeddings and upsampling procedures respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Following Xue et al. (2021) we discard illegal UTF-8 sequences and reuse the final 100 byte IDs as sentinel tokens.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/tensorflow/mesh 7 https://github.com/google-research/text-to-text-transfer-transformer 8 https://github.com/google-research/byt5 9 https://github.com/google-research/google-research/tree/master/ charformer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Cross-lingual Transferability of Monolingual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.11856" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthetic and Natural Noise Both Break Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2018</title>
		<meeting>ICLR 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nuanced metrics for measuring unintended bias with real data for text classification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Borkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.04561" />
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Byte Pair Encoding is Suboptimal for Language Model Pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaj</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.414</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4617" to="4624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Are vowels and consonants processed differently? event-related potential evidence with a delayed letter paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Carreiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Gillon-Dowens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Perea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="288" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking Embedding Coupling in Pre-trained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2021</title>
		<meeting>ICLR 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Canine: Pre-training an efficient tokenization-free encoder for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wieting</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06874</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating Cross-lingual Sentence Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1809.05053" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2018</title>
		<meeting>EMNLP 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01991</idno>
		<title level="m">Sharp models on dull hardware: Fast and accurate neural machine translation decoding on the cpu</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.04805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2019</title>
		<meeting>NAACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CharacterBERT: Reconciling ELMo and BERT for word-level open-vocabulary representations from characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hicham</forename><forename type="middle">El</forename><surname>Boukkouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.609</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.609" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="6903" to="6915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FRAGE: Frequency-Agnostic Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2018</title>
		<meeting>NIPS 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic programming encoding for subword segmentation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.275</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.275" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="3042" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing word segmentation for downstream task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Hiraoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Uchiumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Keyaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.120</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.120" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="1341" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint Optimization of Tokenization and Downstream Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Hiraoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Uchiumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Keyaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2105.12410" />
	</analytic>
	<monogr>
		<title level="m">Findings of ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.06146" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2020</title>
		<meeting>ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Character-level language modeling with hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuyeon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5720" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2021</title>
		<meeting>ICLR 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="0080437516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to discover, ground and use words with segmental neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1645</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1645" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="6429" to="6441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to segment inputs for nmt favors character-level processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1007</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-lingual Language Model Pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/bert" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The relative contribution of consonants and vowels to word identification during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hye-Won</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pollatsek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="205" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00067</idno>
		<ptr target="https://aclanthology.org/Q17-1026" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating Cross-lingual Extractive Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.07475" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CharBERT: Characteraware pre-trained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.4</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2018</title>
		<meeting>NAACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BPE-dropout: Simple and effective subword regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.170</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1882" to="1892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combating adversarial misspellings with robust word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1561</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1561" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="5582" to="5591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.10683" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xtreme-r: Towards more challenging and nuanced multilingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07412</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany, Au</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02084</idno>
		<title level="m">Mesh-tensorflow: Deep learning for supercomputers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04985</idno>
		<title level="m">Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03322</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Are pre-trained convolutions better than pre-trained transformers</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Illia Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sequence modeling via segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-view Subword Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2103.08490" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2021</title>
		<meeting>NAACL 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">International World Wide Web Conferences Steering Committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052591</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052591" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, WWW &apos;17</title>
		<meeting>the 26th International Conference on World Wide Web, WWW &apos;17<address><addrLine>Republic and Canton of Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1391" to="1399" />
		</imprint>
	</monogr>
	<note>Ex machina: Personal attacks seen at scale</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">ByT5: Towards a token-free future with pre-trained byte-to-byte models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13626</idno>
		<ptr target="http://arxiv.org/abs/2105.13626" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.11828" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2019</title>
		<meeting>EMNLP 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Character-level Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.01626#" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
