<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularization for Long Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
							<email>minbyuljeong@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
							<email>kangj@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Regularization for Long Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When performing named entity recognition (NER), entity length is variable and dependent on a specific domain or dataset. Pretrained language models (PLMs) are used to solve NER tasks and tend to be biased toward dataset patterns such as length statistics, surface form, and skewed class distribution. These biases hinder the generalization ability of PLMs, which is necessary to address many unseen mentions in real-world situations. We propose a novel debiasing method RegLER to improve predictions for entities of varying lengths. To close the gap between evaluation and real-world situations, we evaluated PLMs on partitioned benchmark datasets containing unseen mention sets. Here, RegLER shows significant improvement over long-named entities that can predict through debiasing on conjunction or special characters within entities. Furthermore, there is a severe class imbalance in most NER datasets, causing easy-negative examples to dominate during training, such as 'The'. Our approach alleviates skewed class distribution by reducing the influence of easynegative examples. Extensive experiments on the biomedical and general domains demonstrated the generalization capabilities of our method. To facilitate reproducibility and future work, we release our code. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is a core task in information extraction, that is essential for many downstream tasks such as question answering <ref type="bibr">(Moll? et al., 2006;</ref><ref type="bibr">Lee et al., 2020a)</ref>, machine translation <ref type="bibr">(Babych and Hartley, 2003;</ref><ref type="bibr">Ugawa et al., 2018)</ref>, entity linking <ref type="bibr">(Martins et al., 2019)</ref>, and event extraction <ref type="bibr" target="#b1">(Wadden et al., 2019)</ref>. In recent years, with the proliferation of pre-trained language models (PLMs) <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr">Lee et al., 2020b)</ref>, the performance of NER datasets plateaued. 1 https://github.com/minstar/RegLER</p><p>There are several ways to analyze a generalization behavior to see whether the PLMs are able to predict various unseen mentions, such as: breaking down a holistic performance into fine-grained categories <ref type="bibr">(Fu et al., 2020a,b)</ref>, splitting benchmark datasets based on seen and unseen mentions <ref type="bibr">(Lin et al., 2020)</ref>, and partitioning unseen mentions into synonyms and concepts in the biomedical domain <ref type="bibr">(Kim and Kang, 2021)</ref>. In addition, it has been proven that PLMs tend to focus on dataset biases to solve particular benchmark datasets <ref type="bibr">(Jia and Liang, 2017;</ref><ref type="bibr">Gururangan et al., 2018;</ref><ref type="bibr">Nie et al., 2020)</ref>.</p><p>Exploiting dataset biases hinders the generalization capacity of PLMs to address unseen entities, which is critical in real-world situations. Thus, current debiasing methods have been suggested to prevent the overfitting of superficial cues in training datasets <ref type="bibr">(Clark et al., 2019;</ref><ref type="bibr">Kim and Kang, 2021)</ref>. For NER tasks, we found two problems to apply this directly on PLMs. <ref type="bibr">First, Clark et al. (2019)</ref> introduced a product-of-expert to scale down the gradients of biased examples, but they ignored a class prior because the class distributions of particular datasets is uniformly distributed. In contrast, there is a severe class imbalance in the NER task, thus the dominating influence of easy-negative examples must be addressed <ref type="bibr">(Meng et al., 2019;</ref><ref type="bibr">Li et al., 2020b;</ref><ref type="bibr">Mahabadi et al., 2020)</ref>. Second, in biomedical named entity recognition (BioNER), <ref type="bibr">Kim and Kang (2021)</ref> enhanced generalizability through the debiasing framework which replaces a bias-only model as a class distribution given word frequency. However, it is impossible to explicitly provide a training signal to out-of-vocabulary (OOV) words that do not appear in the training. For example, it is difficult to predict 'Latent BHRF1 transcripts' as an entity because the word 'Latent' does not occur in the training dataset but appears in the test dataset.</p><p>In this paper, we introduce a debiasing method RegLER that regularizes models to predict long- named entities by alleviating class imbalance and OOV issues. When designing a class prior, we use pointwise mutual information (PMI) to measure the correlation score between a word and its class <ref type="bibr">(Gururangan et al., 2018)</ref>. Class imbalance is alleviated by reducing the influence of easy-negative examples such as 'The'. We also leverage subword-level PMI scores to handle the dataset shift. For instance, a word 'Latent' can be split into subwords of 'Late' and '##nt', and the word 'Late' only occurs with an 'O' label which can cause it to not be predicted as an entity. Thus, we debias the class distribution of the word 'Late' to predict it as an entity without the 'O' label.</p><p>Finally, previous studies have suggested that entity length can affect the predictions of the models <ref type="bibr">(Fu et al., 2020b;</ref><ref type="bibr">Hong and Lee, 2020;</ref><ref type="bibr" target="#b4">Wei et al., 2020)</ref>. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, we observe that the entity length is variable and dependent on a specific domain or dataset. Biomedical datasets contain longer entities than general domain datasets on average because of adjectives or conjunctions found within the entity <ref type="bibr" target="#b4">(Wei et al., 2020;</ref><ref type="bibr">Cho et al., 2020)</ref>. For example, an entity 'foot -and -mouth -disease virus L. protease' consists of 10 words because of the adjectives (foot-and-mouth) and conjunctions <ref type="bibr">(-or and)</ref>. Thus, we formulate a target model (PLM in our case) that has flexibility depending on whether the entity length is short or long. In other words, our RegLER method adaptively smooths the class distribution of the entity length. The key intuition of our approach, depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, is to explicitly incorporate the dataset biases into the target model to increase robustness against these biases.</p><p>Our contributions are as follows: (1) We introduce a model-agnostic debiasing method, RegLER, to regularize the model to improve prediction for long-named entities by alleviating severe class imbalance and OOV issues in the NER task.</p><p>(2) We demonstrate that our method significantly improves the ability of synonym and concept generalization and shows competitive in-domain performance among the biomedical and general domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Backgrounds</head><p>Task Formulation A NER is a task that identifies a word or phrase that corresponds to instances such as a person, location, organization, or miscellaneous. NER mainly focuses on extracting and classifying named entities in a corpus with predefined entity tags. We use the BIO tagging scheme <ref type="bibr">(Ramshaw and Marcus, 1999)</ref>, where B and I labels are concatenated with all target entity types and the O label indicates that a token belongs to no chunk, for example, B-PER and B-LOC. Formally, a sentence consists of an input sequence X = {x 1 , x 2 , . . . , x N }with length N, its ground truth labels of the input sequence Y = {y 1 , y 2 , . . . , y N }, and its predicted labels of the output sequence? = {? 1 ,? 2 , . . . ,? N } where y i ? {1, 2, . . . , C} and C is the number of classes. There are two widely used decoding strategies: tag-independent decoding (i.e., softmax) and tag-dependent decoding (i.e., conditional random field <ref type="bibr">(Lafferty et al., 2001)</ref>). The goal of NER is to predict mentions by assigning the output label? i for each token x i using the following decoding strategies: P(? i |X) or P(? i |X,? 1 ,? 2 , . . . ,? i?1 ).</p><p>Partitioning Benchmark Datasets Given the annotated entities in the training dataset, we created an entity dictionary to split the evaluation dataset. In other words, using this dictionary generates two mention sets: mentions those that are seen and unseen during training <ref type="bibr">(Lin et al., 2020;</ref><ref type="bibr">Kim and Kang, 2021</ref>). In particular BioNER task, a raw dataset suggests the CUI of each entity that signifies the same concept (or meaning) of different entities, forming a concept dictionary. We define our evaluation dataset to be split based on these dictionaries as follows, (a) PMI reflects a solution of class imbalance, and the subword frequency solves an out-of-vocabulary (OOV) issue. For example, we depict the OOV issue with the word Latent that does not appear in the training dataset but occurs in the test dataset. We designed a class prior to reflecting the class imbalance inherent in the NER task. (b) We substitute a bias-only model with PMI statistic. For example, the word disease is mainly classified with an I (or inside) label, but the class distribution focuses on an O (or outside) label. Our bias-only model reduces the importance of the label I by adding the PMI score to the logit value of the target model ( f ? ). Subsequently, we use temperature scaling <ref type="bibr">(Guo et al., 2017)</ref> to smooth the class distribution of the normalized PMI depending on the entity length. The red line indicates that computing the PMI is only added during the training time.</p><p>where E tr (M) and C tr (M) refer to the entity and concept dictionary of the training dataset respectively, and M indicates the surface form of the entity. Memorization (Mem) contains mentions that appear in the training dataset. In contrast, Unseen contains mentions that do not occur in the training dataset. More specifically, in the BioNER dataset, we can further partition the Unseen mention set into synonym generalization (Syn) and concept generalization (Con) <ref type="bibr">(Kim and Kang, 2021)</ref>. We measure the generalization ability using these partitioned benchmark datasets.</p><p>Evaluation Metrics Evaluation of NER is divided into two parts: boundary detection and type identification <ref type="bibr">(Li et al., 2020a)</ref>. General domain NER identifies and classifies text spans into humanannotated categories <ref type="bibr">(Sang and De Meulder, 2003)</ref>. However, BioNER mostly focuses on computing an entity-level boundary detection. The common evaluation metric of NER and BioNER is to compute the entity-level metric using the micro-averaged F1 score: F1 = 2 * P * R P+R , where P and R are Precision and Recall respectively. Along with the common evaluation metric (F1 score) as an in-domain performance, we use the evaluation metric of (Kim and Kang, 2021) to report the recall score of partitioned mention sets because the predicted entities are likely not to exist in the annotated entities.</p><p>Debiasing Framework We illustrate the overall framework of our debiasing method RegLER in <ref type="figure" target="#fig_1">Figure 2</ref>(b). Given the input sequence of tokens X and its ground truth labels Y, a model has to predict the labels of the output sequence?. As a simple ensemble method, we use a bias product that trains a product of experts <ref type="bibr">(Hinton, 2002;</ref><ref type="bibr">Smith et al., 2005)</ref> and adds the log probabilities of the bias-only model and target model f ? <ref type="bibr">(Clark et al., 2019;</ref><ref type="bibr">He et al., 2019)</ref>. It is intuitive that the target model can prevent a focus on dataset biases during training. We compute a final probability as follows:</p><formula xml:id="formula_0">p i = so f tmax(log(p i ) + log(b i )),<label>(1)</label></formula><p>wherep i ? p i ? b i , a log(p i ) is the log probability of the target model f ? , in which we desire to avoid over-trust in the surface form, and log(b i ) is the log probability of the bias-only model. By converting a bias-only model to word level statistics <ref type="bibr">(Ko et al., 2020;</ref><ref type="bibr">Kim and Kang, 2021)</ref>, the model can operate on these ensemble frameworks without introducing additional parameters and we adopt this to address a various dataset patterns. We define a loss function for our model as follows:</p><formula xml:id="formula_1">L = ? 1 N N i=1 y i ?p i ,<label>(2)</label></formula><formula xml:id="formula_2">p i = so f tmax(log(p i ) + log(b i )),<label>(3)</label></formula><p>where N is the length of an input sequence. This loss function minimizes the negative log-likelihood of the distribution of final probabilityp i and its ground-truth label y i . Note that log(b i ) is only added during the training process, whereas it does not apply at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>Long and Complex Structure Entities In terms of entity length, there are several error cases in which longer terms are not well predicted. This can be easily observed through <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="bibr">Hong and Lee (2020)</ref> shows that conjunctions or the special character "/" within an entity makes it challenging for the models to predict types of diseases and chemicals. <ref type="bibr" target="#b4">Wei et al. (2020)</ref> demonstrated three primary reasons for error as follows: complex and long-named entities, annotation inconsistency on corpora, and abbreviations. As the bias-only model does not over-trust the dataset bias, we hypothesize that relative smoothing on the bias-only model predictions can increase robustness when dealing with long entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subword Frequency</head><p>We depict the OOV issue in <ref type="figure" target="#fig_1">Figure 2</ref>(a). It is impossible to explicitly provide a training signal to OOV words that do not occur during training <ref type="bibr">(Leevy et al., 2018;</ref><ref type="bibr">Li et al., 2020b)</ref>. For example, the word 'Latent' does not appear in the training dataset but occurs as a part of an entity in the test dataset as 'Latent BHRF1 transcripts'. Thus, it is difficult to apply a meaningful statistic based on a word unit. However, if subword statistics are applied, 'Latent' can be split into the subwords 'Late' and '##nt'. As the word 'Late' occurs with only an 'O' label in the training dataset, debiasing to predict without using the 'O' label enables capturing the entity 'Latent BHRF1 transcripts'. This example is fully detailed in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling a Class Prior</head><p>A joint model of f ? (?|X) and P(Y|X) P(Y) is used by current debiasing methods <ref type="bibr">(Clark et al., 2019)</ref>. However, these methods use a bias-only model with learned parameters and ignore the P(Y) factor while designing because it does not need to reflect a class prior in a situation where the class distribution of a particular dataset (e.g., natural language inference) is uniformly distributed. In contrast, class imbalance is severe in NER tasks, increasing the need to address the dominating influence of easy-negative examples (Meng </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>We start by introducing our debiasing method Re-gLER, which utilizes PMI as a bias-only model to alleviate the OOV issue and class imbalance. For long entities, we provide an explanation for smoothing the class distribution via temperature scaling <ref type="bibr">(Guo et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pointwise Mutual Information</head><p>To cope with the class-distribution shift of words between the training and test datasets 2 , we compute the PMI between the subword and class in the training dataset as follows:</p><p>PMI(subword, class) = log P(subword, class) P(subword) ? P(class) .</p><p>(4) For a numerical issue, we apply add-K smoothing to each subword frequency 3 .</p><p>Overall, PMI captures the correlation between subwords and their corresponding classes. We further suggest a method to understand how each mathematical expression operates. A joint distribution P(subword, class) assigns low training signals to subwords in which class distributions are dominant for a particular label. However, P(subword, class) focuses on frequent subwords independent of class, such as 'disease' or 'deficiency'. To handle this problem, a P(subword) alleviates the situation in which subwords appear frequently regardless of their class. Finally, P(class) enhances regularization when the class imbalance between positive examples (B and I labels) and negative examples (O labels) is severe. Because there are many easynegative examples, we want to prevent the training signals from being ruled by these easy-negatives.</p><p>As the PMI is much larger than the logit value of the target model f ? , we normalize the PMI to a probability distribution. We can then compute the normalized PMI (i.e., PMI ic ) and the final probabilityp i as follows:</p><formula xml:id="formula_3">PMI ic = exp(PMI ic ) C c =1 exp(PMI ic ) (5) p i = so f tmax(log(p i ) + log(PMI i ))<label>(6)</label></formula><p>where C is the number of classes in a particular dataset, and PMI ic indicates the probability of the PMI corresponding to each class c. PMI i = {PMI i1 , PMI i2 . . . , PMI iC } is the class distribution of the PMI for each token x i in the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bias-weighted Scaling on Long-Named and Complex Entities</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, our RegLER method involves adaptively smoothing the class distribution of PMI using temperature scaling <ref type="bibr">(Guo et al., 2017)</ref>. Biomedical entities have long names and complex structures that contain conjunctions or adjectives <ref type="bibr" target="#b4">(Wei et al., 2020;</ref><ref type="bibr">Cho et al., 2020)</ref>. Thus, we define a temperature T i that depends on the length L i of the entity to smooth the class distribution of the PMI. Given the PMI probability PMI ic , the new statistic is defined as below,</p><formula xml:id="formula_4">T i := ? ? ? ? ? ? ? 1 + ? ? L i , if L i &gt; 1 1, otherwise (7) PMI ic = exp(PMI ic /T i ) C c =1 exp(PMI ic /T i ) .<label>(8)</label></formula><p>T i is the temperature (Guo et al., 2017) of the token x i to smooth the class distribution of PMI ic depending on the entity length L i , and ? is a hyperparamter. As T ? ?, the probability PMI ic approaches 1/C, which represents the maximum uncertainty. Conversely, as T ? 0, the probability collapses to a point mass.</p><p>Notably, as PMI requires task-related information, it does not introduce additional memory requirements. We can also choose other statistics, such as the random value or word frequency corresponding to its class. However, we observe that using PMI achieves the best results. Moreover, improving the generalizability of our method appears to be beneficial for other class-imbalance tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate our debiasing method RegLER on the biomedical and general domain datasets of the NER task. We address three main questions in the experiments as follows: (1) How effective is our method as a bias-only model using length-dependent scaling, subword frequency, and the class prior? (2) Does bias-weighted scaling predict entities with long names and complex structures? (3) What components can support the capacity of generalization?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Dataset We evaluated our debiasing method on ten benchmark NER datasets including eight biomedical domains and two general domains. For the biomedical domains, we used eight datasets: disease entities NCBI-disease <ref type="figure">(</ref>  <ref type="bibr">, 2013)</ref>. We followed the preprocessed version of the previous work <ref type="bibr">(Crichton et al., 2017)</ref>. 4 For the general domain, we used two English datasets: the CoNLL-2003 shared benchmark dataset <ref type="bibr">(Sang and De Meulder, 2003)</ref> and WNUT-2017 which focuses on identifying unusual, previously unseen entities <ref type="bibr">(Derczynski et al., 2017)</ref>. We focused on improving both the in-domain performance and recall of our partitioned datasets (Mem, Syn, Con, and Unseen). Descriptions of data statistics are noted in Appendix A.</p><p>Baselines Our target models are composed of BERT and RoBERTa architectures, which have been widely used in many studies on NER. We   </p><formula xml:id="formula_5">NCBI-disease BC5CDR-chem LINNAEUS Mem (R) Syn (R) Con (R) Total (F1) Mem (R) Syn (R) Con (R) Total (F1) Mem (R) Unseen (R) Total (F1) S ciFive ? base ? # # # - - - 89.4 - - - 94.2 - - - S parkNLP ? # # # - - - 89.1 - - - - - - 86.3 KeBioLM ? # # # - - - 89.1 - - - 93.3 - - - CL ? KL ? # # # - - - 89.0 - - - - - - - BioFLAIR ? # # # - - - 88.9 - - - 93.5 - - 87.0 BioBERT # # # 95.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness of Our Debiasing Method</head><p>Experimental results on the biomedical domain are summarized in <ref type="table" target="#tab_2">Table 2</ref>. Using three components, namely Subword, Class, and Temp, showed significant improvements in recall on Syn and Con, as well as overall improvements to in-domain performance (F1). In the first block, we report five experimental results to compare our debiasing method RegLER. From the second to the sixth block, we probed the performance of various language models trained on the Bias-Product (BP) framework. Compared to the fine-tuning language model (row 1 in each block), the absolute improvements of our method RegLER in Syn, Con, and in-domain performance (F1) were approximately 1.34%, 1.4% and 0.47%, respectively. In the BP framework, the absolute improvements of our method in Syn, Con, and F1 were approximately 1.01%, 0.82% and 0.66% compared to the (Kim and Kang, 2021) method, respectively.</p><p>While the BP framework is vulnerable to preserving memorization ability and in-domain performance, our method achieves competitive performance on all datasets and achieves state-of-the-art performance on three datasets. Because our method RegLER targets capturing long-named entities, we achieve the ability to generalize but fall behind various baselines in the general domain. We thus report our performance for the general domain in Appendix D. We conduct five runs with different seeds for all experiments. The t-tests indicate that p &lt; 0.05. We report on other datasets owing to the   space limit in Appendix E.</p><formula xml:id="formula_6">L(E)=1 L(E)=2 L(E)=3 L(E)=4 L(E)=5 L(E)=6 L(E)=7 L(E) 8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative Analysis</head><p>Overall, adopting three components not only boosts the performance of memorization ability in all cases but also improves generalizability by preventing dataset bias. In <ref type="table" target="#tab_4">Table 3</ref>, we note that the class prior (Class) and subword frequency (Subword) improve the memorization ability (Mem) but decrease the capacity of concept generalization (Con). However, a combination of Subword and Class shows a consistent improvement in Syn and Con. Furthermore, applying temperature scaling (Temp) based on the entity length enhances the ability of Mem. As we suggest the Temp to improve prediction for the long-named entities, there is an increase in in-domain performance. Although the performance improvement itself appears insignificant, we will show that the improvement is large for long entities in the next paragraph. In summary, it seems that the three components maintain an equilibrium to increase the robustness of the model, allowing for a high capacity for generalization and memorization.</p><p>Adjusting ? for Temperature Scaling As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, for the BC2GM (Smith et al., 2008) dataset, we conducted an ablation study by converting only the ? during training. Following previous work (Fu et al., 2020a), we adopt a finegrained evaluation metric to partition the mention set based on the entity length. In <ref type="figure" target="#fig_0">Figure 1</ref>, since biomedical entities are much longer than general entities, we suggest the plot to select entity length in the range from one to eight. As observed, there is little change in the performance of the mention sets with a small length of entities. However, in the long-named entities (E(L) 8), we investigate that the F1 score improves by up to 2%. Detailed numerical values are reported in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>In this section, we present two intuitive examples to better understand our approach. In <ref type="table" target="#tab_5">Table 4</ref>, we demonstrate two examples of dealing with the OOV issue and predicting a long-named entity. For example, the word 'Latent' does not appear in the training dataset but occurs in the evaluation dataset. Thus, we cannot apply the class distribution of the word frequency based on the word unit. However, 'Latent' can be split into 'Late' and '##nt' using a subword tokenizer. Considering the word 'Late' highly co-occurs with the 'O' label, the bias-only model forces the target model to predict without taking the 'O' label into account. This enables recognition of the entity 'Latent BHRF1 transcripts'. By adopting bias-weighted scaling, our RegLER approach can capture entities that are long or exhibit complex structure. This also allows for the prediction of foot -and -mouth -disease virus L protease which includes conjunction (such as "and") and special symbol (such as "-"). In BioNER tasks, there are many more long entities, JNLPBA (out-of-vocabulary 'Latent')</p><p>Sentence: Latent BHRF1 transcripts encoding bcl -2 homologue and BCRF1 transcripts encoding viral interleukin ( vIL ) -10 were detected in one and two of eight patients , respectively . BioLM ? large: BHRF1 transcripts, bcl -2, BCRF1 transcripts, viral interleukin ( vIL ) -10 BioLM ? large + RegLER (+ None): BHRF1 transcripts, bcl -2 homologue, BCRF1 transcripts, viral interleukin ( vIL ) -10 BioLM ? large + RegLER (+ Subword): Latent BHRF1 transcripts, bcl -2 homologue, BCRF1 transcripts, viral interleukin ( vIL ) -10 BC2GM (Long-Named Entity)</p><p>Sentence: Rhinovirus 2A protease and foot -and -mouth -disease virus L protease were used to analyze the association of eIF4G with eIF4A , eIF4E , and eIF3 . BioLM  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Named Entity Recognition Named Entity Recognition (NER) has long been studied and received attention for being a core task of information extraction. Initially, research of NER focused on constructing a model using handcrafted features <ref type="bibr" target="#b7">(Zhou and Su, 2002;</ref><ref type="bibr">Bender et al., 2003;</ref><ref type="bibr">Settles, 2004)</ref>. Because this is labor-intensive, deep learning approaches automatically extract features and label a sequence <ref type="bibr">(dos Santos and Guimar?es, 2015;</ref><ref type="bibr">Lample et al., 2016;</ref><ref type="bibr">Habibi et al., 2017)</ref>. Recently, with the advent of pre-training on a large corpus, contextualized language models performed significantly better on benchmark datasets and reached a plateau in performance <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr">Beltagy et al., 2019;</ref><ref type="bibr">Lee et al., 2020b)</ref>.</p><p>However, questions were raised about the generalization capabilities of pre-trained language models. Specifically, in NER tasks, <ref type="bibr">Fu et al. (2020b)</ref> divided the set of test entities into different subsets and broke the overall performance down into interpretable categories. To explore the generalization ability, Lin et al. (2020) partitioned the benchmark datasets into seen and unseen mention sets to create situations of erasing name regularity, mention coverage, and context diversity. In BioNER, Kim and Kang (2021) split unseen mention sets into synonyms and concepts based on CUI overlap between the training and test datasets. The authors adopted a debiasing method to enhance generalization behavior.  <ref type="formula" target="#formula_0">2019)</ref> suggested a regularization method to lower the weights of the give-away phrases causing the bias. As previous debiasing methods are vulnerable to preserving in-domain performance while improving out-of-domain performance, a confidence regularization (Utama et al., 2020a) method addresses this trade-off by connecting robustness against dataset biases and overconfident problems in neural models <ref type="bibr">(Papernot et al., 2016;</ref><ref type="bibr">Feng et al., 2018)</ref>. Our work is motivated by a debiasing framework with a prior distribution of task-related statistics <ref type="bibr">(Ko et al., 2020;</ref><ref type="bibr">Kim and Kang, 2021)</ref>, and we introduce another statistic that fits well for the task of class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Bias and Debiasing Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this study, we introduce a debiasing method RegLER to regularize PLMs to predict well for various lengths of entities. We evaluated our models on partitioned benchmark datasets containing unseen mention sets to improve practicality in real-world situations. RegLER shows a significant improvement when predicting long-named entities while preserving performance on short-named entities. While the debiasing framework is vulnerable to preserving memorization ability and in-domain performance, our method achieves competitive performance on all datasets. In our observation, a combination of modeling a class prior and debiasing on subword frequency improves the generalization ability of synonym and concept mention sets. We hope our method is meaningful to those who are working to improve the generalization ability of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethical Considerations</head><p>In this work, we focus on helping a wide range of existing Named Entity Recognition (NER) models to alleviate data bias and enhance generalization behavior. There may exist many different biases, however, we attempt to mitigate the class imbalance problem inherent in the benchmark NER datasets. As a result, our work promotes robustness with a high capacity for generalization and reduces concerns regarding manually annotating entities suffering from high class imbalance. To save energy, we will share our code and dataset publicly, and also suggest hyperparameters for each experiment to prevent additional trials and reduce further carbon emissions.   <ref type="bibr">Kim and Kang, 2021)</ref> utilizing a bias product. We compute a final probability as below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><formula xml:id="formula_7">g(x i ) = so f tplus(w ? h i ),<label>(9)</label></formula><formula xml:id="formula_8">p i = so f tmax(log(p i ) + g(x i ) ? log(b i )),<label>(10)</label></formula><p>where g is a learned function with a learned vector w and the final hidden layer h i of the target model for the token x i . Herein, a so f tplus(x) = log(1 + e x ) function is used to prevent the target model from reversing the bias <ref type="bibr">(Clark et al., 2019)</ref>.</p><p>However, the Learned-Mixin method tends to ignore a bias probability log(b i ). Thus, <ref type="bibr">(Clark et al., 2019)</ref> also suggested the Learned-Mixin+H method which adds the entropy penalty to the objective function, preventing the target model from ignoring the bias probability log(b i ). An objective function with entropy penalty is computed as follows:</p><formula xml:id="formula_9">R = w ? H(so f tmax(g(x i ) ? log(b i )))<label>(11)</label></formula><formula xml:id="formula_10">L = ? 1 N N i=1 y i ?p i + R,<label>(12)</label></formula><p>where H(z) = ? k z k log(z k ) is the entropy and w is a hyperparameter (we select 0.2 in our experiments).</p><p>A Learned-Mixin framework that prevents the target model from reversing the bias decreases generalizability while increasing memorization ability. The experimental results show a similar trend to the bias-product framework thus we did not report this in our table. Furthermore, the Learned-Mixin+H method shows unstable results because the two regularization functions (i.e., entropy penalty and temperature scaling) conflict with each other in a biasonly model. In other words, the entropy penalty encourages the bias-only model to be non-uniform, but the temperature scaling forces the model to be uniform. Owing to the low and unstable results, we removed these two methods from our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameter and Training Details</head><p>We describe our hyperparameter and training details. For all training models, we used at batch size of 32 and the Adam optimizer (Kingma and Ba, 2014) with a learning rate in the range of {5e-6, 1e-5, 3e-5}. We used a warmup rate in the range of {0, 5000} steps for all datasets and a training seed in the range of {1, 2, 3, 4, 5} to conduct five runs. The length of the input sequence was set to 128 in the biomedical domain and 256 in the general domain dataset. If the sentence length is longer than 128 or 256, respectively, then the sentence was divided into multiple sentences. A training epoch was selected in the range of {20, 50}. To adopt temperature scaling <ref type="bibr">(Guo et al., 2017)</ref> depending on the entity length, we used a lambda (?) in the range of {0.01, 0.1}. For training and inference of all the experimental results, we use one graphics processing unit (NVIDIA Titan Xp). Our code is based on the PyTorch implementation of <ref type="bibr">(Lee et al., 2020b)</ref>. <ref type="bibr">5</ref> Most of the previous studies used a training dataset containing the training and development dataset in the BioNER task. In our experiments, we followed previous works to use the development dataset in our training process for the biomedical NER dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Results of the General Domain</head><p>We used <ref type="bibr">BERT (Devlin et al., 2019)</ref> and <ref type="bibr">RoBERTa (Liu et al., 2019)</ref> as our target models. We enumerate our baselines as follows: ACE+document-context <ref type="bibr" target="#b2">(Wang et al., 2020)</ref>, LUKE <ref type="bibr" target="#b5">(Yamada et al., 2020)</ref>, and FLERTXLM-R (Schweter and Akbik, 2020). For the evaluation, the WNUT-2017 dataset has its own evaluation metric that measures how good systems are at correctly recognizing a diverse range of entities, rather than simply measuring the very frequent surface forms. 5 https://github.com/dmis-lab/biobert-pytorch On the CoNLL 2003 dataset, because RegLER aims to predict long-named entities, improvements in in-domain and generalization abilities (Mem and Unseen) are not effective. However, RegLER shows a significant improvement on the WNUT 2017 dataset, which is composed of unseen mentions on the test dataset. Compared to the finetuning language model (the first fow in each block), the absolute improvements of RegLER in the indomain and surface form performance are approximately 1.5% and 2.15%, respectively. In addition, the absolute improvements of RegLER for the indomain and surface form performances are approximately 1.5% and 2.0%, respectively compared to <ref type="bibr">(Kim and Kang, 2021)</ref>. Based on these results, we believe that using RegLER shows little degradation in the surface form performance compared to the in-domain performance. This means that our method learns various mention forms, rather than focusing on a specific mention form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Results of Other</head><p>Biomedical Datasets <ref type="table" target="#tab_14">Table 8</ref> shows our results for the rest of the biomedical NER datasets. The main trend is similar in that our approach enhances the recall of Mem and Unseen while preserving (or sometimes improving) the in-domain performance. Our RegLER method shows competitive results on the BC4CHEMD, BC2GM, and BC5CDR-disease datasets. We conducted five runs with different seeds for all the experiments. The t-test indicated that p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experimental Results of Various Lambda</head><p>As stated, <ref type="table" target="#tab_12">Table 7</ref> shows that in a mention set with a small entity length (i.e., E(L) is low), there is no significant difference in performance even if ? changes. However, we find that a mention set of long-named entities (i.e., E(L) 8) exhibits at highperformance gap between low ? and high ? values. Because it is difficult to find an optimal ? for each mention set, finding it through a validation setting or set a ? as a learnable parameter appears to be a good approach.         </p><formula xml:id="formula_11">ACE + document ? context ? # # # - - 94.6 - - CL ? KL ? # # # - - 93.9 - 60.5 FLERT XLM ? R ? # # # - - 94.1 - - LUKE ? # # # - - 94.3 - - BERT ? base # # #</formula><formula xml:id="formula_12">? Base # # # - - - S parkNLP # # # - - - KeBioLM # # # - - 85.1 CL ? KL # # # - - - BioFLAIR # # # - - -</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Entity length is variable and dependent on a specific domain or dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?Figure 2 :</head><label>2</label><figDesc>Mem := {M|M ? E tr (M)} ? Unseen := {M|M E tr (M)} ? Syn := {M|M E tr (M), M ? C tr (M)} ? Con := {M|M E tr (M), M C tr (M)}, The overview of RegLER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Dogan et al., 2014), BC5CDR-disease (Li et al., 2016), drug and chemical entities BC5CDR-chemical (Li et al., 2016), BC4CHEMD (Krallinger et al., 2015), gene and protein entities BC2GM (Smith et al., 2008), JNLPBA (Kim et al., 2004), and species entities LINNAEUS (Gerner et al., 2010), and Species-800 (Pafilis et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>used BioBERT (Lee et al., 2020b), SciBERT (Beltagy et al., 2019), PubmedBERT (LIU et al., 2020), Method Subword Class Temp</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Bias-weighted scaling on long-named and complex entities. L(E) indicates an entity length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>? large: Rhinovirus 2A protease, mouth -disease virus L protease, eIF4G, eIF4A, eIF4E, eIF3 BioLM ? large + RegLER (+ None): Rhinovirus 2A protease, mouth disease virus L protease, eIF4G, eIF4A, eIF4E, eIF3 BioLM ? large + RegLER (+ Temp): Rhinovirus 2A protease, foot -and -mouth -disease virus L protease, eIF4G, eIF4A, eIF4E, eIF3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>In NLP tasks, dataset bias is a superficial cue or surface form of text associated with specific labels. Recently, pre-trained language models(Devlin et al.,  2019; Liu et al., 2019; Lee et al., 2020b)  have tended to exploit dataset biases to predict rather than understand the relationship between words and labels(Gururangan et al., 2018; McCoy et al.,  2019; Geirhos et al., 2020; Bender and Koller,  2020;<ref type="bibr" target="#b0">Utama et al., 2020b;</ref> Du et al., 2021). This eventually led the advent of debiasing methods to enhance the generalization ability. Clark et al.(2019) introduced a product-of-expert as a modelagnostic approach of debiasing method to scale down the gradients on the biased examples. Mahabadi et al. (2020) proposed a bias-only model to focus on learning hard examples by reducing the relative importance of biased examples using focal loss. Schuster et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of the debiasing method on the biomedical domain NER datasets. Each dataset is partitioned into memorization (Mem), synonym generalization (Syn), and concept generalization (Con). We use and # to show the components that are used. ? signifies the reported performance on the manuscript. Best performances are shown in bold. and BioLM (Lewis et al., 2020) as our target models. To compare our experimental results, we report various strong models with written scores.</figDesc><table><row><cell>These models are enumerated as follows: SciFive-</cell></row><row><cell>base (Phan et al., 2021), Spark NLP (Kocaman and</cell></row><row><cell>Talby, 2021), KeBioLM (Yuan et al., 2021), CL-</cell></row><row><cell>KL (Wang et al., 2021), and BioFLAIR (Sharma</cell></row><row><cell>and Daniel Jr, 2019). In the bias-only model, we</cell></row><row><cell>compare RegLE with a class distribution of word</cell></row><row><cell>frequency (Kim and Kang, 2021) which is sug-</cell></row><row><cell>gested as row 2 of each block in Table 2. Rather</cell></row><row><cell>than attempting PMI solely on the Bias-Product</cell></row><row><cell>(BP), we extend our experiments to show how our</cell></row><row><cell>method RegLE can operate with other debiasing</cell></row><row><cell>methods such as Learned-Mixin (LM) and Learned-</cell></row><row><cell>Mixin+H (Clark et al., 2019). Detailed explana-</cell></row><row><cell>tions are given in Appendix B. We denote and</cell></row><row><cell># when using subword frequency (Subword), class</cell></row><row><cell>prior (Class), and bias-weighted scaling (Temp).</cell></row><row><cell>Description of the training details and hyperparam-</cell></row><row><cell>eters are provided in Appendix C.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>81.2 85.0 88.1 98.4 89.5 88.9 94.2 BioLM ? large + RegLER # # 94.2 80.8 84.6 88.6 98.7 87.4 86.7 93.2 BioLM ? large + RegLER # # 94.7 82.4 84.8 88.6 98.6 88.5 87.5 93.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">NCBI-disease</cell><cell></cell><cell></cell><cell cols="2">BC5CDR-chem</cell><cell></cell></row><row><cell>Method</cell><cell>Subword Class Temp</cell><cell>Mem</cell><cell>Syn</cell><cell>Con</cell><cell>Total</cell><cell>Mem</cell><cell>Syn</cell><cell>Con</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell>(R)</cell><cell>(R)</cell><cell>(R)</cell><cell>(F1)</cell><cell>(R)</cell><cell>(R)</cell><cell>(R)</cell><cell>(F1)</cell></row><row><cell cols="10">BioLM ? large + RegLER 93.5 BioLM ? large + RegLER # # # # # 94.0 81.3 84.5 88.7 97.7 85.4 86.8 93.8</cell></row><row><cell>BioLM ? large + RegLER</cell><cell>#</cell><cell cols="8">95.2 82.5 88.1 88.5 98.6 90.1 89.2 93.6</cell></row><row><cell>BioLM ? large + RegLER</cell><cell>#</cell><cell cols="8">95.0 82.4 84.3 88.8 98.6 88.3 87.4 94.1</cell></row><row><cell>BioLM ? large + RegLER</cell><cell>#</cell><cell cols="8">94.8 83.5 86.2 89.1 98.4 89.9 88.3 94.3</cell></row><row><cell>BioLM ? large + RegLER</cell><cell></cell><cell cols="8">95.1 83.2 87.4 89.5 98.7 90.3 89.1 94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of three components on biomedical domain: subword frequency (Subword), class prior (Class), and length dependent scaling (Temp). Best performances are shown in bold.</figDesc><table><row><cell></cell><cell>86</cell></row><row><cell></cell><cell>84</cell></row><row><cell></cell><cell>82</cell></row><row><cell>F1 score</cell><cell>78 80</cell></row><row><cell></cell><cell>76</cell></row><row><cell></cell><cell>74</cell></row><row><cell></cell><cell>72</cell></row><row><cell></cell><cell>1.01 1.02 1.03 1.04 1.05 1.06 1.07 1.08 1.09 Lambda</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Example of dealing with the OOV issue and prediction for a long-named entity. Ground truth labels are in bold, incorrect answers are in red, and correct answers are in blue. but we only show a relatively simple case to provide an intuition.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Minsoo Cho, Jihwan Ha, Chihyun Park, and Sanghyun Park. 2020. Combinatorial feature embedding based on cnn and lstm for biomedical named entity recognition. Journal of biomedical informatics. Robert Geirhos, J?rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence. Martin Gerner, Goran Nenadic, and Casey M Bergman. 2010. Linnaeus: a species name identification system for biomedical literature. BMC bioinformatics. Kocaman and David Talby. 2021. Spark nlp: Natural language understanding at scale. Software Impacts. Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2020. A rigourous study on named entity recognition: Can fine-tuning pretrained model lead to the promised land? arXiv preprint arXiv:2004.12126. Erol Bahadroglu, Alec Peltekian, and Gr?goire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature. arXiv preprint arXiv:2106.03598.</figDesc><table><row><cell cols="2">TRISTAN NAUMANN LIU, JIANFENG GAO, and HOIFUNG POON. 2020. Domain-specific</cell><cell>Christopher Clark, Mark Yatskar, and Luke Zettle-moyer. 2019. Don't take the easy way out: En-Long N Phan, James T Anibal, Hieu Tran, Shaurya Veysel Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Chanana, Lance A Ramshaw and Mitchell P Marcus. 1999. Text</cell></row><row><cell cols="2">language model pretraining for biomedical</cell><cell>semble based methods for avoiding known dataset Lowe, et al. 2015. The chemdner corpus of chemi-chunking using transformation-based learning. In</cell></row><row><cell>natural language processing.</cell><cell>arXiv preprint</cell><cell>biases. In Proceedings of the 2019 Conference on cals and drugs and its annotation principles. Journal Natural language processing using very large cor-</cell></row><row><cell cols="2">arXiv:2007.15779. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-berger. 2017. On calibration of modern neural net-works. In International Conference on Machine Learning. Suchin Gururangan, Swabha Swayamdipta, Omer Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap-proach. arXiv preprint arXiv:1907.11692. Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural lan-guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. 2020. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Com-putational Linguistics. Maryam Habibi, Leon Weber, Mariana Neves, David Luis Wiegandt, and Ulf Leser. 2017. Deep learning with word embeddings improves biomedi-cal named entity recognition. Bioinformatics. Pedro Henrique Martins, Zita Marinho, and Andr? F. T. Martins. 2019. Joint learning of named en-tity recognition and entity linking. In Proceedings</cell><cell>Empirical Methods in Natural Language Processing of cheminformatics. pora. and the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP). Gamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna Korhonen. 2017. A neural network multi-task learn-ing approach to biomedical named entity recogni-tion. BMC bioinformatics. Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. 2017. Results of the wnut2017 shared task on novel and emerging entity recogni-tion. In Proceedings of the 3rd Workshop on Noisy User-generated Text. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data. Erik Tjong Kim Sang and Fien De Meulder. 2003. In-troduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceed-ings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. Guillaume Lample, Miguel Ballesteros, Sandeep Sub-ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies. Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola Ortiz, Enrico Santus, and Regina Barzilay. 2019. Towards debiasing fact verification models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-Jinhyuk Lee, Sean S. Yi, Minbyul Jeong, Mujeen Sung, WonJin Yoon, Yonghwa Choi, Miyoung Ko, and Jae-woo Kang. 2020a. Answering questions on COVID-ral Language Processing (EMNLP-IJCNLP), pages 3419-3425, Hong Kong, China. Association for Computational Linguistics.</cell></row><row><cell cols="2">He He, Sheng Zha, and Haohan Wang. 2019. Unlearn dataset bias in natural language inference by fitting the residual. In Proceedings of the 2nd Workshop on of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Work-shop, pages 190-196, Florence, Italy. Association for Computational Linguistics. Deep Learning Approaches for Low-Resource NLP (DeepLo 2019). Geoffrey E Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural com-putation. Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceed-ings of the 57th Annual Meeting of the Association for Computational Linguistics.</cell><cell>bidirectional transformers for language understand-ing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, Volume 1 (Long and Short Papers). Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for dis-ease name recognition and concept normalization. Journal of biomedical informatics. 19 in real-time. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, On-line. Association for Computational Linguistics. Stefan Schweter and Alan Akbik. 2020. Flert: Document-level features for named entity recogni-tion. arXiv preprint arXiv:2011.06993. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020b. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics. Burr Settles. 2004. Biomedical named entity recogni-tion using conditional random fields and rich feature sets. In Proceedings of the international joint work-shop on natural language processing in biomedicine and its applications (NLPBA/BioNLP).</cell></row><row><cell cols="2">SK Hong and Jae-Gil Lee. 2020. Dtranner: biomedical named entity recognition with deep learning-based label-label transition model. BMC bioinformatics. Yuxian Meng, Muyu Li, Xiaoya Li, Wei Wu, and Ji-wei Li. 2019. Dsreg: Using distant supervision as a regularizer. arXiv preprint arXiv:1905.11658. Robin Jia and Percy Liang. 2017. Adversarial exam-Bogdan Babych and Anthony Hartley. 2003. Im-proving machine translation quality with automatic named entity recognition. In EAMT. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-ert: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empiri-cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Emily M. Bender and Alexander Koller. 2020. Climb-ing towards NLU: On meaning, form, and under-standing in the age of data. In Proceedings of the ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Hyunjae Kim and Jaewoo Kang. 2021. How do your Diego Moll?, Menno Van Zaanen, Daniel Smith, et al. 2006. Named entity recognition for question an-swering. Yixin Nie, Adina Williams, Emily Dinan, Mohit biomedical named entity models generalize to novel entities? arXiv preprint arXiv:2101.00160. Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduction to the bio-entity recognition task at jnlpba. In Pro-ceedings of the international joint workshop on nat-Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-versarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin-guistics. Evangelos Pafilis, Sune P Frankild, Lucia Fanini, ural language processing in biomedicine and its ap-Sarah Faulwetter, Christina Pavloudi, Aikaterini plications. Vasileiadou, Christos Arvanitidis, and Lars Juhl</cell><cell>Cicero dos Santos and Victor Guimar?es. 2015. Boost-ing named entity recognition with neural character embeddings. In Proceedings of the Fifth Named En-tity Workshop. Joffrey L Leevy, Taghi M Khoshgoftaar, Richard A Bauder, and Naeem Seliya. 2018. A survey on ad-dressing high-class imbalance in big data. Journal of Big Data. Shreyas Sharma and Ron Daniel Jr. 2019. Bioflair: Pretrained pooled contextualized embeddings for biomedical sequence labeling tasks. arXiv preprint arXiv:1908.05760. Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. 2021. Towards interpreting and mitigating shortcut learning behavior of nlu models. Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber. 2018. Pathologies of neural models make interpretations difficult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-ing. Jinlan Fu, Pengfei Liu, and Graham Neubig. 2020a. In-Patrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-anov. 2020. Pretrained language models for biomed-ical and clinical tasks: Understanding and extending Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Logarithmic opinion pools for conditional random fields. In Proceedings of the 43rd Annual Meeting the state-of-the-art. In Proceedings of the 3rd Clini-cal Natural Language Processing Workshop. on Association for Computational Linguistics. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-aky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016. Biocreative v cdr task corpus: Larry Smith, Lorraine K Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, et al. 2008. Overview of biocreative ii gene mention recognition. Genome a resource for chemical disease relation extraction. Database. biology. Arata Ugawa, Akihiro Tamura, Takashi Ninomiya, Hi-Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. roya Takamura, and Manabu Okumura. 2018. Neu-</cell></row><row><cell cols="2">58th Annual Meeting of the Association for Compu-tational Linguistics, pages 5185-5198, Online. As-sociation for Computational Linguistics. Jensen. 2013. The species and organisms resources Diederik P Kingma and Jimmy Ba. 2014. Adam: A for fast and accurate identification of taxonomic method for stochastic optimization. arXiv preprint arXiv:1412.6980. names in text. PloS one.</cell><cell>terpretable multi-dataset evaluation for named entity recognition. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-ing (EMNLP), pages 6058-6069, Online. Associa-2020a. A survey on deep learning for named entity ral machine translation incorporating named entity. recognition. IEEE Transactions on Knowledge and In Proceedings of the 27th International Conference Data Engineering. on Computational Linguistics.</cell></row><row><cell cols="2">Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh</cell><cell>tion for Computational Linguistics. Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang,</cell></row><row><cell cols="2">Jha, and Ananthram Swami. 2016. Distillation as</cell><cell>Fei Wu, and Jiwei Li. 2020b. Dice loss for data-</cell></row><row><cell cols="2">a defense to adversarial perturbations against deep</cell><cell>imbalanced nlp tasks. In Proceedings of the 58th An-</cell></row><row><cell cols="2">neural networks. In 2016 IEEE symposium on secu-</cell><cell>nual Meeting of the Association for Computational</cell></row><row><cell>rity and privacy (SP).</cell><cell></cell><cell>Linguistics.</cell></row></table><note>Oliver Bender, Franz Josef Och, and Hermann Ney. 2003. Maximum entropy models for named entity recognition. In Proceedings of the seventh confer- ence on Natural language learning at HLT-NAACL 2003.Jinlan Fu, Pengfei Liu, and Qi Zhang. 2020b. Rethink- ing generalization of neural models: A named entity recognition case study. In AAAI.Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. 2020. Look at the first sen- tence: Position bias in question answering. In Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Partitioned benchmark datasets to evaluate</cell></row><row><cell>generalization ability following the previous works</cell></row><row><cell>(Lin</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Performance of RegLER on the general domain NER datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>1.09 87.4 / 85.6 / 86.5 85.3 / 83.4 / 84.1 86.2 / 85.9 / 86.0 83.6 / 83.5 / 83.5 81.4 / 81.6 / 81.5 82.2 / 82.1 / 82.2 80.4 / 78.8 / 79.6 69.6 / 74.1 / 71.8 1.08 87.1 / 85.3 / 86.2 85.1 / 83.3 / 84.2 86.0 / 85.5 / 85.8 83.6 / 83.3 / 83.4 81.7 / 81.5 / 81.6 81.5 / 81.8 / 81.6 80.5 / 79.0 / 79.7 70.1 / 73.8 / 72.0 1.07 87.6 / 85.9 / 86.7 85.4 / 83.7 / 84.6 86.1 / 85.7 / 85.9 83.9 / 83.7 / 83.8 82.0 / 81.7 / 81.8 81.2 / 81.6 / 81.4 79.7 / 78.8 / 79.3 70.7 / 73.8 / 72.2 1.06 87.5 / 86.3 / 86.9 85.4 / 83.9 / 84.6 85.7 / 85.7 / 85.7 83.4 / 83.6 / 83.5 82.1 / 82.5 / 82.3 81.6 / 82.1 / 81.9 80.1 / 80.4 / 80.3 70.1 / 73.6 / 71.8 1.05 87.6 / 86.3 / 86.9 85.5 / 83.9 / 84.7 86.2 / 86.0 / 86.1 83.6 / 83.5 / 83.6 81.8 / 81.9 / 81.8 81.2 / 82.2 / 81.7 80.2 / 79.7 / 79.8 69.4 / 73.5 / 71.4 1.04 87.6 / 86.1 / 86.8 85.6 / 84.0 / 84.8 86.1 / 85.7 / 85.9 83.7 / 83.5 / 83.6 81.8 / 81.7 / 81.7 82.3 / 83.0 / 82.6 80.3 / 79.8 / 80.0 71.6 / 74.7 / 73.1 1.03 87.2 / 85.8 / 86.5 85.7 / 83.9 / 84.8 81.2 / 85.9 / 86.0 84.1 / 83.8 / 84.0 81.9 / 81.8 / 81.9 82.0 / 82.5 / 82.3 80.9 / 80.8 / 80.9 71.5 / 74.5 / 72.9 1.02 87.5 / 86.0 / 86.7 85.3 / 83.7 / 84.5 86.1 / 85.7 / 85.9 83.5 / 83.4 / 83.5 82.1 / 81.9 / 82.0 81.5 / 82.0 / 81.8 79.4 / 78.7 / 79.0 70.2 / 73.3 / 71.7 1.01 87.0 / 85.2 / 86.1 85.0 / 83.3 / 84.1 85.8 / 85.5 / 85.7 83.2 / 83.4 / 83.3 82.2 / 82.2 / 82.2 80.5 / 81.6 / 81.0 79.2 / 79.3 / 79.3 69.4 / 72.8 / 71.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of using various ? values. E(L) signifies the length of the entity.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">Subword Class Temp</cell><cell></cell><cell>Eval Metric</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Mem (R) Unseen (R) Total (F1)</cell></row><row><cell></cell><cell>S ciFive ? Base</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>S parkNLP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>93.7</cell></row><row><cell></cell><cell>KeBioLM</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CL ? KL</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BioFLAIR</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BioBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>95.6</cell><cell>82.6</cell><cell>91.1</cell></row><row><cell></cell><cell>BioBERT + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>94.9</cell><cell>83.3</cell><cell>91.0</cell></row><row><cell></cell><cell>BioBERT + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>95.8</cell><cell>83.9</cell><cell>91.4</cell></row><row><cell></cell><cell>S ciBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>94.2</cell><cell>79.2</cell><cell>89.4</cell></row><row><cell>BC4CHEMD</cell><cell>S ciBERT + BP S ciBERT + RegLER</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>94.2 94.2</cell><cell>80.5 81.1</cell><cell>89.4 89.4</cell></row><row><cell></cell><cell>PubmedBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>95.3</cell><cell>82.8</cell><cell>91.2</cell></row><row><cell></cell><cell>PubmedBERT + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>95.3</cell><cell>83.7</cell><cell>91.2</cell></row><row><cell></cell><cell>PubmedBERT + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>95.5</cell><cell>83.5</cell><cell>91.4</cell></row><row><cell></cell><cell>BioLM ? base</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>95.4</cell><cell>86.5</cell><cell>91.4</cell></row><row><cell></cell><cell>BioLM ? base + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>95.1</cell><cell>86.6</cell><cell>91.1</cell></row><row><cell></cell><cell>BioLM ? base + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>96.0</cell><cell>87.1</cell><cell>92.5</cell></row><row><cell></cell><cell>BioLM ? large</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>96.3</cell><cell>88.2</cell><cell>93.2</cell></row><row><cell></cell><cell>BioLM ? large + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>96.3</cell><cell>88.5</cell><cell>93.4</cell></row><row><cell></cell><cell>BioLM ? large + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>96.2</cell><cell>88.9</cell><cell>93.6</cell></row><row><cell></cell><cell>S ciFive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BC2GM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Performance of the debiasing method RegLER on the biomedical domain NER datasets. Each dataset is partitioned into a memorization (Mem) and Unseen mention set. Best performances are shown in bold.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">Subword Class Temp</cell><cell></cell><cell cols="2">Eval Metric</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mem (R) Unseen (R)</cell><cell cols="2">Total (F1)</cell></row><row><cell></cell><cell>S ciFive ? Base</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>S parkNLP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>80.9</cell><cell></cell></row><row><cell></cell><cell>KeBioLM</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>CL ? KL</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>BioFLAIR</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>82.4</cell><cell></cell></row><row><cell></cell><cell>BioBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.6</cell><cell>70.6</cell><cell>74.7</cell><cell></cell></row><row><cell></cell><cell>BioBERT + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.8</cell><cell>71.4</cell><cell>74.4</cell><cell></cell></row><row><cell></cell><cell>BioBERT + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>94.1</cell><cell>72.2</cell><cell>75.3</cell><cell></cell></row><row><cell></cell><cell>S ciBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>89.5</cell><cell>66.5</cell><cell>72.0</cell><cell></cell></row><row><cell>S800</cell><cell>S ciBERT + BP S ciBERT + RegLER</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>89.3 90.7</cell><cell>65.4 68.2</cell><cell>71.7 70.9</cell><cell></cell></row><row><cell></cell><cell>PubmedBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>90.5</cell><cell>72.2</cell><cell>74.2</cell><cell></cell></row><row><cell></cell><cell>PubmedBERT + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>89.3</cell><cell>72.2</cell><cell>73.2</cell><cell></cell></row><row><cell></cell><cell>PubmedBERT + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>91.1</cell><cell>72.4</cell><cell>73.6</cell><cell></cell></row><row><cell></cell><cell>BioLM ? base</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>92.4</cell><cell>70.9</cell><cell>73.2</cell><cell></cell></row><row><cell></cell><cell>BioLM ? base + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>91.9</cell><cell>72.1</cell><cell>72.8</cell><cell></cell></row><row><cell></cell><cell>BioLM ? base + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>92.2</cell><cell>71.7</cell><cell>73.9</cell><cell></cell></row><row><cell></cell><cell>BioLM ? large</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>90.2</cell><cell>73.3</cell><cell>73.3</cell><cell></cell></row><row><cell></cell><cell>BioLM ? large + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>91.5</cell><cell>74.3</cell><cell>73.7</cell><cell></cell></row><row><cell></cell><cell>BioLM ? large + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>92.2</cell><cell>73.7</cell><cell>74.5</cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">Subword Class Temp</cell><cell></cell><cell cols="2">Eval Metric</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mem (R)</cell><cell>Syn (R)</cell><cell cols="2">Con (R) Total (F1)</cell></row><row><cell></cell><cell>S ciFive ? Base</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.2</cell></row><row><cell></cell><cell>S parkNLP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>KeBioLM</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.1</cell></row><row><cell></cell><cell>CL ? KL</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BioFLAIR</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.3</cell></row><row><cell></cell><cell>BioBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>91.3</cell><cell>76.7</cell><cell>73.4</cell><cell>82.5</cell></row><row><cell></cell><cell>BioBERT + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>90.9</cell><cell>77.1</cell><cell>74.3</cell><cell>82.5</cell></row><row><cell></cell><cell>BioBERT + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>92.0</cell><cell>78.5</cell><cell>75.1</cell><cell>83.3</cell></row><row><cell>BC5CDR-disease</cell><cell>S ciBERT S ciBERT + BP S ciBERT + RegLER</cell><cell># #</cell><cell># #</cell><cell># #</cell><cell>93.3 93.2 93.1</cell><cell>71.9 70.8 72.2</cell><cell>73.3 73.3 74.2</cell><cell>84.2 84.1 84.1</cell></row><row><cell></cell><cell>PubmedBERT</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.2</cell><cell>76.5</cell><cell>74.5</cell><cell>85.4</cell></row><row><cell></cell><cell>PubmedBERT + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.1</cell><cell>78.6</cell><cell>75.7</cell><cell>85.3</cell></row><row><cell></cell><cell>PubmedBERT + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>93.4</cell><cell>77.5</cell><cell>75.3</cell><cell>85.5</cell></row><row><cell></cell><cell>BioLM ? base</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.5</cell><cell>79.2</cell><cell>75.6</cell><cell>85.4</cell></row><row><cell></cell><cell>BioLM ? base + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>92.5</cell><cell>79.8</cell><cell>76.7</cell><cell>83.9</cell></row><row><cell></cell><cell>BioLM ? base + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>93.5</cell><cell>80.8</cell><cell>77.5</cell><cell>86.2</cell></row><row><cell></cell><cell>BioLM ? large</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.5</cell><cell>81.5</cell><cell>77.4</cell><cell>86.2</cell></row><row><cell></cell><cell>BioLM ? large + BP</cell><cell>#</cell><cell>#</cell><cell>#</cell><cell>93.0</cell><cell>82.3</cell><cell>77.4</cell><cell>86.4</cell></row><row><cell></cell><cell>BioLM ? large + RegLER</cell><cell></cell><cell></cell><cell></cell><cell>93.7</cell><cell>83.2</cell><cell>78.5</cell><cell>86.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Performance of the debiasing method RegLER on the biomedical domain NER datasets. Each dataset is partitioned into a memorization (Mem) and Unseen mention set. Best performances are shown in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">PMI was used as a correlation score between the word and class in a particular dataset(Gururangan et al., 2018)   3 K is a hyperparameter and we use K=100, similar to(Gururangan et al., 2018)  to emphasize correlations of word and class.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/cambridgeltl/MTL-Bioinformatics-2016</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. 2020a. Mind the trade-off: Debiasing nlu models without degrading the in-distribution performance. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head><p>Following the previous works <ref type="bibr">(Lin et al., 2020;</ref><ref type="bibr">Kim and Kang, 2021)</ref>, in <ref type="table">Table 5</ref>, we partition the top three datasets that suggest the concept unique ID (CUI) in order to split the unseen mention set into Syn and Con. Without the CUI, we partition the benchmark datasets into Mem and Unseen mention sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Learned-Mixin and Learned-Mixin+H</head><p>Following the previous work (Clark et al., 2019), we use an ensemble framework that combines a target model f ? and a bias-only model P(Y|X) P(Y) as a statistic. We also extend our ensemble methods to Learned-Mixin and Learned-Mixin+H without <ref type="bibr">Dataset</ref> Dev Test <ref type="table">Mem Syn Con   NCBI-disease  514  184  87  594  196 164  BC5CDR-disease 2624 899 632  2802 882 631  BC5CDR-chem  3432 460 1394 3293 515 1538   Mem  Unseen  Mem  Unseen   BC4CHEMD  18337  11082  15974  9324  BC2GM  1250  1810  2625  3697  JNLPBA  6356  2214  3616  2614  LINNAEUS  516  189  976  448  Species-800  130  254  270  494  CoNLL-2003  3963  1854  2993  2500</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mem Syn Con</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards debiasing nlu models from unknown biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafise Sadat</forename><surname>Prasetya Ajie Utama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automated concatenation of embeddings for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03654</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A multichannel biomedical named entity recognition model based on multitask learning and contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Wireless Communications and Mobile Computing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Luke: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10344</idno>
		<title level="m">Songfang Huang, and Fei Huang. 2021. Improving biomedical pretrained language models with knowledge</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Named entity recognition using an hmm-based chunk tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
