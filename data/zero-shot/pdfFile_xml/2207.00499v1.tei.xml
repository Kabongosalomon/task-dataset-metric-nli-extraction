<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MotionMixer: MLP-based 3D Human Body Pose Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arij</forename><surname>Bouazizi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ulm University</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Holzbock</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ulm University</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Kressel</surname></persName>
							<email>ulrich.kressel@mercedes-benz.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
							<email>klaus.dietmayer@uni-ulm.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Ulm University</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
							<email>vasileios.belagiannis@ovgu.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Otto von Guericke University Magdeburg</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MotionMixer: MLP-based 3D Human Body Pose Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present MotionMixer, an efficient 3D human body pose forecasting model based solely on multi-layer perceptrons (MLPs). Motion-Mixer learns the spatial-temporal 3D body pose dependencies by sequentially mixing both modalities. Given a stacked sequence of 3D body poses, a spatial-MLP extracts fine-grained spatial dependencies of the body joints. The interaction of the body joints over time is then modelled by a temporal MLP. The spatial-temporal mixed features are finally aggregated and decoded to obtain the future motion. To calibrate the influence of each time step in the pose sequence, we make use of squeeze-and-excitation (SE) blocks. We evaluate our approach on Human3.6M, AMASS, and 3DPW datasets using the standard evaluation protocols. For all evaluations, we demonstrate state-of-the-art performance, while having a model with a smaller number of parameters. Our code is available at: https://github.com/MotionMLP/MotionMixer.</p><p>Recently, the availability of large-scale datasets, e.g. Hu-man3.6M <ref type="bibr" target="#b6">[Ionescu et al., 2013]</ref>, AMASS [Mahmood et al.,  2019]  or 3DPW <ref type="bibr" target="#b9">[von Marcard et al., 2018]</ref>, the development of human pose estimation algorithms <ref type="bibr" target="#b2">[Belagiannis et al., 2014;</ref><ref type="bibr" target="#b3">Bouazizi et al., 2021]</ref> and the advent of deep learning methods pushed the evolution towards forecasting future 3D poses with less priors. Several learning-based approaches were proposed to tackle the problem of 3D human motion prediction. Methods like <ref type="bibr" target="#b3">[Fragkiadaki et al., 2015;</ref><ref type="bibr" target="#b8">Martinez et al., 2017;</ref><ref type="bibr" target="#b8">Tang et al., 2018]</ref> build upon the success of recurrent neural networks (RNNs) to better model the temporal correlation between the human body joints. Nev-arXiv:2207.00499v1 [cs.CV] 1 Jul 2022</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Forecasting 3D human motion is at the core of many different applications ranging from virtual reality to autonomous driving <ref type="bibr">[Wiederer et al., 2020]</ref> and robotics <ref type="bibr" target="#b4">[Gui et al., 2018]</ref>. Fundamentally, the task of human motion prediction is defined as the prediction of future body poses from past ones. To model the human spatial-temporal dynamics, classic approaches adopted Hidden Markov Models <ref type="bibr">[Kuli? et al., 2012]</ref> or Gaussian Processes <ref type="bibr" target="#b10">[Wang et al., 2007]</ref>. Despite the tangible progress in predicting periodic motion, these approaches impose strong assumptions on body pose, leading to performance degradation. Furthermore, it is difficult for these methods to forecast reliable 3D poses because of the complicated bio-mechanical kinematics. Even manually imposing expert knowledge does not help prior-based approaches to generalise to new environments and subjects. * Contact Author ? Most of this work was done while Vasileios Belagiannis was with Ulm University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Motion History Motion</head><p>Figure 1: Long-term predictions of MotionMixer for the actions Directions and Posing of the Human3.6M dataset. The first and the third line indicate the ground-truth 3D human motion. The frames on the left are the observations. The right part, shown in pink is the long-term motion prediction. One every three frames are shown. The model predictions accurately match the ground-truth body poses. ertheless, the use of RNNs come with many drawbacks, including vanishing or exploding gradients <ref type="bibr" target="#b3">[Fragkiadaki et al., 2015]</ref>, first frame discontinuity, problems in processing longer input sequences as well as shorter forecast horizons leading to stationary predictions or frozen motion <ref type="bibr" target="#b8">[Martinez et al., 2017]</ref>. More recent works <ref type="bibr" target="#b8">Sofianos et al., 2021]</ref> propose to replace RNN models with dedicated temporal convolutional architectures. In contrast to RNNs, Convolutional Neural Networks (CNNs) maintain a hierarchical structure, enabling them to capture both spatial and temporal correlations effectively. Lately, graph convolutional networks (GCNs) have received increasing attention. Several works <ref type="bibr" target="#b7">[Mao et al., 2019;</ref><ref type="bibr" target="#b8">Mao et al., 2020;</ref><ref type="bibr" target="#b8">Sofianos et al., 2021;</ref> attempted to utilize GCNs to learn fine-grained spatial relationships among joints. <ref type="bibr" target="#b7">[Mao et al., 2019]</ref> for instance, encoded the joints history in the frequency domain and proposed a GCN with learnable connectivity to predict the future motion. Although effective, these methods still need structural priors <ref type="bibr" target="#b0">[Aksan et al., 2020]</ref> or frequency transformation <ref type="bibr" target="#b7">[Mao et al., 2019]</ref> to address the inherent spatial-temporal dependency of the human motion.</p><p>While RNNs, CNNs and GCNs led to a significant performance gain in forecasting 3D human body poses, the existing methods are unnecessarily complex. In this paper, we present MotionMixer, the first model using exclusively MLPs to address the inherent problems of human motion. Equipped with a simple, yet effective architecture, the model aims to learn the spatial-temporal dependencies of the human body pose. Inspired by <ref type="bibr">[Tolstikhin et al., 2021]</ref>, we propose two types of layers: one with MLPs applied independently to time steps (i.e. "mixing" the temporal information) and another with MLPs applied across body poses (i.e. "mixing" the spatial information). The interchangeable spatial and temporal mixing operations allow the model to access current and past information directly and capture both the structural and the temporal dependencies explicitly.</p><p>Our contributions are summarized as follows: 1) We propose to jointly model the spatial locations of the body joints and their temporal dependency with a spatial-temporal MLP. To the best of our knowledge, MotionMixer is the first 3D body pose forecasting approach based solely on MLPs. 2) We design an efficient architecture, that significantly reduces the computational cost of the pose forecasting model. 3) An extensive evaluation on three challenging large-scale datasets demonstrates state-of-the-art performance for short-term and long-term motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recurrent-based Motion Prediction. 3D human motion prediction with RNNs has been widely studied in the last years. <ref type="bibr" target="#b3">[Fragkiadaki et al., 2015]</ref> proposed a recurrent encoder-decoder model, which incorporates nonlinear encoder and decoder networks before and after recurrent layers. A curriculum learning strategy was adopted to prevent error accumulation during the training. To better model the long-term temporal dependency, <ref type="bibr" target="#b8">[Martinez et al., 2017]</ref> incorporated a residual connection between the RNN units. To make reliable future predictions, <ref type="bibr" target="#b8">[Tang et al., 2018]</ref> proposed a motion context modeling by summarizing the historical human motion with respect to the current prediction within a recurrent prediction framework. Though these methods have also incorporated different modules into RNNs, exploring a recurrent-free backbone to address human motion prediction tasks is rarely studied.</p><p>Convolutional-based Motion Prediction. Thanks to their hierarchical structure and their effectiveness in capturing spatial and temporal correlations, there has been recently a great interest in integrating CNNs in human motion prediction.  for instance, encoded the history motion into a long-term hidden variable, which is used with a decoder to predict the future sequence. The decoder itself also has an encoder-decoder structure, with a short-term encoder and a long-term decoder. <ref type="bibr" target="#b8">[Sofianos et al., 2021]</ref> proposed a spacetime-separable GCN, where the space-time graph connectivity is factored into space and time affinity matrices. <ref type="bibr" target="#b7">[Mao et al., 2019]</ref> designed a fully connected GCN to adaptively learn the spatial connectivity of the human skeletons and converted the joint trajectory to the frequency domain to handle the temporal information. <ref type="bibr" target="#b3">[Dang et al., 2021]</ref> proposed a multi-scale residual graph network with descending and ascending GCNs to extract features in both fine-to-coarse and coarse-to-fine manners. Despite the advantages in capturing long-range temporal correlations, the quite high computational cost of convolution-based approaches remains a bottleneck. Unlike these approaches, we propose an MLP-based model with lower computational complexity, that better exploits the spatial-temporal dependencies of the body pose.</p><p>Attention-and MLP-based Architectures. Inspired by the success of the self-attention mechanism <ref type="bibr" target="#b8">[Vaswani et al., 2017]</ref> in natural language processing, many works have explored its application in human motion prediction. <ref type="bibr" target="#b8">[Mao et al., 2020]</ref> proposed to capture the similarity between the current motion context and the historical motion sub-sequences in the frequency domain with the attention mechanism. <ref type="bibr" target="#b0">[Aksan et al., 2020]</ref> proposed to autoregressively learn spatialtemporal representations with decoupled temporal and spatial self-attention. The key role of self-attention is to re-weight the relative importance of each pose in the sequence with respect to all other poses. This resulted in a high computational memory overhead with increasing number of history poses. On the other end of the spectrum, there have been new works that support replacing self-attention with MLPs. <ref type="bibr">MLP-Mixer [Tolstikhin et al., 2021]</ref> for instance, relaxed the quadratically increasing computational memory by replacing the selfattention module with a two-layer MLP. The idea behind the Mixer architecture is to learn to separate the per-location operations and cross-location operations, allowing communication between different image patches. With a design based solely on MLPs, the model was originally developed for visual recognition tasks. However, unlike image classification, where only spatial correlations exist, there exist complicated spatial-temporal dynamics in human motion. In this work, we delve deeper and propose a new architecture based on MLPs to learn the spatial-temporal dependencies of the human body. Unlike the above-discussed approaches, we show that MLPs are effective in learning human dynamics.</p><p>We define the human body motion as a sequence of T h + T f consecutive frames, where each frame parameterizes the angles or 3D coordinates of the human body joints. Let X 1:T h = {x 1 , x 2 , . . . , x t , . . . x T h } ? R 3?J?T h be the historical motion sequence until the current time step T h , with the 3D body pose x t ? R 3?J and J is the number of body keypoints. Our goal is to learn the mapping that bridges the history sequence X 1:T h to the future sequence X T h +1:</p><formula xml:id="formula_0">T h +T f = x T h +1 , x T h +2 , . . . , x T h +t , . . . x T h +T f ? R 3?J?T f with a pure MLP-based network.</formula><p>Below, we provide the details of our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MotionMixer</head><p>MotionMixer is a sequence to sequence model with mainly three modules: pose embedding, spatial-temporal mixing, and pose prediction, as illustrated in <ref type="figure">Fig. 2</ref>. The pose embedding and the spatial-temporal mixing are coupled together to encode the spatial-temporal dependencies of the human body joints. The pose prediction module, which consists of two fully-connected layers decodes the future 3D motion. Given the historical sequence, each pose is first embedded by a fully-connected layer and given to repeated N STMixer blocks, each of which includes two MLPs with skip connections. The interaction of the body joints over time is modelled by two mixing operations within a single spatial-temporal MLP. The spatial-mixing allows the interplay between the spatial location of the joints, whereas the temporal-mixing allows the long-range interactions of the observed motion. In the pose prediction module, the outputs of the mixing are finally aggregated into a global vector and fed to an MLP to forecast the future motion. Below, we describe each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Embedding</head><p>Given the observed motion sequence X 1:T h , the skeleton of each time step x t is flattened into a vector of length K = 3 ? J. This yields a two-dimensional tensor X 1:T h ? R T h ?K with one temporal dimension T h and one spatial dimension K. For simplicity, we omit the subscript T h , thus replacing X 1:T h with X. The flattened sequence X is then processed by a learnable embedding, which linearly projects each body skeleton x t ? R K through a single fully-connected layer to the hidden dimension C. We refer to the output of the learnable pose embedding</p><formula xml:id="formula_1">Y = {y 1 , . . . , y t , . . . y T h } ? R C?T h as: Y = W 0 X + b 0 , (1) where W 0 ? R C?T h ?K and b 0 ? R C?T h are weights of the fully-connected layer.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-Temporal Mixing</head><p>The proposed mixing module is motivated by the fact that the human spatial-temporal dynamics are contiguous. The spatial-temporal mixing stacks N STMixer blocks of identical size and structure. Each block, as shown in <ref type="figure">Fig. 2</ref> includes two types of MLP layers: spatial-MLP and temporal-MLP, each followed by a squeeze-and-excitation block <ref type="bibr" target="#b6">[Hu et al., 2018]</ref>. The spatial-mixing aims to learn fine-grained spatial dependencies between the body joints by acting on the columns of the pose embedding Y. Each column encodes the spatial information of one timestep. The spatial-mixing operation can be written as follows:</p><formula xml:id="formula_2">Y = Y + W 2 ?(W 1 LN(Y)),<label>(2)</label></formula><p>where W 1 ? R C?C , W 2 ? R C?C , ?(?) is a GELU activation function <ref type="bibr">[Hendrycks and Gimpel, 2016]</ref> and LN(?) denotes the layer normalization <ref type="bibr" target="#b1">[Ba et al., 2016]</ref>. Driven by the fact that the human body joints contribute unequally to the forecasted motion, mixing the columns of Y allows the communication between different spatial pose embeddings. To enable the interchanging between the spatial and temporal domains, the spatial-mixed skeleton features? are transposed and fed to the temporal-mixing MLP. The temporal-mixing MLP, by acting on rows of? aims to learn the temporal correlation of the spatial-mixed features. The temporal-mixing operation is shared across all rows, which encode the temporal information of one body joint. Formally, this can be written as follows:</p><formula xml:id="formula_3">Y =? + (W 4 ?(W 3 LN(? ))) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W 3 ? R T h ?T h , W 4 ? R T h ?T h .</formula><p>Each linear operator of the temporal-mixing assigns each time step as a linear combination of all frames where the linear weights depend on the frame's location. As such, the temporal information can be maintained in each mixing step, allowing the model to capture long-term dependencies by applying long-range interactions between frames.</p><p>In the motion prediction, each time step has a different contribution that is not known in advance. We introduce a squeeze-and-excitation (SE) block <ref type="bibr" target="#b6">[Hu et al., 2018]</ref> into STMixer to automatically regulate the input importance. The SE block, as shown in <ref type="figure">Fig. 2</ref> is added after each mixing operation helping the network to re-weight the influence of each time step. Formally, this is defined as:</p><formula xml:id="formula_5">Y = Y + ?(W s ? R (W e (W 2 ?(W 1 LN(Y)))), (4) Y =? + ?(W s ? R (W e (W 4 ?(W 3 LN(? ))) ),</formula><p>where ?(?) and ? R (?) are respectively the Softmax and ReLU activation functions. The weights W s ? R s?e and W e ? R e?s are shared across the spatial and temporal mixing units. After the mixing operation, an MLP-based pose prediction learns to generate the future human motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Prediction</head><p>Let Y be the output features of the spatial-temporal mixing. An MLP-based decoder further propagates the mixed features for future pose forecasting. Each y t of Y is projected to a vector of length T f based on a two-layer non-linear feedforward network. The computation of the predicted body posesX T h +1:T h +T f is described as:  <ref type="figure">Figure 2</ref>: Overview of the proposed MotionMixer. It mainly consists of three modules: pose embedding, spatial-temporal mixing, and pose prediction. First, the pose embedding module linearly projects each of the past 3D body poses through a single fully-connected layer to a hidden dimension C. The learned features are then fed to N STMixer layers. Equipped with a spatial-MLP, a temporal-MLP and a squeezeand-excitation (SE) block, STMixer aims to learn fine-grained spatial-temporal dependencies of the human motion. The mixing blocks are shown on the right. In each layer, we depict how our framework aggregates information via spatial-temporal mixing. An MLP-based body pose prediction is then applied to the mixed features to forecast the future human motion.</p><formula xml:id="formula_6">X T h +1:T h +T f = W p2 (? R (W p1 ( Y) + b p1 )) + b p2 , (5) where W p1 ? R C?T f , W p2 ? R 3?J?T f and b p2 ? R 3?J , b p2 ? R C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Each joint is represented by the position displacement between two adjacent frames. We train our model by predicting future displacements, which are then added to the most recent pose to get the full-body pose sequence. The loss in terms of Mean Per Joint Position Error (MPJPE) is given by:</p><formula xml:id="formula_7">L 3D = 1 J ? T f J j=1 T h +T f t=T h +1 ?x t,j ? ?x t,j 2 ,<label>(6)</label></formula><p>with ?x t,j and ?x t,j denoting the predicted and groundtruth displacement of a joint j between two adjacent frames. || ? || 2 indicates the 2 norm. For the angle-based representation, the loss between the predicted joint angles and the ground truth in the exponential map representation is given by:</p><formula xml:id="formula_8">L M AE = 1 J ? T f J j=1 T h +T f t=T h +1 x t,j ? x t,j 2 ,<label>(7)</label></formula><p>wherex t,j denotes the predicted angle of the joint j at frame t and x t,j the corresponding ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We evaluate our model on three large-scale public benchmarks. Below, we first introduce the datasets, the evaluation metrics and the baselines we compare with. We then present our results using 3D coordinates and joint angles.  <ref type="bibr" target="#b8">Mao et al., 2020;</ref><ref type="bibr" target="#b8">Sofianos et al., 2021]</ref>, we report the euclidean distance between the predicted and groundtruth joint angles. Due to the inherent ambiguity of the Eulerangle representation <ref type="bibr" target="#b7">[Mao et al., 2019;</ref><ref type="bibr" target="#b8">Mao et al., 2020]</ref>, we further report results in terms of 3D error. We make use of the Mean Per Joint Position Error (MPJPE) in millimeters. We provide the results at the particular frame, as well as  <ref type="table">Table 1</ref>: Performance comparison between different methods in terms of short-term and long-term pose prediction via mean per joint position error for each activity from the Human3.6M dataset. We provide the error results for the particular frame as well as the average over all frames. ( ?) indicates methods that compute the average error over all frames. All other approaches evaluate at the particular frame, where the error is measured between the predictions and ground truth at each frame. The best performance is highlighted in boldface.</p><p>the average over all frames following <ref type="bibr" target="#b8">[Sofianos et al., 2021;</ref><ref type="bibr" target="#b11">Zhong et al., 2022]</ref>. For the particular frame evaluation, the MPJPE is measured between the predicted pose sequence and the corresponding ground truth at each frame, whereas for the average frame evaluation, the errors in all previous frames w.r.t. a considered one are computed and then averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>MotionMixer contains three STMixer blocks with C = 60 channels. In each MLP block, a dropout layer with a rate of 0.1 is added to prevent overfitting. We use Adam [Kingma and <ref type="bibr" target="#b6">Ba, 2014]</ref> as the optimizer. During training, the learning rate is set to 10 ?2 and decayed by a factor of 0.1 every 10 epochs. We train our model for 50 epochs with a batch size of 50 for Human3.6M and 256 for AMASS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>Following previous works <ref type="bibr" target="#b8">[Martinez et al., 2017;</ref><ref type="bibr" target="#b7">Mao et al., 2019]</ref>, we quantitatively evaluate our proposed model on all kinds of actions against the state-of-the-art for 400ms short-term (i.e., 10 frames) and 1000ms long-term (i.e., 25 frames) predictions. We include nine methods with recurrent <ref type="bibr" target="#b8">[Martinez et al., 2017;</ref><ref type="bibr" target="#b8">Mao et al., 2020]</ref>, convolutional <ref type="bibr" target="#b8">Tang et al., 2018]</ref>, graph-convolutional <ref type="bibr" target="#b7">[Mao et al., 2019;</ref><ref type="bibr" target="#b8">Sofianos et al., 2021;</ref><ref type="bibr" target="#b3">Dang et al., 2021;</ref><ref type="bibr" target="#b11">Zhong et al., 2022]</ref>, and attention-based architectures  in our comparison .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Human3.6M. In Tab. 1, we provide the results for each activity of the Human3.6M dataset using 3D body poses. Mo-tionMixer outperforms all previous methods for short-term and long-term prediction in the average-frame evaluation.</p><p>In particular, we outperform the current best-performing approach <ref type="bibr" target="#b11">[Zhong et al., 2022]</ref> by at least 1.3% over all time horizons. The gain over the second-best approach <ref type="bibr" target="#b8">[Sofianos et al., 2021]</ref> ranges from 13% in the case of 400ms, up to 5% for 1000ms. In the particular frame evaluation, we reach very competitive results to the state-of-the-art approaches.</p><p>Only  outperforms MotionMixer in longterm prediction. Our method, however, yields larger improvements on activities with more complex dynamics such as WalkingDog or Purchase. Also on highly aperiodic actions like Posing, our model still produces accurate predictions.  <ref type="table">Table 2</ref>: Average short-term and long-term 3D and mean angle prediction errors over all actions of Human3.6M. We provide the error results for the particular frame as well as the average over all frames. ( ?) indicates methods that compute the average error over all frames. All other approaches evaluate at the particular frame, where the error is measured between the predictions and ground truth at each frame. The best performance is highlighted in boldface.</p><p>AMASS-BMLrub 3DPW milliseconds 80 160 320 400 560 720 880 1000 80 160 320 400 560 720 880 1000 convSeq2Seq  20.6 36.9 59.7 67.6 79.0 87.0 91.5 93.5 18.8 32.9 52.0 58.8 69.4 77.0 83.6 87.8 LTD-10-25 <ref type="bibr" target="#b7">[Mao et al., 2019]</ref> 11.0 20.7 37.8 45.  <ref type="table">Table 3</ref>: Short-term and long-term prediction of 3D body poses on AMASS-BMLrub (left) and 3DPW (right). All results are in millimeters. The best performance is highlighted in boldface. <ref type="figure">Fig. 1</ref> illustrates the future predictions of the Posing action.</p><p>The predicted skeletons accurately match the ground-truth body poses. This demonstrates the effectiveness of spatialtemporal mixing in learning fine-grained motion patterns. In Tab. 2, we additionally provide the results over all actions using respectively the 3D body poses and the joint angles.</p><p>Despite the inherent ambiguity of the angle-based representation, our method outperforms the compared methods in shortterm and yields the lowest angle error of 0.2 at 80ms. In longterm prediction, we reach comparable results with previous approaches.</p><p>AMASS &amp; 3DPW. The results of short-term and long-term prediction in 3D on AMASS and 3DPW are shown in Tab. 3. MotionMixer gets the best average error at all short-term forecast times on the AMASS dataset. For long-term prediction, our method consistently outperforms all previous approaches. The performance gain ranges from 4% for 1000ms up to 36% for 80ms, which further shows the benefits of the proposed spatial-temporal mixing. We further test the generalization of a model trained on AMASS on 3DPW. Without any finetuning, our approach outperforms previous approaches at different forecast times and can therefore better generalize to complex outdoor environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>Model Architecture. We first study the influence of individual components of the proposed method through different ablation studies. Specifically, we report the impact of the number of layers N on the motion prediction. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we show the average prediction errors at different forecast times on the Human3.6M dataset. MotionMixer yields the best average error with N = 3. Stacking more than three layers did not empirically improve the performance. To verify the impact of predicting the pose displacement instead of the 3D body pose, we train our model directly with 3D joints. With a 5mm performance gain, our model takes advantage of the pose displacement representation. This is reasonable, since such transformation may help the network focus more on motion patterns rather than the appearance of the body pose, hence, generalizing better to new environments and subjects.   Computational Complexity. We also evaluate the tradeoff between the model's computational cost and performance.</p><p>The results are shown in Tab. 5. We report the number of parameters and an estimate of the floating operations FLOPs to predict 25 frames (1000ms). We compare our model with the current best approaches. In comparison to , MotionMixer reaches nearly the same performance with only 1.4% of the parameters. We outperform <ref type="bibr" target="#b8">[Sofianos et al., 2021]</ref> and <ref type="bibr" target="#b3">[Dang et al., 2021]</ref> respectively by 4% and 3%, while using only 40% and 0.5% of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameters ? FLOPs Average 3D Motion-Attention  3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Limitations</head><p>In addition to the qualitative results in <ref type="figure">Fig.1</ref>, we examine some failure cases of MotionMixer. <ref type="figure">Fig. 5</ref> illustrates an example of the predicted skeletons for the WalkDog action. As can be seen, the last three frames do not match the groundtruth poses. This failure is also common for previous methods <ref type="bibr" target="#b8">Sofianos et al., 2021]</ref> since various actions in Human3.6M are performed in different arts in the training dataset. In addition, the human motion is highly uncertain. A sequence of past poses may imply various possible futures. Thus, predicting the inter-joint and inter-frame dependencies in long-term becomes even more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented an MLP-based pose forecasting approach that effectively exploits the spatial-temporal dependencies of the 3D human body pose. By learning to mix fea- <ref type="figure">Figure 4</ref>: Example of the failure cases of our 3D pose forecasting approach. The first line indicates the ground-truth 3D human motion. The frames on the left are the observations. The right part, shown in pink is the predicted future motion.</p><p>tures across the spatial and temporal domains, our method improved the state-of-the-art for short-term and long-term forecasting on three large-scale benchmark datasets. Enhanced by squeeze-and-excitation (SE) blocks, which aim to calibrate the influence of each time step in the pose sequence, our model has much less parameters than current best-performing approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>are the weights of the fully-connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of average 3D error in mm over all actions of the Human3.6M dataset at different prediction times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The 3D Pose in the Wild dataset<ref type="bibr" target="#b9">[von Marcard et al., 2018]</ref> consists of video sequences acquired by a moving phone camera. Overall, it contains 51,000 frames of indoor and outdoor actions captured at 30Hz. We use the official test set to test the generalization of a model trained on AMASS.</figDesc><table /><note>4.1 Datasets and Metrics Human3.6M. [Ionescu et al., 2013] consists of 7 actors performing 15 different actions. The original data is trans- formed from exponential map to 3D joint coordinates. We consider 22 joints for forecasting the 3D body poses and 16 for the angle-based prediction. Following [Sofianos et al., 2021; Mao et al., 2020], we use the subject (S11) for valida- tion, (S5) for testing, and the rest of the subjects for training. AMASS. [Mahmood et al., 2019] is a recently published dataset, which consists of 40 subjects performing the action of walking. Following [Sofianos et al., 2021; Mao et al., 2021], we select 8 datasets for training, 4 for validation, and one (BMLrub) as the test set. For each body pose, we con- sider 18 joints. 3DPW.Metrics. Following the standard evaluation protocol [Li</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b7">Mao et al., 2019]</ref> 11.2 23.4 47.9 58.9 78.3 93.3 106.0 114.0 0.32 0.55 0.91 1.04 1.26 1.44 1.59 1.68 RNN-GCN [Mao et al., 2020] 10.4 22.6 47.1 58.3 77.3 91.8 104.1 112.1 0.31 0.55 0.90 1.04 1.25 1.42 1.56 1.65 Motion-Attention [Mao et al., 2021] 11.0 23.6 49.1 60.0 75.9 90.4 102.5 110.1 0.27 0.51 0.81 0.93 1.12 1.27 1.46 1.57 STSGCN [Sofianos et al., 2021]( ?) 10.1 17.1 33.1 38.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Average 3D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average MAE</cell></row><row><cell>milliseconds</cell><cell>80</cell><cell cols="3">160 320 400</cell><cell>560</cell><cell>720</cell><cell>880</cell><cell>1000</cell><cell>80</cell><cell>160 320 400 560 720 880 1000</cell></row><row><cell>Res. sup. [Martinez et al., 2017]</cell><cell cols="10">25.0 46.2 61.4 88.3 106.3 119.4 130.0 136.6 0.36 0.67 1.02 1.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>convSeq2Seq [Li et al., 2018]</cell><cell cols="10">16.6 33.3 77.0 72.7 90.7 104.7 116.7 124.2 0.38 0.68 1.01 1.13 1.35 1.50 1.69 1.82</cell></row><row><cell>MHU [Tang et al., 2018]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.39 0.68 1.01 1.13 1.14 1.28 1.46 1.57</cell></row><row><cell cols="6">LTD-10-25 [3 50.8</cell><cell>60.1</cell><cell>68.9</cell><cell>75.6</cell><cell cols="2">0.24 0.39 0.59 0.66 0.79 0.92 1.00 1.09</cell></row><row><cell>GAGCN [Zhong et al., 2022]( ?)</cell><cell cols="5">10.1 16.9 32.5 38.5 50.0</cell><cell>-</cell><cell>-</cell><cell>72.9</cell><cell cols="2">0.24 0.38 0.54 0.65 0.74</cell><cell>-</cell><cell>-</cell><cell>1.02</cell></row><row><cell>Ours</cell><cell cols="5">11.0 23.6 47.8 59.3 77.8</cell><cell cols="5">91.4 106.0 111.0 0.29 0.54 0.81 0.94 1.20 1.30 1.40 1.57</cell></row><row><cell>Ours ( ?)</cell><cell cols="5">6.9 13.2 26.9 33.6 46.1</cell><cell>56.5</cell><cell>65.7</cell><cell>71.6</cell><cell cols="2">0.20 0.34 0.55 0.63 0.78 0.91 0.99 1.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Spatial-Temporal Mixing. To demonstrate the effect of the STMixer, we train the Spatial-Mix MLP and the Temporal-Mix MLP independently for short-term and long-term prediction and report the results in Tab. 4. By removing the temporal or the spatial mixing, the error increases at 1000ms by 4% and 6%, respectively. The best results are achieved when simultaneously mixing the body pose in time and space. This is expected since the human spatial-temporal dynamics are interleaved. We also empirically evaluate the effect of the squeeze-and-excitation (SE) blocks, which shed new light on LSTMs hidden-state weighting by giving higher importance to influential time steps. With a 2mm performance gain over all time horizons, the SE-blocks help the network re-calibrate the influence of each pose in the sequence and predict more accurate motion patterns.</figDesc><table><row><cell></cell><cell></cell><cell>Human3.6M</cell></row><row><cell>milliseconds</cell><cell>80</cell><cell>160 320 400 560 720 880 1000</cell></row><row><cell>Spatial-Mix MLP</cell><cell cols="2">12.1 17.5 32.6 37.4 51.6 61.9 70.1 77.9</cell></row><row><cell>Temporal-Mix MLP</cell><cell cols="2">10.5 15.2 30.5 36.8 49.5 59.3 68.6 74.0</cell></row><row><cell cols="3">STMixer w/o SE-Block 8.5 14.5 29.1 35.5 48.3 58.6 67.8 73.2</cell></row><row><cell>STMixer</cell><cell cols="2">6.9 12.2 26.9 33.6 46.1 56.5 65.7 71.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Influence of different parts of the STMixer on the performance. "SE-Block" denotes the squeeze-and-excitation blocks. The best results are shown in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Computational complexity analysis. ( ?) indicates results with the average error over all frames.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project "KI Delta Learning" (F?rderkennzeichen 19A19013A). The authors would like to thank the consortium for the successful cooperation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Emre Aksan, Peng Cao, Manuel Kaufmann, and Otmar Hilliges. A spatio-temporal transformer for 3d human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [aksan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08692</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vasileios Belagiannis, Christian Amann, Nassir Navab, and Slobodan Ilic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
	<note>Holistic human pose estimation with regression forests</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Msr-gcn: Multi-scale residual graph convolution networks for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bouazizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Teaching robots to predict human motion</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental learning of full body motion primitives and their sequencing through human motion observation. The International</title>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<editor>Kuli? et al., 2012] Dana Kuli?, Christian Ott, Dongheui Lee, Junichi Ishikawa, and Yoshihiko Nakamura</editor>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5226" to="5234" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning trajectory dependencies for human motion prediction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9489" to="9497" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE/CVF International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term human motion prediction by modeling motion context and enhancing motion dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<editor>Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp</editor>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Traffic control gesture recognition for autonomous vehicles</title>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<publisher>Julian Wiederer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Gaussian process dynamical models for human motion</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Yongjing Ye, and Shihong Xia. Spatial-temporal gating-adjacency gcn for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01474</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10676" to="10683" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
