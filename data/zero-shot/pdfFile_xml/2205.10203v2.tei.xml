<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Count Anything: Reference-less Class-agnostic Counting with Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hobley</surname></persName>
							<email>[mahobley@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Active Vision Laboratory</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
							<email>victor]@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Active Vision Laboratory</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Count Anything: Reference-less Class-agnostic Counting with Weak Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current class-agnostic counting methods can generalise to unseen classes but usually require reference images to define the type of object to be counted, as well as instance annotations during training. Reference-less class-agnostic counting is an emerging field that identifies counting as, at its core, a repetition-recognition task. Such methods facilitate counting on a changing set composition. We show that a general feature space with global context can enumerate instances in an image without a prior on the object type present. Specifically, we demonstrate that regression from vision transformer features without pointlevel supervision or reference images is superior to other reference-less methods and is competitive with methods that use reference images. We show this on the current standard few-shot counting dataset FSC-147. We also propose an improved dataset, FSC-133, which removes errors, ambiguities, and repeated images from FSC-147 and demonstrate similar performance on it. To the best of our knowledge, we are the first weakly-supervised reference-less class-agnostic counting method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Counting is one of the first abstract tasks people learn. Once learnt, the concept is simple. Its aim is to find the number of instances of an object class. Simple though it is, counting has diverse applications including: crowdcounting, traffic-monitoring, conservation, microscopy, and inventory management. Significantly, whereas people can generally count objects without a prior understanding of the type of the object to be counted, most current automated methods cannot.</p><p>Presented with a set of novel objects and asked to 'count', a person would know what is to be counted. This does not require a reference example or prior understanding of object type to clarify that we do not want to find the repetitions of a self-similar background. The ability to count is thus at its core comprised of two components: an understanding of what could be worth counting and an ability to identify repetitions of those countables. We demonstrate that self-supervised vision transformer features, specifically their use of self-attention with a global <ref type="figure">Fig. 1</ref>: The RCC pipeline. Our method learns to count objects of novel classes without reference images, using only the ground truth count, c, as supervision. We also visualise these features to show they suffice to localise instances of the counted object. receptive field, are crucial for both conditions to be met. We prove experimentally that, given general and globally contextual features, enumerating instances is simple and requires only minimal training and supervision.</p><p>Previous methods, whether detection-based, regressionbased, or classification-based, generally focus on enumerating the instances of a single or small set of known classes, such as people <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b5">5]</ref>, vehicles <ref type="bibr" target="#b30">[30]</ref>, animals <ref type="bibr" target="#b16">[16]</ref>, or cells <ref type="bibr" target="#b46">[46]</ref>. This requires an individually trained network for each type of object with limited to no capacity to adapt to novel classes. Individually trained networks require gathering new data and retraining whenever a new type is considered, which is difficult and expensive. Additionally, these methods generally aim to localise instances before enumerating them, requiring point-level annotations to supervise the training. These class-specific and pointlevel supervised systems are only feasible when the composition and appearance of object types will remain the same indefinitely and point-level annotations exist, which is often not the case in real-world applications. In contrast, class-agnostic counting methods <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b32">32]</ref> do not require a static class composition as they adapt an understanding of counting learnt on a set of known classes arXiv:2205.10203v2 [cs.CV] 5 Sep 2022 to objects of unseen classes. However, most class-agnostic methods still require training-time point-level annotations and reference images of the class to count. The only exception is RepRPN <ref type="bibr" target="#b31">[31]</ref>, a twostage method that does not need reference images but uses point-level annotations. Weakly-supervised counting methods <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b49">48]</ref> relax the need for point-level annotations but do not currently generalise to unseen classes.</p><p>Given the cost of gathering reference images and point annotations in dynamic, real-world applications, we present a method that accurately enumerates instances of an unseen class without using such images or annotations. Our simple method achieves this by creating a general, global-context aware feature space using a vision transformer that can then be enumerated with a linear projection, see <ref type="figure">Figure 1</ref>.</p><p>Our key contributions are as follows: 1) We propose RCC, a reference-less class-agnostic counter, and show it can be trained without instance annotations. 2) We demonstrate that RCC outperforms the only other reference-less class-agnostic counting method and is competitive with current methods that use reference images as a prior and train with full point-level supervision. 3) We present FSC-133, an improved dataset for class agnostic counting without the errors, ambiguities, and repeated images present in FSC-147.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Class-specific counting. Class-specific counting methods enumerate instances of known object types in an image. These approaches can be broadly grouped into detection-based, regression-based, and classificationbased methods. Detection-based methods use standard detection or segmentation approaches <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b29">29]</ref> to find all objects of a type <ref type="bibr" target="#b18">[18]</ref> or set of types <ref type="bibr" target="#b11">[11]</ref>, and then enumerate them. Detection is itself a broad field with significant recent developments that can be beneficial for counting applications, such as in handling overlap or occlusion. However, detection-based counting methods are still unsatisfactory in high-density applications. Classification-based methods <ref type="bibr" target="#b30">[30]</ref> generate a discrete classification of an image's global count. Their most obvious flaw is that they treat all incorrect counts, independent of proximity to the ground-truth count, equally. This makes training difficult, needs large amounts of data, and requires a low maximum count to ensure each discrete count-class is correctly trained. Regressionbased counting methods aim to regress a single global count <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b43">43]</ref> or a pixel-level density map prediction, which can be enumerated by integration <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b46">46]</ref> or instance detection <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">9]</ref>. These can use complex features <ref type="bibr">[49,</ref><ref type="bibr">50]</ref> or simpler low-level features <ref type="bibr" target="#b8">[8]</ref>, such as texture <ref type="bibr" target="#b28">[28]</ref>. A regressed density map can also be used as a rudimentary object detector, followed by cross-correlation with a reference to find instances of the desired class <ref type="bibr" target="#b39">[39]</ref>.</p><p>Weakly-supervised counting. While the majority of counting methods above use some form of instance-wise positional information, gathering this information is costly. Weakly-supervised counting methods aim to generate an accurate count with minimal <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b35">35]</ref> or no point-level annotations <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b43">43]</ref>.</p><p>Class-agnostic counting. The above methods assume prior understanding of all object classes to be identified. Generally, this requires an individually trained system for each class of objects, with limited capacity to adapt to novel classes. To avoid the cost of retraining these non-generalisable methods to new classes, Lu et al. <ref type="bibr" target="#b27">[27]</ref> proposed class-agnostic counting, a framework in which test-time classes are not present during training. However, this and most subsequent class-agnostic methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b39">39]</ref> require a prior of object class at test time, in the form of reference images. The exception is a concurrent work <ref type="bibr" target="#b31">[31]</ref>, which uses a two-step process which proposes regions likely to contain an object of interest, and then uses these regions for a reference-based method. Classagnostic counting has so far been achieved by creating a sufficiently general feature space and applying some form of matching to the whole feature map <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b37">37]</ref> or to proposed regions of interest <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Contextual features and vision transformers. Convolutional neural networks, which most of the above methods use, fail to accurately understand global context due to their localised receptive fields. Methods have tried to solve this using dilation <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b2">2]</ref> or rank-based <ref type="bibr" target="#b25">[25]</ref> systems. Attention-based architectures that model global context inherently have been used to generate more contextuallyaware features to aid counting <ref type="bibr" target="#b38">[38]</ref>, to perform feature matching <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b24">24]</ref> and to generate reference image proposals <ref type="bibr" target="#b31">[31]</ref>. Inspired by vision transformers such as ViT <ref type="bibr" target="#b13">[13]</ref> and DETR <ref type="bibr" target="#b6">[6]</ref>, there have been various counting developments <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b40">40]</ref>, including some <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b44">44]</ref> with a specific focus on weakly-supervised counting. These methods, however, focus on crowd-counting, a class specific task with limited generalisability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this paper, we approach the challenging task of counting instances of unseen classes without reference images or point-level supervision. We believe that referenceless counting can be broken into two fundamental questions: 'Where are object instances we may want to count?' and 'Is the class of this given instance repeated elsewhere?'. Unlike RepPRN <ref type="bibr" target="#b31">[31]</ref>, the only other reference-less method, we solve these questions simultaneously. The former, in a class-agnostic context, requires a general, informative feature space. We identify that self-supervised knowledge distillation is well-suited to learning such a feature space. The latter requires an understanding of the global context of an image. Vision transformers, and their use of attention, are perfectly suited to this task. Given a feature space that is general and globally aware, we find that regressing a count is simple and can be achieved with minimal training and a single linear projection without point-level supervision.</p><p>Self-supervised knowledge distillation. In order to learn a general, informative feature space without the use of labels, we use a training methodology of selfsupervised knowledge distillation based on Caron et al. <ref type="bibr" target="#b7">[7]</ref>. We learn informative representations of images by encouraging consensus between a fixed 'teacher' network, g t parameterised by ? t , which operates on large 'global' crops, x g 1 , x g 2 , of the image and a 'student' network, g s parameterised by ? s , which operates on a set, V , of smaller, 'local' crops of the image. This is achieved by minimising the cross-entropy between the probability distributions of these networks, P t and P s , as in Equation 1. These probability distributions, defined in Equation 2, act as 'unsupervised classification predictions'.</p><formula xml:id="formula_0">min ?s x?{x g 1 ,x g 2 } x ?V x =x H(P t (x), P s (x )) (1) P s (x) (i) = exp(g s (x) (i) /? s ) dp d=0 exp(g s (x) (d) /? s )<label>(2)</label></formula><p>Here, H(a, b) = ?a log b, d p is the dimensionality of the probability distribution, and ? s &gt; 0 is a temperature parameter that controls the sharpness of the distribution. A similar formula to Equation 2 holds for P t with temperature ? t . We iteratively update the weights of the teacher network using an exponential moving average of the student network's weights.  </p><formula xml:id="formula_1">head i = Attention(q i , k i , v i ; ? s )<label>(3)</label></formula><p>where q i , k i , and v i , the queries, keys, and values, are each a linear projection of a patch of x. Both q i and k i have the dimensionality d k . In the case of self-attention, q i = k i . In practice, each head's attention is calculated simultaneously for Q i , K i , and V i , sets of q i , k i , and v i respectively, as:</p><formula xml:id="formula_3">Attention(Q i , K i , V i ) = softmax( Q i K T i ? d k )V i<label>(5)</label></formula><p>Count regression. We directly regress a single, scalar count prediction for the whole input image, x, from the latent features of g s (x) without point-level annotations, which are difficult and expensive to gather. Although it may seem that a location-based loss should aid in training, we found that point-level annotations and Gaussian density maps were unnecessary and often detrimental in a class-agnostic setting. Since positional annotations are often arbitrarily placed and contain at most limited information about the size and shape of an object, identifying different parts of all correct objects would be punished by a positional loss function. This arbitrary punishment hinders the network's understanding of the counting task. We allow the network to develop its own conceptual representation of the task by regressing an estimate for the count directly. To this end, we use one of the simplest possible loss functions, the absolute percentage error, defined as: L = Absolute Percentage Error = |c ??|/c, where c is the ground truth count and the predicted count? = F ? g s (x), where F is a learnt linear projection. This was a slight improvement over Absolute Error as it limited the disproportionate effect of very high-density images on the gradients. We found that the network, without the imposition of human ideas of positional salience, learnt to implicitly localise instances of the counted class in a meaningful way, as further discussed in Section 6.4 and seen in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>Tiling augmentation.] While multi-scale systems have been used to improve detection for objects of varying size, they require a complex spatial loss <ref type="bibr" target="#b33">[33]</ref> or non-maximal suppression <ref type="bibr" target="#b34">[34]</ref>, which has a large computational cost. As our method regresses a single count rather than a set of object locations, these approaches are not feasible. To allow the network to better understand multiple scales and densities, we instead increase the diversity of image densities at training time by tiling resized versions of the input image into a (2?2) grid for 50% of instances. This increased the representation of high-density images. This augmentation improved our methods MAE and RMSE by about 10% and 20% respectively. See Appendix B for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FSC-147 and FSC-133</head><p>FSC-147 <ref type="bibr" target="#b32">[32]</ref> is a recent dataset widely used in the field of class-agnostic counting. It is meant to contain 6135 unique images from 147 distinct classes. To evaluate the generalisability of a method to unseen classes, the classes and images for training, validation, and testing are not meant to overlap. Although our method does not use them, this dataset also includes point annotations for each instance and three random instance bounding box annotations per image.</p><p>We found 159 images that appear 334 times in the dataset with a pixel-wise difference of 0 with at least one other image when compared at a 224 ? 224 resolution. If we include images that are close to identical but do not have a zero pixel-wise difference, these numbers increase to 211 images that appear 448 times. See Appendix A.1, A.5 for illustration and the complete lists of duplicates. Further, 11 images appear in the training set and one of the validation or testing sets. This significant issue undermines the purpose of these splits. In addition, in 71 instances, an image repeats with different counts, with discrepancies of up to 25%, including 5 which appear in both the training and the validation sets with 9%-21% discrepancy. See Appendix A.3 for the full list.</p><p>We, therefore, propose a revised version of this dataset, FSC-133, which corrects for these issues. The data split overlap stems from misclassification. This is likely due to ambiguous class distinctions (eg. 'kidney beans' and 'red beans') or hierarchical classes (eg. 'cranes', 'seagulls', and 'geese' could all also be classified as 'birds'). In both cases, it can be difficult or even arbitrary to identify which class is most appropriate. 'Cranes' and 'geese' or 'bread rolls', 'buns', and 'baguette rolls' are similar enough that one might well classify instances of each as of the type with which they are most familiar. See <ref type="figure">Figure  8</ref> in Appendix A.1 for visual examples of unclear class divisions. To disambiguate the dataset and help it more reliably measure a method's generalisability, FSC-133 combines categories that are similar enough to be easily confused, see Appendix A.4 for details. The count discrepancies seem to occur when there is occlusion between objects or partial objects appear at the edges of the image, see Appendix A.1 for examples. While these images are difficult for a human to accurately count, we believe that they are of value to the dataset. Instead of removing them, FSC-133 includes only the most accurate count.</p><p>FSC-133 has 5898 images in 133 classes. The training, validation, and testing sets have 3877, 954, and 1067 images from 82, 26, and 25 classes respectively. When combining classes that overlapped data splits, we merged them into the training split so that methods trained on FSC-147 be tested on FCS-133. While disadvantaged, these tests would still fairly evaluate the method's ability on completely unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we discuss the details of our implementation, training, and the metrics we use for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture</head><p>We use a ViT-small backbone inspired by DeiT-S <ref type="bibr" target="#b41">[41]</ref>. This was chosen due to its efficiency and to provide a good comparison to other counting methods. ViT-S has a similar number of parameters (21M vs 23M), throughput (1237im/sec vs 1007im/sec), and supervised ImageNet performance (79.3% vs 79.8%) as ResNet-50 <ref type="bibr" target="#b41">[41]</ref>, which is used by contemporary counting methods.</p><p>As even data efficient transformers require relatively large amounts of data to train and the datasets on which we evaluate are small, we initialised our transformer backbone, g s , using weights from Caron et al. <ref type="bibr" target="#b7">[7]</ref>. This self-supervised pre-training gives the network an understanding of meaningful image features without supervision and prior to exposure to our limited datasets, minimising the chance of overfitting. Our linear count projection F , projects from a p ? d m feature space to a scalar count prediction, where d m is the dimensionality of the transformer features and p is the number of patches of the vision transformer. We found d m = 384 and p = 28 2 sufficient to achieve competitive results while also being lightweight enough to be trainable on a single 1080Ti. It should be noted that this vision transformer configuration limits the resolution of our input image to (224?224) as opposed to the ?(384?384) used by contemporary methods with ResNet-50 backbones. The code to reproduce our results will be made publicly available at: https://github.com/ActiveVisionLab/ LearningToCountAnything.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training</head><p>We used a batch size of 2, distributed over 2 GPUs (Titan X). We trained for 80 epochs with a learning rate of 3e ? 5, which takes 2.5 hours. To increase the diversity of object densities at training time, we applied our 2 ? 2 tiling augmentation to 50% of the iterations. We applied random reflections and rotations to the images and, when tiled, to each tile independently. Colourbased augmentations (colour-jitter, Gaussian blur, and solarisation) had negligible effect on our results. We did not apply random crops as this would require point-level annotations to adjust the count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics and Trivial Baselines</head><p>In accordance with previous works on class-agnostic counting <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b31">31]</ref>, we use Mean Absolute Error</p><formula xml:id="formula_4">(MAE = ( ntest i=1 |c i ?? i |)/n test ) and Root Mean Squared Error (RMSE = ( ntest i=1 (c i ?? i ) 2 )/n test )</formula><p>to evaluate our performance, where c i and? i are the ground truth and predicted count for image x i , and n test is the number of images in the validation or test set.</p><p>We compare our method and previous methods to two trivial baselines. Both predict the same value,?, for all test images, as follows:? mean = ( C train )/n train and c median = C train [n train /2], where C train is an ordered list of all ground truth counts for the training set and n train is the number of images in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In this section, we show that RCC dramatically outperforms other reference-less class-agnostic counting methods and is competitive with reference-based methods. Since our method does not need reference images or pointlevel annotations, it can be applied to broader real-world applications where the objects to count are ever-changing. We validate this by showing our method's ability to generalise to a novel domain. We also use our learnt latent features to localise instances in an image as this may have real-world utility and to substantiate that RCC uses meaningful information to count. We then discuss the failure cases and limitations of our method to inform and motivate possible future research. We finally validate specific components of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmarking Methods</head><p>We evaluate our method against two trivial baseline methods, two few-shot detection methods, FR <ref type="bibr" target="#b19">[19]</ref> and FSOD <ref type="bibr" target="#b14">[14]</ref>, six reference-based class-agnostic counting methods, GMN <ref type="bibr" target="#b27">[27]</ref>, MAML <ref type="bibr" target="#b15">[15]</ref>, FamNet <ref type="bibr" target="#b32">[32]</ref>, CFOC-Net <ref type="bibr" target="#b47">[47]</ref>, BMNet <ref type="bibr" target="#b37">[37]</ref>, LaoNet <ref type="bibr" target="#b24">[24]</ref>, and the only other class-agnostic counting method which doesnt require a reference image, RepRPN-Counter <ref type="bibr" target="#b31">[31]</ref>, see <ref type="table" target="#tab_1">Table 1</ref>. We also include reference-less modifications to referencebased methods implemented by Ranjan and Hoai <ref type="bibr" target="#b31">[31]</ref>, denoted by a *. Since Ranjan and Hoai <ref type="bibr" target="#b31">[31]</ref> have not yet released the implementations of their method and of the modified reference-based methods they evaluate, the experiments we could run comparing to them were limited. To ensure fair comparison, we also train recent, high performing methods with released implementations using the same vision transformer backbone, ViT-S, as our method, denoted by ? in the results tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">FSC-147 and FSC-133.</head><p>We achieve significantly better result than RepRPN and all reference-less versions of reference-based methods on FSC-147 without the need for point-level annotations. We are also competitive with previous few-shot or referencebased methods on FSC-147 and FSC-133 without the need for reference images, point-level annotations, or test-time adaptation, see <ref type="table" target="#tab_1">Tables 1 and 2</ref>. This result demonstrates that the combination of an architecture wellsuited for counting and a simple, count-centred objective function can learn a meaningful conceptual representation of counting without any location-based information and so does not require reference images. As will be discussed in Section 6.5, we believe that the discrepancy in FSC-147 test-set RMSE is due to poor performance on the high-density images in the test set, as these outliers have a greater effect on RMSE than MAE.</p><p>We found that methods that were modified to use ViT-S in place of their ResNet-50 backbones performed worse; this is likely due to the significant decrease in input image resolution. Since FamNet was previously using features from two sequential layers of a ResNet backbone, this modification also halved the dimensionality of the features used to regress the count, decreasing their scale invariant performance.</p><p>In general, the methods perform better on FSC-133 than FSC-147 with the exception of the validation MAE. The greater validation MAE is likely due to the removal of duplicate images and similar classes. The other metric improvements are likely due to a few high-density images being moved to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Cross-Dataset Generalisability</head><p>To confirm our cross-dataset generalisability, we test our model on CARPK <ref type="bibr" target="#b18">[18]</ref>, a car counting dataset comprised of birds-eye views of parking lots which significantly differ from the appearance of any objects in FSC-147 or FSC-133. To ensure we are fairly testing genrealisability, we exclude the "car" class from our FSC-133 pretraining. We significantly outperform FamNet with and without fine-tuning, and we are competitive with BMNet without fine-tuning. We believe the finetuning result discrepancy with BMNet is caused by out-ofdistribution high-density images as 70% of the test images have more instances than the maximum count seen during training. Both other methods use reference images which aid in the cross-dataset generalisation and with issues of out of distribution high-density test images. Further, while our method improves after fine-tuning, showing the benefit of task-specific information, the improvements    "Pretrained" models were trained on FSC-147 or, for our method, FSC-133. "Fine-tuned" models were further trained on the CARPK dataset. We highlight the top results with and without reference images and with and without fine-tuning.</p><p>are smaller than with FamNet and BMNet. This indicates our method's generalisability is relatively optimised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Feature Visualisation</head><p>In order to validate that our network is learning meaningful information, we visualised the most significant value from a singular value decomposition of the counting features weighted by the linear projection weights, see <ref type="figure" target="#fig_3">Figure 2</ref>. As our patch-wise counting features are relatively low resolution and there may be utility in more accurately localising instances in an image, we trained a localisation head. This head, comprised of 3 Conv-ReLU-Upsample blocks, increases the patch-wise resolution of the trained counting features (28?28) to a pixel-wise density map prediction (224?224), see <ref type="figure" target="#fig_3">Figure  2</ref>. This is trained using the pixel-wise mean squared error of the predicted density map and a ground truth Gaussian density map as is standard in the field <ref type="bibr" target="#b32">[32]</ref>. Since this training is not weakly-supervised, we freeze the feature backbone. This training takes 10 epochs and has significantly better results than the same training on a self-supervised backbone, validating our network's ability to count rather than just detect objects. We also show that the backbone and localisation head trained on FSC-133 are generalisable to other domains by testing on the CARPK and ShanghaiTech [50] datasets, see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Failure Cases and Limitations</head><p>A clear failure case of RCC is its difficulty with highdensity images. As shown in <ref type="table" target="#tab_5">Table 4</ref>, the metrics on FSC-133 improve dramatically when the few images with over 1000 objects are removed. These images, presented in  of the test set. This failure case is likely because our patch-based approach limits the functional resolution to a (28 ? 28) feature map. With many of these high-density images, the same object is found in all patches and across all patch boundaries equally, making individual instances indistinguishable. Ideally, we would use a smaller patch size, but due to computational constraints, we could not test this. As a proxy for this test, we split the high-density images into multiple sub-images that were processed independently, and combined their counts. This improved the MAE and RMSE by 45.6% and 64.9% respectively. However, as this does not generalise in a principled manner to all cases, especially very low density cases, we did not include this as a contribution.</p><p>This failure case could also be attributed to the underrepresentation of very high-density images in the training data; only 9 (0.23%) of 3877 training counts are over 1000. This is supported by the fact that these effects appear more dramatically in FSC-147 where highdensity images have even lower representation. This is also supported by the improvements found from applying the tiled image augmentation, which artificially inflates the count of some training iterations.</p><p>The main and most obvious limitation of RCC, and indeed any reference-less method, is that it is singlecount. It finds the class most likely to be of interest and enumerates it, see <ref type="figure" target="#fig_6">Figure 4</ref>. Given the distribution of objects in FSC-147, this does not pose a problem during our evaluation. Furthermore, the core achievement of this work is negating the requirement for test-time reference images and point-level annotations. There are also a wide range of applications to singular classes of   novel objects, e.g. medical imaging <ref type="bibr" target="#b46">[46]</ref>. Nevertheless, adapting this method to generate multiple counts or a hierarchical class-count structure is scope for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Ablation Studies</head><p>Attention-less features. To demonstrate that the global context provided by the attention mechanism present in vision transformers is a critical component to referenceless class-agnostic counting, we evaluate RCC with ResNet-50 <ref type="bibr" target="#b17">[17]</ref> and ConvNeXt <ref type="bibr" target="#b26">[26]</ref> backbones in place of our transformer. We initialised both architectures with standard weights pre-trained on ImageNet <ref type="bibr" target="#b10">[10]</ref>. While using weights generated by a supervised task that overlaps classes with our test and validation sets means that the results are not directly comparable to our self-supervised method, we believe that this comparison provides a bestcase for these architectures. Resnet-50 has a comparable accuracy to our vision transformer and ConvNeXt has a higher classification accuracy than our architecture when trained comparably <ref type="bibr" target="#b26">[26]</ref>. To enable comparison across backbones, we use the same input feature size (224?224) and take latent features at the same (28?28) resolution as the patched features used in our backbone. As seen in <ref type="table" target="#tab_7">Table 5</ref>, even with the advantageous supervised pretraining, the attention-less features perform significantly worse than the self-supervised vision transformer.</p><p>Count regression complexity. We assert that given sufficiently general and globally aware features, regressing an accurate count should be straightforward. To validate this, we compare our linear projection against two count regression heads. We found that more complex counting heads achieved equivalent or worse results. In <ref type="table" target="#tab_7">Table 5</ref>, we present results for a simple architecture, Conv(3?3)-ReLU-Linear-Relu-Linear, and a more complex architecture comprised of four (3?3) convolutional layers followed by three linear layers with ReLU activations. The convolutional heads perform worse than our projection on all three backbones. It appears that the extra computational capacity of the complex architecture is not only unnecessary for the task of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present RCC, one of the first referenceless class-agnostic counting methods, and show that it can be trained without point-level annotations. This is based on the confirmed intuition that well-trained vision transformer features are both general enough and contextually aware enough to implicitly understand the underlying basis of counting, namely object detection and repetition identification. To evaluate and compare RCC against other methods, we use FSC-147, a standard counting dataset, and our proposed improved dataset FSC-133. We demonstrate on both datasets that RCC is superior to the only other reference-less method and is competitive with current reference-based class-agnostic counting approaches that use full point-level supervision. We also show its cross-domain gerenalisability on CARPK and that it can localise appropriate object instances without any location-based intervention during training. We believe that due to our lack of reliance on object class priors, reference images, and positional annotations, our method has significantly greater utility than other counting methods, especially when the composition or appearance of objects is uncertain.</p><p>Several extensions are possible for future research: first, a multi-class output would be of clear utility as many real world applications will have a diverse range of objects; second, a hierarchical system that is able to organise a multi-count output into likely groupings of objects could be useful to better understand the distribution of present types; third, regressing an image of the type of object being counted could also aid in understanding the generated count.    The breakdown of classes in FSC-147. Note there are identical categories eg. 'red beans' and 'kidney beans'; 'baguette rolls', 'buns' and 'bread rolls' are semantically the same, there are also hierarchical categories e.g. 'cranes', 'geese', 'pigeons', 'seagulls', 'crows', 'swans' and 'flamingos' could also be classified as also 'birds'. We show the classes that are combined in FSC-133 in bold and show the classes that are moved to the training set in FSC-133 in blue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Vision transformer backbone. The crucial global context afforded by transformers stems from their use of an attention mechanism. Attention generates a new feature from linear projections of all other features based on their similarity. Inspired by Vaswani et al. [42], our vision transformers, g t and g s , use multiple attention heads to generate informative, diverse features, as in Equation 3 and Equation 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>g s (x; ? s ) = Concat(head 1 , ..., head h )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Localisation of latent counting features. Columns 1-4: patch-wise principal components of the counting backbone features, Columns 5-8, 9, 10: pixel-wise localisation predictions of unseen dataset classes from FSC-133, CARPK and ShanghaiTech respectively. The latent counting feature density maps are generated by a localisation head trained on FSC-133. The count and prediction of each image are in the top left and the top right respectively. Best viewed in colour and by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 ,</head><label>3</label><figDesc>constitute 0.31% of the validation set and 0.09%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>High-density images from FSC-133. The three images from the validation set (left) and the single test image (right) with associated counts of over 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Localisation of counting features with multiple classes present. The network either ignores all but one class (left and center) or it groups very similar classes and counts the superset (right). The left and center images have high levels of occlusion leading to low predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Val Set</cell><cell cols="2">Test Set</cell></row><row><cell>Method</cell><cell cols="4">MAE RMSE MAE RMSE</cell></row><row><cell>Mean</cell><cell cols="4">54.39 112.68 44.76 104.28</cell></row><row><cell>Median</cell><cell cols="4">51.29 119.29 43.15 109.83</cell></row><row><cell cols="2">Reference-based (3-shot)</cell><cell></cell><cell></cell></row><row><cell>FamNet [32]</cell><cell>25.49</cell><cell cols="2">68.21 21.05</cell><cell>43.48</cell></row><row><cell>FamNet+ [32]</cell><cell>24.75</cell><cell cols="2">67.04 20.20</cell><cell>41.76</cell></row><row><cell>FamNet ? [32]</cell><cell>36.89</cell><cell cols="2">92.76 30.09</cell><cell>90.95</cell></row><row><cell cols="2">FamNet+ ? [32] 36.36</cell><cell cols="2">89.22 30.79</cell><cell>90.71</cell></row><row><cell>BMNet [37]</cell><cell>20.16</cell><cell cols="2">54.83 14.61</cell><cell>41.11</cell></row><row><cell>BMNet+ [37]</cell><cell>16.54</cell><cell cols="2">50.65 13.85</cell><cell>40.61</cell></row><row><cell>BMNet ? [37]</cell><cell>22.06</cell><cell cols="2">61.01 16.38</cell><cell>54.13</cell></row><row><cell>BMNet+ ? [37]</cell><cell>18.78</cell><cell cols="2">59.76 13.87</cell><cell>47.27</cell></row><row><cell>Reference-less</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RCC (ours)</cell><cell>19.84</cell><cell cols="2">55.81 14.23</cell><cell>43.83</cell></row></table><note>Comparison to state-of-the-art methods on FSC-147. We outperform other reference-less methods and achieve competitive results with methods which use reference images and test-time adaptation. We highlight the best results for reference-based and reference-less methods. ? denotes methods trained using the same back- bone as ours. * indicates a reference-less modification of a reference-based method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">Fine-Tuned MAE RMSE</cell></row><row><cell>Mean</cell><cell>N/A</cell><cell>65.63</cell><cell>72.26</cell></row><row><cell>Median</cell><cell>N/A</cell><cell>67.88</cell><cell>74.58</cell></row><row><cell>Reference-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FamNet [32]</cell><cell>?</cell><cell>28.84</cell><cell>44.47</cell></row><row><cell>BMNet [37]</cell><cell>?</cell><cell>14.61</cell><cell>24.60</cell></row><row><cell>BMNet+ [37]</cell><cell>?</cell><cell>10.44</cell><cell>13.77</cell></row><row><cell>FamNet [32]</cell><cell></cell><cell>18.19</cell><cell>33.66</cell></row><row><cell>BMNet [37]</cell><cell></cell><cell>8.05</cell><cell>9.70</cell></row><row><cell>BMNet+ [37]</cell><cell></cell><cell>5.76</cell><cell>7.83</cell></row><row><cell>Reference-less</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RCC (ours)</cell><cell>?</cell><cell>12.31</cell><cell>15.40</cell></row><row><cell>RCC (ours)</cell><cell></cell><cell>9.21</cell><cell>11.33</cell></row></table><note>Comparison to available state-of-the-art methods on FSC-133. We are competitive with reference- based methods without reference images or test-time adaptation. ? denotes methods trained using the same backbone as ours.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Generalisation performance on CARPK.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>The effect of removing high-density images from FSC-133. Excluding the small number of very dense images significantly improves our results, showing that these are a weakness of our method. # and % denote the total number and percentage of images excluded respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Performance</figDesc><table><row><cell>of different feature backbones,</cell></row><row><cell>and two more complex count regression head archi-</cell></row><row><cell>tectures on FSC-133.</cell></row><row><cell>counting but is in fact detrimental because it overfits</cell></row><row><cell>to the training classes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>on Computer Vision, pages 1-17. Springer, 2020. [49] C. Zhang, H. Li, X. Wang, and X. Yang. Crossscene crowd counting via deep convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 833-841, 2015. [50] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma.</figDesc><table><row><cell>Appendix A A.2 Similar Classes A.3 Duplicate Discrepancies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSC-147 and FSC-133</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.1 Similar and Different Images Image A ID Class</cell><cell>Count Set</cell><cell cols="2">Image B Bread Rolls (Train) Id Class</cell><cell cols="2">Count Diff Count Set Abs Rel</cell><cell>Kept</cell></row><row><cell>4399 cranes</cell><cell cols="3">26 train 4549 flamingos</cell><cell>33 val</cell><cell>7 0.21</cell><cell>A</cell></row><row><cell>4399 cranes</cell><cell cols="3">26 train 4719 seagulls</cell><cell>33 val</cell><cell>7 0.21</cell><cell>A</cell></row><row><cell>2891 caps 4386 cranes</cell><cell cols="3">144 train 1896 bottle caps Train 20 train 7415 birds</cell><cell>123 val 23 val</cell><cell>21 0.15 3 0.13</cell><cell>B A</cell></row><row><cell>6567 pigeons</cell><cell>21 train</cell><cell>929</cell><cell>birds</cell><cell>23 val</cell><cell>2 0.09</cell><cell>A</cell></row><row><cell>4664 geese</cell><cell cols="3">11 train 6873 birds</cell><cell>11 val</cell><cell>0 0.00</cell><cell>A</cell></row><row><cell cols="4">4613 geese 4350 cranes Single-image crowd counting via multi-column 26 train 6714 birds Buns (Train) 8 train 4704 seagulls 4350 cranes 8 train 4707 seagulls 3506 m&amp;m pieces 111 train 3698 candy pieces convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern 3791 kidney beans 37 train 3494 red beans</cell><cell>26 val 8 val 8 val 111 test 37 test</cell><cell cols="2">0 0.00 0 0.00 4683 B 0 0.00 4683 0 0.00 A 0 0.00 A</cell></row><row><cell>recognition, pages 589-597, 2016.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Baguette Rolls (Train)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Red Beans (Test)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Fig. 6: Images that have a low pixel-wise difference that we deemed to be different.</cell></row><row><cell></cell><cell cols="3">Kidney Beans (Train)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Val</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Birds (Val)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Test Cranes (Train)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Flamingos (Val)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Fig. 5: Examples of some of the 448 identical or close to identical images that appear in FSC-147 with different</cell></row><row><cell cols="7">image IDs and their associated 'ground truth' counts . Duplicates can occur with different count labels and/or in different splits. Geese (Train)</cell></row><row><cell></cell><cell></cell><cell cols="2">Seagulls (Val)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Fig. 7: Examples of images at the upper bound of pixel-wise difference we considered that we deemed the same</cell></row><row><cell cols="7">image. The pixel-wise differences of the images in each column when resized to 224?224 are 5207, 5403, 7551,</cell></row><row><cell>8224 and 9396 respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 8: Examples of clear overlap or similarity between classes. Each row has a different class label in FSC-147. In FSC-133 these are simplified to Bread Rolls (Train), Kidney Beans (Train) and Birds (Train).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>The 17 cases where the same image appears in more than on of the: train set, validation or test set. The right column denotes which of these images, or if a third images appears in FSC-133.</figDesc><table><row><cell></cell><cell>Image A</cell><cell></cell><cell></cell><cell>Image B</cell><cell></cell><cell>Count Diff</cell><cell></cell></row><row><cell>ID</cell><cell>Class</cell><cell>Count Set</cell><cell>Id</cell><cell>Class</cell><cell>Count Set</cell><cell>Abs Rel</cell><cell>Kept</cell></row><row><cell cols="2">5223 donuts tray</cell><cell>9 val</cell><cell cols="2">5664 donuts tray</cell><cell>12 val</cell><cell>3 0.25</cell><cell>A</cell></row><row><cell>613</cell><cell>seagulls</cell><cell>7 val</cell><cell>623</cell><cell>seagulls</cell><cell>9 val</cell><cell>2 0.22</cell><cell>B</cell></row><row><cell cols="2">4399 cranes</cell><cell cols="3">26 train 4549 flamingos</cell><cell>33 val</cell><cell cols="2">7 0.21 4683</cell></row><row><cell cols="2">4399 cranes</cell><cell cols="3">26 train 4620 geese</cell><cell>33 train</cell><cell cols="2">7 0.21 4683</cell></row><row><cell cols="2">4399 cranes</cell><cell cols="3">26 train 4719 seagulls</cell><cell>33 val</cell><cell cols="2">7 0.21 4683</cell></row><row><cell cols="2">3812 cupcakes</cell><cell cols="3">10 train 5238 cupcakes</cell><cell>8 train</cell><cell>2 0.20</cell><cell>B</cell></row><row><cell cols="2">5142 cashew nuts</cell><cell>193 test</cell><cell cols="2">5801 cashew nuts</cell><cell>157 test</cell><cell>36 0.19</cell><cell>B</cell></row><row><cell cols="2">4634 geese</cell><cell cols="3">18 train 6143 geese</cell><cell>15 train</cell><cell>3 0.17</cell><cell>A</cell></row><row><cell cols="2">4634 geese</cell><cell cols="3">18 train 6669 geese</cell><cell>15 train</cell><cell>3 0.17</cell><cell>A</cell></row><row><cell cols="2">6850 cereals</cell><cell cols="3">25 train 7337 cereals</cell><cell>21 train</cell><cell>4 0.16</cell><cell>B</cell></row><row><cell cols="2">6839 watches</cell><cell>58 test</cell><cell>989</cell><cell>watches</cell><cell>69 test</cell><cell>11 0.16</cell><cell>A</cell></row><row><cell cols="2">1896 bottle caps</cell><cell>123 val</cell><cell cols="2">2891 caps</cell><cell>144 train</cell><cell>21 0.15</cell><cell>A</cell></row><row><cell cols="2">2717 oranges</cell><cell cols="3">94 train 6573 oranges</cell><cell>110 train</cell><cell>16 0.15</cell><cell>A</cell></row><row><cell cols="2">3648 cashew nuts</cell><cell>35 test</cell><cell cols="2">5141 cashew nuts</cell><cell>30 test</cell><cell>5 0.14</cell><cell>B</cell></row><row><cell cols="2">3014 mini blinds</cell><cell cols="3">26 train 7307 mini blinds</cell><cell>30 train</cell><cell>4 0.13</cell><cell>A</cell></row><row><cell cols="2">4386 cranes</cell><cell cols="3">20 train 7415 birds</cell><cell>23 val</cell><cell>3 0.13</cell><cell>A</cell></row><row><cell cols="2">2970 fishes</cell><cell cols="3">16 train 6953 fishes</cell><cell>18 train</cell><cell>2 0.11</cell><cell>B</cell></row><row><cell cols="2">5929 eggs</cell><cell>76 test</cell><cell cols="2">6852 eggs</cell><cell>68 test</cell><cell>8 0.11</cell><cell>B</cell></row><row><cell cols="2">4295 green peas</cell><cell>110 test</cell><cell cols="2">5504 green peas</cell><cell>122 test</cell><cell>12 0.10</cell><cell>A</cell></row><row><cell cols="2">6567 pigeons</cell><cell>21 train</cell><cell>929</cell><cell>birds</cell><cell>23 val</cell><cell>2 0.09</cell><cell>A</cell></row><row><cell cols="2">2743 oranges</cell><cell cols="3">57 train 7363 oranges</cell><cell>52 train</cell><cell>5 0.09</cell><cell>B</cell></row><row><cell cols="2">6138 cars</cell><cell cols="3">122 train 6864 cars</cell><cell>111 train</cell><cell>11 0.09</cell><cell>A</cell></row><row><cell cols="2">4220 biscuits</cell><cell cols="3">11 train 5422 biscuits</cell><cell>10 train</cell><cell>1 0.09</cell><cell>B</cell></row><row><cell cols="2">3282 finger foods</cell><cell>31 test</cell><cell cols="2">3285 finger foods</cell><cell>34 test</cell><cell>3 0.09</cell><cell>A</cell></row><row><cell cols="2">3462 beads</cell><cell cols="3">92 train 5638 beads</cell><cell>85 train</cell><cell>7 0.08</cell><cell>A</cell></row><row><cell cols="2">4040 beads</cell><cell cols="3">117 train 5760 beads</cell><cell>108 train</cell><cell>9 0.08</cell><cell>A</cell></row><row><cell cols="2">3492 polka dots</cell><cell>25 val</cell><cell cols="2">5738 polka dots</cell><cell>27 val</cell><cell>2 0.07</cell><cell>A</cell></row><row><cell cols="2">3486 polka dots</cell><cell>81 val</cell><cell cols="2">5740 polka dots</cell><cell>87 val</cell><cell>6 0.07</cell><cell>A</cell></row><row><cell cols="2">6917 cereals</cell><cell cols="3">29 train 7353 cereals</cell><cell>27 train</cell><cell>2 0.07</cell><cell>B</cell></row><row><cell cols="2">5209 donuts tray</cell><cell>17 val</cell><cell cols="2">5656 donuts tray</cell><cell>16 val</cell><cell>1 0.06</cell><cell>B</cell></row><row><cell cols="2">4013 beads</cell><cell cols="3">115 train 5754 beads</cell><cell>108 train</cell><cell>7 0.06</cell><cell>B</cell></row><row><cell>524</cell><cell>geese</cell><cell>30 train</cell><cell>635</cell><cell>cranes</cell><cell>32 train</cell><cell>2 0.06</cell><cell>B</cell></row><row><cell cols="2">4300 green peas</cell><cell>83 test</cell><cell cols="2">5506 green peas</cell><cell>88 test</cell><cell>5 0.06</cell><cell>A</cell></row><row><cell cols="2">3287 macarons</cell><cell>16 train</cell><cell>428</cell><cell>macarons</cell><cell>17 train</cell><cell>1 0.06</cell><cell>A</cell></row><row><cell cols="2">4634 geese</cell><cell cols="3">18 train 4675 geese</cell><cell>17 train</cell><cell>1 0.06</cell><cell>A</cell></row><row><cell cols="2">6602 pencils</cell><cell cols="3">38 train 7400 pencils</cell><cell>36 train</cell><cell>2 0.05</cell><cell>B</cell></row><row><cell cols="2">3475 beads</cell><cell cols="3">108 train 5637 beads</cell><cell>103 train</cell><cell>5 0.05</cell><cell>A</cell></row><row><cell cols="2">2357 bricks</cell><cell cols="3">276 train 2401 bricks</cell><cell>263 train</cell><cell>13 0.05</cell><cell>B</cell></row><row><cell cols="2">4044 beads</cell><cell cols="3">115 train 5335 beads</cell><cell>109 train</cell><cell>6 0.05</cell><cell>A</cell></row><row><cell cols="2">4019 beads</cell><cell cols="3">105 train 5771 beads</cell><cell>109 train</cell><cell>4 0.04</cell><cell>A</cell></row><row><cell cols="2">4049 beads</cell><cell cols="3">26 train 5755 beads</cell><cell>27 train</cell><cell>1 0.04</cell><cell>B</cell></row><row><cell cols="2">3956 candles</cell><cell cols="3">46 train 5309 candles</cell><cell>44 train</cell><cell>2 0.04</cell><cell>B</cell></row><row><cell cols="2">3969 candles</cell><cell cols="3">23 train 5317 candles</cell><cell>24 train</cell><cell>1 0.04</cell><cell>A</cell></row><row><cell cols="2">3950 candles</cell><cell cols="3">24 train 5312 candles</cell><cell>25 train</cell><cell>1 0.04</cell><cell>A</cell></row><row><cell cols="2">3020 mini blinds</cell><cell cols="3">34 train 7630 mini blinds</cell><cell>35 train</cell><cell>1 0.03</cell><cell>A</cell></row><row><cell cols="2">3668 toilet paper rolls</cell><cell>31 val</cell><cell cols="2">5154 toilet paper rolls</cell><cell>32 val</cell><cell>1 0.03</cell><cell>A</cell></row><row><cell cols="2">3679 buns</cell><cell cols="3">35 train 5026 bread rolls</cell><cell>36 train</cell><cell>1 0.03</cell><cell>A</cell></row><row><cell cols="2">4014 beads</cell><cell cols="3">108 train 5749 beads</cell><cell>105 train</cell><cell>3 0.03</cell><cell>A</cell></row><row><cell cols="2">3645 cashew nuts</cell><cell>60 test</cell><cell cols="2">5797 cashew nuts</cell><cell>58 test</cell><cell>2 0.03</cell><cell>A</cell></row><row><cell cols="2">5148 cashew nuts</cell><cell>36 test</cell><cell cols="2">5803 cashew nuts</cell><cell>35 test</cell><cell>1 0.03</cell><cell>A</cell></row><row><cell cols="2">7640 apples</cell><cell>32 test</cell><cell cols="2">7665 apples</cell><cell>33 test</cell><cell>1 0.03</cell><cell>B</cell></row><row><cell cols="2">3819 cupcakes</cell><cell cols="3">36 train 5241 cupcakes</cell><cell>35 train</cell><cell>1 0.03</cell><cell>B</cell></row><row><cell cols="2">3754 pearls</cell><cell cols="3">131 train 5185 pearls</cell><cell>127 train</cell><cell>4 0.03</cell><cell>B</cell></row><row><cell cols="2">3666 toilet paper rolls</cell><cell>32 val</cell><cell cols="2">5157 toilet paper rolls</cell><cell>31 val</cell><cell>1 0.03</cell><cell>B</cell></row><row><cell cols="2">3639 jade stones</cell><cell cols="3">31 train 5134 jade stones</cell><cell>32 train</cell><cell>1 0.03</cell><cell>A</cell></row><row><cell cols="2">6200 coins</cell><cell cols="3">52 train 6228 coins</cell><cell>53 train</cell><cell>1 0.02</cell><cell>A</cell></row><row><cell>267</cell><cell>beads</cell><cell cols="3">59 train 3242 beads</cell><cell>60 train</cell><cell>1 0.02</cell><cell>A</cell></row><row><cell cols="2">6798 birds</cell><cell>87 val</cell><cell>976</cell><cell>birds</cell><cell>85 val</cell><cell>2 0.02</cell><cell>B</cell></row><row><cell cols="2">3795 kidney beans</cell><cell cols="3">53 train 5547 kidney beans</cell><cell>52 train</cell><cell>1 0.02</cell><cell>B</cell></row><row><cell cols="2">3955 candles</cell><cell cols="3">60 train 5306 candles</cell><cell>61 train</cell><cell>1 0.02</cell><cell>A</cell></row><row><cell cols="2">2671 bowls</cell><cell cols="3">60 train 6724 bowls</cell><cell>59 train</cell><cell>1 0.02</cell><cell>B</cell></row><row><cell cols="2">6738 mini blinds</cell><cell cols="3">53 train 7130 mini blinds</cell><cell>52 train</cell><cell>1 0.02</cell><cell>B</cell></row><row><cell cols="2">5053 beads</cell><cell cols="3">139 train 5644 beads</cell><cell>136 train</cell><cell>3 0.02</cell><cell>B</cell></row><row><cell cols="2">3426 polka dots</cell><cell>219 val</cell><cell cols="2">5046 polka dots</cell><cell>215 val</cell><cell>4 0.02</cell><cell>B</cell></row><row><cell cols="2">3122 coffee beans</cell><cell cols="3">90 train 3134 coffee beans</cell><cell>89 train</cell><cell>1 0.01</cell><cell>B</cell></row><row><cell cols="2">6961 cartridges</cell><cell cols="3">178 train 7680 cartridges</cell><cell>179 train</cell><cell>1 0.01</cell><cell>A</cell></row><row><cell cols="2">4016 beads</cell><cell cols="3">109 train 5333 beads</cell><cell>108 train</cell><cell>1 0.01</cell><cell>B</cell></row><row><cell cols="2">3469 beads</cell><cell cols="3">74 train 5050 beads</cell><cell>75 train</cell><cell>1 0.01</cell><cell>A</cell></row><row><cell cols="2">4021 beads</cell><cell cols="3">106 train 5759 beads</cell><cell>107 train</cell><cell>1 0.01</cell><cell>B</cell></row><row><cell cols="2">5149 cashew nuts</cell><cell>111 test</cell><cell cols="2">5807 cashew nuts</cell><cell>110 test</cell><cell>1 0.01</cell><cell>B</cell></row><row><cell cols="2">4044 beads</cell><cell cols="3">115 train 5767 beads</cell><cell>116 train</cell><cell>1 0.01</cell><cell>A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>The 71 cases where the same image appears more than once in FSC-147 with different associated counts. The right column denotes which of these images, or if a third images appears in FSC-133.A.4 FSC-147 and FSC-133 Class Split</figDesc><table><row><cell>Train</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8 :</head><label>8</label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Tiling Augmentation Ablation.</head><p>We validate the use of our image tiling augmentation by testing various frequencies and tiling configurations, as shown in <ref type="table">Table 9</ref>. We found that introducing the denser (4?4) tiling had less effect. At this density of tiling, each object instance is very small within the total image, so it is no longer interpretable, providing little meaningful training.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zisserman. Interactive object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="504" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive dilated network with self-correction supervision for counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4594" to="4603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian process density counting from weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Borstel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="365" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards partial supervision for generic object counting in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention in crowd counting using the transformer and density map to improve counting result</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 8th NAFOSTED Conference on Information and Computer Science (NICS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained multi-class object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="509" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Drone-based object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-R</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4145" to="4153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards using count-level weak supervision for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">107616</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transcrowd: Weakly-supervised crowd counting with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09116</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An end-to-end transformer model for crowd localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13065</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Object counting: You only need to look at one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05993</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging unlabeled data for crowd counting by learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7661" to="7669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimation of crowd density using image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Marana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing for Security Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">One-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11507</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14212</idno>
		<title level="m">Exemplar free class agnostic counting</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to count everything</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3394" to="3403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Almost unsupervised learning for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maurya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8868" to="8875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Crowdformer: Weakly-supervised crowd counting with improved generalizability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Savner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kanhangad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03768</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Represent, compare, and learn: A similarity-aware framework for class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9529" to="9538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ha-ccn: Hierarchical attention-based crowd counting network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A few-shot sequential approach for object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sokhandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kamousi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Posada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01899</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10926</idno>
		<title level="m">Boosting crowd counting with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Joint cnn and transformer network via weakly supervised learning for efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06388</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Class-agnostic few-shot object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="870" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<title level="m">Weakly-supervised crowd counting learns from sorting rather than locations</title>
		<imprint/>
	</monogr>
	<note>European Conference A.5 Sets of repeated Images Sets of Identical Images. 1896, 2891], [19, 3092], [20, 3093], [22, 3094], [23, 3095], [26, 3097], [267, 3242], [269, 3255], [27, 3098], [9, 3084], [3282, 3285], [427, 3286], [428, 3287], [429, 3288], [3353, 5033, 5717], [3361, 5036], [3362, 5027], [3370, 5722], [3372, 5734], [3375, 5716], [3392, 5732], [3397, 5035. 3400, 3687. 3426, 5046. 3462, 5638. 3465, 5643. 3469, 5050. 3472, 5761. 3475, 5637. 3486, 5740. 3492, 5738. 3494, 3791. 3521, 5079. 3522, 5097. 3531, 5089. 3534, 5093. 3535, 5073. 3538, 5064. 3540, 5084. 3542, 5071. 3543, 5098. 3544, 5085. 3546, 5068. 3549, 5076. 3553, 5095. 3556, 3564, 5069. 3558, 5078. 3562, 5114. 3565, 5115. 3570, 5086, 5107. 3578, 5082, 5117. 3585, 5108. 3587, 5110. 3592, 5111. 3607, 5125], [3625, 5136], [3626, 5126. 3627, 5518], [3628, 5129], [3639, 5134, 5528], [3644, 5794], [3645, 5797], [3646, 5145. 3647, 5144], [3648, 5141], [3652, 5147, 5806. 3656, 5798], [3666, 5157], [3668, 5154], [3669, 5152], [3671, 5153], [3672, 5156], [3675, 5155], [3679, 5026], [3754, 5185], [3759, 5229], [3760, 5221], [3762, 5203], [3764, 5670], [3766, 5202. 3767, 5228, 5673. 3768, 5211, 5655. 3769, 5220], [3772, 5213, 5659. 3773, 5201, 5671], [3775, 5215], [3778, 5212, 5661], [3781, 5217], [3782, 5210], [3790, 5230, 5548], [3793, 5233], [3795, 5547], [3800, 5235, 5553], [3812, 5238], [3815, 5247], [3816, 4196, 5257], [3818, 5243], [3819, 5241], [3824, 5253], [3833, 5254], [3950, 5312], [3955, 5306], [3956, 5309], [3959, 5320], [3964, 5307], [3969, 5317], [3995, 5355], [4013, 5754], [4014, 5749], [4016, 5333], [4019, 5771], [4021, 5759], [4040, 5760], [4044, 5335, 5767], [4049, 5755], [4052, 5334], [4114, 5391], [4134, 5393. 4147, 5398. 4159, 5407. 4197, 5419. 4205, 5436. 4206, 5437. 4218, 5434. 4220, 5422. 4241, 5448. 4248, 5446. 4262, 5780. 4290, 5458. 4300, 5506. 4337, 4666. 4373, 4692], [4374, 4392. 4375, 4683, 4707. 4377, 4658. 4399, 4549. 4620, 4719], [5023, 5712], [5034, 5727], [5044, 5746], [5053, 5644], [5090, 5113], [5101, 5106], [5122, 5680], [5142, 5801], [5148, 5803], [5149, 5807], [5200, 5666], [5204, 5662], [5207, 5667], [5209, 5656], [5214, 5658], [524, 635], [5278, 5332], [5344, 5750], [5411, 5609], [5460, 5503. 6917, 7353], [7640, 7665] Sets Duplicates Images [3184, 3198], [3628, 5129], [3565, 5115], [3648, 5141. 4019, 5771], [3020, 7630], [3646, 5145. 6567, 929], [3362, 5027], [3800, 5235, 5553. 3666, 5157, 5151], [6554, 6566. 4238, 4243. 4049, 5755], [3638, 5525. 3587, 5110], [3286, 427], [3542, 5071], [3818, 5243. 4350, 4375, 4683, 4704, 4707, 6684], [6200, 6228], [3534, 5093], [3553, 5095], [267, 3242], [2895, 7441], [4534, 4570], [3947, 5318], [6798, 976], [3795, 5547], [3668, 5154], [2674, 6759], [3824, 5253], [5460, 5503], [3288, 429], [3679, 5026], [3543, 5098], [3955, 5306], [5209, 5656], [3647, 5144], [4014, 5749], [3522, 5097], [3556, 3564, 5069], [4044, 5335, 5767], [3762, 5203], [3759, 5229], [3675, 5155], [4262, 5780], [3778, 5212, 5661], [4013, 5754], [5207, 5667], [4241, 5448], [6602, 7400], [5452, 5779], [3535, 5073], [3781, 5217], [3956, 5309], [3014, 7307], [4248, 5446], [3392, 5732], [4664, 6873], [2743, 7363], [524, 635], [3531, 5089], [1896, 2891], [5200, 5666], [3969, 5317], [3492, 5738], [3816, 4196, 5257], [5090, 5113], [5142, 5801], [5278, 5332], [4206, 5437], [3375, 5716], [3549, 5076], [6850, 7337], [5344, 5750], [3949, 3960], [5101, 5106], [3122, 3134], [3084, 9. 3782, 5210], [4399, 4549, 4620, 4719], [3544, 5085], [4244, 5777], [5204, 5662], [3717, 5161], [4147, 5398], [4377, 4658], [2671, 6724], [6743, 7506], [3353, 5033, 5717], [3743, 5168], [6138, 6864], [3361, 5036], [3656, 5798], [2009, 2038], [6961, 7680], [4337, 4666], [3833, 5254], [4218, 5434], [3475, 5637], [3645, 5797], [6006, 6010], [4300, 5506], [3627, 5518], [3287, 428], [6280, 7538], [4016, 5333], [5223, 5664], [3570, 5086, 5107], [3626, 5126, 5519], [5148, 5803], [5411, 5609], [4373, 4692], [3486, 5740], [3793, 5233], [3546, 5068], [3370, 5722], [3767, 5228, 5673], [4220, 5422], [3773, 5201, 5671], [22, 3094], [6738, 7130], [3397, 5035], [3585, 5108], [2717, 6573], [2970, 6953], [2357, 2401], [3672, 5156], [5053, 5644], [5122, 5680], [3775, 5215], [26, 3097], [3540, 5084], [6198, 6226], [6834, 6870], [19, 3092], [4134, 5393], [3644, 5794], [27, 3098], [3815, 5247], [3760, 5221], [4114, 5391], [3959, 5320], [4197, 5419], [4386, 7415], [7640, 7665], [3639, 5134, 5528], [3768, 5211, 5655], [3462, 5638], [3812, 5238], [2289, 7172], [3671, 5153], [3995, 5355], [269, 3255], [5034, 5727], [1949, 6765], [3629, 5128], [3521, 5079], [4629, 547], [4290, 5458], [3372, 5734], [3625, 5136], [3950, 5312], [5023, 5712], [4159, 5407, 5604], [4295, 5504], [3469, 5050], [3607, 5125], [5929, 6852], [3426, 5046], [3578, 5082, 5117], [3506, 3698], [3669, 5152], [3554, 5092], [3494, 3791], [3769, 5220], [3764, 5670], [5044, 5746], [4374, 4392], [3558, 5078], [6839, 989], [3465, 5643], [3538, 5064], [4040, 5760], [23, 3095], [4613, 6714], [20, 3093], [4021, 5759], [4357, 4380], [3652, 5147, 5806], [3282, 3285], [4052, 5334], [5149, 5807], [3964, 5307], [3766, 5202], [5214, 5658], [6917, 7353], [613, 623], [3772, 5213, 5659], [3562, 5114], [3400, 3687], [3790, 5230, 5548], [4205, 5436], [3592, 5111], [3819, 5241], [3472, 5761], [4634, 4675, 6143, 6669], [3723, 5616], [3958, 5313], [3754, 5185</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
