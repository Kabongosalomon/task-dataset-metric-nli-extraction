<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pythia v0.1: the Winning Entry to the VQA Challenge 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pythia v0.1: the Winning Entry to the VQA Challenge 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This document describes Pythia v0.1, the winning entry from Facebook AI Research (FAIR)'s A-STAR team to the VQA Challenge 2018 1 .</p><p>Our starting point is a modular re-implementation of the bottom-up top-down (up-down) model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. We demonstrate that by making subtle but important changes to the model architecture and the learning rate schedule, finetuning image features, and adding data augmentation, we can significantly improve the performance of the up-down model on VQA v2.0 dataset [6] -from 65.67% to 70.24%.</p><p>Furthermore, by using a diverse ensemble of models trained with different features and on different datasets, we are able to significantly improve over the 'standard' way of ensembling (i.e. same model with different random seeds) by 1.31%. Overall, we achieve 72.27% on the teststd split of the VQA v2.0 dataset. Our code in its entirety (training, evaluation, data-augmentation, ensembling) and pre-trained models are publicly available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Chaerephon: Pythia -Is there any man alive wiser than Socrates? Pythia: None.</p><p>We present Pythia v0.1, a modular framework for Visual Question Answering research, which formed the basis for the winning entry to the VQA Challenge 2018 from Facebook AI Research (FAIR)'s A-STAR 2 team.</p><p>The motivation for Pythia comes from the following observation -a majority of today's Visual Question Answering (VQA) models fit a particular design paradigm, with modules for question encoding, image feature extraction, fusion of the two (typically with attention), and classification over the space of answers. The long-term goal of Pythia is to serve as a platform for easy and modular research &amp; * indicates equal contributions. <ref type="bibr" target="#b0">1</ref> and changes made after the challenge deadline. <ref type="bibr" target="#b1">2</ref> Agents that See, Talk, Act, and Reason. development in VQA <ref type="bibr" target="#b1">[2]</ref> and related directions like visual dialog <ref type="bibr" target="#b2">[3]</ref>. The name 'Pythia' is an homage to the Oracle of Apollo at Delphi, who answered questions in Ancient Greece. The starting point for Pythia v0.1 is a modular reimplementation of the bottom-up top-down (up-down) model <ref type="bibr" target="#b13">[14]</ref>. In this study, we demonstrate that by making a sequence of subtle but important changes, we can significantly improve the performance as summarized in <ref type="table" target="#tab_0">Table 1</ref> 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bottom-Up and Top-Down Attention</head><p>We perform ablations and augmentations over the baseline system of the up-down model <ref type="bibr" target="#b0">[1]</ref>, which was the basis of the winning entry to the 2017 VQA challenge. The key idea in up-down is the use of an object detector -Faster RCNN <ref type="bibr" target="#b11">[12]</ref> pre-trained on the Visual Genome dataset <ref type="bibr" target="#b8">[9]</ref> -to extract image features with bottom-up attention, i.e., visual feed-forward attention. Specifically, a ResNet-101 was chosen as the backbone network, and its entire Res-5 block was used as the second-stage region classifier for detection. After training, each region was then represented by the 2048D feature after average pooling from a 7?7 grid.</p><p>The question text is then used to compute the top-down attention, i.e., task specific attention, for each object in the image. Multi-modal fusion is done through a simple Hadamard product followed by a multi-label classifier using a sigmoid activation function to predict the answer scores. Their performance reached 70.34% on VQA 2.0 test-std split with an ensemble of 30 models trained with different seeds. For presentation clarity, we present our proposed changes (and the respective improvements) in a sequence; however, we also found them to be independently useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Architecture</head><p>We made a few changes to the up-down model to improve training speed and accuracy. Instead of using the gated hyperbolic tangent activation <ref type="bibr" target="#b0">[1]</ref>, we use weight normalization <ref type="bibr" target="#b12">[13]</ref> followed by ReLU to reduce computation 4 . We also replaced feature concatenation with element-wise To compute the question representation, we used 300D GloVe <ref type="bibr" target="#b10">[11]</ref> vectors to initialize the word embeddings and then passed it to a GRU network and a question attention module to extract attentive text features <ref type="bibr" target="#b15">[16]</ref>. For fusing the image and text information, we found the best-performing hidden size to be 5000. With these modifications, we were able to improve the performance of the model from 65.32% to 66.91% on VQA v2.0 test-dev.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning Schedule</head><p>Our model is optimized by Adamax, a variant of Adam with infinite norm <ref type="bibr" target="#b7">[8]</ref>. In one popular implementation of up-down 4 learning rate is set to 0.002 with a batch size of 512. We found that reducing the batch size improves performance -which suggests that there is potential for improving performance by increasing the learning rate. However, naively increasing the learning rate resulted in divergence. To increase the learning rate, we thus deployed the warm up strategy <ref type="bibr" target="#b4">[5]</ref> commonly used for large learning-rate training of networks. Specifically, we begin with a learning rate of 0.002, linearly increasing it at each iteration till it reaches 0.01 at iteration 1000. Next, we first reduce the learning rate by a factor of 0.1 at 5K and then reduce it every 2K iterations, and stop training at 12K. With this we increase the performance from 66.91% to 68.05% on test-dev.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fine-Tuning Bottom-Up Features</head><p>Fine tuning pre-trained features is a well known technique to better tailor the features to the task at hand and thus improve model performance <ref type="bibr" target="#b11">[12]</ref>.</p><p>Different from Anderson et al. <ref type="bibr" target="#b0">[1]</ref>, we also used the new state-of-the-art detectors based on feature pyramid net- works (FPN) <ref type="bibr" target="#b9">[10]</ref> from Detectron 5 , which uses ResNeXt <ref type="bibr" target="#b14">[15]</ref> as backbone and has two fully connected layers (fc6 and fc7) for region classification. This allows us to extract the 2048D fc6 features and fine-tune the fc7 parameters, as opposed to the original up-down <ref type="bibr" target="#b0">[1]</ref>, where fine-tuning previous layers requires significantly more storage/IO and computation on 7?7?2048 convolutional feature maps. Similar to up-down, we also used Visual Genome (VG) <ref type="bibr" target="#b8">[9]</ref> with both objects and attributes annotations to train the detector.</p><p>We set the fine-tune learning rate as 0.1 times the overall learning rate. We are able to reach a performance of 68.49% on test-dev with this fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Data Augmentation</head><p>We added additional training data from Visual Genome <ref type="bibr" target="#b8">[9]</ref> and Visual Dialog (VisDial v0.9) <ref type="bibr" target="#b2">[3]</ref> datasets. For VisDial, we converted the 10 turns in a dialog to 10 independent question-answer pairs. Since both VG and VisDial datasets only have a single ground-truth answer while VQA has 10, we simply replicated the answer to each question in VG and VisDial 10 times to make the data format compatible with the VQA evaluation protocol.</p><p>We also performed additional data augmentation by mirroring the images in the VQA dataset. We do some basic processing of the questions and answers for the mirrored images by interchanging the tokens "left" and "right" in the questions and answers which contain them. When adding these additional datasets, we reduce the learning rate as we described in Section 2.2 first at 15K iterations, respectively, and stop training at 22K iterations. As a result of data augmentation, we are able to improve our single model performance from 68.49% to 69.24% on test-dev.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Post-Challenge Improvements</head><p>Anderson et al. <ref type="bibr" target="#b0">[1]</ref> uses only the features pooled from object proposals (called bottom-up features) to represent an image. Our hypothesis is that such a representation does not fully capture a holistic spatial information about the image and visual representations from image regions not covered by the proposals. To test this hypothesis, we combined grid-level image features together with bottom-up features. We follow the same procedure as <ref type="bibr" target="#b3">[4]</ref> to extract grid-level features from ResNet152 <ref type="bibr" target="#b6">[7]</ref>. Object-level features and gridlevel features are separately fused with features from questions and then are concatenated to fed to classification. Before the challenge deadline, we had experimented with this only on images from the VQA dataset without fine-tuning. After the challenge, we performed more comprehensive experiments and found that adding grid level features helps to further improve the performance to 69.81%.</p><p>Instead of using an adaptive protocol for choosing the number of object proposals (between 10 and 100) per image as as done in <ref type="bibr" target="#b13">[14]</ref>, we also experimented with using a simpler (but slower) strategy of using 100 objects proposals for all images. As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, with features from 100 bounding-boxes, we reach 70.01% for test-dev and 70.24% for test-std on VQA 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Model Ensembling</head><p>All ensembling experiments described below involve models trained before the challenge deadline. That is, they do not include the two after-challenge experiments described in Section 2.5. We tried two strategies for ensembling. First, we choose our best single model and train the same network with different seeds, and finally average the predictions from each model. As can be seen from <ref type="figure" target="#fig_0">Fig 1,</ref> the performance plateaus at 70.96%. Second, we choose models trained with different settings, i.e., the tweaked updown model trained on the VQA dataset with/without data augmentation and models trained with image features extracted from different Detectron models with/without data augmentation. As can be seen, this ensembling strategy is much more effective than the previous one. Ensembling 30 diverse models, we reach 72.18% on test-dev and 72.27% on test-std of VQA v2.0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Performance with different ensemble strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy (%) on VQA v2.0. For ease of presentation, our changes are presented as a sequence building on top of previous changes. * denotes that these models are not included in our ensemble results submitted to the challenge.</figDesc><table><row><cell>Model</cell><cell cols="2">test-dev test-std</cell></row><row><cell>up-down [1]</cell><cell>65.32</cell><cell>65.67</cell></row><row><cell>up-down Model Adaptation ( ?2.1)</cell><cell>66.91</cell><cell></cell></row><row><cell>+ Learning Schedule ( ?2.2)</cell><cell>68.05</cell><cell></cell></row><row><cell>+ Detectron &amp; Fine-tuning ( ?2.3)</cell><cell>68.49</cell><cell></cell></row><row><cell>+ Data Augmentation  *  ( ?2.4)</cell><cell>69.24</cell><cell></cell></row><row><cell>+ Grid Feature  *  ( ?2.5)</cell><cell>69.81</cell><cell></cell></row><row><cell>+ 100 bboxes  *  ( ?2.5)</cell><cell>70.01</cell><cell>70.24</cell></row><row><cell>Ensemble, 30? same model ( ?2.6)</cell><cell>70.96</cell><cell></cell></row><row><cell>Ensemble, 30? diverse model ( ?2.6)</cell><cell>72.18</cell><cell>72.27</cell></row><row><cell cols="3">multiplication to combine the features from text and vi-</cell></row><row><cell cols="3">sual modalities when computing the top-down attention.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">FAIR A-STAR's entry in the VQA 2018 Challenge was 72.25%. This document describes results produced by our code release which reaches 72.27%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/hengyuan-hu/ bottom-up-attention-vqa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/facebookresearch/Detectron</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Peter Anderson, Abhishek Das, Stefan Lee, Jiasen Lu, Jianwei Yang, Licheng Yu, Luowei Zhou for helpful discussions, Peter Anderson for providing training data for the Visual Genome detector, Deshraj Yadav for responses on EvalAI related questions, Stefan Lee for suggesting the name 'Pythia', Abhishek Das, Abhishek Kadian for feedback on our codebase and Meet Shah for making a docker image for our demo.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tips and tricks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1708.02711</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
