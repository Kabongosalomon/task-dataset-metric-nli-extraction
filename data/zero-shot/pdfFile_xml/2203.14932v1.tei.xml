<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attributable Visual Similarity Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Zhang</surname></persName>
							<email>zhang-br21@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attributable Visual Similarity Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes an attributable visual similarity learning (AVSL) framework for a more accurate and explainable similarity measure between images. Most existing similarity learning methods exacerbate the unexplainability by mapping each sample to a single point in the embedding space with a distance metric (e.g., Mahalanobis distance, Euclidean distance). Motivated by the human semantic similarity cognition, we propose a generalized similarity learning paradigm to represent the similarity between two images with a graph and then infer the overall similarity accordingly. Furthermore, we establish a bottom-up similarity construction and top-down similarity inference framework to infer the similarity based on semantic hierarchy consistency. We first identify unreliable higher-level similarity nodes and then correct them using the most coherent adjacent lower-level similarity nodes, which simultaneously preserve traces for similarity attribution. Extensive experiments on the CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate significant improvements over existing deep similarity learning methods and verify the interpretability of our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Similarity learning is a fundamental task in the field of computer vision, where most prevalent works (i.e., metric learning methods) employ a distance metric to measure the similarities between samples. They transform features into an embedding space and define the dissimilarity as the Euclidean distance in this space, where the objective is to cluster similar samples together and separate dissimilar ones apart from each other. While conventional methods use hand-crafted features like SIFT <ref type="bibr" target="#b20">[21]</ref> and LBP <ref type="bibr" target="#b0">[1]</ref>, deep metric learning methods employ convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16]</ref> to extract more representative features and demonstrate superior performance. In recent years, similarity learning has been widely applied to various <ref type="bibr">Figure 1</ref>. The motivation of the proposed AVSL framework. Humans recognize each image as a complex set of concepts and compare two images hierarchically <ref type="bibr" target="#b16">[17]</ref>. For example, when inferring the similarity between two cars, humans usually first compare high-level features such as shapes or colors and then turn to finer features such as wheel structures when a coarse observation does not distinctly distinguish them. Motivated by this, we propose to employ a graph structure to decompose sample pairs into discriminative concept nodes, which is more consistent with how humans perceive the cognitive distance and is beneficial to the attribution of the similarity measurement. vision tasks such as face recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>, person reidentification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47]</ref>, and image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The essential goal of visual similarity learning is to obtain a similarity measure that generalizes well to unseen data. It has been shown that the good generalization of the human visual system comes from the ability to parse objects into parts and relations and learn the underlying concepts <ref type="bibr" target="#b16">[17]</ref>. Humans also infer the similarity between two images hierarchically by first comparing high-level features and then delving into lower-level features, as illustrated in <ref type="figure">Figure 1</ref>. However, most existing similarity learning methods simply project each sample to one single vector and employ the Mahalanobis distance or Euclidean distance as the similarity function. They only use the top-level feature to represent an image and directly compute the similarity without inference. Also, using a single vector for similarity measure exacerbates the unexplainability caused by the blackbox CNNs and leads to untraceable similarity measurement, i.e., we can hardly attribute the overall similarity to specific features. To alleviate this issue, some methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59]</ref> attempt to extend neural network visualization techniques to deep metric learning and generate a saliency map for each image. Still, they treat the similarity computing model as a black box and can only explain it subjectively in a post hoc way, where the similarity computing process remains untraceable and unexplainable.</p><p>In this paper, we propose an attributable visual similarity learning (AVSL) framework to actively explain the learned similarity measurement. We generalize the prevalent metric learning paradigm to represent the similarity between images by a graph and then analyze it to infer the overall similarity. We use CNNs to extract hierarchical visual features in a bottom-up manner, where higher-level features encode more abstract concepts <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50]</ref> and can be regarded as a combination of low-level features <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. We further construct an undirected graph to represent the similarity between images. We then propose a top-down similarity inference method based on hierarchy consistency. We start from high-level similarity nodes and rectify identified unreliable nodes using adjacent low-level similarity nodes until reaching the lowest level, which is similar to how humans compare two objects from coarse to fine. The overall similarity can be easily attributed to the effect of each similarity node corresponding to certain visual concepts. Our framework can be readily applied to existing deep metric learning methods with various loss functions and sampling strategies. Extensive experiments on the widely used CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref>, Cars196 <ref type="bibr" target="#b14">[15]</ref>, and Stanford Online Products <ref type="bibr" target="#b23">[24]</ref> datasets demonstrate that our AVSL framework can significantly boost the performance of various deep metric learning methods and achieve state-of-the-art results. We also conduct visualization experiments to demonstrate the attributability and interpretability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Similarity learning: Similarity learning aims to learn a similarity function to accurately measure the semantic similarities between images. Conventional methods adopt the Mahalanobis distance to learn linear metric functions and further use kernel tricks to model nonlinear relations. Recent deep metric learning methods employ convolutional neural networks to learn an embedding space and use the Euclidean distance for similarity measurement, where the majority of works focus on designing different loss functions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> and sampling strategies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref> for more effective training of the metric. For example, the contrastive loss <ref type="bibr" target="#b6">[7]</ref> pulls positive pairs together while pushing negative ones farther than a fixed margin. Song et al. <ref type="bibr" target="#b23">[24]</ref> further proposed a lifted structured loss considering the global connections among a mini-batch. Movshovitz et al. <ref type="bibr" target="#b22">[23]</ref> simplified the pair sampling to linear complexity by including proxies in the loss formulations. Still, an appropriate sampling strategy has been proven to be effective to boost performance. For example, Schroff et al. <ref type="bibr" target="#b29">[30]</ref> presented a semi-hard sampling strategy to select informative samples while discarding outliers. Harwood et al. <ref type="bibr" target="#b7">[8]</ref> proposed a smart sampling strategy adaptive to different training stages.</p><p>Other works explore different designs of the similarity function to improve the performance. For example, Yuan et al. <ref type="bibr" target="#b47">[48]</ref> and Huang et al. <ref type="bibr" target="#b11">[12]</ref> proposed an SNR distance and a PDDM module, respectively, to better guide the training process but still uses the conventional Euclidean distance during testing. Verma et al. <ref type="bibr" target="#b35">[36]</ref> learned hierarchical distance metrics based on the class taxonomy. Ye et al. <ref type="bibr" target="#b43">[44]</ref> employed a set of metrics to describe similarities from different perspectives. However, all the aforementioned methods represent dissimilarity by projecting samples into single points in the Euclidean distance which implies the triangle inequation, while the proposed AVSL framework represents samples in a graph manner to model relations between concepts. Zheng et al. <ref type="bibr" target="#b56">[57]</ref> also exploited relations by projecting samples with multiply embedders to learn a sub-space structure. Differently, we propose to decompose the overall similarity hierarchically with hierarchy consistency as the inductive bias and employ a top-down similarity structure compatible with bottom-up similarity construction.</p><p>Explainable artificial intelligence: Explainable artificial intelligence (XAI) has attracted considerable attention in recent years, resulting from the demand for stabler and safer AI applications. One category of works aims to interpret the outputs of black-box models by visualization or imitation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref> (i.e., passive methods). For example, Zeiler et al. <ref type="bibr" target="#b49">[50]</ref> and Selvaraju et al. <ref type="bibr" target="#b30">[31]</ref> projected hidden feature maps into the input space using deconvolution and gradients, respectively, which can assist humans to understand the semantics of the hidden layers. Ribeiro et al. <ref type="bibr" target="#b27">[28]</ref> and Zhang et al. <ref type="bibr" target="#b51">[52]</ref> employed linear regressions and graph models to imitate complex rules in the black-box inference process. Another category of works attempts to modify the model architecture to improve its explainability <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53]</ref> (i.e., active methods). For instance, Zhang et al. <ref type="bibr" target="#b52">[53]</ref> restricted each kernel of hidden layers to encoding a single concept. Wu et al. <ref type="bibr" target="#b41">[42]</ref> proposed a tree regularization loss to favor models that can be more easily approximated by a simple decision tree.</p><p>A few works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b58">59]</ref> seek to extend neural network visualization techniques for deep metric learning. Nevertheless, they can only obtain global saliency maps and can hardly conduct quantitive attribution analysis of overall similarities, which cannot provide detailed interpretations of similarity models. To the best of our knowledge, we are the first to explore an attributable and explainable similarity learning framework. Imitating humans to compare objects from coarse to fine, the proposed AVSL can attribute overall similarities to hierarchical hidden concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we first present a generalized similarity learning paradigm and then elaborate on the proposed bottom-up similarity construction and top-down similarity inference. Finally, we present the AVSL framework and demonstrate how to quantitatively attribute the similarity to different levels of features under our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generalized Similarity Learning Paradigm</head><formula xml:id="formula_0">Let X = {x (1) , x (2) , ? ? ? , x (N) } denotes the image set,</formula><p>where sample x (n) ? X has a label l (n) ? {l 1 , l 2 , ? ? ? , l C } with C being the number of classes. Given an L-layer CNN f and a sample x, we call the l-layer outputs as feature maps, denoted as z l = f l (x) ? R c l ?h l ?w l , where c l , h l , and w l denote channel, height, and weight respectively. Then a pooling operation g l (?) reduces feature maps to vectors v l = g l (z l ) ? R c l . Existing deep metric learning methods usually add a linear projector h l (?) to map v l into an r-dimension embedding space: e l = h l (v l ) ? R r , where the dissimilarity between two images x, x isd(x, x ) = d(e l , e l ) = e l ? e l 2 . For simplicity, We use similarity and dissimilarity interchangeably unless stated otherwise.</p><p>However, existing deep similarity learning methods only utilize the features from the top layer while discarding those from the hidden layers, which might contain complementary information. To address this, we propose a generalized similarity learning (GSL) paradigm, which constructs an undirected graph H involving embeddings from each layer to compute overall similarities. We denote each element of the embedding e l as e l i , and define the similarity node of H as ? l i = |e l i ? e l i |. In addition, the edge ? ij of H will be elaborated in Section 3.2. To sum up, the GSL paradigm is composed of two modules:</p><p>? Similarity construction: compute similarity nodes ? l i and edges ? ij to construct an undirected graph H.</p><p>? Similarity inference: infer the overall similarity d according to the graph H.</p><p>The conventional metric learning methods can be regarded as a special case of GSL paradigm as shown in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bottom-Up Similarity Construction</head><p>Conventional similarity learning methods only use embeddings from the top layer to compute the overall similarity, making it difficult to trace back to different concepts, which are encoded by embeddings from all layers as evidenced by Zhang et al. <ref type="bibr" target="#b51">[52]</ref>. Different levels of features encode different levels of concepts containing complementary information, where the large receptive fields of highlevel features enable them to represent high-level semantic information and omit some high-frequency details, while low-level features can capture detailed information such as textures but fail to perceive global semantics due to the restricted receptive fields. Still, high-level features can be regarded as the combination of low-level features <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, and their connections can be exploited for the subsequent similarity inference. Therefore, we propose a bottom-up similarity construction method to compute different levels of similarity nodes and the connections between them.</p><p>The first step is to compute similarity nodes ? l i . We extract the feature map z l of the l-th layer by convolutional blocks from bottom to top, and then obtain the feature vector v l using global pooling. Subsequently, we employ a fully connected layer to map the feature vector to the corresponding embedding e l . Finally we obtain the similarity nodes by computing the square of the difference between normalized embeddings? l ,? l : The second step is to compute edge w l ij between nodes ? l i and ? l?1 j . Since pooling operation erases the spatial information, which encodes relations between different nodes, we propose to utilize CAMs <ref type="bibr" target="#b57">[58]</ref> of each node to recover relations as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. We first compute CAMs of nodes as follows:</p><formula xml:id="formula_1">? l i = |? l i ?? l i | 2 . (1) ! !"#$% ! ? 1 #$% !"#$% ! #$% ? NOT Related ? Related</formula><formula xml:id="formula_2">u l i = c l j=1 a ij z l j ? R h l ?w l ,<label>(2)</label></formula><p>where z l j denotes the j-th slice of the feature map z l , and a ij indicates weights of the linear layer h l (?). We consider two nodes correlated if the two distributions of the corresponding CAMs are statistically similar. After rescaling and vectorizing CAMs to the same scale vectors? l i ,? l?1 j ? R p , where p = min{h l , h l?1 } ? min{w l , w l?1 }, we establish the correlation? l ij by computing the inner product of? l i and? l?1 j as follows:?</p><formula xml:id="formula_3">l ij = ? l i ,? l?1 j .<label>(3)</label></formula><p>To obtain the final edges ? l ij , we adopt the momentum updating strategy to gradually incorporate all training samples:</p><formula xml:id="formula_4">? l ij ? ?? l ij + (1 ? ?)? l ij ,<label>(4)</label></formula><p>where ? is a momentum factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Top-Down Similarity Inference</head><p>Having constructed the graph H composed of similarity nodes ? l i and correlation edges ? l ij , we want to incorporate them to compute an overall similarity. Taking full advantage of the hierarchy consistency of CNNs, we propose a top-down similarity inference method based on the graph H. On the one hand, we argue that different levels of features encode complementary information, enabling the corresponding similarity nodes to produce relatively independent similarity judgment. On the other hand, the most correlated similarity nodes of adjacent levels should be consistent, which can be used as a non-trivial constraint to restrict the overall similarity. Motivated by this, we propose to identify unreliable higher-level similarity nodes and rectify them using adjacent lower-level similarity nodes with the largest correlations.</p><p>Analogous to the process of humans comparing images from coarse to fine, we infer the overall similarity from top to bottom. We first estimate the reliability of the similarity nodes at the l-th layer to identify the unreliable ones. Intuitively, we deem a similarity node unreliable if its corresponding CAM is unable to focus clearly on specific regions. Formally, we employ the standard deviation of normalized CAMs to compute the reliability as:</p><formula xml:id="formula_5">? l i = std(? l i ) ? std(? l i ),<label>(5)</label></formula><p>where std(?) denotes the standard deviation and? l i ,? l i indicate normalized CAMs of samples x, x . Then we apply a sigmoid function to map ? l i to the range of (0, 1) as:</p><formula xml:id="formula_6">p l i = ?(? l i ? l i + ? l i ) = e ? l i ? l i +? l i e ? l i ? l i +? l i + 1 ? (0, 1),<label>(6)</label></formula><p>where ? l i , ? l i are node-wise learnable parameters. We then rectify unreliable nodes at the higher-level layer with the correlated ones at the adjacent lower-level layer. For an unreliable ? l i at the l-th layer, we denote the index set of k most correlated nodes at the (l ? 1)-th layer as:</p><formula xml:id="formula_7">I(? l i ) = {j|? l ij ? max k {? l im:m=1,2,??? ,r }},<label>(7)</label></formula><p>where max k (?) denotes the set of k largest values. Subsequently, we compute the rectified similarity node? l i by the sum of the original one ? l i and adjacent related low-level ones ? l?1 j weighted by the unreliability p l i as follows:</p><formula xml:id="formula_8">? l i = ? ? ? ? ? ? ? p l i ? l i + (1 ? p l i ) r j=1? l ij? l?1 j , 2 ? l ? L ? l i , l = 1 (8) where? l ij = I j?I(? l i ) ? l ij r k=1 I k?I(? l i ) ? l ik ? [0, 1] denotes the nor- malized edges, I (?)</formula><p>is the indicative function, and ? l i , ? l ij are nodes and edges of the graph H. Since ? l i ? 0, p l i ? (0, 1),? l ij ? 0, we know that? l i ? 0. For convenience, we reorganize (8) in a matrix format as follows:  <ref type="figure">Figure 4</ref>. An illustration of the architecture of the proposed AVSL framework. We first extract a set of feature maps from multiple layers of a CNN network and perform global pooling followed by linear projection to obtain the set of embedding. We then compute the square of the absolute differences between the corresponding embeddings as similarity nodes. For similarity inference, we first estimate the reliability of the similarity nodes and rectify them using the most correlated ones in the adjacent lower level. We compute the overall similarity as the sum of the rectified similarity nodes of the top layer which can be conveniently attributed to specific similarity nodes in different levels.</p><formula xml:id="formula_9">? ? ? l = P l ? ? ? l + (I ? P l )W l? ? ? l?1 (9) where? ? ? l = [? l 1? l 1 ? ? ?? l r ] ? R r ? ? ? l = [? l 1 ? l 2 ? ? ? ? l r ] ? R r P l = diag(p l 1 , ? ? ? , p l r ), l ? 2 I, l = 1 W l = (? l ij ) ? R r?r .</formula><p>Finally, we define the overall similarity between two images as the sum of rectified top-level similarity nodes a? d = r i=1? L i , while the smaller value indicates more similarity. Following (9), we can infer it recursively in a topdown manner efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Attributable Visual Similarity Learning</head><p>The proposed AVSL framework employs a bottom-up similarity construction and top-down similarity inference method based on hierarchy consistency to extend the conventional similarity learning, as shown in <ref type="figure">Figure 4</ref>. We divide our framework into three phases: training, attribution, and evaluation.</p><p>Training: In order to learn network parameters, We define l-th level similarity as d l = r i=1 ? l i . The proposed AVSL is compatible with existing deep metric learning methods with various loss functions and sampling strategies to further improve their performance. For a particular loss function L(?), the overall objective of the proposed AVSL framework is formulated as follows:</p><formula xml:id="formula_10">min ?1,?2 J = min ?1 L l=1 L m (d l ) + min ?2 L f (d)<label>(10)</label></formula><p>where ? 1 corresponds to the CNN network parameters and ? 2 represents the parameters of the similarity inference module including ? l i , ? l i of the reliability estimatation modules in <ref type="bibr" target="#b5">(6)</ref>. We only use the loss on overall similarities to train the similarity inference module and the loss on level similarities to train the similarity construction module. L m targets at learning discriminative embeddings for each layer, while L f only aims at learning the similarity inference process to obtain an accurate and robust overall similarity.</p><p>Attribution: It is essential to analyze how the model infers the similarity. We reorganize the overall similarityd in a linear combination format following (9) as:</p><formula xml:id="formula_11">d = r i=1? L i = 1? ? ? L (11) = 1P L ? ? ? L + 1(I ? P L )W L? ? ? L?1 = L l=1 1? l ? ? ? l = L l=1 r i=1 ? l i ? l i , where ? l = (I ? P L )W L ? ? ? (I ? P l+1 )W l+1 P l , and ? ? ? l = [? l 1 ? l 1 ? ? ? ? l r ] = 1 T ? l .</formula><p>The weight ? l i represents the sensitivity of the overall similarity to each similarity node ? l i , which means the change of the similarity node ? l i will contribute more to the change of the overall similarityd if its corresponding weight ? l i is larger. Finally, saliency maps are generated to demonstrate the attribution process. The proposed AVSL framework is convenient for visualization since we compute similarity nodes and corresponding CAMs simultaneously in a single forward propagation.</p><p>Evaluation: During the evaluation, we freeze all parameters and only compute the overall similaritiesd using <ref type="bibr" target="#b21">(22)</ref> to represent the similarities between the given query samples and gallery samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we conducted experiments on three widely used datasets including CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref>, Cars196 <ref type="bibr" target="#b14">[15]</ref>, and Stanford Online Products <ref type="bibr" target="#b23">[24]</ref> to evaluate the accuracy and interpretability of our AVSL framework. We used the Recall@Ks as the performance metrics, which compute the percentage of well-separated samples acknowledged if we can find at least one corrected retrieved sample in the K nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>For quantitative evaluation, we conducted experiments under a zero-shot setting following the non-intersecting dataset partition protocol <ref type="bibr" target="#b23">[24]</ref>. The split scheme of datasets are as follows:</p><p>? CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref> consists of 200 bird species and 11,788 images. We split the first 100 species <ref type="bibr">(</ref> For qualitative demonstration, we further visualized the similarity attribution results of some randomly selected samples in CUB-200-2011 and Cars196. All datasets are publicly available for non-commercial research and educational purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We conducted all the experiments using the PyTorch package <ref type="bibr" target="#b25">[26]</ref> on an NVIDIA RTX 3090 GPU and employed the ResNet50 <ref type="bibr" target="#b8">[9]</ref> as the CNN feature extractor (i.e., f m ) for fair comparisons. Limited by the GPU device memory, we only selected feature maps for every three layers for similarity construction (i.e., layers 3, 4, and 5). We employed a global pooling operation (i.e., g l ) and a linear layer (i.e., h l ) after each selected layer. We fixed the embedding size to 512 for all selected layers. For data argumentation, we first resized images to 256 by 256 to apply random reshaping and horizontal flip and then randomly cropped them to 224 by 224. Before training, we initialized the CNN with weights pre-trained on ImageNet ILSVRC dataset <ref type="bibr" target="#b28">[29]</ref>. We adopted AdamW <ref type="bibr" target="#b19">[20]</ref> to train our model with an initial learning rate 1 ? 10 ?4 and a weight decay of 0.0001. We fixed the batch size to 180 and set the momentum factor ? to 0.5. For the margin loss <ref type="bibr" target="#b40">[41]</ref>, we set the margin factors ? and ? to 1.2 and 0.2, respectively. For the ProxyAnchor loss <ref type="bibr" target="#b12">[13]</ref>, we set the temperature ? = 16, positive margin ? pos = 1.8, and negative margin ? neg = 2.2. We tuned all hyperparameters by grid search on a reserved validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results and Analysis</head><p>Comparisons with existing methods: We applied the proposed AVSL framework to the margin loss <ref type="bibr" target="#b40">[41]</ref> and the ProxyAnchor loss <ref type="bibr" target="#b12">[13]</ref> for demonstration and compared our framework with several baseline methods. <ref type="table">Table 1</ref> shows the image retrieval performance on the CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref>, Cars196 <ref type="bibr" target="#b14">[15]</ref>, and Stanford Online Products <ref type="bibr" target="#b23">[24]</ref> respectively. We mark the best results with bold red and highlight our superior results over the associated methods without AVSL in bold black.</p><p>We observe that our AVSL framework can greatly improve the original deep metric learning methods by a large margin and achieve state-of-the-art performance on three datasets. We ascribe the improvement to exploiting the graph structure by employing the hierarchy consistency between different similarity nodes as the inductive bias which is consistent with how humans perceive the semantic visual similarity. By rectifying unreliable higher-level similarity nodes with the most correlated ones in the lower-level layer, we achieve a more accurate and robust similarity measure with the proposed top-down similarity inference.</p><p>Ablation study: We first performed an ablation study to evaluate the contribution of each component of the proposed AVSL framework. We report the experimental results on the Cars196 <ref type="bibr" target="#b14">[15]</ref> dataset with the ProxyAnchor loss <ref type="bibr" target="#b12">[13]</ref>, as shown in <ref type="table">Table 2</ref>, but we observe similar outcomes with the other loss functions. We highlight the best results using bold numbers.</p><p>ProxyAnchor denotes the baseline method of using the ProxyAnchor loss. + M is short for 'multi-layer'. In this trial, we exerted extra loss constraints on embeddings of all layers (i.e., layers 3, 4, and 5). + R stands for 'reliability', which means that we utilize the reliabilities defined in <ref type="bibr" target="#b5">(6)</ref> and only keep reliable nodes to compute the similarities. Based on + M &amp; R setting, if we further consider edges between similarity nodes, we can get complete components of the proposed AVSL framework. In the + M (concat) setting, we concatenated embeddings of all three layers (i.e., the final dimension equals to 3 ? 512 = 1536) to compute similarities, which is a strong baseline to further demonstrate the effectiveness of the proposed AVSL framework.</p><p>We observe that the proposed AVSL framework achieves better performance than all the compared counterparts and all modules contribute to the overall improvement. In particular, imposing loss constraints on the embeddings of hidden layers can boost the performance of the original method by 2.0%. It is also beneficial to employ reliabilities defined in <ref type="bibr" target="#b5">(6)</ref> to guide the selection of informative nodes. Subsequently, exploiting the relations among similarity nodes and employing the hierarchy consistency for similarity inference can further improve the performance by 1.6%. We also see that our method suppresses + M (concat), which demonstrates that learning informative relations and reliabilities is crucial for effective inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of hyperparameters:</head><p>The k value defined in <ref type="bibr" target="#b6">(7)</ref> controls how many adjacent related nodes participate in rectification. <ref type="figure" target="#fig_2">Figure 5a</ref> reveals the continuous improvement when increasing the k value. And we can further discover that the influence of k shows diminishing marginal effects, thus we fixed k to 128 to complete all other experiments. In addition, the dimension of embeddings significantly im-  pacts the performance as shown in <ref type="figure" target="#fig_2">Figure 5b</ref>, and larger embedding size leads to higher performance. In particular, when only fixing the dimension to 128, our proposed AVSL could surpass all other methods with a recall@1 score of 88.4% on the Cars196 dataset, which further demonstrates the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>To verify the interpretability of the proposed AVSL framework, we randomly selected a triplet from CUB-200-2011 to show the attribution results, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>. For the pairs in the triplet, we first selected the 128 most reliable similarity nodes, and then ranked those nodes according to their similarities. We observe that most of the CAMs of nodes focus on specific parts of images while some saliency maps are unrecognizable. We think that this phenomenon is due to the singularity of the relationships between spatial coordinates and concepts. Also, we discover that the dissimilarity distribution of the negative pair  is more dispersed than the positive one as shown on the left of <ref type="figure" target="#fig_4">Figure 6</ref>. This means that the nodes of the negative pair are more likely to be dissimilar, which is beneficial to classifying samples from different classes.</p><p>To further understand the underlying mechanism of the inference process, we also randomly selected a sample pair from Cars196 for similarity attribution, as shown in <ref type="figure">Figure 7 2</ref> . From top to bottom, we first selected top-128 reliable nodes with high p l i scores among 512 nodes and further displayed the two most similar nodes framed in the green dotted box as well as the two most dissimilar ones framed in the red dotted box. Subsequently, we decompose one unreliable node into adjacent related nodes. We observe that similarity nodes with higher sensitivity value ? l i are more likely positioned in a higher layer and correspond to clearer concepts such as "headlight", "wheel", and "door" and low-level saliency maps are difficult to distinguish concepts. This demonstrates that high features tend to encode object-level patterns while low features focus on pixel-level patterns. In addition, we discover that nodes and concepts may not correspond to each other one-to-one. For example, multiple nodes may focus on the "wheel" part of cars, which indicates that concepts extracted by CNNs are not well disentangled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>During evaluation, our AVSL framework needs to maintain a similarity matrix with the spatial complexity of O(N 2 ) to compute the similarity between two images, where N denotes the number of samples. When dealing</p><formula xml:id="formula_12">!"#$% &amp; !"#$% ' Similar Dissimilar Unreliable "car door"</formula><p>Unreliable "headlight" "wheel 1" "wheel 2" "rear wheel" Can't recognize "car frame" "front wheel" Similar Dissimilar <ref type="figure">Figure 7</ref>. Visualization of the attribution process. We randomly select 2 samples from Cars196 and attribute the overall similarity to the specific similarity nodes in an undirected graph manner. Best viewed in color.</p><p>with large-scale datasets, we use computation tricks such as matrix slicing to reduce the memory usage to achieve partially parallel computing. This also affects training of proxy-based methods (e.g., the ProxyAnchor loss) when the number of classes are large. On the Stanford Online Products dataset, it is impossible to maintain the similarities between samples in a mini-batch and 11,318 proxies simultaneously on a 24GB-memory device. We thus tailor the loss to only constrain the similarities between samples and positive proxies, which may lead to inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented an attributable visual similarity learning (AVSL) framework to learn a more accurate and interpretable similarity. We adopt a hierarchy consistency as the inductive bias and employ a bottom-up similarity construction and top-down similarity inference method to model the visual similarity, which first estimates the reliability of similarity nodes at a higher level and then rectifies the unreliable ones using the correlated ones in the adjacent lower level. We have conducted experiments on three widely used datasets to demonstrate the superiority of our framework on both accuracy and interpretability. While our framework is motivated by human visual similarity perception, we believe it can also be adapted to other modalities of information such as text and speech for better interpretability, which is an interesting future work.</p><p>Loss functions in deep metric learning can be categorized into pair-based methods and proxy-based methods. During training, we apply the proposed AVSL framework to the margin loss <ref type="bibr" target="#b40">[41]</ref> and the ProxyAnchor loss <ref type="bibr" target="#b12">[13]</ref> as the representative pair-based and proxy-based methods, respectively, to verify the effectiveness.</p><p>Margin loss <ref type="bibr" target="#b40">[41]</ref> compresses positive pairs while repelling negative pairs in the embedding space as follows:</p><formula xml:id="formula_13">L margin = 1 |P| (x,x + )?P [d(x, x + ) ? (? yx ? ?)] + 1 |N| (x,x ? )?N [(? yx + ?) ? d(x, x ? )] + ,<label>(12)</label></formula><p>where [?] + is the hinge function (i.e., [x] + = max{x, 0}) and d(?, ?) denotes the Euclidean distance. We use P and N to indicate the set of positive pairs and negative pairs and | ? | to denote the set of the size. To address the variable intra-class distributions, the margin loss introduce a learnable parameter ? ? ? ? R C to adaptively control the range of each class, where C denotes the number of classes. ? is a fixed parameter to enforce a large margin between classes.</p><p>ProxyAnchor loss <ref type="bibr" target="#b12">[13]</ref> instead constrains the relations between proxies and samples as follows:</p><formula xml:id="formula_14">L pa = 1 |P + | p?P + log ? ? 1 + x?X + p e ??(s(x,p)??) ? ? + 1 |P| p?P log ? ? 1 + x?X ? p e ?(s(x,p)+?) ? ? ,<label>(13)</label></formula><p>where ? is a scaling factor, ? is the margin, s(?, ?) is the cosine similarity function, P denotes the proxy set, and P + denotes the positive proxy set where each proxy has at least one positive samples in the current batch. Also, X + p includes the positive samples for a proxy p and X ? p contains the rest negative samples in the batch. However, the original form of ProxyAnchor loss <ref type="bibr" target="#b12">(13)</ref> is defined with the cosine similarity, while our proposed AVSL is defined in the context of dissimilarity. To address this, we reformulate the ProxyAnchor loss as follows:</p><formula xml:id="formula_15">L pa = 1 |P + | p?P + log ? ? 1 + x?X + p e ?(d(x,p)?(??? )) ? ? + 1 |P| p?P log ? ? 1 + x?X ? p e ??(d(x,p)?(?+? )) ? ? ,<label>(14)</label></formula><p>where d(?, ?) indicates the dissimilarity and ? and ? control the interclass margin similar to ? in <ref type="bibr" target="#b12">(13)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Pooling Linearization</head><p>To construct the similarity graph, we first employ a CNN to extract the feature map z = f (x) at each level and then reduce the feature map to a feature vector using pooling operation as v = g(z). Specifically, we use both max pooling g max and average pooling g avg operations following <ref type="bibr" target="#b12">[13]</ref> as follows:</p><formula xml:id="formula_16">v i = max h,w z ihw + 1 HW H h=1 W w=1 z ihw .<label>(15)</label></formula><p>Finally, we adopt a linear layer h to map v into an embedding space as:</p><formula xml:id="formula_17">e = h(v) = (h ? g)(z).<label>(16)</label></formula><p>In addition, we need to compute CAMs <ref type="bibr" target="#b57">[58]</ref> as follows:</p><formula xml:id="formula_18">u ihw = h(z ?hw ) = c j=1 a ij z jhw ,<label>(17)</label></formula><p>where c is the number of channels and a ij indicates the weights of the linear layer h. In order to maintain the spatial information of e, we want to ensure the following property:</p><formula xml:id="formula_19">g(u) = (g ? h)(z) = v.<label>(18)</label></formula><p>However, the pooling operation and the linear mapping is not commutative (i.e., g ? h = h ? g) since the pooling operation is a nonlinear function. To address this, we propose a linearization operationg as follows:</p><formula xml:id="formula_20">z i =g(z i ) = K ? z ihw , if z ihw = max kl z ikl 0, Otherwise<label>(19)</label></formula><p>where K = HW #{z ihw |z ihw =max kl z ikl } . Thus, we can decompose the pooling operation as follows: where I denotes the identity mapping. By employing this linearization trick, we first preprocess the feature map as follows:z = (g + I)(z) =g(z) + z.</p><formula xml:id="formula_21">g = g max + g avg = g avg ?g + g avg = g avg ? (g + I),<label>(20)</label></formula><p>We then rewrite <ref type="formula" target="#formula_6">(16)</ref> and <ref type="formula" target="#formula_19">(18)</ref> as:</p><formula xml:id="formula_23">e = (h ? g)(z) = (h ? g avg )(z) g avg (u) = g avg (h(z)) = (g avg ? h)(z),</formula><p>The g avg and h are now commutative (i.e., g avg ? h = h ? g avg ) so that the CAMs can preserve the spatial information of the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attribution Property</head><p>The proposed AVSL can attribute the overall similarity to specific similarity nodes quantitively as:</p><formula xml:id="formula_24">d = r i=1? L i = 1? ? ? L = 1P L ? ? ? L + 1(I ? P L )W L? ? ? L?1 = L l=1 r i=1 ? l i ? l i ,<label>(22)</label></formula><p>whered is the overall similarity, ? l i is the similarity node, and ? l i denotes the sensitivity of the corresponding node. ? l i represents the influence of the corresponding node on the overall similarity. The sensitivities have the following property: Property 1. The sum of ? l i of all nodes is a constant. Proof. We rewrite <ref type="bibr" target="#b21">(22)</ref> as follows:</p><formula xml:id="formula_25">d = L l=1 1? l ? ? ? l ,<label>(23)</label></formula><p>where ? l = (I?P L )W L ? ? ? (I?P l+1 )W l+1 P l , and ? ? ? l = [? l 1 ? l 1 ? ? ? ? l r ] = 1 T ? l . Let? l = (I ? P L )W L ? ? ? (I ? P l+1 )W l+1 . SinceW l is normalized by row (i.e.,W l 1 = 1), we can derive that:</p><formula xml:id="formula_26">(? l+1 +? l )1 =(I ? P L )W L ? ? ? (I ? P l+2 )W l+2 P l+1 1 + (I ? P l+1 )W l+1 1 =(I ? P L )W L ? ? ? (I ? P l+2 )W l+2 1 =? l+1 1<label>(24)</label></formula><p>Therefore, the sum of ? l i is computed as:</p><formula xml:id="formula_27">L l=1 r i=1 ? l i = L l=1 1 T ? l 1 =1 T L l=2 ? l 1 +? 1 1 =1 T?L 1 = 1 T 1 = r,<label>(25)</label></formula><p>where r is the dimension of embeddings.  Property 1 ensures that the absolute value of the sensitive ? l i is meaningful across samples and can directly indicate the significance of the corresponding similarity node when inferring the overall similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Detailed Visualization</head><p>We provide more detailed visualization results of the similarity inferring and attribution. we randomly selected two sample pairs from CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref> and Cars196 <ref type="bibr" target="#b14">[15]</ref> for similarity attribution, as shown in <ref type="figure" target="#fig_5">Figure 8</ref>. From top to bottom, we first selected the top-128 reliable nodes with a high p l i among all the 512 nodes and further displayed the three most similar nodes framed in green dotted boxes and the three most dissimilar ones framed in red dotted boxes. Subsequently, we decompose one unreliable node to the adjacent related nodes. We quantitatively show the reliabilities p l i , similarity nodes ? l i , and sensitivities ? l i under each box. We observe that the similarity nodes with higher sensitivity value ? l i are more likely positioned in higher layers, which correspond to clearer concepts such as "wing", "head", and "feet" as shown on the left of <ref type="figure" target="#fig_5">Figure 8</ref>. In addition, patterns of low-level features are relatively difficult to recognize. This demonstrates that high-level features tend to encode object-level concepts while low-level features focus on pixel-level concepts. In addition, we discover that nodes and concepts may not correspond to each other one-  to-one. For example, multiple nodes may all focus on the "wheel" part of cars as shown on the right of <ref type="figure" target="#fig_5">Figure 8</ref>, which indicates that concepts extracted by CNNs are not completely consistent with humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Further Analysis</head><p>The strategy of edge construction : Due to the image noise, computing edges only based on a single sample may cause large fluctuation. Instead, we propose to learn the edges dependent on the entire dataset. We adopt a momentum updating strategy (MUS) to filter the image noise formulated by <ref type="bibr" target="#b3">(4)</ref>. We plot the fluctuation amplitude curves of edges in <ref type="figure" target="#fig_6">Figure 9</ref> and see that the proposed momentum updating strategy obtains more stable edges. We further conducted an ablation study <ref type="table" target="#tab_4">(Table 3)</ref> to analyze the influence of MUS on the performance, where "PA-AVSL (w/o MUS)" denotes our framework without the momentum updating and ? is the momentum factor. We see that using the momentum updating strategy with a large momentum factor leads to the best performance, indicating the importance of stable edges.</p><p>The design of reliability: We conducted an ablation study about different designs of computing reliability as shown in <ref type="table" target="#tab_5">Table 4</ref>, where "PA-AVSL (LR)" represents learning the reliability by a fully-connected layer (i.e., ? l i = h(? l i )h(? l i )). The comparison demonstrates that a priori design is more effective than a learning-based one.</p><p>The effectiveness of reliability detection: We show the distribution of the reliability in <ref type="figure" target="#fig_7">Figure 10a</ref> and observe that only a few significantly unreliable nodes will trigger the top-down rectification. We further show the distribution of the coefficient of the sigmoid regression as <ref type="bibr" target="#b4">(5)</ref> in <ref type="figure" target="#fig_7">Figure 10b</ref>. The sigmoid regression acts like a filter and assigns a small number of inaccurate reliability estimations with small coefficients close to zero.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>- ure 2 Figure 2 .</head><label>22</label><figDesc>when only constructing H with the top layer similarity nodes ? L i and defining the overall similarity as d = r i=1 (? L i ) 2 . Taking full advantage of the hierarchy consistency in deep CNNs, we further propose an attributable visual simi-An illustration to show the difference between the proposed GSL and conventional DML. After extracting hierarchical features by applying CNNs, the conventional metric learning methods mainly focus on top-level features while the proposed GSL takes full advantage of features from all layers and the interactions between them to construct the similarity graph. larity learning (AVSL) framework composed of bottom-up similarity construction and top-down similarity inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An illustration to show how to construct edges. (a)The basic idea of how to compute correlations. We propose to recover nodes to spatial distributions employing CAMs and regard the overlap degree of corresponding distributions as the correlation between nodes. (b) The detailed operations. We first rescale each CAMs into the same size and then compute the convolution of two normalized CAMs as the correlation value of the edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Influence of hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the attribution result. We randomly select a triplet from CUB-200-2011 and rank the results according to the similarity among the 128 most reliable nodes for each sample pair. We use green and red boxes to denote positive and negative pairs, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of the similarity inference and attribution. We randomly selected two sample pairs from CUB-200-2011 and Cars196. For each pair of images, we attribute the overall similarity to the specific similarity nodes in an undirected graph. We report the corresponding values of reliabilities, nodes, and sensitivities under each node. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Fluctuation of edges during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Frequency histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Recall@K(%) on the test sets of CUB-200-2011, Cars196, and Stanford Online Products. Ablation study with different model settings.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CUB-200-2011</cell><cell></cell><cell></cell><cell cols="2">Cars196</cell><cell></cell><cell cols="3">Stanford Online Products</cell></row><row><cell>Methods</cell><cell cols="13">Setting R@1 R@2 R@4 R@8 R@1 R@2 R@4 R@8 R@1 R@10 R@100</cell></row><row><cell>HDC [49]</cell><cell cols="2">384G</cell><cell>53.6</cell><cell>65.7</cell><cell>77.0</cell><cell>85.6</cell><cell>73.7</cell><cell>83.2</cell><cell>89.5</cell><cell>93.8</cell><cell>70.1</cell><cell>84.9</cell><cell>93.2</cell></row><row><cell>DAML [6]</cell><cell cols="2">512BN</cell><cell>52.7</cell><cell>65.4</cell><cell>75.5</cell><cell>84.3</cell><cell>75.1</cell><cell>83.8</cell><cell>89.7</cell><cell>93.5</cell><cell>68.4</cell><cell>83.5</cell><cell>92.3</cell></row><row><cell>DVML [19]</cell><cell cols="2">512BN</cell><cell>52.7</cell><cell>65.1</cell><cell>75.5</cell><cell>84.3</cell><cell>82.0</cell><cell>88.4</cell><cell>93.3</cell><cell>96.3</cell><cell>70.2</cell><cell>85.2</cell><cell>93.8</cell></row><row><cell>Angular [39]</cell><cell cols="2">512G</cell><cell>53.6</cell><cell>65.0</cell><cell>75.3</cell><cell>83.7</cell><cell>71.3</cell><cell>80.7</cell><cell>87.0</cell><cell>91.8</cell><cell>67.9</cell><cell>83.2</cell><cell>92.2</cell></row><row><cell>DAMLRRM [43]</cell><cell cols="2">512G</cell><cell>55.1</cell><cell>66.5</cell><cell>76.8</cell><cell>85.3</cell><cell>73.5</cell><cell>82.6</cell><cell>89.1</cell><cell>93.5</cell><cell>69.7</cell><cell>85.2</cell><cell>93.2</cell></row><row><cell>DE-DSP [5]</cell><cell cols="2">512G</cell><cell>53.6</cell><cell>65.5</cell><cell>76.9</cell><cell>-</cell><cell>72.9</cell><cell>81.6</cell><cell>88.8</cell><cell>-</cell><cell>68.9</cell><cell>84.0</cell><cell>92.6</cell></row><row><cell>HDML [55]</cell><cell cols="2">512BN</cell><cell>53.7</cell><cell>65.7</cell><cell>76.7</cell><cell>85.7</cell><cell>79.1</cell><cell>87.1</cell><cell>92.1</cell><cell>95.5</cell><cell>68.7</cell><cell>83.2</cell><cell>92.4</cell></row><row><cell>A-BIER [25]</cell><cell cols="2">512G</cell><cell>57.5</cell><cell>68.7</cell><cell>78.3</cell><cell>86.2</cell><cell>82.0</cell><cell>89.0</cell><cell>93.2</cell><cell>96.1</cell><cell>74.2</cell><cell>86.9</cell><cell>94.0</cell></row><row><cell>ABE [14]</cell><cell cols="2">512G</cell><cell>60.6</cell><cell>71.5</cell><cell>79.8</cell><cell>87.4</cell><cell>85.2</cell><cell>90.5</cell><cell>94.0</cell><cell>96.1</cell><cell>76.3</cell><cell>88.4</cell><cell>94.8</cell></row><row><cell>MS [40]</cell><cell cols="2">512BN</cell><cell>65.7</cell><cell>77.0</cell><cell>86.3</cell><cell>91.2</cell><cell>84.1</cell><cell>90.4</cell><cell>94.0</cell><cell>96.5</cell><cell>78.2</cell><cell>90.5</cell><cell>96.0</cell></row><row><cell>SoftTriple [27]</cell><cell cols="2">512BN</cell><cell>65.4</cell><cell>76.4</cell><cell>84.5</cell><cell>91.6</cell><cell>86.1</cell><cell>91.7</cell><cell>95.0</cell><cell>97.3</cell><cell>78.3</cell><cell>90.3</cell><cell>95.9</cell></row><row><cell>Circle [34]</cell><cell cols="2">512BN</cell><cell>66.7</cell><cell>77.4</cell><cell>86.2</cell><cell>91.2</cell><cell>83.4</cell><cell>89.8</cell><cell>94.1</cell><cell>96.5</cell><cell>78.3</cell><cell>90.5</cell><cell>96.1</cell></row><row><cell>DCML [56]</cell><cell cols="2">512R</cell><cell>68.4</cell><cell>77.9</cell><cell>86.1</cell><cell>91.7</cell><cell>85.2</cell><cell>91.8</cell><cell>96.0</cell><cell>98.0</cell><cell>79.8</cell><cell>90.8</cell><cell>95.8</cell></row><row><cell>DIML [54]</cell><cell cols="2">512R</cell><cell>68.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.3</cell><cell>-</cell><cell>-</cell></row><row><cell>DRML [57]</cell><cell cols="2">512R</cell><cell>68.7</cell><cell>78.6</cell><cell>86.3</cell><cell>91.6</cell><cell>86.9</cell><cell>92.1</cell><cell>95.2</cell><cell>97.4</cell><cell>79.9</cell><cell>90.7</cell><cell>96.1</cell></row><row><cell>Margin [41]</cell><cell cols="2">512R</cell><cell>65.6</cell><cell>75.9</cell><cell>84.3</cell><cell>90.8</cell><cell>78.2</cell><cell>86.7</cell><cell>92.3</cell><cell>95.3</cell><cell>72.4</cell><cell>85.3</cell><cell>92.8</cell></row><row><cell>Margin-AVSL</cell><cell cols="2">512R</cell><cell>68.8</cell><cell>79.2</cell><cell>87.3</cell><cell>92.7</cell><cell>81.1</cell><cell>88.8</cell><cell>93.4</cell><cell>96.4</cell><cell>76.8</cell><cell>89.2</cell><cell>95.4</cell></row><row><cell>ProxyAnchor [13]</cell><cell cols="2">512R</cell><cell>69.7</cell><cell>80.0</cell><cell>87.0</cell><cell>92.4</cell><cell>87.7</cell><cell>92.9</cell><cell>95.8</cell><cell>97.9</cell><cell>78.4</cell><cell>90.5</cell><cell>96.2</cell></row><row><cell>ProxyAnchor-AVSL</cell><cell cols="2">512R</cell><cell>71.9</cell><cell>81.7</cell><cell>88.1</cell><cell>93.2</cell><cell>91.5</cell><cell>95.0</cell><cell>97.0</cell><cell>98.4</cell><cell>79.6</cell><cell>91.4</cell><cell>96.4</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">R@1 R@2 R@4 R@8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxyAnchor</cell><cell></cell><cell>87.7</cell><cell>92.9</cell><cell>95.8</cell><cell>97.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxyAnchor + M</cell><cell></cell><cell>89.7</cell><cell>93.9</cell><cell>96.3</cell><cell>97.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxyAnchor + M &amp; R</cell><cell></cell><cell>89.9</cell><cell>94.0</cell><cell>96.4</cell><cell>98.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ProxyAnchor + M (concat)</cell><cell>90.6</cell><cell>94.6</cell><cell>96.8</cell><cell>98.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProxyAnchor + AVSL</cell><cell></cell><cell>91.5</cell><cell>94.8</cell><cell>96.9</cell><cell>98.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of the edge construction.</figDesc><table><row><cell>Method</cell><cell cols="4">R@1 R@2 R@4 R@8</cell></row><row><cell>PA</cell><cell>87.7</cell><cell>92.9</cell><cell>95.8</cell><cell>97.9</cell></row><row><cell cols="2">PA-AVSL (w/o MUS) 91.0</cell><cell>94.6</cell><cell>96.7</cell><cell>98.1</cell></row><row><cell>PA-AVSL (? = 0.50)</cell><cell>91.5</cell><cell>95.0</cell><cell>97.0</cell><cell>98.4</cell></row><row><cell>PA-AVSL (? = 0.95)</cell><cell>91.6</cell><cell>95.2</cell><cell>97.2</cell><cell>98.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of the reliability estimation.</figDesc><table><row><cell></cell><cell cols="4">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">R@1 R@2 R@4 R@8</cell></row><row><cell></cell><cell cols="2">PA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">87.7</cell><cell>92.9</cell><cell>95.8</cell><cell>97.9</cell></row><row><cell></cell><cell cols="11">PA-AVSL (LR) 90.9</cell><cell>94.6</cell><cell>96.6</cell><cell>98.0</cell></row><row><cell></cell><cell cols="5">PA-AVSL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">91.5</cell><cell>95.0</cell><cell>97.0</cell><cell>98.4</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Probability</cell><cell>0.08 0.1 0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Reliability</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">(a) Reliability</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">More detailed inference graphs of samples from both CUB-200-2011 and Cars196 are included in the appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 62125603 and Grant U1813218, and in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Loss Functions</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hybrid-attention based decoupled metric learning for zero-shot image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2750" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting grad-cam for embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2794" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep embedding learning with discriminative sampling policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep adversarial metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Local similarity-aware deep feature embedding. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1610.08904</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Proxy anchor loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="736" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consistent-aware deep learning for person reidentification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5771" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-manifold deep metric learning for image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1137" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno>abs/1912.01703</idno>
		<title level="m">An imperative style, high-performance deep learning library. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing deep similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2029" to="2037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning hierarchical similarity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakul</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2280" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Petryk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>abs/2004.00221</idno>
		<title level="m">Nbdt: neural-backed decision trees. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2593" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond sparsity: Tree regularization of deep models for interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep asymmetric metric learning via rich relationship mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What makes objects similar: A unified multimetric learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue-Min</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1235" to="1243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno>abs/1411.1792</idno>
		<title level="m">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<idno>abs/1506.06579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hard-aware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Signal-to-noise ratio: A robust distance metric for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4815" to="4824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Matthew D Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpreting cnn knowledge via an explanatory graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">terpretable convolutional neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards interpretable deep metric learning with structural matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaodong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep compositional metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9320" to="9329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep relational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Visual explanation for deep metric learning. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
