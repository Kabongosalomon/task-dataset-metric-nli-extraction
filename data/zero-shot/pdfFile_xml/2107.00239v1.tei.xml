<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generic Event Boundary Detection Challenge at CVPR 2021 Technical Report: Cascaded Temporal Attention Network (CASTANET)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiang</forename><surname>Hong</surname></persName>
							<email>hongdexiang19@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
							<email>licongcong18@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
							<email>longyin.wen@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Bytedance Inc. Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
							<email>xinyao.wang@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Bytedance Inc. Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generic Event Boundary Detection Challenge at CVPR 2021 Technical Report: Cascaded Temporal Attention Network (CASTANET)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report presents the approach used in the submission of Generic Event Boundary Detection (GEBD) Challenge at CVPR21. In this work, we design a Cascaded Temporal Attention Network (CASTANET) for GEBD, which is formed by three parts, the backbone network, the temporal attention module, and the classification module. Specifically, the Channel-Separated Convolutional Network (CSN) is used as the backbone network to extract features, and the temporal attention module is designed to enforce the network to focus on the discriminative features. After that, the cascaded architecture is used in the classification module to generate more accurate boundaries. In addition, the ensemble strategy is used to further improve the performance of the proposed method. The proposed method achieves 83.30% F1 score on Kinetics-GEBD test set, which improves 20.5% F1 score compared to the baseline method. Code is available at https://github.com/DexiangHong/Cascade-PC. arXiv:2107.00239v1 [cs.CV] 1 Jul 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Approach</head><p>The task of Generic Event Boundary Detection (GEBD) is introduced by <ref type="bibr" target="#b4">[5]</ref>, which aims to localize the moments where humans naturally perceive taxonomy-free event boundaries that break a longer event into shorter temporal segments. The main challenge of GEBD is the taxonomy-free nature of event boundary, which requires the predictions of the detector matching at least one annotator. To tackle this problem, we design a Cascaded Temporal Attention Network (CASTANET), which consists of three parts, the backbone network, the temporal attention mod-* Both authors contributed equally to this work. ule and the cascaded classification module. The Channel-Separated Convolutional Network (CSN) <ref type="bibr" target="#b6">[7]</ref> is used as the backbone network to extract features. After that, the temporal attention module is designed to enforce the network to focus on the discriminative features. Finally, the cascaded classification module is used to generate the accurate boundaries. Notably, the ensemble strategy is used to further improve the performance of the proposed method. The overall architecture of the proposed method is presented in <ref type="figure" target="#fig_1">Figure 1</ref>. We will describe each module in more detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Backbone Network</head><p>The Channel-Separated Convolutional Networks (CSN) <ref type="bibr" target="#b6">[7]</ref> is first designed for video classification, which separates the channel interactions and spatiotemporal interactions to balance the accuracy and efficiency. Specifically, all the convolutional operations in CSN are separated into the pointwise 1 ? 1 ? 1 convolutions for channel interactions and depthwise 3 ? 3 ? 3 convolutions for local spatiotemporal interactions. To extract more discriminative features for GEBD, we use CSN as the backbone network to extract features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Temporal Attention Module</head><p>Temporal component. Temporal context information is crucial to determine the event boundaries. Some event boundaries may need more contextual frames while others may need only a few frames to determine whether are boundaries. To increase the receptive field of different layers and capture contextual information of different resolutions, we stack several dilated 1D convolution layers in temporal domain. Inspired by the MS-TCN++ <ref type="bibr" target="#b3">[4]</ref> architecture, we use a dilation factor that is doubled at each layer, i.e. {1, 2, ? ? ? , 2 L?1 }, where L is the total number  of layers(L = 4 in our experiments). Each layer has the same number of convolutional filters and applies a dilated convolution with ReLU activation to the output of the previous layer. We further use residual connections to facilitate gradients flow. The set of operations at each layer can be formally described as follows:</p><formula xml:id="formula_0">C l = ReLU(W d * C l?1 ),<label>(1)</label></formula><formula xml:id="formula_1">C l = C l?1 + W * ? l ,<label>(2)</label></formula><p>where C l is the output of layer l, * denotes the convolution operator, W d ? R 3?D?D are the weights of the dilated convolution filters with kernel size 3 and D is the number of convolutional filters, W ? R 1?D?D are the weights of a 1 ? 1 convolution, and bias parameters are omitted for simplicity. Meanwhile, we use a bi-directional LSTM <ref type="bibr" target="#b2">[3]</ref> to capture temporal information, which can process sequences of video frames in the temporal space. Then a 3d max pooling is applied on the LSTM output, which extracts the max activations among video frames at different time point. This operation flattens the sequential input video frames into the features v tem ? R c and maximize event boundary activations when there exists a boundary in the input video frames. Attention component. GEBD requires to fully understand the semantics of each input video frame. Thus, it is necessary to learn the textural attention for boundary detection simultaneously. The Transformer <ref type="bibr" target="#b5">[6]</ref> is a model that uses self attention to boost the performance of neural machine translation. We adapt the same architecture as in <ref type="bibr" target="#b5">[6]</ref> and stack 4 self attention layers to process the input frames. Then a 3d max pooling is also applied on the attention output, producing the attention features v att ? R c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Cascaded Classification Module</head><p>Taking the extract features from the temporal attention module, we use a fully connected layer to produce the clas- sification scores of the boundaries, which is computed as follows:</p><formula xml:id="formula_2">H 1 S 1 H 2 S 2 H 3 S 3 ! " ! " # ! " #</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Samples</head><formula xml:id="formula_3">S = softmax(FC([v tem , v att ])),<label>(3)</label></formula><p>where v tem and v att are the features produced by the temporal and attention components, and [?, ?] indicates concatenation in the channel dimensions. Improving both recall and precision for GEBD is a dilemma. Since the event boundary is taxonomy-free, it is difficult to determine if a frame is the event boundary or not. The official evaluation protocol <ref type="bibr" target="#b4">[5]</ref> addresses this problem by using Rel.Dis., which indicates the error between the detected and ground truth timestamps, divided by the length of the corresponding action. Then given a fixed threshold u for Rel.Dis., we can determine whether a detection is correct (i.e. ? threshold) or not (i.e. &gt; threshold). When u is high, the positive samples contain more ambiguous frames, and make the classifiers confused and have little incentive to reject close false positives. When u is low, the positive is more accurate, but it is difficult to assemble enough positive training examples. Inspire by Cascade R-CNN <ref type="bibr" target="#b0">[1]</ref>, we propose the cascaded architecture to gradually refine the classification results. The cascaded classifies are trained sequentially, using the output from the previous stages.</p><p>Specifically, given a sequential of classification heads {H 1 , H 2 , ? ? ? , H N } of identical architecture (gray dashed box in <ref type="figure" target="#fig_1">Figure 1</ref>, initialized with different parameters) where N is the number of cascaded heads, we train these classifier heads using different ground truth labels {L 1 , L 2 , ? ? ? , L N } which are produced by decreased ground truth Rel.Dis. threshold {u 1 , u 2 , ? ? ? , u N }, where u 1 ? u 2 ? ? ? ? ? u N . Then the output classification score S n of H n is used as a mask with threshold ? n to filter out easy negative samples and the next stage detection head H n+1 further refines positive samples using previous head's mask by utilizing more accurate ground truth labels L n+1 . The mask thresholds {? 1 , ? 2 , ? ? ? , ? N ?1 } used to filter out easy negative samples are monotonically increasing, i.e., ? 1 &lt; ? 2 &lt; ? ? ? &lt; ? N ?1 . The cascaded classifier architecture is shown in <ref type="figure" target="#fig_2">figure 2</ref>.</p><p>During inference, the final output is computed by averaging all the outputs of the detection heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Audio Features</head><p>Joint audiovisual learning is the core to human perception. For many video understanding tasks, audio could be very helpful <ref type="bibr" target="#b7">[8]</ref>. We extract the audio from the original videos and use STFT to convert the audio representation into the frequency domain. Then four 1D convolution layers are adopted to extract the temporal feature of the audio inputs. Then we concatenate the audio feature with visual features and feed them to the linear classification layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Implementation Details</head><p>Dataset. We randomly sample 2000 videos from kinetics-GEBD validation set to construct local validation set and use all the other data for training.</p><p>Network architecture. We use Channel-Separated Convolutional Networks (CSN) <ref type="bibr" target="#b6">[7]</ref> pretrained on the IG-65M <ref type="bibr" target="#b1">[2]</ref> and kinetics-400 as the feature encoder. The pretrained weight is released in the mmaction2 repository 1 . Specifically, we modify the temporal strides of the CSN backbone from [1, 2, 2, 2] to [1, 1, 1, 1]. As a result, The CSN backbone produces feature embeddings with the original temporal resolution. We use 4 dilated temporal convolutional layers with dilation rates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> followed by 1 bidirectional LSTM layer in temporal module and 4 self atten-  <ref type="bibr" target="#b5">[6]</ref> in attention module. The outputs of temporal module and attention module are concatenated and then fed into a fully connected layer to make prediction. For each training sample, the input dimension is 2n ? 224 ? 224, where n = 8 is the number of frames before and after the candidate time position t, 224 is the height and width of the resized frame. Dynamic sampling. Videos in the Kinetics-GEBD dataset are recorded in different FPS. Different from the baseline method that samples 1 frame for every 3 frames, we determine the sampling frequencies based on the recorded FPS of videos. Notably, given the video recorded with f FPS, we sample 1 frame for every f /8 frames. Intuitively, the larger the FPS is, the more repeated frames it contains. So fixed frame sampling strategy may include redundant information for larger FPS while lack necessary contextual information for smaller FPS. Our dynamic sampling strategy captures contextual information with a fixed ratio, which makes input information consistent and is benefit for network convergence.</p><p>Training. We train our model on 8 Tesla V100 GPUs using PyTorch 1.8 and use a mini-batch of 32 clips per GPU, thus making a total mini-batch of 256 clips. Training is done in 1 epoch since the performance drops if training longer. We use SGD optimizer with the momentum of 0.9 and the weight decay of 5e ?4 . The learning rate is set to 1e ?2 . Automatic mixed precision training 2 is also adapted to reduce GPU memory burden.</p><p>Testing. For each candidate boundary time t, we average all the scores of the cascade classifiers which are trained using different threshold-level ground truth labels presented in section1.3. Then we watershed the probability sequence to obtain internals above 0.5 and each internal's center is treated as an event boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ablation Studies</head><p>In this section, we conduct several ablation experiments to comprehensively understand the performance contribution of the proposed framework. The contributions of each module are shown in <ref type="table">Table 1</ref>. We report the results on the local validation set. We can observe that with these methods, we improve the F1 score from 0.615 to 0.814. Specifically, we set cascade classifier's mask threshold ? to [0.4, 0.3] and ground truth Rel.Dis. threshold u to [0.5, 0.4, 0.3]. Finally, we finetune the mask threshold and ground truth Rel.Dis. threshold and ensemble 9 models to produce the final results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The architecture of the proposed cascaded temporal attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The architectures of cascaded classification module."H" indicates classification head, "L" indicates ground truth labels and "S" indicates the classification scores. u and ? are the ground truth threshold and mask threshold, respectively.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https : / / pytorch . org / docs / stable / notes / amp _ examples.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Overall Results</head><p>We submit our results on the Kinetics-GEBD test set and achieve 83.30 F1 scores. Specifically, we ensemble 9 models with different parameters. The results are shown in <ref type="table">Table  2</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MS-TCN++: multi-stage temporal convolutional network for action segmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generic event boundary detection: A benchmark for event segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno>abs/2101.10511</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audiovisual slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08740</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
