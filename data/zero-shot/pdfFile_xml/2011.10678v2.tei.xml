<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open-Vocabulary Object Detection Using Captions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
							<email>azareian@snap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
							<email>kevin.delarosa@snap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Open-Vocabulary Object Detection Using Captions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the remarkable accuracy of deep neural networks in object detection, they are costly to train and scale due to supervision requirements. Particularly, learning more object categories typically requires proportionally more bounding box annotations. Weakly supervised and zero-shot learning techniques have been explored to scale object detectors to more categories with less supervision, but they have not been as successful and widely adopted as supervised models. In this paper, we put forth a novel formulation of the object detection problem, namely openvocabulary object detection, which is more general, more practical, and more effective than weakly supervised and zero-shot approaches. We propose a new method to train object detectors using bounding box annotations for a limited set of object categories, as well as image-caption pairs that cover a larger variety of objects at a significantly lower cost. We show that the proposed method can detect and localize objects for which no bounding box annotation is provided during training, at a significantly higher accuracy than zero-shot approaches. Meanwhile, objects with bounding box annotation can be detected almost as accurately as supervised methods, which is significantly better than weakly supervised baselines. Accordingly, we establish a new state of the art for scalable object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the most prominent applications of artificial intelligence, and one of the most successful tasks for deep neural networks. However, despite the tremendous progress in deep object detection, such as Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> and its impressive accuracy, training such models requires expensive and time-consuming human supervision. Particularly, one needs to manually annotate at least thousands of bounding boxes for each object category of interest. Although such efforts have been already made and there are valuable datasets publicly available, such as Open Images <ref type="bibr" target="#b20">[21]</ref> and MSCOCO <ref type="bibr" target="#b24">[25]</ref>, these datasets cover <ref type="bibr">Figure</ref> 1. An overview of Open-Vocabulary Object Detection. We propose a two-stage training framework where we first (1) construct a visual-semantic space using low-cost image-caption pairs, and then (2) learn object detection using object annotations for a set of base classes. During test (3), the goal is to detect object categories beyond base classes, by exploiting the semantic space. a limited set of object categories (e.g. 600), despite requiring extensive resources. Extending object detection from 600 to 60,000 categories requires 100 times more resources, which makes versatile object detection out of reach.</p><p>Nevertheless, humans learn to recognize and localize objects effortlessly through natural supervision, i.e., exploring the visual world and listening to others describing situations. Their lifelong learning of visual patterns and associating them with spoken words results in a rich visual and semantic vocabulary that can be used not only for detecting objects, but for other tasks too, such as describing objects and reasoning about their attributes and affordances. Although drawing bounding boxes around objects is not a task that humans naturally learn, they can quickly learn it using few examples, and generalize it well to all types of objects, without needing examples for each object class.</p><p>In this paper, we imitate this human ability, by designing a two-stage framework named Open-Vocabulary object Detection (OVD). We propose to first use a corpus of image-caption pairs to acquire an unbounded vocabulary of concepts, simulating how humans learn by natural supervision, and then use that knowledge to learn object detection (or any other downstream task) using annotation for only some object categories. This way, costly annotation is only needed for some categories, and the rest can be learned using captions, which are much easier to collect, and in many cases freely available on the web <ref type="bibr" target="#b32">[33]</ref>. <ref type="figure">Figure 1</ref> illustrates the proposed OVD framework, which is novel and efficient, enables versatile real-world applications, and can be generalized to other computer vision tasks.</p><p>More specifically, we train a model that takes an image and detects any object within a given target vocabulary V T . To train such a model, we use an image-caption dataset covering a large variety of words denoted as V C as well as a much smaller dataset with localized object annotations from a set of base classes V B . Note that in this task, target classes are not known during training, and can be any subset of the entire language vocabulary V ? . This is in contrast with most existing object detection settings including weakly supervised transfer learning methods, where V T should be known beforehand <ref type="bibr" target="#b36">[37]</ref>. The most similar task to OVD is zero-shot object detection, which also generalizes to any given target set, but cannot utilize captions. <ref type="figure">Figure 2</ref> illustrates an intuitive abstraction of our proposed task compared to zero-shot and weakly supervised detection. Despite close connections to those well-known ideas, OVD is novel and uniquely positioned in the literature, as we elaborate in Section 2.</p><p>To address the task of OVD, we propose a novel method based on Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, which is first pretrained on an image-caption dataset, and then fine-tuned on a bounding box dataset, in a particular way that maintains the rich vocabulary learned during pretraining, enabling generalization to object categories without annotation. Through extensive experiments, we evaluate our method, Open Vocabulary R-CNN (OVR-CNN), and show that it achieves significantly higher performance than the state of the art in zero-shot learning (27% mAP compared to 10%). We also show that it outperforms weakly supervised object detectors by a significant margin in generalized zero-shot settings (40% mAP compared to 26%). We supplement the paper with comprehensive open-source code to reproduce results. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Zero-shot object detection (ZSD) aims to generalize from annotated (seen) object classes to other (unseen) categories. The key idea is to use zero-shot learning techniques (e.g. word embedding projection <ref type="bibr" target="#b12">[13]</ref>) to learn object proposal classification. Bansal et al. <ref type="bibr" target="#b2">[3]</ref> argued the main challenge in ZSD is modeling the background class, which is hard to separate from unseen classes. They defined background as a mixture model, which was later improved by <ref type="bibr">Figure 2</ref>. A comparison of our proposed OVD with existing ZSD and WSD paradigms. While zero-shot detection methods learn a limited set of base classes VB and struggle to generalize to target classes VT , we acquire a much larger vocabulary VC by learning from low-cost image-caption pairs. Although there are weakly supervised approaches that can learn from captions, they cannot use bounding box supervision from base classes, and need to know VT before training. Hence, our OVD formulation is a generalization of ZSD and WSD, which can use both data sources to reach an outstanding performance on target classes not known in advance.</p><p>the introduction of polarity loss <ref type="bibr" target="#b29">[30]</ref>. On the other hand, Zhu et al. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> argued the key to ZSD is to improve the generalization ability of object proposal models. They employed a generative model to hallucinate unseen classes and augment seen examples when training the proposal model. Nevertheless, ZSD methods are still far from practical performance, due to their unnecessarily harsh constraint, i.e., not having any example of unseen objects, and having to guess how they look like solely based on their word embeddings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> or textual descriptions <ref type="bibr" target="#b23">[24]</ref>. This has motivated recent papers to simplify the task by making unrealistic assumptions, such as the availability of test data during training <ref type="bibr" target="#b28">[29]</ref>, or the availability of unseen class annotations to filter images with unseen object instances <ref type="bibr" target="#b14">[15]</ref>. Considering datasets with natural, weak supervision are abundant and cheap, we propose an alternative, more realistic problem: Besides annotated data for "seen" classes, we assume an image-caption dataset is available that covers a larger variety of objects with an open vocabulary. This allows us to achieve 27% mAP on unseen classes, compared to the 10% state of the art, without much extra annotation effort. To this end, we address the open problem of knowledge transfer from image-caption pretraining to object detection.</p><p>Weakly supervised object detection (WSD) is the most widely used approach to train object detectors without bounding box annotations, by using image-level labels instead. The main challenge of WSD is localization, as each label may refer to any object in the image. This problem is typically addressed using multiple instance learning, which is a well-studied topic <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8]</ref>. Although image-level labels are easier to collect than bounding boxes, they still require manual effort, and they are typically limited to a predefined taxonomy. In contrast, we use captions, which are more natural to annotate and often freely available on the web, while also featuring a rich, open vocabulary of concepts. Learning object detection from captions has been studied at a limited scale. Cap2Det <ref type="bibr" target="#b42">[43]</ref> parses captions into multi-label classification targets, which can be used to train a WSD model. However, that requires image-level labels to train the caption parser, and is limited to a closed vocabulary. Amrani et al. <ref type="bibr" target="#b1">[2]</ref> train a WSD model based on the presence of a predefined set of words in captions, which is similarly closed-vocabulary, and discards the rich semantic content of captions, which we exploit through transformers. In contrast, Sun et al. <ref type="bibr" target="#b34">[35]</ref> and Ye et al. <ref type="bibr" target="#b43">[44]</ref> aim to discover an open set of object classes from image-caption corpora, and learn detectors for each discovered class. A key limitation of all such WSD methods is their inferior object localization accuracy. In contrast, we disentangle object recognition and localization into two independent problems. We learn recognition using open-vocabulary captions, while learning localization using a fully annotated dataset from a small subset of classes.</p><p>Object detection using mixed supervision has been studied in order to exploit both weak and full supervision. However, most existing methods need bounding box annotations for all classes, and use weak supervision only as auxiliary data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>. More similar to our work are those which transfer a detector trained on supervised base classes to weakly supervised target classes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. These methods usually lose performance on base classes as we show in Section 4. In contrast, we treat this problem as an opposite knowledge transfer process: Instead of training on base classes first, and transferring to target classes using weakly supervised learning, we first use captions to learn an open-vocabulary semantic space that includes target classes, and transfer that to the task of object detection via supervised learning. Another limitation of all weakly supervised and mixed-supervision methods is that they require image-level annotations within a predefined taxonomy, and they only learn those predefined classes. In contrast, we use captions which are open-vocabulary and also more prevalent on the web, and we learn to generalize to any set of target classes on demand, without having to know them beforehand. VirTex <ref type="bibr" target="#b10">[11]</ref> is the only method that uses captions as well as object annotations to train a detector, but it needs annotation for all object classes while we can generalize from a subset of annotated categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual grounding of referring expressions can be seen as an open-vocabulary object localization problem:</head><p>Given an image and a noun phrase that refers to an object, usually within the context of a full caption, the goal is to localize the referred object in the image using a bounding box. We are inspired by the rich literature of weakly supervised visual grounding methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref> to design our imagecaption pretraining technique. More specifically, we learn to map caption words to image regions, by learning a visualsemantic common space. However, such a mapping alone cannot be used to detect objects during inference when no caption is provided. Therefore, we propose to transfer visual grounding knowledge to the task of object detection through another phase of training.</p><p>Vision-language transformers Our framework of pretraining with image-captions and transferring the learned knowledge to the downstream task is inspired by the recent success of multimodal transformers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref>. These methods train transformers in a self-supervised manner to take image-caption pairs as input and extract versatile features that can be fine-tuned on various downstream vision-language tasks. However, they have not been applied to object detection yet, since they need both image and caption as input, and also because they rely on a pretrained object detector to articulate the image before feeding into transformers. Recently, PixelBERT <ref type="bibr" target="#b18">[19]</ref> removed the latter requirement by applying transformers directly on the feature map. We utilize and extend PixelBERT by devising a vision-to-language projection module before feeding visual features into the transformer, and by incorporating a visual grounding cost function into the pretraining process. Moreover, we propose to transfer the weights pretrained via multi-modality transformers to the single-modality downstream task of object detection. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the architecture of our proposed method, which is based on a Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> trained in a zero-shot manner. More specifically, it is trained on a set of base classes V B , and tested on another set of target classes V T . To this end, pretrained word embeddings (e.g. GloVE <ref type="bibr" target="#b27">[28]</ref>) are often used instead of conventional, trainable classifiers, so that target class embeddings can replace base class embeddings during testing, without changing the model's output semantic space. Nevertheless, this practice often leads to severe overfitting due to the small sample of base classes, to the point where the state-of-the-art mAP on target classes is 9 times lower than base classes <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>To alleviate this problem, our key idea is to pretrain the visual backbone on a larger vocabulary V C to learn a more complete semantic space rather than a small number of base classes. Since captions are naturally written without much constraint on the vocabulary, they are a perfect source for learning a rich and complete visual-semantic space. We name this framework Open Vocabulary Object Detection (OVD), as there are no explicit limits on the vocabulary of objects that can be learned through captions. In practice, our vocabulary is not literally "open", as it is limited to pretrained word embeddings. However, word embeddings are typically trained on very large text corpora such as Wikipedia that cover nearly every word <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In the rest of this section, we elaborate how we pretrain our Open Vocabulary faster R-CNN (OVR-CNN) on imagecaption pairs, and how we transfer the pretraining knowledge to the downstream task. In Section 4, we demonstrate that our method closes the base-target performance gap from a ratio of 9 to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning a visual-semantic space</head><p>Object detectors typically use a CNN backbone that is often pretrained for ImageNet classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>. Pretraining results in a backbone that can extract features optimized for object recognition, which is then used to train a new classification head for a fixed set of annotated classes. This is problematic in zero-shot settings, as a classifier trained on base classes cannot recognize target classes. Therefore, zero-shot methods learn a linear projection from visual features to pretrained base class embeddings by replacing classifier weights with a fixed embeddings matrix <ref type="bibr" target="#b12">[13]</ref>. This way, the network is expected to generalize to target classes by assuming the continuity of the embedding space. Nevertheless, this approach is prone to overfitting, as projecting to a small number of the embedding space (base class embeddings) is an under-determined problem <ref type="bibr" target="#b2">[3]</ref>.</p><p>To prevent overfitting, we propose to learn the aforementioned Vision to Language (V2L) projection layer along with the CNN backbone during pretraining, where the data is not limited to a small set of base classes. To this end, we use an image-caption dataset, since captions contain a rich vocabulary and semantic structure that can be used to learn the meaning of words, including object names. To effectively learn from the rich supervision that captions provide, we exploit recent advances in visual grounding and visionlanguage transformers. We use a main (grounding) task as well as a set of auxiliary self-supervision tasks to learn a robust CNN backbone and V2L layer. In the next subsection, we elaborate how we transfer the pretrained modules to learn open-vocabulary object detection.</p><p>Our pretraining architecture resembles PixelBERT <ref type="bibr" target="#b18">[19]</ref>: it takes image-caption pairs as input, feeds the image into a visual backbone and the caption into a language backbone, which results in a set of token embeddings for the image and caption, and then feeds those token embeddings into a multimodal transformer to extract multimodal embeddings. Our visual backbone is a ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, which takes a w ? h image I as input and extracts a grid of w/32?h/32 regions, where each region i is represented by a d v -dimensional feature vector, r I i . For the language backbone, we use a pretrained BERT <ref type="bibr" target="#b11">[12]</ref>, which takes a tokenized caption C as input, extracts a d l -dimensional word embedding e C j for each token j, augments that with position embeddings, and applies several layers of multi-head self-attention to extract d l -dimensional contextualized token embeddings f C j . Furthermore, we devise a linear V2L layer that maps each visual region representation r I i into the language embedding space e I i . The final embeddings of image regions {e I i } and caption tokens {f C j } are then aggregated and fed into a multimodal transformer, which is similar to BERT in architecture, but performs attention not only within each modality but also across the two modelities. The output of the multimodal transformer is {m I i } and {m C j } for the regions and words respectively, which can be used for various pretraining tasks, as we discuss later in this section.</p><p>Once we extract the aforementioned stages of unimodal and multimodal embeddings from a batch of image-caption pairs, we define a main objective function as well as various auxiliary objectives to ensure an effective training for the ResNet parameters, as well as the V2L layer. Our main objective is visual grounding, i.e., word embeddings from each caption e C j should be close to their corresponding image regions e I i . Since the correspondence of words and regions is not given, we employ a weakly supervised grounding technique to learn it. Specifically, we define a global grounding score for each image-caption pair, that is a weighted average of local grounding scores for word-region pairs:</p><formula xml:id="formula_0">I, C G = 1 n C n C j=1 n I i=1 a i,j e I i , e C j L ,<label>(1)</label></formula><p>where ., . L is the dot product of two vectors, n I and n C are the number of image and caption tokens, and</p><formula xml:id="formula_1">a i,j = exp e I i , e C j L n I i =1 exp e I i , e C j L .<label>(2)</label></formula><p>The global grounding score for a matching image-caption pair should be maximized, while it should be minimized for a non-matching pair. Hence, we use other images in the batch as negative examples for each caption, and use other captions in the batch as negative examples for each image.</p><p>Accordingly, we define two grounding objective functions:</p><formula xml:id="formula_2">L G (I) = ? log exp I, C G C ?B C exp I, C G ,<label>(3)</label></formula><p>and</p><formula xml:id="formula_3">L G (C) = ? log exp I, C G I ?B I exp I , C G ,<label>(4)</label></formula><p>where B I and B C are the image and caption batch. We validated the described formulation by completing extensive experimentation with various other alternatives, such as other similarity metrics (e.g. cosine instead of dot product), other loss functions (e.g. triplet loss instead of negative log likelihood) and other word-to-region alignment mechanisms (e.g. hard alignment instead of softmax).</p><p>Optimizing the grounding objectives results in a learned visual backbone and V2L layer that can map regions in the image into words that best describe them, without limiting to a closed vocabulary. However, since we induce a weak, indirect supervision, a local optima might be achieved where the model only learns the minimum concepts necessary to choose the right image/caption. To more directly learn each word, we employ masked language modeling following PixelBERT <ref type="bibr" target="#b18">[19]</ref>. Specifically, we randomly replace some words j in each caption C with a [MASK] token, and try to use the multimodal embedding of the masked token m C j to guess the word that was masked. To this end, the visual backbone and the V2L layer should learn to extract all objects that might be described in captions, and the multimodal transformer should learn to use those along with the language understanding ability of BERT to determine what word completes the caption best.</p><p>Accordingly, we apply a fully connected layer on m C j , compare its output to all word embeddings using dot product, and apply softmax to compute a probability score for each word. We define masked language modeling L M LM as a cross-entropy loss comparing the predicted distribution with the actual word that was masked. PixelBERT also employs an image-text matching loss L IT M , but does not use masked visual modeling that is common in vision-language transformers <ref type="bibr" target="#b25">[26]</ref>. We follow their choices for our auxiliary objectives, although other combinations are possible. We train the visual backbone, V2L layer, and the multimedia transformer jointly by minimizing the total loss for each image-caption pair:</p><formula xml:id="formula_4">L(I, C) = L G (I) + L G (C) + L M LM + L IT M . (5)</formula><p>Note that our language backbone (BERT) and its word embeddings are fixed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning open-vocabulary detection</head><p>Once the ResNet visual backbone and V2L layer are trained, we transfer them to the task of object detection, by initializing a Faster R-CNN. Following <ref type="bibr" target="#b31">[32]</ref>, we use the stem and the first 3 blocks of our pretrained ResNet to extract a feature map from a given image. Next, a region proposal network slides anchor boxes on the feature map to predict objectness scores and bounding box coordinates, followed by non-max suppression and region-of-interest pooling to get a feature map for each potential object. Finally, following <ref type="bibr" target="#b31">[32]</ref>, the 4th block of our pretrained ResNet is applied on each proposal followed by pooling to get a final feature vector r I i for each proposal box, which is typically fed into a classifier in supervised settings.</p><p>Nevertheless, in our zero-shot setting, a linear layer is applied on the visual features r I i to map each proposal onto a word embedding space e I i , so they can be compared to base or target class embeddings in the training or testing phase respectively. In all ZSD methods, the aforementioned linear layer is trained from scratch on base classes, which struggles to generalize. In contrast, we have already trained the V2L layer in the pretraining phase, on a much broader semantic space. The main difference of this phase with pretraining is that instead of the grid-structured feature map, r I i represents a bounding box of arbitrary shape. However, due to the linear characteristics of RoI-Align <ref type="bibr" target="#b15">[16]</ref>, r I i is on the same space as in pretraining, with minimal domain shift that can be eliminated by fine-tuning the ResNet backbone.</p><p>During training, we compare e I i to each base class k to compute classification scores:</p><formula xml:id="formula_5">p(i classified as k) = exp e I i , e V k 1 + k ?V B exp e I i , e V k ,<label>(6)</label></formula><p>where e V k is the pretrained embedding of word k, V B is the set of base classes, and ., . denotes dot product. The addition of 1 in the denominator is because we set the background class to have a fixed, all-zero embedding, which makes any dot product zero, and is exponentiated to 1. We found that a fixed all-zero background embedding performs better than a trainable one as it does not push non-foreground bounding boxes, which may contain target classes, to an arbitrary region of the embedding space.</p><p>Except for the aforementioned changes in the classification head, the rest of our network exactly follows Faster R-CNN, and is trained in the exact same way with the same objective functions. Empirically, we found that multiplying a ratio ? to the classification loss of background proposals (i.e., proposal boxes that are not matched with any ground truth bounding box) can improve the performance on target classes significantly, while slightly lowering base class performance. Hence, we use cross-validation to find the best ? for each model. The ResNet parameters are finetuned, while the region proposal network and the regression head are trained from scratch. The classifier head is fully fixed, as it consists of a pretrained V2L layer and word embeddings, which are especially prone to overfitting. During testing, we use the model just like a Faster R-CNN, except we can replace word embeddings in Eq. (6) with any set of target classes V T . While we evaluate on a fixed, annotated target set, the model is not particularly tuned for those classes, and hence can be deployed on the entire vocabulary V ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we demonstrate our method's ability to detect objects of the target classes accurately, while not losing its accuracy on the base classes compared to supervised approaches. Particularly, we show significant quantitative improvements compared to zero-shot and weakly supervised object detection methods, followed by a comprehensive analysis including ablation and visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data and metrics</head><p>We base our experiments on the challenging and widely used COCO Objects dataset <ref type="bibr" target="#b24">[25]</ref>. We use their 2017 training and validation split for training and evaluation respectively. To select base and target classes, we adopt the split proposed by <ref type="bibr" target="#b2">[3]</ref> and used by all other ZSD methods. Their splits includes 48 base classes and 17 target classes, which are both subsets of COCO object classes. We remove any bounding box that is not labeled with a base class from training data, and remove images that are left with no bounding boxes. This leaves us with 107,761 training images that contain 665,387 instances of base classes, and 4,836 test images that contain 28,538 instances of base classes and 4,614 instances of target classes.</p><p>Unless otherwise mentioned, for pretraining we use COCO Captions <ref type="bibr" target="#b5">[6]</ref>, which is based on the same images and same train/test split as COCO Objects. This dataset is preferred due to the matching domain with the downstream task. However, to study more realistic settings, we also report results by pretraining on Conceptual Captions (CC) <ref type="bibr" target="#b32">[33]</ref>, which was automatically collected from the web. CC is larger with 2,749,293 training image-caption pairs, compared to COCO with 118,287 images and 5x captions. Both COCO and CC cover very broad vocabularies that include all base and target classes in our experiments. Although there is no theoretical limit for our model to predict words outside the caption vocabulary V C <ref type="figure">(Figure 2</ref>), we do not study those unusual cases in this paper.</p><p>Following most ZSD and WSD methods, we evaluate using mean Average Precision (mAP) at an IoU of 0.5. We compute mAP on base classes by directly applying the model on COCO validation images and using base class annotations to evaluate. Then we replace the classifier head with target class embeddings and apply on the same dataset, but this time compare with target class annotations. These result in base and target mAP, which resemble supervised and zero-shot settings respectively. We also replace the classifier head with the union of base and target class em-beddings, to mimic generalized zero-shot settings <ref type="bibr" target="#b29">[30]</ref>. In that case, we report total mAP, as well as separately computing the mean of AP over base and target classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We used the maskrcnn-benchmark code base <ref type="bibr" target="#b26">[27]</ref>, and particularly the R 50 C4 configuration to implement our system.</p><p>We also used a pretrained and frozen BERT-Base <ref type="bibr" target="#b40">[41]</ref> as our language backbone. For the multimodal transformer, we use the same architecture as BERT-Base, except we use only 6 layers and 8 attention heads at each layer, and we train it from scratch. Our base learning rate for pretraining is 0.01 which drops to 0.001 and 0.0001 after sufficient training. We use a batch size of 64 and train on 8 V-100 GPUs which takes about 10 hours. We use spatial dropout following <ref type="bibr" target="#b18">[19]</ref> to subsample visual regions during pretraining. For masked language modeling, we mask each word with the likelihood of 0.135. We use gradient clipping at 5.0 for pretraining.</p><p>During downstream training, we use the BERT embeddings (i.e., pretrained input embeddings, not the output of BERT transformers) of the base classes to initialize and fix the classifier weights. We found the best background weight is ? = 0.2 for most experiments, except the ablations without a fixed, pretrained V2L layer, where ? = 0.0 works best. We only fine-tune the third and forth block of ResNet, and keep the stem and first two blocks fixed. We train using a learning rate of 0.005 and drop to 0.0005 and 0.00005 when appropriate. We train with a batch size of 8 on 8 V-100 GPUs which takes about 18 hours to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>Because our proposed OVD task utilizes a unique combination of training data and supervision that has not been studied before, there are no baselines with identical training settings for an entirely fair comparison. Therefore, we use baselines from a variety of similar but not identical tasks. Firstly, we compare to zero-shot detection methods, as ZSD is the closest area to our work. Particularly, we compare to SB <ref type="bibr" target="#b2">[3]</ref>, which is the first and simplest ZSD method, projecting CNN features of EdgeBox proposals <ref type="bibr" target="#b46">[47]</ref> to word embeddings. Then we compare to LAB <ref type="bibr" target="#b2">[3]</ref>, which attempts to better model the background class using a mixture model. We also compare to DSES <ref type="bibr" target="#b2">[3]</ref>, which uses additional classes from Visual Genome <ref type="bibr" target="#b19">[20]</ref> to augment base classes. Then we compare to PL <ref type="bibr" target="#b29">[30]</ref>, which proposes polarity loss to address the object-background imbalance, and to DELO <ref type="bibr" target="#b45">[46]</ref>, which employs a generative approach to prepare the model for certain target classes through feature hallucination. Note that DELO needs to know target classes beforehand, which makes it not truly open-vocabulary.</p><p>It is important to note that our approach utilizes extra data (COCO Captions or Conceptual Captions) that is not available to ZSD baselines, and may include examples of target categories. Therefore, we also compare to weakly supervised detection (WSD) methods, by converting captions into image-level labels using exact matching or a classifier <ref type="bibr" target="#b42">[43]</ref>. We compare to WSDDN <ref type="bibr" target="#b3">[4]</ref>, as well as Cap2Det <ref type="bibr" target="#b42">[43]</ref> which better utilizes captions. WSD methods do not utilize bounding boxes for base classes, which can be an advantage in situations where no such annotation is available, but it results in poor localization performance compared to our method, which is able to utilize bounding boxes. Hence, we also compare to transfer learning methods that utilize a mixture of weak and full supervision (denoted as MSD). Particularly, we compare to LSDA <ref type="bibr" target="#b17">[18]</ref>, which learns a transformation from classifier weights into detector weights, its extension <ref type="bibr" target="#b35">[36]</ref> to utilize semantic class relationships (LSDA+), and a more recent work <ref type="bibr" target="#b36">[37]</ref> which uses multiple instance learning along with a region proposal network trained on base classes (MIL+RPN). Note that since WSD and MSD methods require imagelevel labels, target classes should be known in advance during pretraining, and the models are particularly adapted to those classes. In contrast, our method and most ZSD methods have no access to such information, and can be applied to any novel class without retraining. <ref type="table">Table 1</ref> demonstrates our main results compared to the baselines. Particularly, we observe a significant improvement on target class performance and generalized target performance compared to all ZSD baselines. This is mainly due to our ability to utilize additional, low-cost training data. We also outperform WSD and MSD baselines on target classes, despite their access to information about target classes during training, and we significantly outperform them on base classes and therefore overall, due to our effective exploitation of bounding box supervision for base classes. Note that WSD and MSD models cannot be evaluated on base-only or target-only classes since they have a fixed classifier trained on all 65 classes. Moreover, we have a FSD (fully supervised detection) baseline to measure the performance drop on base classes. Furthermore, we present ablation experiments in <ref type="table">Table 2</ref> to show the effectiveness of each design choice. Particularly, we observe that without pretraining our model on image-caption datasets, the model performs poorly. This confirms the remarkable efficacy of multimodal pretraining for open-vocabulary generalization. We also observe that grounding is the main component of pretraining, which has a much larger effect than the auxiliary objectives that are optimized through the multimedia transformer module. Moreover, we show that transferring ResNet weights alone (from pretraining to downstream task) is not enough for effective knowledge transfer, and we must transfer the V2L layer as well. Additionally, if the V2L layer is not frozen during downstream training, it loses its ability to generalize to target classes, in order to slightly improve on base classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>We also try initializing the model randomly during pretraining instead of using widely used Imagenet weights, and despite the performance drop, we still perform better than most ZSD baselines that use Imagenet. We also observe that if we use the automatically collected Conceptual Captions instead of the carefully annotated COCO Captions, the performance drops, but still outperforms all ZSD baselines significantly, proving that even low-quality, cheap data can be utilized by OVR-CNN to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization and discussion</head><p>To gain deeper insight about what OVR-CNN learns, we depict the visual-semantic embedding space that is learned by our model in <ref type="figure" target="#fig_1">Figure 4</ref>. More specifically, we apply our trained model (after downstream training) on all COCO validation images, get the embeddings of all output bounding boxes after the V2L layer e I i , and reduce their dimensionality to 2 using t-SNE <ref type="bibr" target="#b37">[38]</ref>. We color-code them based on their ground truth label and overlay class embeddings e V k on the same space. We only show target classes and their instances to reduce clutter. Ideally, instances of each target class should form distinct clusters, and each class embed- ding (prototype) should fall inside the cluster formed by its instances. This is particularly difficult to achieve for target classes due to the lack of direct supervision. We compare our method to a ZSD baseline that is identical to our model except without pretraining on image-caption pairs.</p><p>We observe that in the baseline, target classes form convoluted clusters and their prototypes are randomly distributed or collapsed. On the other hand, our full model creates well-defined clusters that contain their prototypes in most cases. This is consistent with our intuition and our quantitative results that suggest zero-shot learning is not sufficient for learning a smooth and generalizable mapping from visual features to semantic embeddings, and learning a larger vocabulary through multimodal data is crucial for a more coherent space and generalizing beyond base classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We called attention to the new task of Open-Vocabulary Object Detection (OVD), as an attempt to disentangle object detection into recognition and localization, and learn them separately using two different sources of supervision that are perfect for each corresponding task. In OVD, recognition is learned from captions, which are general-purpose and open-vocabulary, while localization is learned from bounding box annotations, which are accurate and directly designed for the downstream task. We proposed OVR-CNN which pretrains a Faster R-CNN on an image-caption dataset and carefully transfers the open-vocabulary visualsemantic knowledge learned from captions to the downstream task of object detection. We demonstrated record performance compared to zero-shot and weakly supervised baselines, establishing a new state of the art for scalable object detection. Nevertheless, OVR-CNN is merely one possible implementation of our general idea, which can be extended to other downstream tasks too, enabling more human-like, open-vocabulary computer vision technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head><p>In this section, we provide statistical and qualitative analysis to gain additional insights about the performance of the proposed method. Since one of the most critical issues of deep learning is bias, we start by analyzing the effect of training data bias on our per-class performance. Since we have two training phases, the class frequency during pretraining and downstream training should be separately analyzed. <ref type="figure" target="#fig_2">Figure 5</ref> shows our per-class performance (right), along with the frequency of bounding box instances during downstream training (left), and the frequency of words during pretraining (center).</p><p>Our first observation is that our performance is not affected by the bias in downstream training data. As we move down the list, classes become exponentially less frequent, but the performance does not drop at all, except target (red) classes which have exactly zero examples during downstream training, and are inevitably less accurate. Our robustness to data bias is most likely due to the fact that we fix the classification head during downstream training, including both the V2L layer and the class embeddings. This is in contrast with conventional classifiers which fully adapt the classifier parameters, including an explicit bias term, to the biased training data.</p><p>Nevertheless, when we compare the performance to word frequency during pretraining, we do observe a correlation between the least frequent words and the lest accurate classes. This correlation is not very strong, but it motivates our future work on bias mitigation mechanisms that can be used in naturally supervised (image-caption) settings.</p><p>Furthermore, we observe that smaller objects such as knife and tie have lower performance, which is to some extent consistent with supervised object detection, but is fueled by the fact that our grounding mechanism is weakly supervised, and is less likely to correctly align smaller objects to words, because they take a smaller portion of the feature map.</p><p>To get a qualitative look at the performance, we deploy our model on the COCO validation set and visualize its detection outputs in <ref type="figure" target="#fig_3">Figure 6</ref>. We use the generalized version which selects the category of each object from the union of base and target classes. We emphasize target classes for better visibility, and analyse the quality of the predictions. Based on our observation, the main limitation of our method is localization accuracy for target classes. There are several cases of overly loose or overly tight bounding boxes, which is due to the fact that we have no ground truth bounding boxes for target classes. This motivates future work on class-agnostic boundary refinement.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of our OVR-CNN during pretraining (top) and downstream training (bottom). We first train the ResNet and the V2L layer on image-caption pairs via grounding, masked language modeling (MLM) and image-text matching (ITM). Then we use the trained ResNet and V2L to initialize a Faster R-CNN in order to learn open-vocabulary object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The embedding space learned by OVR-CNN (right) compared to a baseline without pretraining (left). Each color represents a target class, each dot represents the e I i embedding of a bounding box and each star represents a class prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Performance for each class along with data frequency during pretraining and downstream training. Green and red show base and target classes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results of our OVR-CNN model, detecting both base and target classes. Target classes are shown with larger font, thicker border, and uppercase.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/alirezazareian/ovr-cnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-level multimodal common semantic space for image-phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surabhi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12476" to="12486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect and retrieve objects from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge aided consistency for weakly supervised phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4042" to="4050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multifold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="189" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Align2ground: Weakly supervised phrase grounding guided by image-caption alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karuna</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2601" to="2610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06666,2020.3</idno>
		<title level="m">Virtex: Learning visual representations from textual annotations</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Note-rcnn: Noise tolerant ensemble rcnn for semisupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9508" to="9517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multi-space approach to zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dikshant</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nehal</forename><surname>Mamgain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1209" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>Liunian Harold Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhecan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12831</idno>
		<title level="m">Weaklysupervised visualbert: Pre-training without parallel images and captions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salil</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8690" to="8697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Insert date here</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transductive learning for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6082" to="6091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dlwl: Improving detection for lowshot classes with weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9342" to="9352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic concept discovery from parallel text and visual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2596" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Revisiting knowledge transfer for training object class detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jianbin Jiao, and Qixiang Ye. C-mil: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2199" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model recommendation: Generating object detectors from few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1619" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weaklysupervised visual grounding of phrases with linguistic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5945" to="5954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cap2det: Learning to amplify weak caption supervision for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Berent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to discover and localize visual objects with open vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Berent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10080</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="998" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
