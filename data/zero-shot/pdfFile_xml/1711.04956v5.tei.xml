<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classical Structured Prediction Losses for Sequence to Sequence Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, New York</settlement>
									<region>CA, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, New York</settlement>
									<region>CA, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, New York</settlement>
									<region>CA, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, New York</settlement>
									<region>CA, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">&amp;apos; Aurelio</forename><surname>Ranzato</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park, New York</settlement>
									<region>CA, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classical Structured Prediction Losses for Sequence to Sequence Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been much recent work on training neural attention models at the sequencelevel using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT'14 German-English translation as well as Gigaword abstractive summarization. On the large WMT'14 English-French task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence to sequence models are usually trained with a simple token-level likelihood loss <ref type="bibr" target="#b2">Bahdanau et al., 2014)</ref>. However, at test time, these models do not produce a single token but a whole sequence. In order to resolve this inconsistency and to potentially improve generation, recent work has focused on training these models at the sequence-level, for instance using REINFORCE <ref type="bibr" target="#b23">(Ranzato et al., 2015)</ref>, actor-critic <ref type="bibr" target="#b1">(Bahdanau et al., 2016)</ref>, or with beam search optimization <ref type="bibr" target="#b39">(Wiseman and Rush, 2016)</ref>.</p><p>Before the recent work on sequence level training for neural networks, there has been a large body of research on training linear models at the * Equal contribution. <ref type="bibr">1</ref> An implementation of the losses is available as part of fairseq at: https://github.com/pytorch/fairseq/tree/ classic_seqlevel sequence level. For example, direct loss optimization has been popularized in machine translation with the Minimum Error Rate Training algorithm (MERT; <ref type="bibr" target="#b19">Och 2003)</ref> and expected risk minimization has an extensive history in NLP <ref type="bibr" target="#b29">(Smith and Eisner, 2006;</ref><ref type="bibr" target="#b24">Rosti et al., 2010;</ref><ref type="bibr" target="#b11">Green et al., 2014)</ref>. This paper revisits several objective functions that have been commonly used for structured prediction tasks in NLP <ref type="bibr" target="#b9">(Gimpel and Smith, 2010)</ref> and apply them to a neural sequence to sequence model <ref type="bibr">(Gehring et al., 2017b) ( ?2)</ref>. Specifically, we consider likelihood training at the sequencelevel, a margin loss as well as expected risk training. We also investigate several combinations of global losses with token-level likelihood. This is, to our knowledge, the most comprehensive comparison of structured losses in the context of neural sequence to sequence models ( ?3).</p><p>We experiment on the IWSLT'14 German-English translation task <ref type="bibr" target="#b3">(Cettolo et al., 2014)</ref> as well as the Gigaword abstractive summarization task <ref type="bibr" target="#b25">(Rush et al., 2015)</ref>. We achieve the best reported accuracy to date on both tasks. We find that the sequence level losses we survey perform similarly to one another and outperform beam search optimization <ref type="bibr" target="#b39">(Wiseman and Rush, 2016</ref>) on a comparable setup. On WMT'14 English-French, we also illustrate the effectiveness of risk minimization on a larger translation task. Classical losses for structured prediction are still very competitive and effective for neural models ( ?5, ?6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sequence to Sequence Learning</head><p>The general architecture of our sequence to sequence models follows the encoder-decoder approach with soft attention first introduced in <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref>. As a main difference, in most of our experiments we parameterize the encoder and the decoder as convolutional neural networks instead of recurrent networks <ref type="bibr">(Gehring et al., 2017a,b)</ref>. Our use of convolution is motivated by computational and accuracy considerations. However, the objective functions we present are model agnostic and equally applicable to recurrent and convolutional models. We demonstrate the applicability of our objective functions to recurrent models (LSTM) in our comparison to <ref type="bibr" target="#b39">Wiseman and Rush (2016)</ref> in ?6.6.</p><p>Notation. We denote the source sentence as x, an output sentence of our model as u, and the reference or target sentence as t. For some objectives, we choose a pseudo reference u * instead, such as a model output with the highest BLEU or ROUGE score among a set of candidate outputs, U, generated by our model.</p><p>Concretely, the encoder processes a source sentence x = (x 1 , . . . , x m ) containing m words and outputs a sequence of states z = (z 1 . . . . , z m ). The decoder takes z and generates the output sequence u = (u 1 , . . . , u n ) left to right, one element at a time. For each output u i , the decoder computes hidden state h i based on the previous state h i?1 , an embedding g i?1 of the previous target language word u i?1 , as well as a conditional input c i derived from the encoder output z. The attention context c i is computed as a weighted sum of (z 1 , . . . , z m ) at each time step. The weights of this sum are referred to as attention scores and allow the network to focus on the most relevant parts of the input at each generation step. Attention scores are computed by comparing each encoder state z j to a combination of the previous decoder state h i and the last prediction u i ; the result is normalized to be a distribution over input elements. At each generation step, the model scores for the V possible next target words u i by transforming the decoder output h i via a linear layer with weights W o and bias b o :</p><formula xml:id="formula_0">s i = W o h i + b o .</formula><p>This is turned into a distribution via a softmax: p(u i |u 1 , . . . , u i?1 , x) = softmax(s i ).</p><p>Our encoder and decoder use gated convolutional neural networks which enable fast and accurate generation <ref type="bibr" target="#b8">(Gehring et al., 2017b)</ref>. Fast generation is essential to efficiently train on the model output as is done in this work as sequence-level losses require generating at training time. Both encoder and decoder networks share a simple block structure that computes intermediate states based on a fixed number of input tokens and we stack several blocks on top of each other. Each block contains a 1-D convolution that takes as input k feature vectors and outputs another vector; subsequent layers operate over the k output elements of the previous layer. The output of the convolution is then fed into a gated linear unit <ref type="bibr" target="#b5">(Dauphin et al., 2017)</ref>. In the decoder network, we rely on causal convolution which rely only on states from the previous time steps. The parameters ? of our model are all the weight matrices in the encoder and decoder networks. Further details can be found in <ref type="bibr" target="#b8">Gehring et al. (2017b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Objective Functions</head><p>We compare several objective functions for training the model architecture described in ?2. The corresponding loss functions are either computed over individual tokens ( ?3.1), over entire sequences ( ?3.2) or over a combination of tokens and sequences ( ?3.3). An overview of these loss functions is given in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token-Level Objectives</head><p>Most prior work on sequence to sequence learning has focused on optimizing token-level loss functions, i.e., functions for which the loss is computed additively over individual tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token Negative Log Likelihood (TokNLL)</head><p>Token-level likelihood (TokNLL, Equation 1) minimizes the negative log likelihood of individual reference tokens t = (t 1 , . . . , t n ). It is the most common loss function optimized in related work and serves as a baseline for our comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token NLL with Label Smoothing (TokLS)</head><p>Likelihood training forces the model to make extreme zero or one predictions to distinguish between the ground truth and alternatives. This may result in a model that is too confident in its training predictions, which may hurt its generalization performance. Label smoothing addresses this by acting as a regularizer that makes the model less confident in its predictions. Specifically, we smooth the target distribution with a prior distribution f that is independent of the current input x <ref type="bibr">(Szegedy et al., 2015;</ref><ref type="bibr" target="#b22">Pereyra et al., 2017;</ref><ref type="bibr" target="#b37">Vaswani et al., 2017)</ref>. We use a uniform prior distribution over all words in the vocabulary, f = 1 V . One may also use a unigram distribution which has been shown to work better on some tasks <ref type="bibr" target="#b22">(Pereyra et al., 2017)</ref>. Label smoothing is equivalent to adding the KL divergence between f and the model prediction . We denote the source as x, the reference target as t, the set of candidate outputs as U and the best candidate (pseudo reference) as u * . For max-margin we denote the candidate with the highest model score as?. p(u|x) to the negative log likelihood (TokLS, Equation 2). In practice, we implement label smoothing by modifying the ground truth distribution for word u to be q(u) = 1 ? and q(u ) = V for u = u instead of q(u) = 1 and q(u ) = 0 where is a smoothing parameter.</p><formula xml:id="formula_1">L TokNLL = ? n i=1 log p(t i |t 1 , . . . , t i?1 , x) (1) L TokLS = ? n i=1 log p(t i |t 1 , . . . , t i?1 , x) ? D KL (f p(t i |t 1 , . . . , t i?1 , x)) (2) L SeqNLL = ? log p(u * |x) + log u?U (x) p(u|x) (3) L Risk = u?U (x) cost(t, u) p(u|x) u ?U (x) p(u |x) (4) L MaxMargin = max [0, cost(t,?) ? cost(t, u * ) ? s(u * |x) + s(?|x)]<label>(5)</label></formula><formula xml:id="formula_2">L MultiMargin = u?U (x) max [0, cost(t, u) ? cost(t, u * ) ? s(u * |x) + s(u|x)]<label>(6)</label></formula><formula xml:id="formula_3">L SoftmaxMargin = ? log p(u * |x) + log u?U (x) exp [s(u|x) + cost(t, u)]<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequence-Level Objectives</head><p>We also consider a class of objective functions that are computed over entire sequences, i.e., sequence-level objectives. Training with these objectives requires generating and scoring multiple candidate output sequences for each input sequence during training, which is computationally expensive but allows us to directly optimize taskspecific metrics such as BLEU or ROUGE. Unfortunately, these objectives are also typically defined over the entire space of possible output sequences, which is intractable to enumerate or score with our models. Instead, we compute our sequence losses over a subset of the output space, U(x), generated by the model. We discuss approaches for generating this subset in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Negative Log Likelihood (SeqNLL)</head><p>Similar to TokNLL, we can minimize the negative log likelihood of an entire sequence rather than individual tokens (SeqNLL, Equation 3). The log-likelihood of sequence u is the sum of individual token log probabilities, normalized by the number of tokens to avoid bias towards shorter sequences:</p><formula xml:id="formula_4">p(u|x) = exp 1 n n i=1 log p(u i |u 1 , . . . , u i?1 , x)</formula><p>As target we choose a pseudo reference 2 amongst the candidates which maximizes either BLEU or ROUGE with respect to t, the gold reference:</p><formula xml:id="formula_5">u * (x) = arg max u?U (x) BLEU(t, u)</formula><p>As is common practice when computing BLEU at the sentence-level, we smooth all initial counts to one (except for unigram counts) so that the geometric mean is not dominated by zero-valued ngram match counts <ref type="bibr" target="#b17">(Lin and Och, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Risk Minimization (Risk)</head><p>This objective minimizes the expected value of a given cost function over the space of candidate sequences <ref type="bibr">(Risk,</ref><ref type="bibr">Equation 4</ref>). In this work we use task-specific cost functions designed to maximize BLEU or ROUGE <ref type="bibr" target="#b16">(Lin, 2004)</ref>, e.g., cost(t, u) = 1 ? BLEU(t, u), for a given a candidate sequence u and target t. Different to SeqNLL ( ?3.2), this loss may increase the score of several candidates that have low cost, instead of focusing on a single sequence which may only be marginally better than any alternatives. Optimizing this loss is a particularly good strategy if the reference is not always reachable, although compared to classical phrase-based models, this is less of an issue with neural sequence to sequence models that predict individual words or even sub-word units.</p><p>The Risk objective is similar to the REIN-FORCE objective used in Ranzato et al. <ref type="formula" target="#formula_1">(2015)</ref>, since both objectives optimize an expected cost or reward <ref type="bibr" target="#b38">(Williams, 1992)</ref>. However, there are a few important differences: (1) whereas REINFORCE typically approximates the expectation with a single sampled sequence, the Risk objective considers multiple sequences; (2) whereas REINFORCE relies on a baseline reward 3 to determine the sign of the gradients for the current sequence, for the Risk objective we instead estimate the expected cost over a set of candidate output sequences (see ?4); and (3) while the baseline reward is different for every word in REINFORCE, the expected cost is the same for every word in risk minimization since it is computed on the sequence level based on the actual cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-Margin</head><p>MaxMargin <ref type="formula" target="#formula_1">(Equation 5</ref>) is a classical margin loss for structured prediction <ref type="bibr" target="#b35">(Taskar et al., 2003;</ref><ref type="bibr" target="#b36">Tsochantaridis et al., 2005)</ref> which enforces a margin between the model scores of the highest scoring candidate sequence? and a reference sequence. We replace the human reference t with a pseudo-reference u * since this setting performed slightly better in early experiments; u * is the candidate sequence with the highest BLEU score. The size of the margin varies between samples and is given by the difference between the cost of u * and the cost of?. In practice, we scale the margin by a hyper-parameter ? determined on the validation set: ?(cost(t,?) ? cost(t, u * )). For this loss we use the unnormalized scores computed by the model before the final softmax: </p><formula xml:id="formula_6">s(u|x) = 1 n n i=1 s(u i |u 1 , . . . , u i?1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Margin</head><p>MaxMargin only updates two elements in the candidate set.</p><p>We therefore consider MultiMargin (Equation 6) which enforces a margin between every candidate sequence u and a reference sequence <ref type="bibr" target="#b12">(Herbrich et al., 1999)</ref>, hence the name Multi-Margin. Similar to MaxMargin, we replace the reference t with the pseudoreference u * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax-Margin</head><p>Finally, SoftmaxMargin <ref type="formula" target="#formula_3">(Equation 7)</ref> is another classic loss that has been proposed by <ref type="bibr" target="#b9">Gimpel and Smith (2010)</ref> as another way to optimize task-specific costs. The loss augments the scores inside the exp of SeqNLL (Equation 3) by a cost. The intuition is that we want to penalize high cost outputs proportional to their cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combined Objectives</head><p>We also experiment with two variants of combining sequence-level objectives ( ?3.2) with tokenlevel objectives ( ?3.1). First, we consider a weighted combination (Weighted) of both a sequence-level and token-level objective , e.g., for TokLS and Risk we have:</p><formula xml:id="formula_7">L Weighted = ?L TokLS + (1 ? ?)L Risk (8)</formula><p>where ? is a scaling constant that is tuned on a held-out validation set.</p><p>Second, we consider a constrained combination (Constrained), where for any given input we use either the token-level or sequence-level loss, but not both. The motivation is to maintain good token-level accuracy while optimizing on the sequence-level. In particular, a sample is processed with the sequence loss if the token loss under the current model is at least as good as the token loss of a baseline model L b</p><p>TokLS . Otherwise, we update according to the token loss:</p><formula xml:id="formula_8">L Constrained = L Risk L TokLS ? L b</formula><p>TokLS L TokLS otherwise (9) In this work we use a fixed baseline model that was trained with a token-level loss to convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Candidate Generation Strategies</head><p>The sequence-level objectives we consider ( ?3.2) are defined over the entire space of possible output sequences, which is intractable to enumerate or score with our models. We therefore use a subset of K candidate sequences U(x) = {u 1 , . . . , u K }, which we generate with our models.</p><p>We consider two search strategies for generating the set of candidate sequences. The first is beam search, a greedy breadth-first search that maintains a "beam" of the top-K scoring candidates at each generation step. Beam search is the de facto decoding strategy for achieving state-ofthe-art results in machine translation. The second strategy is sampling <ref type="bibr" target="#b4">(Chatterjee and Cancedda, 2010)</ref>, which produces K independent output sequences by sampling from the model's conditional distribution. Whereas beam search focuses on high probability candidates, sampling introduces more diverse candidates (see comparison in ?6.5).</p><p>We also consider both online and offline candidate generation settings in ?6.4. In the online setting, we regenerate the candidate set every time we encounter an input sentence x during training. In the offline setting, candidates are generated before training and are never regenerated. Offline generation is also embarrassingly parallel because all samples use the same model. The disadvantage is that the candidates become stale. Our model may perfectly be able to discriminate between them after only a single update, hindering the ability of the loss to correct eventual search errors. <ref type="bibr">4</ref> Finally, while some past work has added the reference target to the candidate set, i.e., U (x) = U(x) ? {t}, we find this can destabilize training since the model learns to assign low probabilities nearly everywhere, ruining the candidates generated by the model, while still assigning a slightly higher score to the reference (cf. ). Accordingly we do not add the reference translation to our candidate sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Translation</head><p>We experiment on the IWSLT'14 German to English <ref type="bibr" target="#b3">(Cettolo et al., 2014)</ref> task using a similar setup as <ref type="bibr" target="#b23">Ranzato et al. (2015)</ref>, which allows us to compare to other recent studies that also adopted this setup, e.g., <ref type="bibr" target="#b39">Wiseman and Rush (2016)</ref>. <ref type="bibr">5</ref> The training data consists of 160K sentence pairs and the validation set comprises 7K sentences randomly sampled and held-out from the train data. We test on the concatenation of all available test and dev sets of IWSLT 2014, that is TED.tst2010, TED.tst2011, TED.tst2012 and TED.dev2010, TEDX.dev2012 which is of similar size to the validation set. 6 All data is lowercased and tokenized with a byte-pair encoding (BPE) of 14,000 types <ref type="bibr" target="#b27">(Sennrich et al., 2016)</ref> and we evaluate with case-insensitive BLEU.</p><p>We also experiment on the much larger WMT'14 English-French task. We remove sentences longer than 175 words as well as pairs with a source/target length ratio exceeding 1.5 resulting in 35.5M sentence-pairs for training. The source and target vocabulary is based on 40K BPE types. Results are reported on both newstest2014 and a validation set held-out from the training data comprising 26,658 sentence pairs.</p><p>We modify the fairseq-py toolkit to implement the objectives described in ?3. 7 Our translation models have four convolutional encoder layers and three convolutional decoder layers with a kernel width of 3 and 256 dimensional hidden states and word embeddings. We optimize these models using Nesterov's accelerated gradient method <ref type="bibr" target="#b31">(Sutskever et al., 2013)</ref> with a learning rate of 0.25 and momentum of 0.99. Gradient vectors are renormalized to norm 0.1 <ref type="bibr" target="#b20">(Pascanu et al., 2013)</ref>.</p><p>We train our baseline token-level models for 200 epochs and then anneal the learning by shrinking it by a factor of 10 after each subsequent epoch until the learning rate falls below 10 ?4 . All sequence-level models are initialized with parameters of a token-level model before annealing. We then train sequence-level models for another 10 to 20 epochs depending on the objective. Our batches contain 8K tokens and we normalize gradients by the number of non-padding tokens per mini-batch. We use weight normalization for all layers except for lookup tables <ref type="bibr" target="#b26">(Salimans and Kingma, 2016)</ref>. Besides dropout on the embeddings and the decoder output, we also apply dropout to the input of the convolutional blocks at a rate of 0.3 <ref type="bibr" target="#b30">(Srivastava et al., 2014)</ref>. We tuned the various parameters above and report accuracy on the test set by choosing the best configuration based on the validation set.</p><p>We length normalize all scores and probabilities in the sequence-level losses by dividing by the number of tokens in the sequence so that scores are comparable between different lengths. Additionally, when generating candidate output sequences during training we limit the output sequence length to be less than 200 tokens for efficiency. We generally use 16 candidate sequences per training example, except for the ablations where we use 5 for faster experimental turnaround.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Abstractive Summarization</head><p>For summarization we use the Gigaword corpus as training data <ref type="bibr" target="#b10">(Graff et al., 2003)</ref> and pre-process it identically to <ref type="bibr" target="#b25">Rush et al. (2015)</ref> resulting in 3.8M training and 190K validation examples. We evaluate on a Gigaword test set of 2,000 pairs identical to the one used by <ref type="bibr" target="#b25">Rush et al. (2015)</ref> and report F1 ROUGE similar to prior work. Our results are in terms of three variants of ROUGE <ref type="bibr" target="#b16">(Lin, 2004)</ref>, namely, ROUGE-1 (RG-1, unigrams), ROUGE-2 (RG-2, bigrams), and ROUGE-L (RG-L, longestcommon substring). Similar to <ref type="bibr" target="#b0">Ayana et al. (2016)</ref> we use a source and target vocabulary of 30k words. Our models for this task have 12 layers in the encoder and decoder each with 256 hidden units and kernel width 3. We train on batches of 8,000 tokens with a learning rate of 0.25 for 20 epochs and then anneal as in ?5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison of Sequence Level Losses</head><p>First, we compare all objectives based on a weighted combination with token-level label smoothing <ref type="bibr">(Equation 8</ref>). We also show the likelihood baseline (MLE) of <ref type="bibr" target="#b39">Wiseman and Rush (2016)</ref>, their beam search optimization method (BSO), the actor critic result of <ref type="bibr" target="#b1">Bahdanau et al. (2016)</ref> as well as the best reported result on this dataset to date by <ref type="bibr" target="#b13">Huang et al. (2017)</ref>. We show a like-for-like comparison to <ref type="bibr" target="#b39">Wiseman and Rush (2016)</ref> with a similar baseline model below ( ?6.6). <ref type="table">Table 1</ref> shows that all sequence-level losses outperform token-level losses. Our baseline tokenlevel results are several points above other figures in the literature and we further improve these results by up to 0.61 BLEU with Risk training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Combination with Token-Level Loss</head><p>Next, we compare various strategies to combine sequence-level and token-level objectives (cf.   <ref type="formula">9</ref>). We also compare to randomly choosing between token-level and sequence-level updates and find it underperforms the more principled constrained strategy. In the remaining experiments we use the weighted strategy.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of initialization</head><p>So far we initialized sequence-level models with parameters from a token-level model trained with label smoothing. <ref type="table" target="#tab_4">Table 3</ref> shows that initializing weighted Risk with token-level label smoothing achieves 0.7-0.8 better BLEU compared to initializing with parameters from token-level likelihood. The improvement of initializing with TokNLL is only 0.3 BLEU with respect to the TokNLL baseline, whereas, the improvement from initializing with TokLS is 0.6-0.8 BLEU. We believe that the regularization provided by label smoothing leads to models with less sharp distributions that are a better starting point for sequence-level training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Online vs. Offline Candidate Generation</head><p>Next, we consider the question if refreshing the candidate subset at every training step (online) results in better accuracy compared to generating candidates before training and keeping the set static throughout training (offline). <ref type="table" target="#tab_5">Table 4</ref> shows that offline generation gives lower accuracy. However the online setting is much slower, since regenerating the candidate set requires incremental (left to right) inference with our model which is very slow compared to efficient forward/backward over large batches of pre-generated hypothesis. In our setting, offline generation has 26 times higher throughput than the online generation setting, despite the high inference speed of fairseq <ref type="bibr" target="#b8">(Gehring et al., 2017b)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Beam Search vs. Sampling and Candidate Set Size</head><p>So far we generated candidates with beam search, however, we may also sample to obtain a more diverse set of candidates  In other experiments, we rely on a candidate set size of 16 which strikes a good balance between efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Comparison to Beam-Search Optimization</head><p>Next, we compare classical sequence-level training to the recently proposed Beam Search Optimization <ref type="bibr" target="#b39">(Wiseman and Rush, 2016)</ref>. To enable a fair comparison, we re-implement their baseline, a single layer LSTM encoder/decoder model with 256-dimensional hidden layers and word embeddings as well as attention and input feeding (Lu-  <ref type="table">Table 6</ref>: Accuracy on Gigaword abstractive summarization in terms of F-measure Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L) for token-level label smoothing, and Risk optimization of all three ROUGE F1 metrics.</p><p>[T] indicates a token-level objective and [S] indicates a sequence level objectives. ABS+ refers to <ref type="bibr" target="#b25">Rush et al. (2015)</ref>, RNN MLE/MRT <ref type="bibr" target="#b0">(Ayana et al., 2016)</ref>, WFE <ref type="bibr" target="#b33">(Suzuki and Nagata, 2017)</ref>, SEASS , DRGD . <ref type="bibr">ong et al., 2015)</ref>. This baseline is trained with Adagrad <ref type="bibr" target="#b6">(Duchi et al., 2011)</ref> using a learning rate of 0.05 for five epochs, with batches of 64 sequences. For sequence-level training we initialize weights with the baseline parameters and train with Adam (Kingma and Ba, 2014) for another 10 epochs with learning rate 0.00003 and 16 candidate sequences per training example. We conduct experiments with Risk since it performed best in trial experiments. Different from other sequence-level experiments ( ?5), we rescale the BLEU scores in each candidate set by the difference between the maximum and minimum scores of each sentence. This avoids short sentences dominating the sequence updates, since candidate sets for short sentences have a wider range of BLEU scores compared to longer sentences; a similar rescaling was used by <ref type="bibr" target="#b1">Bahdanau et al. (2016)</ref>. <ref type="table" target="#tab_7">Table 5</ref> shows the results from <ref type="bibr" target="#b39">Wiseman and Rush (2016)</ref> for their token-level likelihood baseline (MLE), best beam search optimization results (BSO), as well as our reimplemented baseline. Risk significantly improves BLEU compared to our baseline at +2.75 BLEU, which is slightly better than the +2.33 BLEU improvement reported for Beam Search Optimization (cf. <ref type="bibr" target="#b39">Wiseman and Rush (2016)</ref>). This shows that classical objectives for structured prediction are still very competitive.  <ref type="table">Table 7</ref>: Test and valid BLEU on WMT'14 English-French with and without decoder self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">WMT'14 English-French results</head><p>Next, we experiment on the much larger WMT'14 English-French task using the same model setup as <ref type="bibr" target="#b8">Gehring et al. (2017b)</ref>. We TokLSfor 15 epochs and then switch to sequence-level training for another epoch. <ref type="table">Table 7</ref> shows that sequence-level training can improve an already very strong model by another +0.37 BLEU. Next, we improve the baseline by adding self-attention <ref type="bibr" target="#b21">(Paulus et al., 2017;</ref><ref type="bibr" target="#b37">Vaswani et al., 2017)</ref> to the decoder network (TokLS + selfatt) which results in a smaller gain of +0.2 BLEU by Risk. If we train Risk only on the news-commentary portion of the training data, then we achieve state of the art accuracy on this dataset of 41.5 BLEU <ref type="bibr" target="#b41">(Xia et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Abstractive Summarization</head><p>Our final experiment evaluates sequence-level training on Gigaword headline summarization. There has been much prior art on this dataset originally introduced by Rush et al. <ref type="formula" target="#formula_1">(2015)</ref> who experiment with a feed-forward network (ABS+). <ref type="bibr" target="#b0">Ayana et al. (2016)</ref> report a likelihood baseline (RNN MLE) and also experiment with risk training (RNN MRT). Different to their setup we did not find a softmax temperature to be beneficial, and we use beam search instead of sampling to obtain the candidate set (cf. ?6.5). Suzuki and Nagata (2017) improve over an MLE RNN baseline by limiting generation of repeated phrases.  also consider an MLE RNN baseline and add an additional gating mechanism for the encoder.  equip the decoder of a similar network with additional latent variables to accommodate the uncertainty of this task. <ref type="table">Table 6</ref> shows that our baseline (TokLS) outperforms all prior approaches in terms of ROUGE-2 and ROUGE-L and it is on par to the best previous result for ROUGE-1. We optimize all three ROUGE metrics separately and find that Risk can further improve our strong baseline. We also compared Risk only training to Weighted on this dataset (cf. ?6.2) but accuracy was generally lower on the validation set: RG-1 (36.59 Risk only vs. 36.67 Weighted), , and RG-L (33.66 vs. 33.98).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a comprehensive comparison of classical losses for structured prediction and apply them to a strong neural sequence to sequence model. We found that combining sequence-level and tokenlevel losses is necessary to perform best, and so is training on candidates decoded with the current model.</p><p>We show that sequence-level training improves state-of-the-art baselines both for IWSLT'14 German-English translation and Gigaword abstractive sentence summarization. Structured prediction losses are very competitive to recent work on reinforcement or beam optimization. Classical expected risk can slightly outperform beam search optimization <ref type="bibr" target="#b39">(Wiseman and Rush, 2016)</ref> in a likefor-like setup. Future work may investigate better use of already generated candidates since invoking generation for each batch slows down training by a large factor, e.g., mixing with fresh and older candidates inspired by MERT <ref type="bibr" target="#b19">(Och, 2003)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Token and sequence negative log-likelihood (Equations 1 and 3), token-level label smoothing (Equation 2), expected risk (Equation 4), max-margin (Equation 5), multi-margin (Equation 6), softmax-margin (Equation 7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Validation and test BLEU for loss combina-</cell></row><row><cell>tion strategies. We either use token-level TokLS and</cell></row><row><cell>sequence-level Riskindividually or combine them as</cell></row><row><cell>a weighted combination, a constrained combination, a</cell></row><row><cell>random choice for each sample, cf.  ?3.3.</cell></row><row><cell>?3.3). For these experiments we use 5 candi-</cell></row><row><cell>date sequences per training example for faster ex-</cell></row><row><cell>perimental turnaround. We consider Risk as</cell></row><row><cell>sequence-level loss and label smoothing as token-</cell></row><row><cell>level loss. Table 2 shows that combined objectives</cell></row><row><cell>perform better than pure Risk. The weighted</cell></row><row><cell>combination (Equation 8) with ? = 0.3 per-</cell></row><row><cell>forms best, outperforming constrained combina-</cell></row><row><cell>tion (Equation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">: Effect of initializing sequence-level training</cell></row><row><cell cols="2">(Risk) with parameters from token-level likelihood</cell></row><row><cell>(TokNLL) or label smoothing (TokLS).</cell><cell></cell></row><row><cell>valid</cell><cell>test</cell></row><row><cell cols="2">Online generation 33.91 32.85</cell></row><row><cell cols="2">Offline generation 33.52 32.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Generating candidates online or offline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison to Beam Search Optimization.</figDesc><table><row><cell>We report the best likelihood (MLE) and BSO results</cell></row><row><cell>from Wiseman and Rush (2016), as well as results from</cell></row><row><cell>our MLE reimplementation and training with Risk.</cell></row><row><cell>Results based on unnormalized beam search (k = 5).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Another option is to use the gold reference target, t, but in practice this can lead to degenerate solutions in which the model assigns low probabilities to nearly all outputs. This is discussed further in ?4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We can mitigate this issue by regenerating infrequently, i.e., once every b batches but we leave this to future work. 5 Different to<ref type="bibr" target="#b23">Ranzato et al. (2015)</ref> we train on sentences of up to 175 rather than 50 tokens.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In a previous version of this paper, we erroneously quoted the use of tst2013. We are using TEDX.dev2012 instead. 7 https://github.com/pytorch/fairseq/ tree/classic_seqlevel.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural headline generation with sentence-wise optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Actor-Critic Algorithm for Sequence Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Minimum error rate training by sampling the translation lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samidh</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language Modeling with Gated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Convolutional Encoder Model for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Softmax-margin crfs: Training log-linear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English gigaword. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT. Association for Computational Linguistics</title>
		<meeting>of WMT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICANN</title>
		<meeting>of ICANN</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Phrase-based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1706.05565</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Deep recurrent generative decoder for abstractive text summarization. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orange: a method for evaluating automatic evaluation metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR Workshop</title>
		<meeting>of ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence level Training with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BBN System Description for WMT10 System Combination Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Antti-Veikko I Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT. Association for Computational Linguistics</title>
		<meeting>of WMT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07868</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimum Risk Training for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimum Risk Annealing for Training Log-Linear Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent Neural Networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cutting-off redundant repeating generations for neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<title level="m">Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2015. Rethinking the inception architecture for computer vision. arXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Selective encoding for abstractive sentence summarization. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
