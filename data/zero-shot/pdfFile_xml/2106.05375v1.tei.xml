<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Plan2Scene: Converting Floorplans to 3D Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhawa</forename><surname>Vidanapathirana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirui</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Plan2Scene: Converting Floorplans to 3D Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the task of converting a floorplan and a set of associated photos of a residence into a textured 3D mesh model, a task which we call Plan2Scene. Our system 1) lifts a floorplan image to a 3D mesh model; 2) synthesizes surface textures based on the input photos; and 3) infers textures for unobserved surfaces using a graph neural network architecture. To train and evaluate our system we create indoor surface texture datasets, and augment a dataset of floorplans and photos from prior work with rectified surface crops and additional annotations. Our approach handles the challenge of producing tileable textures for dominant surfaces such as floors, walls, and ceilings from a sparse set of unaligned photos that only partially cover the residence. Qualitative and quantitative evaluations show that our system produces realistic 3D interior models, outperforming baseline approaches on a suite of texture quality metrics and as measured by a holistic user study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digital 3D scene representations of interiors are key to emerging application areas such as AI assistants, online product marketing, and augmented reality. Private residences are predominantly designed with CAD software. However, texture-mapped 3D scene models of the built interiors are rarely available. Despite recent progress in indoor reconstruction techniques and depth-sensing hardware, the state-of-the-art in room layout inference and photogrammetry-based 3D modeling is still far from reliable and practical for non-expert users.</p><p>This paper explores a novel path for 3D interior digitization by utilizing a residential floorplan and a sparse set of photos without camera pose as input, which are prevalent in online real estate listings. More precisely, we present the Plan2Scene task: conversion of a residential floorplan and photos of a residence to a textured 3D scene model (see <ref type="figure" target="#fig_0">Figure 1</ref>). In the context of this task, we focus on texturing of architectural surfaces. This task is challenging due to 1) the lack of camera poses for the photos; 2) the challenge of photometric calibration under varying lighting conditions and 3) limited photo coverage, leaving many surfaces unobserved. The Plan2Scene task allows us to articulate these challenges for residential interiors and to identify suitable texture appropriateness metrics.</p><p>Our key idea is to model the architectural surfaces and identify appropriate textures for each surface. We treat photos as sparse and partial observations of surface textures and formulate a texture inference task, instead of relying on exact camera poses and texture-mapping as in prior work <ref type="bibr" target="#b20">[21]</ref>. Textures for observed surfaces are generated using an encoder-decoder architecture. Unobserved surfaces (i.e., surfaces for which a photo is not available) are handled by a graph neural network (GNN) that propagates information while learning inter/intra-room consistency.</p><p>The paper also makes three dataset contributions. First, we extend an existing database of floorplans and photos (Rent3D) <ref type="bibr" target="#b20">[21]</ref> by annotating more room boundaries, assigning more photos to the rooms, and annotating object-icons indicated on the floorplans. We also curate two datasets of textures of common indoor substances (e.g., 'painted walls', 'tiles', 'carpets') from various online sources for training our texture synthesis method.</p><p>Through qualitative and quantitative evaluations using a suite of metrics characterizing texture appropriateness and quality, and a user study, we demonstrate that the proposed approach outperforms baselines including rectified image patch texturing, and direct texture retrieval. We release all our code, data, and pretrained models to the community. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Room layout estimation and 3D reconstruction. A long line of work exists on coarse 3D layout estimation from indoor perspective images or panoramas: Sun et al. <ref type="bibr" target="#b25">[26]</ref>, Yang et al. <ref type="bibr" target="#b29">[30]</ref>, Zhang et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, Zou et al. <ref type="bibr" target="#b35">[36]</ref>. In contrast to our task, these methods 1) generate coarse geometry for a room as a set of planes 2) process only a single image and a single room instead of an entire residence; and 3) simply project image pixels onto the layout planes without separating objects from architectural surfaces. There is also a rich literature of house-scale 3D reconstruction and modeling methods. Different methods take as input a set of RGB images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, RGBD videos <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, RGBD panoramas <ref type="bibr" target="#b22">[23]</ref>, dense point clouds <ref type="bibr" target="#b28">[29]</ref> or partial reconstructions <ref type="bibr" target="#b18">[19]</ref>. In contrast, our input is a floorplan and a sparse set of RGB images without precise camera pose partially covering the interior, which is the typical data available on real estate websites and we produce as output a 3D textured mesh for the entire residence. Texture synthesis. There is a rich literature of work on exemplar-based texture synthesis. We refer readers to the survey by Akl et al. <ref type="bibr" target="#b1">[2]</ref>. Recent work has adapted neural networks architectures for this task, allowing the use of embeddings to represent textures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>. Chen et al. <ref type="bibr" target="#b5">[6]</ref> generate tileable textures from text descriptions. Other work has focused on inferring SVBRDF models from a single image <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> or from multi-illumination images <ref type="bibr" target="#b23">[24]</ref>. We adopt an approach inspired by recent work that utilizes a compact texture embedding <ref type="bibr" target="#b9">[10]</ref> as we rely on embedding propagation to generate textures for unobserved surfaces. 3D scene generation from photo and floorplan data. Our Plan2Scene task is closely related to the work of Izadinia et al. <ref type="bibr" target="#b12">[13]</ref> on IM2CAD and Liu et al. <ref type="bibr" target="#b20">[21]</ref> on Rent3D. IM2CAD infers a 3D room layout as well as 3D object placements from a single image. Objects and walls are colored using the medoid of each color channel in the input RGB image, making it impossible to represent common material types such as wood, tile, or carpet. Furthermore, the approach only handles surfaces and objects visible in a single input image. In contrast, our approach generates textured materials for both observed and unobserved surfaces. We also handle multi-room interiors. Rent3D has a similar problem formulation as us, taking a floorplan and set of photos as input and producing a coarse 3D mesh. However, it focuses on estimating the camera pose for each image and directly projecting pixels onto the mesh as an appearance model. This results in unrealistic rooms with sofas, beds and other objects projected onto surfaces (see <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Plan2Scene Task</head><p>The Plan2Scene task involves several steps (see <ref type="figure">Figure</ref> 3). Here, we provide an overview of these steps.  <ref type="figure">Figure 2</ref>: Comparison of (a) output from Liu et al. <ref type="bibr" target="#b20">[21]</ref> to (b) our output. We produce textured 3D meshes of the residence that do not exhibit distortions due to direct warping of photos onto walls, and that cover all surfaces.</p><p>Overview. Floorplans are usually available as raster images, requiring vectorization. Raster-to-vector floorplan conversion is the focus of prior work <ref type="bibr" target="#b19">[20]</ref>, so we assume a vector floorplan as input. We use 'floorplan' to mean a vector floorplan from here on. We convert the floorplan to 3D geometry and place fixed 3D objects (e.g., doors, windows, toilets) in the floorplan using a rule-based approach that retrieves objects from ShapeNet <ref type="bibr" target="#b4">[5]</ref> (see supplement for details). In this work, we focus on computing textures for architectural surfaces in each room, including both surfaces observed in photos and entirely unobserved surfaces.</p><p>Input and output assumptions. The input is a floorplan and a set of photos taken inside a residence, with photos assigned to rooms. The set of photos are taken from a subset of the rooms. Some rooms do not have any photos, and some surfaces in a room may not be visible in a photo. Room type information and correspondence between photos and rooms is commonly available for photos on real-estate websites, so we assume this information in our input. The floorplan specifies walls and openings (i.e., doors and windows) as line segments, provides a category for each room (e.g., bedroom), and contains structurally fixed objects (e.g., toilets), each of which is given a position. The output is a 3D mesh of the house with textures for all architectural surfaces in each room. As a simplifying assumption, we only represent three surface types in each room: 'floor', 'wall', and 'ceiling', indicated by s ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> and assign the same texture for all surfaces of the same type in a room. Thus, given the set of rooms of a house R = {1, 2, . . . , r, . . . , |R|} where r is a room index, we uniquely identify a surface through (r, s). Each texture is an RGB image assigned to surface s of room r, so the complete output texture set is Y r,s ? R 3?H?W . A good texture set is tileable (i.e. does not exhibit seams or look unnecessarily repetitive when tiled), matches the color, pattern, and substance of the input photo surfaces, while correcting artifacts due to illumination conditions and imaging noise in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vectorization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Geometry Construction Photo Assignment</head><p>Texture Generation Texture Propagation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><p>Object Placement <ref type="figure">Figure 3</ref>: In the Plan2Scene task we produce a textured 3D mesh of a residence from a floorplan and set of photos. This process involves several steps: floorplan vectorization, 3D geometry construction, object placement, photo assignment, texture generation, and texture propagation. In this paper, we use simple solutions for earlier steps (blue), and focus primarily on the last two steps (orange). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data</head><p>In this section we describe the datasets used in our experiments: a dataset of floorplans and photos based on Rent3D <ref type="bibr" target="#b20">[21]</ref> that we call Rent3D++, and two curated datasets of textures that we respectively use for training our texture synthesis approach and for establishing a texture retrieval baseline. Rent3D++ floorplan and photos dataset. The Rent3D dataset consists of floorplans and photos from 215 apartments. However, we found that: i) some rooms were unannotated; ii) not all portals (windows, doors, room-room openings) are annotated and; iii) some photos were not assigned to rooms. We correct these issues through reannotation, and extend the dataset by:</p><p>? Fixing incorrectly categorized rooms and adding wall outlines and categories from missing rooms.  <ref type="figure">Figure 5</ref>: Surface crop extraction approach. We extract rectified surface patches from photos for conditioning texture generation and for use as a reference in our evaluation.</p><p>Surface crop extraction. To facilitate texture generation, we extract a set of rectified surface crops for all architectural surfaces observed in photos. These are square patches which we use to condition the generation of textures and also as 'reference crops' for evaluation. <ref type="figure">Figure 5</ref> shows the approach we adopt. We first segment floor, ceiling, and wall surfaces using a semantic segmentation model (HRNet-v2 <ref type="bibr" target="#b26">[27]</ref> trained on ADE20K <ref type="bibr" target="#b33">[34]</ref>). We then use the approach of Yu et al. <ref type="bibr" target="#b30">[31]</ref> to estimate normals and depth for the surface planes (10 largest wall masks, one mask for floor and one for ceiling) and rectify the surface masks using the rectification implementation by Bell et al. <ref type="bibr" target="#b2">[3]</ref> and a predefined constant camera field of view. Rectified surfaces are upscaled by a factor of 3 to sample up to 10 random 256 ? 256 crops (max 1000 attempts to obtain complete crop, per crop). The crops are resized to 128 ? 128 and assigned to the corresponding room surface. We treat the medoid crop 2 from each surface as a reference crop for evaluation purposes. The medoid is less likely to be affected by shadows, reflections and specular highlights, while also being representative of the surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference photo</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surface crop tiling</head><p>Texture retrieval Texture synthesis <ref type="figure">Figure 6</ref>: Three approaches to creating textures: i) direct surface crop tiling, ii) retrieval of best matching texture, and iii) texture synthesis conditioned on surface crop. Directly using the crop creates obvious tiling artifacts. Texture retrieval cannot precisely match the surface. Texture synthesis can better match the surface with fewer visible artifacts.</p><p>Stationary textures dataset. We curated 516 texture exemplars for four substance types: 'wood', 'plaster', 'carpet' and 'tile' from various online texture libraries <ref type="bibr" target="#b2">3</ref> . <ref type="figure" target="#fig_2">Figure 4</ref> shows examples from each texture substance type. We divide this dataset into a training and validation split of 452 and 64 textures respectively. We train our texture synthesis model using crops extracted from these textures so that we can generate stationary textures that can be seam-corrected to tile without artifacts. Substance-mapped textures dataset. We also curated a broader dataset of tileable (seamless and stationary) textures from ArchiveTextures 4 for our retrieval-based texturing baseline. It consists of 146 textures from substances such as 'carpet', 'concrete', 'granite', 'metal', 'painted', 'plastic', 'tiles' and 'wood'. See the supplement for examples. These textures are seamless and scaled so that they can be directly tiled as textures on our 3D geometry. Images in this dataset roughly correspond in size to a cropped patch from the stationary textures dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Approach</head><p>Here, we focus on the texture generation and propagation stages in Plan2Scene. See the supplement for implementation details of the other stages.</p><p>We consider three families of approaches for texture generation: i) direct crop; ii) retrieval; and iii) synthesis. The first and simplest approach is to directly use the extracted rectified crop from the input surface. Unfortunately, this can result in obvious seams and repeating artifacts when tiling the texture. The second approach retrieves a texture that best represents the surface from a dataset. It can attain high quality output but is limited by the size and diversity of the texture dataset, and may not match the input surface well. The third approach generates textures conditioned on the input surface and can potentially achieve high quality output that also matches the input surface. <ref type="figure">Figure 6</ref> illustrates these three approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Overview</head><p>There are three components to our approach: neural embedding-based tileable texture synthesis, texture synthesis for observed surfaces, and texture propagation to unobserved surfaces. As the basis to our texture synthesis approach, we enhance an embedding based texture synthesis model <ref type="bibr" target="#b9">[10]</ref> so that given an input crop, we can embed it to a latent code and synthesize a tileable texture (of finite resolution) from the latent code (Section 5.2). Using our tileable neural texture synthesis network, we can then take rectified surface crops for observed surfaces and synthesize an appropriate texture (Section 5.3). As the photos may not cover all surfaces (roughly 60% are unobserved), we create a graph using the room as nodes, and connectivity between the rooms as edges. We encode the room type and the texture embeddings of the 3 surface types ('wall', 'floor', 'ceiling') for the room, and use a graph neural network (GNN) to propagate embeddings to unobserved surfaces. We can then synthesize texture for all unobserved surfaces using these propagated texture embeddings (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning an embedding for tileable textures</head><p>We extend recent work on neural textures synthesis <ref type="bibr" target="#b9">[10]</ref> to address our problem setting. To condition our texture on the input surface, we compute a texture embedding t = E(I) for each surface crop I using an encoder E. Then, given an embedding t, we decode a texture Y = D( t) for each surface. As this output is not guaranteed to be seamless, we post-process the texture to make it seamless.</p><p>Henzler et al. <ref type="bibr" target="#b9">[10]</ref>'s original approach accepts a 128 ? 128 input, which it encodes to an 8-dimensional embedding. This embedding is combined with a random noise vector to sample an infinite 2D or 3D texture. A weakness of this approach is that a separate model is trained for each substance. Moreover, the approach has difficulty disentangling color, pattern, and substance, especially for surfaces with a variety of colors (e.g., tiles, and painted walls which are common in our setting).</p><p>We extend this approach in several ways to address these limitations (see <ref type="figure">Figure 7</ref>). To separate color from texture pattern, we separate into median color and offset and convert both to the HSV color space to obtain the median HSV color and offset from the median ?HSV. A differentiable HSV to RGB conversion layer allows gradient back propa-  <ref type="figure">Figure 7</ref>: Our texture synthesis architecture employs an encoder-decoder network inspired by recent work <ref type="bibr" target="#b9">[10]</ref>, adding modules to compute ? HSV (blue) and a substance classifier (green). Components that are only used during training have dashed outlines.</p><p>gation at training time. To allow the use of a single model across different substances, we also introduce a substance classification branch. The substance classification branch is trained using a cross-entropy loss over the substance category ('wood', 'plaster', 'carpet', and 'tile'), encouraging a structured latent space based on substance type which is used for texture propagation. This setup is used to train an embedding for ?HSV using the sum of the VGG statistics loss from Henzler et al. <ref type="bibr" target="#b9">[10]</ref> and the cross-entropy loss above. We convert the median color to RGB and concatenate it with the learned embedding into a final texture embedding t ? R 8+3 which we use for synthesis of observed surfaces and propagation for unobserved surfaces.</p><p>To ensure that the generated texture has stationary statistics (e.g. there are no sharp gradient changes across the image), we train on crops from our stationary textures dataset. We use random crops from the textures to match inference time when we will be taking random crops from the input surfaces. We do not train directly on the crops from Rent3D as that produces outputs that are unsuitable for use as tileable textures. In early experiments, we observed training directly on surface crops tends to produce nonstationary textures with 'blob' artifacts caused by light gradients, shadows and specular highlights. Finally, we postprocess the generated texture with the Embark Studios Texture Synthesis Library 5 to ensure that they are seamless and exhibit no seams when tiled (see supplement for example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Texture synthesis for observed surfaces</head><p>Now that we can embed a single image crop I to a vector t = E(I), let us consider how we can synthesize textures for observed surfaces. Note that for a given surface, we may have multiple crops {I i }. From these crops {I i }, we compute a single representative surface texture embedding t * for each surface.</p><p>A naive approach is to use the mean crop embedding of 5 https : / / github . com / EmbarkStudios / texturesynthesis  <ref type="figure">Figure 8</ref>: We employ a VGG statistics-based textureness score to characterize the appropriateness of a surface crop as conditioning input for texture generation. The textureness scores are indicated on the arrows (lower is better). Wall Floor <ref type="figure">Figure 9</ref>: Our graph representation and GNN architecture for texture propagation. Nodes correspond to rooms (with matching colors). Edges indicated by orange lines with kinks at doors. Here, the GNN computes the three surface texture embeddings for the blue node and the red node.</p><p>all the crops assigned to a surface. This works poorly if a large fraction of crops are affected by artifacts such as shadows and reflections, or if the mean crop interpolates over qualitatively different texture regions. Furthermore, we want a crop which when embedded will give a good synthesized texture. Thus, instead of taking the mean, we can select a representative crop for the surface using a textureness score <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>. We use the difference of VGG statistics (employed as a similarity metric by Henzler et al. <ref type="bibr" target="#b9">[10]</ref>) between the synthesized texture and the conditioned surface crop as a proxy for the textureness score. For each crop I i , we generate an output texture D( t i ) and then select the surface crop I k that has the least L2 difference of VGG Gram Matrices with the synthesized texture. This textureness score encourages the selection of a surface crop that can be best represented by our texture synthesis model (see <ref type="figure">Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Texture propagation for unobserved surfaces</head><p>Approximately 60% of the surfaces in our dataset are unobserved by any photo. For these surfaces, we cannot directly compute t * as above since we do not have any surface crops. We instead use other observed surface embeddings and the floorplan data (room type and room-door-room connectivity) to propagate a texture embedding. To do this, we build a room-door-room connectivity graph with rooms as nodes, and edges between two rooms if they are connected by a door. We encode the room type and the texture embeddings of the three surface types ('wall', 'floor', 'ceiling') for the room, and use a graph neural network (GNN) to infer the texture embeddings of all unobserved surfaces.</p><p>We construct a room-door-room connectivity graph G = (V, E) for each floorplan. Each room r is represented by a node v r . An edge exists between rooms connected by a door (see <ref type="figure">Figure 9</ref>). Each node v r stores a feature vector x r ? R d which concatenates the room type ? r as a multi-hot encoding (?-dims) and the floor, wall and ceiling surface texture embeddings t * r,? each concatenated with a 'presence cell' (indicating if the corresponding surface is observed by setting the cell to 1, otherwise zeroing out both the embedding and the cell.) The overall feature vector is d-dimensional where d = ? + 3 ? (11 + 1).</p><p>We use a gated graph convolutional architecture based on Li et al. <ref type="bibr" target="#b16">[17]</ref>, with additional linear layers (see <ref type="figure">Figure 9</ref>). To train this GNN, we take each observed surface s of each room r in the training set as unobserved and treat it as a prediction target. As data augmentation, we mask additional surfaces to produce versions of the graph with additional unobserved surfaces. We use the L1 loss on the target surface texture embedding t * r,s and train with Adam <ref type="bibr" target="#b13">[14]</ref> (weight decay 0.0001, batch size 32, learning rate 0.0005). The architecture was implemented using PyTorch Geometric <ref type="bibr" target="#b7">[8]</ref>. See the supplement for the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Methods</head><p>We compare the following methods in our experiments. Crop: Assign surface reference crop as texture. For unobserved surfaces, we randomly select a crop with the same room and surface type (from the house if available, else from training set, relaxing room type match if necessary). Retrieve: Retrieve textures in the Substance Mapped Textures dataset with lowest pixel-wise L1 loss to the reference crop. For unobserved surfaces, we employ a similar input selection strategy as Crop. NaiveSynth: Synthesize textures using Henzler et al. <ref type="bibr" target="#b9">[10]</ref> approach trained on our textures dataset. For observed surfaces, conditioning input is the mean crop embedding. For unobserved surfaces, we compute a mean crop embedding considering surface crops assigned to all the surfaces in the training set, of the same room type and surface type. Finally, we correct the seams of synthesized texture crops as before to make them tileable. Synth: Our improved texture synthesis module for observed surfaces leveraging the textureness score, using propagation of improved embeddings with our GNN architecture to handle unobserved surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Metrics</head><p>Texture synthesis approaches are typically evaluated in terms of similarity to reference images and output diversity. In our setting, we want to 'match' the input surface crops while correcting artifacts such as shadows, specular highlights, and making the texture tileable. These criteria make pixel-wise similarity metrics insufficient, and diversity induced by a random noise vector is inappropriate. We define a suite of metrics that measure: i) how well a texture matches properties of the input surface such as color, pattern, and substance type; ii) the tileability of the textures; and iii) how well the texture set distribution matches the input surface distribution. The following metrics define each of these axes of output quality against the reference crop for each surface (or all crops from a surface for FID). COLOR: L1 distance between histograms of pixel values in HSL color space (10 bins for hue, 3 bins each for saturation and lightness), normalized to [0, 1]. FREQ: Measures pattern match. Extract periodic component of grayscale image in frequency domain <ref type="bibr" target="#b21">[22]</ref> and take L1 difference of azimuthally averaged frequency amplitude histograms (with DC component set to zero), taking mean across frequencies. SUBS: Substance classification error rate. Uses a VGG16based network to classify whether texture matches reference crop's substance type. See supplement for details. FID: Fr?chet Inception Distance <ref type="bibr" target="#b10">[11]</ref>. Measures distribution similarity between synthesized texture set and all surface crops. Lower is better. TILE: Measures uniformity of local color averages as a proxy for tileability. Based on 'mean prior loss' <ref type="bibr" target="#b0">[1]</ref>, and implemented as Tile(I gray ) = ||w f F {I gray }|| 2 where F is the Fourier operator, I gray is crop I in grayscale, and w f is the magnitude spectrum of a Gaussian with ? = 21 (1/6 texture size, as recommended by <ref type="bibr" target="#b0">[1]</ref>). Lower is better. <ref type="table" target="#tab_3">Table 1</ref> shows the overall performance of our method compared against various baseline approaches. Similarity metrics for the 'all' and 'unobserved' surfaces settings are computed by simulating a further 60% unobserved photos since we do not have reference crops for truly unobserved surfaces. Crop does well on similarity metrics by virtue of directly using the reference crop as a texure. However, it does not produce tileable textures as seen by the high TILE metric values. Overall, our Synth outperforms other baselines on both observed and unobserved surfaces. The Retrieve approach is quite competitive and even outperforms NaiveSynth on many metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Quantitative Evaluation</head><p>The supplement provides additional analyses and ablations showing that these trends hold across a range of unobserved photo fractions, and quantifying improvements due   to individual components in our approach. <ref type="figure" target="#fig_0">Figure 10</ref> compares full textured 3D house results using various methods. Crop produces obvious tiling artifacts such as notable repetitiveness and visible seams. NaiveSynth does a poor job in capturing the input appearance, in this case generating red shifted textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Qualitative Evaluation</head><p>Retrieve produces high quality textures, but it fails to match the input as well as our Synth method. For instance looking at the green shaded bathroom (which is observed) and the blue shaded bedroom (which is unobserved) in the first example, cyan room (observed) in the second example, yellow room (unobserved) in the third example, and red room (observed) in the last example, Synth better matches the appearance of the floor. See the supplement for more examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">User Study</head><p>To holistically evaluate the results of our approach against baselines, we also carry out a user study. Setup. We randomly sampled 20 houses from the test set and conducted a forced A-B choice user study. The participants were university students who were not involved with this work. They were asked to choose between two top-down 3D renderings of a textured house output (with the ceiling removed), given an input floorplan overlaid with photos assigned to the rooms. The pair of renderings was from Synth and one of the three baselines, presented in random order. Users were instructed to consider the quality of the textures (i.e. absence of discontinuities and unnecessary repetitiveness), and similarity to the surfaces observed in the photos. As the pairs contrasted the three baselines against Synth, each user made 20 ? 3 = 60 choices in shuffled order. Results. A total of 18 users participated the study. Our Synth results were preferred relative to the baselines about 70% of the time (69.4% against Retrieve, 66.9% against Crop and 72.8% against NaiveSynth). See the supplement for additional analysis of the user study results. <ref type="figure" target="#fig_0">Figure 11</ref> shows two failure cases. In the first case, a wooden floor is textured entirely as a white carpet. Here, the semantic segmentation we use included the carpet mat in the floor mask, which led to a crop from the carpet being used for floor texture synthesis. In the second case, the walls exhibit a blue tint due to the illumination and color balance of the input photo. Our crop selection helps mitigate localized illumination anomalies, but it does not correct photowide color shifts, or otherwise handle the general problem of accounting for illumination. The floor here is also light cyan due to poor crop selection caused by a severe specular Rectify (a) Semantic Mask Failure (b) Illumination Failure <ref type="figure" target="#fig_0">Figure 11</ref>: Example failure cases. Selected crops shown in blue outline. In the first case, a semantic segmentation issue leads to a carpet crop being used for the wooden floor. In the second case, a blue color shift in the photos and strong specular highlights cause poor crop selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Failure cases and limitations</head><p>highlight (crop shown in popup is entirely solid color and is likely interpreted as a plastered surface).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented the Plan2Scene task: conversion of a floorplan and set of sparse photos to a textured 3D mesh. We focused on the texture generation stages in this task, defining a suite of texture quality metrics, and proposing a texture synthesis approach with GNN-based propagation to unobserved surfaces. Our experiments show that our approach leads to higher quality textures compared to simpler baselines. Several stages of the task were simplified and are open for future investigation, including improved generation of regular patterns such as tiles. We believe conversion of floorplan and photo data to textured 3D scenes is a promising avenue for creating large volumes of 3D interiors, and will enable future work relying on large-scale 3D interior datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our system addresses the Plan2Scene task by converting a floorplan and set of photos to a textured 3D mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Selected textures from curated textures dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative comparison of results on test set, with simulated unobserved photos (dashed lines indicate unobserved photos). Crop produces textures that do not tile well. Retrieve is a competitive baseline, but does not match the input as well as our Synth approach does.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Overall results on test set. Lower values are better. Similarity metrics in the 'Unobserved' and 'All' columns are reported by simulating 60% of photos as unobserved. All similarity metrics treat the reference crop as the ground truth (note that this benefits Crop which uses the reference crop as a texture). We observe that Synth outperforms Retrieve and NaiveSynth across all metrics, and outperforms all approaches in terms of tileability.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Observed</cell><cell></cell><cell></cell><cell>Unobserved</cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">COLOR FREQ SUBS</cell><cell cols="2">FID TILE</cell><cell>COLOR FREQ SUBS</cell><cell cols="2">FID TILE</cell><cell>COLOR FREQ SUBS</cell><cell cols="2">FID TILE</cell></row><row><cell>Crop</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>38.1</cell><cell>0.768 0.026 0.345</cell><cell>57.2</cell><cell>40.6</cell><cell>0.459 0.016 0.208</cell><cell>35.6</cell><cell>39.5</cell></row><row><cell>Retrieve</cell><cell cols="4">0.561 0.054 0.473 238.2</cell><cell>17.3</cell><cell cols="2">0.751 0.040 0.437 261.5</cell><cell>19.1</cell><cell cols="2">0.680 0.046 0.458 243.2</cell><cell>18.3</cell></row><row><cell>NaiveSynth</cell><cell cols="4">0.694 0.046 0.385 239.3</cell><cell>21.7</cell><cell cols="2">0.714 0.044 0.427 245.4</cell><cell>19.8</cell><cell cols="2">0.709 0.046 0.404 239.4</cell><cell>20.6</cell></row><row><cell>Synth (ours)</cell><cell cols="4">0.431 0.035 0.350 196.1</cell><cell>16.4</cell><cell cols="2">0.653 0.032 0.393 199.4</cell><cell>18.6</cell><cell cols="2">0.591 0.034 0.392 196.2</cell><cell>17.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">By projecting the crops into the 3 ? H ? W RGB vector space and selecting the crop closest to the mean vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https : / / www . pexels . com/, https : / / www . sketchuptextureclub.com/, https://www.freepik.com/, https://3djungle.net/ 4 https://archivetextures.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>We thank all the participants in our user study: Akshit Sharma, Andy Wang </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reflectance modeling by neural texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey of exemplarbased texture synthesis methods. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adib</forename><surname>Akl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Yaacoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Donias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre Da</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Germain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="12" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenSurfaces: A richly annotated catalog of surface appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piecewise planar and compact floorplan reconstruction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
	</analytic>
	<monogr>
		<title level="m">ShapeNet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intelligent home 3D: Automatic 3D-house design from linguistic descriptions only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12622" to="12631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The synthesizability of texture examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3027" to="3034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reconstructing building interiors from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a neural 3D texture space from 2D exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3Dlite: towards commodity 3D scanning for content creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="203" to="204" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5134" to="5143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modeling surface appearance from a single photograph using selfaugmented convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diversified texture synthesis with feed-forward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Materials for masses: SVBRDF acquisition with a single mobile phone image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="72" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Floorplanjigsaw: Jointly estimating scene layout and aligning partial scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5674" to="5683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Raster-to-vector: Revisiting floorplan transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2214" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rent3D: Floor-plan priors for monocular layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Periodic plus smooth image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Moisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="161" to="179" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic room detection and reconstruction in cluttered indoor environments with complex room layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Jaspe</forename><surname>Villanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Pajarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset of multi-illumination images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Murmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4080" to="4089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Texture fields: Learning texture representations in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4530" to="4539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HorizonNet: Learning room layout with 1D representation and pano stretch data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wei</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1047" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic texture exemplar extraction based on global and local textureness measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenkun</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="173" to="184" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reconstructing the world&apos;s museums. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="243" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DuLa-Net: A dual-projection network for estimating room layouts from a single RGB panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Ta</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Han</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3363" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-image piece-wise planar 3D reconstruction via associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GeoLayout: Geometry driven room layout estimation based on depth maps of planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="632" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PanoContext: A whole-room 3D context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="668" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">State of the art on 3D reconstruction with RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Stotko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>G?rlitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="625" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LayoutNet: Reconstructing the 3D room layout from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2051" to="2059" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
