<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Cai</surname></persName>
							<email>caiyangang@pku.edu.cnrgwang@pkusz.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised methods play an increasingly important role in monocular depth estimation due to their great potential and low annotation cost. To close the gap with supervised methods, recent works take advantage of extra constraints, e.g., semantic segmentation. However, these methods will inevitably increase the burden on the model. In this paper, we show theoretical and empirical evidence that the potential capacity of self-supervised monocular depth estimation can be excavated without increasing this cost. In particular, we propose (1) a novel data augmentation approach called data grafting, which forces the model to explore more cues to infer depth besides the vertical image position, (2) an exploratory self-distillation loss, which is supervised by the self-distillation label generated by our new post-processing method -selective post-processing, and (3) the full-scale network, designed to endow the encoder with the specialization of depth estimation task and enhance the representational power of the model. Extensive experiments show that our contributions can bring significant performance improvement to the baseline with even less computational overhead, and our model, named EPCDepth, surpasses the previous state-of-the-art methods even those supervised by additional constraints. Code is available at https://github.com/prstrive/EPCDepth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth estimation has always been a fundamental problem of computer vision, which dominates the performance of various applications, e.g., virtual reality, autonomous driving, robotics, etc. As the cheapest solution, monocular depth estimation (MDE) has made considerable progress due to the evolvement of Convolution Neural Networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16]</ref>. However, most existing state-of-the-art approaches rely on supervised training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b0">1]</ref>, whose training datasets collection is a cumbersome and formidable challenge. As an alternative, self-supervised methods elimi-Monodepth <ref type="bibr" target="#b11">[12]</ref> Monodepth2 <ref type="bibr" target="#b12">[13]</ref> DepthHint <ref type="bibr" target="#b48">[49]</ref> EdgeDepth <ref type="bibr" target="#b67">[68]</ref> EPCDepth nate the need for ground-truth depth through recasting depth estimation as the reconstruction problem among stereo images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">68]</ref>, monocular video <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref> or a combination of both <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In terms of performance alone, recent works have shown that the gap between self-supervision and full-supervision has made a de facto reduction. But on the other hand, this reduction largely benefits from the sophisticated model architecture and extra constraints from external modalities, e.g., semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b14">15]</ref>, optical flow <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b39">40]</ref>, depth normal <ref type="bibr" target="#b53">[54]</ref>, etc. Apparently, these factors substantially increase the burden of the model training and run counter to the concept of self-supervision to some extent. In this paper, we show the potential of self-supervised monocular depth estimation even without these additional constraints from three aspects: data augmentation, selfdistillation, and model architecture.</p><p>Generally, the closer the projection on the image is to the lower boundary, the smaller the depth of the object. This feature of vertical image position has been proven to be the main cue adopted by the MDE model to infer depth, while the apparent size and other cues that humans will rely on are ignored <ref type="bibr" target="#b46">[47]</ref>. We conjecture that the reason is that in the traditional training mechanism that takes the entire image as input, the feature of vertical image position exists in almost every training sample, while the number of samples for other cues is relatively small, which leads to a longtailed distribution on cues. Obviously, this kind of paranoia tends to damage the generalization ability of the model. To solve this, we propose a novel data augmentation method called Data Grafting, which breaks this dilemma by vertically grafting a certain proportion from another image to appropriately weaken the relationship between depth and vertical image position. Moreover, there is another fact that the precision of different scales output by the multi-scale network is inconsistent at different pixels, and this motivates us to generate better disparity maps as pseudo-labels to realize the self-distillation of the model. Concretely, we propose Selective Post-Processing (SPP) to select the best prediction for each pixel among all scales according to the reconstruction error, which is inspired by the availability of all views during training, and the similar idea has been proven effective in the field of multi-view stereo <ref type="bibr" target="#b54">[55]</ref>. Finally, we extend the traditional multi-scale network to the full-scale network by inserting prediction modules not only on the decoder but also on the encoder to advance the specialization of depth prediction from decoder to encoder and absorb the representational power of the model. The superior result of our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To summarize, our main contributions are listed below in fourfold:</p><p>? We introduce a conceptually simple but empirically efficient data augmentation approach, which enables the model to learn more effective cues besides the vertical image position.</p><p>? We apply self-distillation to MDE for the first time without any auxiliary network and generate better pseudolabels based on our training-oriented selective postprocessing method.</p><p>? We propose a more efficient full-scale network to strengthen the constraints on the model and enhance the encoder's specificity of depth estimation.</p><p>? Without bells and whistles, we achieve state-of-the-art performance within self-supervised methods even compared to those high-performance models that are trained by extra constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Self-Supervised Monocular Depth Estimation. The depth is predicted as an intermediate in self-supervised MDE to synthesize the reconstructed view from the source view, and the photometric loss between the target view and the reconstructed view is calculated as the target of minimization. There are mainly two kinds of self-supervised methods: trained by synchronized stereo images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">68]</ref> or monocular video <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42]</ref>. For the first category, the model with known relative placement only needs to predict the disparity, that is, the inverse of the depth. For the second category, additional predictions of the relative pose of the camera are required. Recently, abundant works have improved the performance of self-supervised MDE through new loss function <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b67">68]</ref>, new architecture <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref> and new supervision from extra constraints <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>In this paper, we further excavate the potential capacity of self-supervised MDE with the realization of training on stereo images. Self-Distillation. Knowledge distillation is a pioneering work to transfer knowledge from powerful teacher networks to student networks using the softmax output <ref type="bibr" target="#b17">[18]</ref>, intermediate feature <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17]</ref>, attention <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b20">21]</ref>, relationship <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, etc. Self-distillation is a special case where the model itself is used as a teacher. Intuitively, the model can be distilled by the same model trained previously <ref type="bibr" target="#b8">[9]</ref>, but these approaches are inefficient because they need to train multiple generations synchronously. Therefore, some recent works advocate distilling the model within one generation, which take supervision from prior iterations <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b23">24]</ref>, consistency of distorted data <ref type="bibr" target="#b50">[51]</ref>, invariance among intraclass <ref type="bibr" target="#b59">[60]</ref> and the output of deeper portion <ref type="bibr" target="#b63">[64]</ref>.</p><p>These methods only focus on the self-distillation of the classification task. In this work, we applied self-distillation to the regression task of depth estimation. Different from the method of using the whole network to promote subnetworks in <ref type="bibr" target="#b37">[38]</ref>, we select the optimal disparity map from all output scales as the self-distillation label to distill the whole network in one generation. Data Augmentation. For overfitting, data augmentation is an efficient approach to mitigate this drawback by implicitly increasing the total amount of training data and teaching models about the invariance of the data domain. Common data augmentation methods can be summarized into two categories: learnable <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4]</ref> and parameter learning free <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65]</ref>. Learnable methods are more universal and work out of the box, while the subsequent methods are easier to be implemented and most of them are tailored to specific datasets.</p><p>Motivated by the fact that the monocular depth estimation model mainly relies on the vertical image position and tends to overlook other useful cues, we propose a new pa-  <ref type="figure">Figure 2</ref>. Framework illustration. The proposed approach is mainly composed of three procedures. The input batch data is first refactored by data grafting, and here we take the grafting ratio of 0.6 as an example. Immediately after that, the full-scale network will estimate the disparity map at all scales, which means that not only the decoder but also the encoder will infer the disparity. Finally, the full-scale disparity will be used to generate the self-distillation label through selective post-processing for the encoder and decoder scale separately and calculate the loss l sd . Meanwhile, the model will be trained with the assistance of photometric loss l ph and depth hint loss l h , and it is worth noting that these losses are executed on all scales. rameter learning free data augmentation method, called data grafting, to force the model to explore more cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We adopt rectified stereo pairs as the input of our selfsupervised model in training, while only a single image is required to infer depth at test time. This kind of selfsupervised method is mainly divided into three steps. The model F : I ? d ? R H?W , that will first estimate the disparity map d, which represents the offset of the corresponding pixel between the stereo pair, from the target view I ? R C?H?W . Next, the model will be trained iteratively by minimizing the discrepancy between the target view and the view? reconstructed from the source view I with differentiable warping f w (I , d). The photometric loss measured with the combination of SSIM <ref type="bibr" target="#b47">[48]</ref> and L1 is often adopted to express the discrepancy between the target view and the reconstructed view just as:</p><formula xml:id="formula_0">l ph (d) = l ph (I,?) = ? 1 ? SSIM (I,?) 2 + ?|I ??| (1)</formula><p>where SSIM() is computed over a 3 ? 3 pixel window, with ? = 0.85 and ? = 0.15. Finally, the depth map z ? R H?W will be recovered from d, which is outputted by the trained model, with known baseline b and focal length f under formula z = bf /d.</p><p>In this section, we will introduce the main contributions of this paper in detail. The framework pipeline is just shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Grafting</head><p>Lack of data in both quantity and diversity is the first tricky obstacle faced by monocular depth estimation, which will damage the generalization ability of the model. One of the significant overfitting risks in MDE is the excessive dependence on the vertical image position as described in Sec. 1. Although data augmentation is the most cost-effective and ubiquitous solution, there is almost no relevant research on existing self-supervised MDE methods, and only some simple data perturbations such as horizontal flipping are used. The reason mainly lies in that self-supervised MDE methods generate supervisory signals based on the degree of matching between views, which requires strict pixel correspondence (epipolar constraint) to ensure that the matching error only comes from the estimated disparity. Obviously, the traditional data augmentation method will break this correspondence, thereby damaging the performance of the model as shown in our experiments in Sec. 5.2.</p><p>However, we note that this restriction is relaxed in the category with stereo pairs as input. Because the two views were taken with parallel cameras and rectified, the match between them will only occur in the horizontal direction, e.g., panning left or right. Therefore, we can do perturbation in the vertical direction to augment our data.</p><p>To this end, we found that grafting two images with different semantics together can effectively alleviate the overfitting risk in MDE and encourage the model to better utilize the full context of the input without destroying the epipolar constraint. We conduct the data grafting within a minibatch, and it is determined by two hyper-parameters: the grafting ratio r and the corresponding uniform probability p. We reconstruct the input by vertically grafting an area with a proportion of r from another input with the probability of p, and randomly flip these two parts vertically, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Meanwhile, grafting is not only for the target view, but also for its corresponding Depth Hint, which will be introduced in Sec. 3.4, and the source view. But each grafting operation can only be performed between the same category, e.g., both are target views. And the grafting config of all inputs in a batch is the same. The grafting detail of a single input is shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Full-scale Network</head><p>The coarse-to-fine strategy has been proven effective in MDE which continuously refines the estimation with iterative warping <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49]</ref>. The common practice is to output multi-scale disparity prediction in the decoder, whose spatial size is incremental. In this scenario, the knowledge learned by the encoder is more abstract and general, while that in the decoder is more specific to the depth estimation task.</p><p>Intuitively, advancing the specialization of depth estimation to the encoder can give stronger constraints to the model and further improve its performance. Therefore, we extend the traditional multi-scale to full-scale, which means that we also add the multi-scale disparity prediction block to the encoder. Meanwhile, we insert a residual block, or more precisely an RSU block <ref type="bibr" target="#b38">[39]</ref>, between the prediction block and the residual stage in the encoder as the bridge to mitigate the impacts between different scales.</p><p>Furthermore, just as depicted in <ref type="figure" target="#fig_4">Figure 4</ref>, we adopt the RSU block, which is more powerful and more lightweight, to construct the decoder to draw the representational capacity of our full-scale network. After training, we can discard the encoder-scale or even part of the decoder-scale, and only retain the largest scale of the decoder, which means that the full-scale network will not bring more parameters or computation than the traditional network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-distillation</head><p>Self-distillation is an effective way to generate more supervised signals for the model, and it is particularly important for self-supervised learning. Here, we propose selective post-processing to generate the self-distillation label, and with which we create a new loss, termed Self-Distillation Loss l sd , for the model.</p><p>Selective Post-Processing aims to filter out the optimal disparity at each pixel from multiple disparity scales. Actually, the largest disparity map in the decoder that we of- ten output is not always the best at all pixels, as shown in <ref type="table">Table 5</ref>. Maybe the "d0" scale is better at pixel a but the "d3" scale is better at pixel b. Hence, to distinguish the optimal scale at each pixel, we adopt the reconstruction error or the photometric loss as our criterion, which is inspired by <ref type="bibr" target="#b48">[49]</ref>. Given the full-scale disparity maps D = [d d0 , . . . , d d4 , d e0 , . . . , d e4 ], we will calculate a reconstruction error map for each scale according to Equation <ref type="bibr" target="#b0">(1)</ref>. Then, the self-distillation label of encoder y e and decoder y d can be constructed based on the assumption that the smaller the error, the better the predicted disparity. The detailed procedure of SPP, which is the same between the encoder-scale and decoder-scale, is shown in Algorithm 2. The statistic result in <ref type="figure" target="#fig_5">Figure 5</ref> shows that the SPP can get the most precise results. Find all the pixels where e &lt; e min ; <ref type="bibr" target="#b10">11</ref> Update y with d at these pixels; <ref type="bibr" target="#b11">12</ref> Update e min with e at these pixels; Self-Distillation Loss is the differenc between the disparity map and the self-distillation label for each scale, and it can be modeled as:</p><formula xml:id="formula_1">l sd (d) = log (|y c(d) ? d| + 1)<label>(2)</label></formula><p>where c(?) is used to determine whether d belongs to the decoder-scale or the encoder-scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss</head><p>Following <ref type="bibr" target="#b48">[49]</ref>, we incorporate the hint loss that has been proven effective for thin structures into our model. The Depth Hint h is generated by the Semi-Global Matching (SGM) algorithm <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and be consulted only when the reconstruction error can be improved upon. It can be formulated for pixel i in each scale as:</p><formula xml:id="formula_2">l h (d i ) = log (|h i ? d i | + 1), if l ph (I,? h ) i &lt; l ph (I,?) i 0, otherwise<label>(3)</label></formula><p>where? h denotes the reconstructed view with Hint h.</p><p>Therefore, the final training loss is composed of the average of the three items of photometric loss, self-distillation loss and hint loss at each scale:</p><formula xml:id="formula_3">l = 1 |D| d?D (l ph (d) + l sd (d) + l h (d))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>We implement our model in PyTorch <ref type="bibr" target="#b34">[35]</ref>. The procedure of calculating Depth Hint is the same as that of <ref type="bibr" target="#b48">[49]</ref>. We use Adam <ref type="bibr" target="#b24">[25]</ref> optimizer with the base learning rate of 1e-4 and train the joint loss for 20 epochs. Besides our new data augmentation approach, we adopted the preprocessing techniques in <ref type="bibr" target="#b12">[13]</ref>. In data grafting, we found that the grafting ratio r = 0.2 ? n, where n ? N and r &lt; 1, can get the best effect, as shown in Algorithm 1. Unless otherwise specified, we take ResNet-18 which is pre-trained on Ima-geNet <ref type="bibr" target="#b22">[23]</ref> as the encoder and resize the input to 320?1024. As for the RSU block <ref type="bibr" target="#b38">[39]</ref>, we remove the Batch Normalization layer <ref type="bibr" target="#b21">[22]</ref> and replace the ReLU <ref type="bibr" target="#b32">[33]</ref> with ELU <ref type="bibr" target="#b2">[3]</ref> activation. More specifically, we take RSU3 ? RSU7 to construct the decoder's layers and the encoder's bridges from minimum scale to maximum scale respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first verify the performance of our model on the KITTI dataset <ref type="bibr" target="#b10">[11]</ref>, and perform a comprehensive ablation study on each component. Finally, the generalization ability of our model is validated on the NYU-Depth-v2 dataset <ref type="bibr" target="#b42">[43]</ref>. KITTI Stereo was recorded from a driving vehicle and contains 42,382 rectified stereo pairs from 61 scenes. To ensure the objectivity of comparison, we utilize the Eigen split <ref type="bibr" target="#b5">[6]</ref>, which is composed of 22,600 training image pairs in 32 scenes, and 697 test pairs in other 29 scenes. We report all seven of the standard metrics <ref type="bibr" target="#b6">[7]</ref> with Garg's crop <ref type="bibr" target="#b9">[10]</ref> and a standard distance cap of 80 meters <ref type="bibr" target="#b11">[12]</ref>.  <ref type="table">Table 1</ref>. Quantitative results on the KITTI dataset <ref type="bibr" target="#b10">[11]</ref> using the split of Eigen et al. <ref type="bibr" target="#b5">[6]</ref>. Best results in each category are in bold. For red metrics, lower is better. And higer is better for blue metrics. Abbreviation in Data column: D refers to methods that are supervised by the ground truth depth, D ? use auxiliary depth supervision from SLAM, D * use auxiliary depth supervision from synthetic depth labels, C for supervision from segmentation labels, C ? for supervision from predicted segmentation labels, S refers to the supervision from stereo images and M for models trained by monocular video. PP represents post-processing <ref type="bibr" target="#b11">[12]</ref>. The underlined model is our baseline. We annotate all the methods that use extra tricks, e.g., fine-tuning and teacher model.</p><p>NYU-Depth-v2 was captured with a Microsoft Kinect sensor and consists of a total 582 indoor scenes. We validate our model on the official test set using the same standard metrics as in KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Depth Estimation Performance</head><p>We conduct a comprehensive comparison with multifarious methods on the KITTI benchmark to verify our depth estimation performance. First of all, we need to emphasize that our model is only trained on KITTI stereo data and is trick-free. We compare our approach with the recent self-, semi-and fully-supervised monocular depth estimation methods in <ref type="table">Table 1</ref>. And the results show that our approach outperforms all existing self-supervised methods on all metrics and even some of the fully-supervised methods. Our approach of training only on stereo pairs improves 0.013 on the ? &lt; 1.25 compared to our baseline model <ref type="bibr" target="#b48">[49]</ref>, and this improvement is 225% (= 0.899?0.886 0.890?0.886 ? 1) higher than that of <ref type="bibr" target="#b67">[68]</ref>, which has finetuned the model and used additional constraints. Furthermore, our method is not only outstanding in the category trained with stereo images, but also has a major advantage in the category of methods trained with stereo video (MS). Even if compared with the best score of each metric in the MS category, our approach won out in most metrics. Moreover, we have done more experiments on low-resolution and complex backbones to demonstrate the generality and robustness of our model, and the consistent performance improvement obtained just proves it. It's worth noting that we have further reduced the gap between full-supervision and self-supervision by nearly 79% (= 1? 0.904?0.901 0.904?0.890 ) compared to our baseline <ref type="bibr" target="#b48">[49]</ref>. Besides, the qualitative results in <ref type="figure">Figure 6</ref> show that our model predicts more accurately in challenging areas.</p><p>While our model significantly improves the performance of the baseline, it also retains the advantages of simple implementation. Each plug-and-play improvement can be easily integrated into other models, which is critical for future in-depth studies of monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>We perform ablation analysis on the KITTI. The results in <ref type="table" target="#tab_3">Table 6</ref> show that our full model combining all components has leading performance, and the baseline model, without any of our contributions, performs the worst. Benefits of data grafting. With data grafting, we can implicitly increase the amount of data by 1/p times on the basis of our baseline. The results in <ref type="table">Table 2</ref> show that only 20% of the data is used to obtain competitive performance to the model without data grafting under 100% of the data, which just verifies the strong generalization ability of our model. Moreover, we make a comparison with other similar augmentation methods to demonstrate our effectiveness in <ref type="table">Table 3</ref>. The result just shows that breaking the relationship between the depth and the vertical image position with a certain probability, which is the uniqueness of data grafting, can allow the model to potentially grasp more effective cues. The unsatisfaction of other methods may lie in the lack of regularization ability for the vertical image position and the damage of the epipolar constraint between views at the edge of the hole. Meanwhile, we conduct a sensitivity experiment on the grafting ratio. The resuls in <ref type="figure" target="#fig_8">Figure 7</ref> show that the odd setting (e.g. n/3) is generally better than even setting (e.g. n/2), and performs best when r = n/5, which indicates that the grafting result holding a piece of dominant semantic information is more effective. Benefits of self-distillation. From <ref type="table">Table 5</ref>, we expect to select the optimal scale for each pixel through selective post-processing. The comparison results in <ref type="table">Table 4</ref> between different label generation methods show that the SPP can get more stable improvement and distilling encoder and decoder separately is more effective. Meanwhile, we no-   ticed that the magnitude of its performance improvement is minimal and is affected by the capacity of the model. But we hope that our exploration can open the door to selfdistillation in this regression task.</p><p>Benefits of full-scale network. Our full-scale network draws on some advantages of the multi-generation strategy, that is to impose more constraints on the model, and the results in <ref type="table" target="#tab_3">Table 6</ref> just prove its power. Furthermore, we explore the effectiveness of the encoder scale, RSU blocks and the encoder's bridges respectively, by ablating their effects in <ref type="table" target="#tab_4">Table 7</ref>. Note that each experiment is carried out on the basis of the previous experiment. The continuous performance improvement of each module proves their effectiveness. Meanwhile, our full-scale network achieves superior performance with 9.88 GFLOPS at test time, compared to 10.1 GFLOPS of the traditional network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>Monodepth2 <ref type="bibr" target="#b12">[13]</ref> DepthHint <ref type="bibr" target="#b48">[49]</ref> EdgeDepth <ref type="bibr" target="#b67">[68]</ref> Ours (EPCDepth) GT Input <ref type="figure">Figure 8</ref>. Qualitative results on the NYU-Depth-v2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.  <ref type="table">Table 8</ref>. Quantitative results on the NYU-Depth-v2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalizing to NYU-Depth-v2</head><p>Since there are no stereo pairs in NYU-Depth-v2 dataset, we train on the KITTI dataset and then test on it just as Monodepth <ref type="bibr" target="#b11">[12]</ref> did on Make3D. The preprocessing strategy we adopt is the same as that of <ref type="bibr" target="#b57">[58]</ref>, and median scaling is applied for all models. The results shown in <ref type="table">Table 8</ref> and <ref type="figure">Figure 8</ref> just verify our strong generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We extracted the potential capacity of self-supervised monocular depth estimation through our novel data augmentation method, exploratory self-distillation and efficient full-scale network. The experiments demonstrate that our model (EPCDepth) can yield the best performance with the least computational cost. In future work, we will try to further improve the performance of self-distillation by exploring more accurate label generation methods. Besides, applying our contributions to other categories, e.g., M, MS and even supervised method, is also a potential direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Depth estimation from a single image. Our model (EPCDepth), trained only on stereo data, performs the best and produces the sharpest and most complete result with the least computational cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of data grafting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 6 I 1 8 T = I 1 ; 9 I 1</head><label>1618191</label><figDesc>Data Grafting Input: Input I 1 ; Another input of the same category randomly sampled from the same batch I 2 ; Shape of input (c, h, w); Random vertical flip factor f lip. Output: Grafted input I 1 . 1 Random sampling r from {0, 0.2, 0.4, 0.6, 0.8} with the uniform probability of 0.2; 2 if r = 0 then 3 return I 1 . 4 else 5 graf t h = Ceil(h ? r); [:, graf t h :, :] ? I 2 [:, graf t h :, :]; 7 if f lip &lt; 0.5 then [:, h ? graf t h :, :] ? T [:, : graf t h, :]; 10 I 1 [:, : h ? graf t h, :] ? T [:, graf t h :, :];</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Full-scale network. The "e0 ? e4" represents the scale in the encoder and "d0 ? d4" represents the scale in the decoder. The spatial size of each scale increases with the decrease of serial number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Precision improvement statistics of SPP result on each scale for all test samples in Eigen split<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2 : 7 y = d; 8 e</head><label>278</label><figDesc>Selective Post-Processing Input: The target view I; The source view I ; Multi-scale disparity maps D . Output: Self-distillation label y. 1 Initialization: e min = N one; 2 for d in D do 3 Upsample d to the same size as I; 4 Reconstruct target view? = f w (I , d); 5 Calculate the reconstruction error e = l ph (I,?); 6 if d = D [0] then min = e;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Sensitivity analysis of grafting ratio r. The smaller the value, the better in the red line chart, and the worse in the blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25<ref type="bibr" target="#b2">3</ref> Eigen et al.</figDesc><table><row><cell>Method</cell><cell>PP</cell><cell>Data</cell><cell>H ? W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[7]</cell><cell></cell><cell>D</cell><cell>184 ? 612</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell><cell>0.702</cell><cell>0.890</cell><cell>0.890</cell></row><row><cell>Kuznietsov et al. [28]</cell><cell></cell><cell>DS</cell><cell>187 ? 621</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>Yang et al. [53]</cell><cell></cell><cell>D  ? S</cell><cell>256 ? 512</cell><cell>0.097</cell><cell>0.734</cell><cell>4.442</cell><cell>0.187</cell><cell>0.888</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>Luo et al. [31]</cell><cell></cell><cell cols="2">D  *  DS 192 ? 640 crop</cell><cell>0.094</cell><cell>0.626</cell><cell>4.252</cell><cell>0.177</cell><cell>0.891</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell>Fu et al. [8]</cell><cell></cell><cell>D</cell><cell>385 ? 513 crop</cell><cell>0.099</cell><cell>0.593</cell><cell>3.714</cell><cell>0.161</cell><cell>0.897</cell><cell>0.966</cell><cell>0.986</cell></row><row><cell>Lee et al. [30]</cell><cell></cell><cell>D</cell><cell>352 ? 1216</cell><cell>0.091</cell><cell>0.555</cell><cell>4.033</cell><cell>0.174</cell><cell>0.904</cell><cell>0.967</cell><cell>0.984</cell></row><row><cell>Zhan et al. [62]</cell><cell></cell><cell>MS</cell><cell>160 ? 608</cell><cell>0.135</cell><cell>1.132</cell><cell>5.585</cell><cell>0.229</cell><cell>0.820</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell>Godard et al. [13]</cell><cell></cell><cell>MS</cell><cell>320 ? 1024</cell><cell>0.104</cell><cell>0.775</cell><cell>4.562</cell><cell>0.191</cell><cell>0.878</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>Watson et al. [49]</cell><cell></cell><cell>MS</cell><cell>320 ? 1024</cell><cell>0.098</cell><cell>0.702</cell><cell>4.398</cell><cell>0.183</cell><cell>0.887</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>Shu et al. [42]</cell><cell></cell><cell>MS</cell><cell>320 ? 1024</cell><cell>0.099</cell><cell>0.697</cell><cell>4.427</cell><cell>0.184</cell><cell>0.889</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Lyu et al. [32]</cell><cell></cell><cell>MS</cell><cell>320 ? 1024</cell><cell>0.101</cell><cell>0.716</cell><cell>4.395</cell><cell>0.179</cell><cell>0.899</cell><cell>0.966</cell><cell>0.983</cell></row><row><cell>Garg et al. [10]</cell><cell></cell><cell>S</cell><cell>188 ? 620</cell><cell>0.169</cell><cell>1.080</cell><cell>5.104</cell><cell>0.273</cell><cell>0.740</cell><cell>0.904</cell><cell>0.962</cell></row><row><cell>Godard et al. [12]</cell><cell></cell><cell>S</cell><cell>256 ? 512</cell><cell>0.138</cell><cell>1.186</cell><cell>5.650</cell><cell>0.234</cell><cell>0.813</cell><cell>0.930</cell><cell>0.969</cell></row><row><cell>Wong et al. [50]</cell><cell></cell><cell>S</cell><cell>256 ? 512</cell><cell>0.133</cell><cell>1.126</cell><cell>5.515</cell><cell>0.231</cell><cell>0.826</cell><cell>0.934</cell><cell>0.969</cell></row><row><cell>Pilzer et al. [38] Teacher</cell><cell></cell><cell>S</cell><cell>256 ? 512</cell><cell>0.098</cell><cell>0.831</cell><cell>4.656</cell><cell>0.202</cell><cell>0.882</cell><cell>0.948</cell><cell>0.973</cell></row><row><cell>Chen et al. [2]</cell><cell></cell><cell>SC</cell><cell>256 ? 512</cell><cell>0.118</cell><cell>0.905</cell><cell>5.096</cell><cell>0.211</cell><cell>0.839</cell><cell>0.945</cell><cell>0.977</cell></row><row><cell>Godard et al. [13]</cell><cell></cell><cell>S</cell><cell>192 ? 640</cell><cell>0.108</cell><cell>0.842</cell><cell>4.891</cell><cell>0.207</cell><cell>0.866</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Watson et al. [49]</cell><cell></cell><cell>S</cell><cell>192 ? 640</cell><cell>0.106</cell><cell>0.780</cell><cell>4.695</cell><cell>0.193</cell><cell>0.875</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>Ours</cell><cell></cell><cell>S</cell><cell>192 ? 640</cell><cell>0.099</cell><cell>0.754</cell><cell>4.490</cell><cell>0.183</cell><cell>0.888</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Pillai et al. [37]</cell><cell></cell><cell>S</cell><cell>384 ? 1024</cell><cell>0.112</cell><cell>0.875</cell><cell>4.958</cell><cell>0.207</cell><cell>0.852</cell><cell>0.947</cell><cell>0.977</cell></row><row><cell>Godard et al. [13]</cell><cell></cell><cell>S</cell><cell>320 ? 1024</cell><cell>0.105</cell><cell>0.822</cell><cell>4.692</cell><cell>0.199</cell><cell>0.876</cell><cell>0.954</cell><cell>0.977</cell></row><row><cell>Watson et al. [49]</cell><cell></cell><cell>S</cell><cell>320 ? 1024</cell><cell>0.099</cell><cell>0.723</cell><cell>4.445</cell><cell>0.187</cell><cell>0.886</cell><cell>0.962</cell><cell>0.981</cell></row><row><cell>Zhu et al. [68] Finetuned</cell><cell></cell><cell>SC  ?</cell><cell>320 ? 1024</cell><cell>0.097</cell><cell>0.675</cell><cell>4.350</cell><cell>0.180</cell><cell>0.890</cell><cell>0.964</cell><cell>0.983</cell></row><row><cell>Ours</cell><cell></cell><cell>S</cell><cell>320 ? 1024</cell><cell>0.093</cell><cell>0.671</cell><cell>4.297</cell><cell>0.178</cell><cell>0.899</cell><cell>0.965</cell><cell>0.983</cell></row><row><cell>Watson et al. [49] ResNet50</cell><cell></cell><cell>S</cell><cell>320 ? 1024</cell><cell>0.096</cell><cell>0.710</cell><cell>4.393</cell><cell>0.185</cell><cell>0.890</cell><cell>0.962</cell><cell>0.981</cell></row><row><cell>Zhu et al. [68] Finetuned ResNet50</cell><cell></cell><cell>SC  ?</cell><cell>320 ? 1024</cell><cell>0.091</cell><cell>0.646</cell><cell>4.244</cell><cell>0.177</cell><cell>0.898</cell><cell>0.966</cell><cell>0.983</cell></row><row><cell>Ours ResNet50</cell><cell></cell><cell>S</cell><cell>320 ? 1024</cell><cell>0.091</cell><cell>0.646</cell><cell>4.207</cell><cell>0.176</cell><cell>0.901</cell><cell>0.966</cell><cell>0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Qualitative results. Our model (EPCDepth) in the last column produces the most accurate and sharpest results, especially in challenging areas, e.g., tree trunks, cars, etc. Ablation study on training data amount. DG refers to data grafting. And the % means the percentage of data amount. Comparison against other similar augmentation methods. And the input size is 192 ? 640.</figDesc><table><row><cell>Input</cell><cell>Godard</cell><cell>. [12]</cell><cell>Godard</cell><cell>. [13]</cell><cell>Watson</cell><cell>. [49]</cell><cell>Zhu</cell><cell>. [68]</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Ablation study on distillation source. PP refers to the post-processing result of the largest scale in the decoder. Quantitative results of different scales.</figDesc><table><row><cell>Source</cell><cell></cell><cell cols="4">Abs Rel Sq Rel RMSE ? &lt; 1.25</cell></row><row><cell>PP</cell><cell></cell><cell>0.094</cell><cell>0.680</cell><cell>4.320</cell><cell>0.898</cell></row><row><cell>SPP</cell><cell></cell><cell>0.094</cell><cell>0.675</cell><cell>4.312</cell><cell>0.899</cell></row><row><cell cols="2">SPP separate</cell><cell>0.093</cell><cell>0.671</cell><cell>4.297</cell><cell>0.899</cell></row><row><cell cols="6">Scale Abs Rel Sq Rel RMSE ? &lt; 1.25 ? &lt; 1.25 2</cell></row><row><cell>d0</cell><cell>0.0925</cell><cell>0.671</cell><cell>4.297</cell><cell>0.899</cell><cell>0.965</cell></row><row><cell>d1</cell><cell>0.0922</cell><cell>0.668</cell><cell>4.292</cell><cell>0.899</cell><cell>0.965</cell></row><row><cell>d2</cell><cell>0.092</cell><cell>0.655</cell><cell>4.268</cell><cell>0.898</cell><cell>0.965</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>MethodDG SD FS HR Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25<ref type="bibr" target="#b2">3</ref> Ablation resuls amongst variants of our model (EPCDepth) on the KITTI dataset. DG refers to data grafting, SD refers to self-distillation, FS refers to full-scale and HR refers to high resolution.</figDesc><table><row><cell>Baseline</cell><cell>0.107</cell><cell>0.848</cell><cell>4.745</cell><cell>0.194</cell><cell>0.875</cell><cell>0.957</cell><cell>0.980</cell></row><row><cell>Baseline + DG</cell><cell>0.102</cell><cell>0.782</cell><cell>4.581</cell><cell>0.188</cell><cell>0.883</cell><cell>0.960</cell><cell>0.981</cell></row><row><cell>Baseline + SD</cell><cell>0.105</cell><cell>0.822</cell><cell>4.708</cell><cell>0.193</cell><cell>0.876</cell><cell>0.958</cell><cell>0.981</cell></row><row><cell>Baseline + FS</cell><cell>0.103</cell><cell>0.785</cell><cell>4.628</cell><cell>0.189</cell><cell>0.881</cell><cell>0.960</cell><cell>0.981</cell></row><row><cell>Baseline HR</cell><cell>0.101</cell><cell>0.758</cell><cell>4.497</cell><cell>0.187</cell><cell>0.886</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell>Baseline HR + DG</cell><cell>0.098</cell><cell>0.694</cell><cell>4.371</cell><cell>0.182</cell><cell>0.890</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>Baseline HR + SD</cell><cell>0.099</cell><cell>0.744</cell><cell>4.465</cell><cell>0.186</cell><cell>0.888</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell>Baseline HR + FS</cell><cell>0.097</cell><cell>0.701</cell><cell>4.364</cell><cell>0.182</cell><cell>0.892</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Full HR w/o FS</cell><cell>0.098</cell><cell>0.702</cell><cell>4.377</cell><cell>0.184</cell><cell>0.888</cell><cell>0.963</cell><cell>0.983</cell></row><row><cell>Full HR w/o SD</cell><cell>0.094</cell><cell>0.678</cell><cell>4.312</cell><cell>0.180</cell><cell>0.898</cell><cell>0.965</cell><cell>0.982</cell></row><row><cell>Full HR w/o DG</cell><cell>0.096</cell><cell>0.696</cell><cell>4.368</cell><cell>0.182</cell><cell>0.892</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Full HR</cell><cell>0.093</cell><cell>0.671</cell><cell>4.297</cell><cell>0.178</cell><cell>0.899</cell><cell>0.965</cell><cell>0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on full-scale network. Conducted by continuously accumulating each module with input size 192?640.</figDesc><table><row><cell>Full-Scale</cell><cell cols="4">Abs Rel Sq Rel RMSE ? &lt; 1.25</cell></row><row><cell>+ Encoder Scale</cell><cell>0.105</cell><cell>0.811</cell><cell>4.668</cell><cell>0.877</cell></row><row><cell>+ Bridges</cell><cell>0.104</cell><cell>0.798</cell><cell>4.655</cell><cell>0.878</cell></row><row><cell>+ RSU</cell><cell>0.103</cell><cell>0.785</cell><cell>4.628</cell><cell>0.881</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>25 ? &lt; 1.25 2 ? &lt; 1.25 3</figDesc><table><row><cell>Monodepth2 [13]</cell><cell>0.355</cell><cell>0.673</cell><cell>1.252</cell><cell>0.373</cell><cell>0.485</cell><cell>0.771</cell><cell>0.907</cell></row><row><cell>DepthHint [49]</cell><cell>0.298</cell><cell>0.457</cell><cell>1.043</cell><cell>0.331</cell><cell>0.539</cell><cell>0.821</cell><cell>0.937</cell></row><row><cell>EdgeDepth [68]</cell><cell>0.292</cell><cell>0.437</cell><cell>1.018</cell><cell>0.319</cell><cell>0.563</cell><cell>0.834</cell><cell>0.941</cell></row><row><cell>Ours (EPCDepth)</cell><cell>0.247</cell><cell>0.277</cell><cell>0.818</cell><cell>0.285</cell><cell>0.605</cell><cell>0.869</cell><cell>0.961</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Thanks to National Natural Science Foundation of China 61672063 and 62072013, Shenzhen Research Projects of JCYJ20180503182128089, 201806080921419290 and RCJC20200714114435057. In addition, we thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><forename type="middle">Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu Chiang Frank</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2624" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Djork Arn? Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Vijay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ambrus Rares, and Adrien Gaidon. Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection CNNS by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-knowledge distillation: A simple way for better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungyul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyoung</forename><surname>Byeong Moon Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangheum</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HR-Depth: High resolution self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2294" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve Restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5007" to="5016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Su-perDepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rare?</forename><surname>Ambru?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9250" to="9256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9768" to="9777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">U2-Net: Going deeper with nested U-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PR</publisher>
			<biblScope unit="page">107404</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FitNets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Bayesian data augmentation approach for learning deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How do neural networks see depth in single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H E</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2183" to="2191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Rahim</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2162" to="2171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5644" to="5653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data-distortion guided self-distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Snapshot distillation: Teacher-student optimization in one generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2859" to="2868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="817" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of geometry from videos with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7493" to="7500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid multi-view stereo net with self-adaptive view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">P2Net: Patch-match and plane-regularization for unsupervised indoor depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="206" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via self-knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongjin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13876" to="13885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">M</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihuai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6872" to="6881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The edge of depth: Explicit constraints between segmentation and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13116" to="13125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
