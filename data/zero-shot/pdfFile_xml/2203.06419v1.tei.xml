<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivani</forename><surname>Kumar</surname></persName>
							<email>shivaniku@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atharva</forename><surname>Kulkarni</surname></persName>
							<email>atharvak@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Shad</forename><surname>Akhtar</surname></persName>
							<email>shad.akhtar@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Chakraborty</surname></persName>
							<email>tanmoy@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indraprastha Institute of Information Technology Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication. While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication. Though sarcasm identification has been a well-explored topic in dialogue analysis, for conversational systems to truly grasp a conversation's innate meaning and generate appropriate responses, simply detecting sarcasm is not enough; it is vital to explain its underlying sarcastic connotation to capture its true essence. In this work, we study the discourse structure of sarcastic conversations and propose a novel task -Sarcasm Explanation in Dialogue (SED). Set in a multimodal and code-mixed setting, the task aims to generate natural language explanations of satirical conversations. To this end, we curate WITS, a new dataset to support our task. We propose MAF (Modality Aware Fusion), a multimodal context-aware attention and global information fusion module to capture multimodality and use it to benchmark WITS. The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics. Lastly, we carry out detailed analyses both quantitatively and qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The use of figurative language serves many communicative purposes and is a regular feature of both oral and written communication (Roberts and <ref type="bibr" target="#b26">Kreuz, 1994)</ref>. Predominantly used to induce humour, criticism, or mockery <ref type="bibr" target="#b1">(Colston, 1997)</ref>, paradoxical language is also used in concurrence with hyperbole to show surprise <ref type="bibr" target="#b2">(Colston and Keller, 1998)</ref> as well as highlight the disparity between expectations and reality <ref type="bibr" target="#b10">(Ivanko and Pexman, 2003)</ref>. While the use and comprehension of sarcasm is a * Equal contribution <ref type="figure">Figure 1</ref>: Sarcasm Explanation in Dialogues (SED). Given a sarcastic dialogue, the aim is to generate a natural language explanation for the sarcasm in it. Blue text represents the English translation for the text.</p><p>cognitively taxing process <ref type="bibr" target="#b19">(Olkoniemi et al., 2016)</ref>, psychological evidence advocate that it positively correlates with the receiver's theory of mind (ToM) (Wellman, 2014), i.e., the capability to interpret and understand another person's state of mind. Thus, for NLP systems to emulate such anthropomorphic intelligent behavior, they must not only be potent enough to identify sarcasm but also possess the ability to comprehend it in its entirety. To this end, moving forward from sarcasm identification, we propose the novel task of Sarcasm Explanation in Dialogue (SED).</p><p>For dialogue agents, understanding sarcasm is even more crucial as there is a need to normalize its sarcastic undertone and deliver appropriate responses. Conversations interspersed with sarcastic statements often use contrastive language to convey the opposite of what is being said. In a real-world setting, understanding sarcasm goes beyond negat-ing a dialogue's language and involves the acute comprehension of audio-visual cues. Additionally, due to the presence of essential temporal, contextual, and speaker-dependent information, sarcasm understanding in conversation manifests as a challenging problem. Consequently, many studies in the domain of dialogue systems have investigated sarcasm from textual, multimodal, and conversational standpoints <ref type="bibr" target="#b6">(Ghosh et al., 2018;</ref><ref type="bibr">Castro et al., 2019;</ref><ref type="bibr" target="#b20">Oraby et al., 2017;</ref><ref type="bibr">Bedi et al., 2021)</ref>. However, baring some exceptions <ref type="bibr" target="#b18">(Mishra et al., 2019;</ref><ref type="bibr" target="#b4">Dubey et al., 2019;</ref><ref type="bibr">Chakrabarty et al., 2020)</ref>, research on figurative language has focused predominantly on its identification rather than its comprehension and normalization. This paper addresses this gap by attempting to generate natural language explanations of satirical dialogues.</p><p>To illustrate the proposed problem statement, we show an example in <ref type="figure">Figure 1</ref>. It contains a dyadic conversation of four utterances u 1 , u 2 , u 3 , u 4 , where the last utterance (u 4 ) is a sarcastic remark. Note that in this example, although the opposite of what is being said is, "I don't have to think about it," it is not what the speaker means; thus, it enforces our hypothesis that sarcasm explanation goes beyond simply negating the dialogue's language. The discourse is also accompanied by ancillary audio-visual markers of satire such as an ironical intonation of the pitch, a blank face, or roll of the eyes. Thus, conglomerating the conversation history, multimodal signals, and speaker information, SED aims to generate a coherent and cohesive natural language explanation associated with sarcastic dialogues.</p><p>For the task at hand, we extend MASAC (Bedi et al., 2021) -a sarcasm detection dataset for codemixed conversations -by augmenting it with natural language explanations for each sarcastic dialogue. We name the dataset WITS 1 . The dataset is a compilation of sarcastic dialogues from a popular Indian TV show. Along with the textual transcripts of the conversations, the dataset also contains multimodal signals of audio and video.</p><p>We experiment with unimodal as well as multimodal models to benchmark WITS. Text, being the driving force of the explanations, is given the primary importance, and thus, we compare a number of established text-based sequence-to-sequence systems on WITS. To incorporate multimodal information, we propose a unique fusion scheme of 1 WITS: "Why Is This Sarcastic" Multimodal Context-Aware Attention (MCA2). Inspired by , this attention variant facilitates deep semantic interaction between the multimodal signals and textual representations by conditioning the key and value vectors with audio-visual information and then performing dot product attention with these modified vectors. The generated audio and video information-informed textual representations are then combined using the Global Information Fusion Mechanism (GIF). The gating mechanism of GIF allows for the selective inclusion of information relevant to the satirical language and also prohibits any multimodal noise from seeping into the model. We further propose MAF (Modality Aware Fusion) module where the aforementioned mechanisms are introduced in the Generative Pretrained Models (GPLMs) as adapter modules. Our fusion strategy outperforms the textbased baselines and the traditional multimodal fusion schemes in terms of multiple text-generation metrics. Finally, we conduct a comprehensive quantitative and qualitative analysis of the generated explanations.</p><p>In a nutshell, our contributions are four fold: ? We propose Sarcasm Explanation in Dialogue (SED), a novel task aimed at generating a natural language explanation for a given sarcastic dialogue, elucidating the intended irony. ? We extend an existing sarcastic dialogue dataset, to curate WITS, a novel dataset containing human annotated gold standard explanations. ? We benchmark our dataset using MAF-TAV B and MAF-TAV M variants of BART and mBART, respectively, that incorporate the audio-visual cues using a unique context-aware attention mechanism. ? We carry out extensive quantitative and qualitative analysis along with human evaluation to assess the quality of the generated explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility:</head><p>The source codes and the dataset can be found here: https://github.com/LCS2-IIITD/MAF.git.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sarcasm and Text: <ref type="bibr" target="#b11">Joshi et al. (2017)</ref> presented a well-compiled survey on computational sarcasm where the authors expanded on the relevant datasets, trends, and issues for automatic sarcasm identification. Early work in sarcasm detection dealt with standalone text inputs like tweets and reviews <ref type="bibr" target="#b14">(Kreuz and Caucci, 2007;</ref><ref type="bibr" target="#b33">Tsur et al., 2010;</ref><ref type="bibr" target="#b12">Joshi et al., 2015;</ref><ref type="bibr" target="#b24">Peled and Reichart, 2017)</ref>. These initial works mostly focused on the use of linguistic and lexical features to spot the markers of sarcasm <ref type="bibr" target="#b14">(Kreuz and Caucci, 2007;</ref><ref type="bibr" target="#b33">Tsur et al., 2010)</ref>. More recently, attention-based architectures are proposed to harness the inter-and intra-sentence relationships in texts for efficient sarcasm identification <ref type="bibr" target="#b32">(Tay et al., 2018;</ref><ref type="bibr" target="#b37">Xiong et al., 2019;</ref><ref type="bibr" target="#b29">Srivastava et al., 2020)</ref>. Analysis of figurative language has also been extensively explored in conversational AI setting. <ref type="bibr" target="#b7">Ghosh et al. (2017)</ref> utilised attentionbased RNNs to identify sarcasm in the presence of context. Two separate LSTMs-with-attention were trained for the two inputs (sentence and context) and their hidden representations were combined during the prediction.</p><p>The study of sarcasm identification has also expanded beyond the English language. Bharti et al.  <ref type="formula" target="#formula_0">2021)</ref> proposed a code-mixed multi-party dialogue dataset, MASAC, for sarcasm and humor detection. In the bimodal setting, sarcasm identification with tweets containing images has also been well explored <ref type="bibr">(Cai et al., 2019;</ref><ref type="bibr" target="#b38">Xu et al., 2020;</ref><ref type="bibr" target="#b22">Pan et al., 2020)</ref> .</p><p>Beyond Sarcasm Identification: While studies in computational sarcasm have predominantly focused on sarcasm identification, some forays have been made into other domains of figurative language analysis. <ref type="bibr" target="#b4">Dubey et al. (2019)</ref> initiated the work of converting sarcastic utterances into their non-sarcastic interpretations using deep learning.  In summary, much work has been done in sarcasm detection, but little, if any, effort has been placed into explaining the irony behind sarcasm. This paper attempts to fill this gap by proposing a new problem definition and a supporting dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>Situational comedies, or 'Sitcoms', vividly depict human behaviour and mannerism in everyday reallife settings. Consequently, the NLP research community has successfully used such data for sarcasm identification <ref type="bibr">(Castro et al., 2019;</ref><ref type="bibr">Bedi et al., 2021)</ref>. However, as there is no current dataset tailored for the proposed task, we curate a new dataset named WITS, where we augment the already existing MASAC dataset <ref type="bibr">(Bedi et al., 2021)</ref> with explanations for our task. MASAC is a multimodal, multi-party, Hindi-English code-mixed dialogue dataset compiled from the popular Indian TV show, 'Sarabhai v/s Sarabhai' 2 . We manually analyze the data and clean it for our task. While the original dataset contained 45 episodes of the TV series, we add 10 more episodes along with their transcription and audio-visual boundaries. Subsequently, we select the sarcastic utterances from this augmented dataset and manually define the utterances to be included in the dialogue context for each of them. Finally, we are left with 2240 sarcastic dialogues with the number of contextual utterances ranging from 2 to 27. Each of these instances is manually   annotated with a corresponding natural language explanation interpreting its sarcasm. Each explanation contains four primary attributes -source and target of sarcasm, action word for sarcasm, and an optional description for the satire as illustrated in <ref type="figure">Figure 1</ref>. In the explanation "Indu implies that Maya is not looking good.", 'Indu' is the sarcasm source, 'Maya' is the target, 'implies' is the action word, while 'is not looking good' forms the description part of the explanation. We collect explanations in code-mixed format to keep consistency with the dialogue language. We split the data into train/val/test sets in an 80:10:10 ratio for our experiments, resulting in 1792 dialogues in the train set and 224 dialogues each in the validation and test sets. The next section illustrates the annotation process in more detail. <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure" target="#fig_2">Figure 2</ref> show detailed statistics of WITS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation Guidelines</head><p>Each of the instance in WITS is associated with a corresponding video, audio, and textual transcript such that the last utterance is sarcastic in nature. We first manually define the number of contextual utterances required to understand the sarcasm present in the last utterance of each dialogue. Further, we provide each of these sarcastic statements, along with their context, to the annotators who are asked to generate an explanation for these instances based on the audio, video, and text cues. Two annotators were asked to annotate the entire dataset. The target explanation is selected by calculating the cosine similarity between the two explanations. If the cosine similarity is greater than 90% then the shorter length explanation is selected as the target explanation. Otherwise, a third annotator goes through the dialogue along with the explanations and resolves the conflict. The average cosine similarity after the first pass is 87.67%. All the final selected explanations contain the following attributes:</p><p>? Sarcasm source: The speaker in the dialog who is being sarcastic. ? Sarcasm target: The person/ thing towards whom the sarcasm is directed. ? Action word: Verb/ action used to describe how the sarcasm is taking place. For e.g. mocks, insults, taunts, etc. ? Description: A description about the scene which helps in understanding the sarcasm. <ref type="figure">Figure 1</ref> represents an example annotation from WITS with its attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Methodology</head><p>In this section, we present our model and its nuances. The primary goal is to smoothly integrate multimodal knowledge into the BART architecture. To this end, we introduce Multimodal Aware Fusion (MAF), an adapter-based module that comprises of Multimodal Context-Aware Attention (MCA2) and Global Information Fusion (GIF) mechanisms.  Given the textual input sarcastic dialogue along with the audio-video cues, the former aptly introduces multimodal information in the textual representations, while the latter conglomerates the audiovisual information infused textual representations. This adapter module can be readily incorporated at multiple layers of BART/mBART to facilitate various levels of multimodal interaction. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates our model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Context Aware Attention</head><p>The traditional dot-product-based cross-modal attention scheme leads to the direct interaction of textual representations with other modalities. Here the text representations act as the query against the multimodal representations, which serve as the key and value. As each modality comes from a different embedding subspace, a direct fusion of multimodal information might not retain maximum contextual information and can also leak substantial noise in the final representations. Thus, based on the findings of , we propose multimodal fusion through Context Aware Attention. We first generate multimodal information conditioned key and value vectors and then perform the traditional scaled dot-product attention. We elaborate on the process below. Given the intermediate representation H generated by the GPLMs at a specific layer, we calculate the query, key, and value vectors Q, K, and V ? R n?d , respectively, as given in Equation 1, where W Q , W K , and W V ? R d?d are learnable parameters. Here, n denotes the maximum sequence length of the text, and d denotes the dimensionality of the GPLM generated vector.</p><formula xml:id="formula_0">QKV = H W Q W K W V<label>(1)</label></formula><p>Let C ? R n?dc denote the vector obtained from audio or visual representation. We generate multimodal information informed key and value vec-torsK andV , respectively, as given by . To decide how much information to integrate from the multimodal source and how much information to retain from the textual modality, we learn matrix ? ? R n?1 (Equation 3). Note that U k and U v ? R dc?d are learnable matrices.</p><formula xml:id="formula_1">K V = (1 ? ? k ? v ) K V + ? k ? v (C U k U v )<label>(2)</label></formula><p>Instead of making ? k and ? v as hyperparameters, we let the model decide their values using a gating mechanism as computed in Equation 3. The</p><formula xml:id="formula_2">matrices of W k 1 , W k 2 , W v 1 , and W v 2 ? R d?1 are trained along with the model. ? k ? v = ?( K V W k 1 W v 1 + C U k U v W k 2 W v 2 )<label>(3)</label></formula><p>Finally, the multimodal information infused vec-torsK andV are used to compute the traditional scaled dot-product attention. For our case, we have two modalities -audio and video. Using the context-aware attention mechanism, we obtain the acoustic-information-infused and visualinformation infused vectors H A and H V , respectively (c.f. Equations 4 and 5).</p><formula xml:id="formula_3">H a = Sof tmax( QK T a ? d k )V a<label>(4)</label></formula><formula xml:id="formula_4">H v = Sof tmax( QK T v ? d k )V v (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Global Information Fusion</head><p>In order to combine the information from both the acoustic and visual modalities, we design the GIF block. We propose two gates, namely the acoustic gate (g a ) and the visual gate (g v ) to control the amount of information transmitted by each modality. They are as follows:</p><formula xml:id="formula_5">g a = [H ? H a ]W a + b a (6) g v = [H ? H v ]W v + b v (7) Here, W a , W v ? R 2d?d and b a , b v ? R d?1</formula><p>are trainable parameters, and ? denotes concatenation. The final multimodal information fused representa-tion? is given by Equation <ref type="formula" target="#formula_6">8</ref>.</p><formula xml:id="formula_6">H = H + g a H a + g v H v<label>(8)</label></formula><p>This vector? is inserted back into GPLM for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments, Results and Analysis</head><p>In this section, we illustrate our feature extraction strategy, the comparative systems, followed by the results and its analysis. For a quantitative analysis of the generated explanations, we use the standard metrics for generative tasks -ROUGE-1/2/L <ref type="bibr" target="#b16">(Lin, 2004)</ref>, BLEU-1/2/3/4 <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref>, and METEOR <ref type="bibr" target="#b3">(Denkowski and Lavie, 2014)</ref>. To capture the semantic similarity, we use the multilingual version of the BERTScore .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Extraction</head><p>Audio: Acoustic representations for each instance are obtained using the openSMILE python library 3 . We use a window size of 25 ms and a window shift of 10 ms to get the non-overlapping frames. Further, we employ the eGeMAPS model <ref type="bibr" target="#b5">(Eyben et al., 2016)</ref> and extract 154 dimensional functional features such as Mel Frequency Cepstral Coefficients (MFCCs) and loudness for each frame of the instance. These features are then fed to a Transformer encoder <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> for further processing.</p><p>Video: We use a pre-trained action recognition model, ResNext-101 <ref type="bibr" target="#b8">(Hara et al., 2018)</ref>, trained on the Kinetics dataset <ref type="bibr" target="#b13">(Kay et al., 2017)</ref> which can recognise 101 different actions. We use a frame rate of 1.5, a resolution of 720 pixels, and a window length of 16 to extract the 2048 dimensional visual features. Similar to audio feature extraction, we employ a Transformer encoder <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> to capture the sequential dialogue context in the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparative Systems</head><p>To get the best textual representations for the dialogues, we experiment with various sequence-tosequence (seq2seq) architectures. RNN: We use the openNMT 4 implementation of the RNN seqto-seq architecture. Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>: The standard Transformer encoder and decoder are used to generate explanations in this case. Pointer Generator Network <ref type="bibr" target="#b28">(See et al., 2017)</ref>: A seq-to-seq architecture that allows the generation of new words as well as copying words from the input text for generating accurate summaries. BART : It is a denoising autoencoder model with standard machine translation architecture with a bidirectional encoder and an auto-regressive left-to-right decoder. We use its base version. mBART : Following the same architecture and objective as BART, mBART is trained on large-scale monolingual corpora in different languages 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Text Based: As evident from  Multimodality: Psychological and linguistic literature suggests that there exist distinct paralinguistic cues that aid in comprehending sarcasm and humour <ref type="bibr">(Attardo et al., 2003;</ref><ref type="bibr" target="#b31">Tabacaru and Lemmens, 2014)</ref>. Thus, we gradually merge auditory and visual modalities using MAF module and obtain MAF-TAV B and MAF-TAV M for BART and mBART, respectively. We observe that the inclusion of acoustic signals leads to noticeable gains of 2-3% across the ROUGE, BLEU, and METEOR scores. The rise in BERTScore also suggests that the multimodal variant generates a tad more coherent explanations. As ironical intonations such as mimicry, monotone, flat contour, extremes of pitch, long pauses, and exaggerated pitch (Rockwell, 2007) form a significant component in sarcasm understanding, we surmise that our model, to some extent, is able to spot such markers and identify the intended sarcasm behind them.</p><p>We notice that visual information also contributes to our cause. Significant performance gains are observed for MAF-TV B and MAF-TV M , as all the metrics show a rise of about 3-4%. While MAF-TA B gives marginally better performance over MAF-TV B in terms of R1, RL, and B1, we see that MAF-TV B performs better in terms of the rest of the metrics. Often, sarcasm is depicted through gestural cues such as raised eyebrows, a straight face, or an eye roll <ref type="bibr">(Attardo et al., 2003)</ref>. Moreover, when satire is conveyed by mocking someone's looks or physical appearances, it becomes essential to incorporate information expressed through visual media. Thus, we can say that, to some extent, our model is able to capture these nuances of non-verbal cues and use them well to normalize the sarcasm in a dialogue. In summary, we conjecture that whether independent or together, audio-visual signals bring essential information to the table for understanding sarcasm.   <ref type="table" target="#tab_7">Table 3</ref> reports the ablation study. CONCAT1 represents the case where we perform bimodal concatenation ((T ? A), (T ? V )) instead of the MCA2 mechanism, followed by the GIF module, whereas, CONCAT2 represents the simple trimodal concatenation (T ? A ? V ) of acoustic, visual, and textual representations followed by a linear layer for dimensionality reduction. In comparison with MCA2, CONCAT2 reports a below-average performance with a significant drop of more than 14% for MAF-TAV B and MAF-TAV M . This highlights the need to have deftly crafted multimodal fusion mechanisms. CONCAT1, on the other hand, gives good performance and is competitive with DPA and MAF-TAV B . We speculate that treating the audio and video modalities separately and then merging them to retain the complimentary and differential features lead to this performance gain. Our proposed MAF outperforms DPA with gains of 1-3%. This underlines that our unique multimodal fusion strategy is aptly able to capture the contextual information provided by the audio and video signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Replacing the GIF module with simple addition, we observe a noticeable decline in the performance across almost all metrics by about 2-3%. This attests to the inclusion of GIF module over simple addition. We also experiment with fusing multimodal information using MAF before different layers of the BART encoder. The best performance was obtained when the fusion was done before the sixth layer of the architecture (c.f. Appendix A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Result Analysis</head><p>We evaluate the generated explanations based on their ability to correctly identify the source and target of a sarcastic comment in a conversation. We report such results for mBART, BART, MAF-TA B , MAF-TV B , and MAF-TAV B . BART performs better than mBART for the source as well as target identification. We observe that the inclusion of audio (? 10%) and video (? 8%) information dras-   tically improves the source identification capability of the model. The combination of both these nonverbal cues leads to a whopping improvement of more than 13% for the same. As a result, we infer that multimodal fusion enables the model to incorporate audio-visual peculiarities unique to each speaker, resulting in improved source identification. The performance for target identification, however, drops slightly on the inclusion of multimodality. We encourage future work in this direction.</p><p>Qualitative Analysis. We analyze the best performing model, MAF-TAV B , and its corresponding unimodal model, BART, and present some examples in <ref type="table" target="#tab_9">Table 4</ref>. In <ref type="table" target="#tab_9">Table 4a</ref>, we show one instance where the explanations generated by the BART as well as MAF-TAV B are neither coherent nor comply with the dialogue context and contain much scope of improvement. On the other hand, <ref type="table" target="#tab_9">Table 4b</ref> illustrates an instance where the explanation generated by MAF-TAV B adheres to the topic of the dialogue, unlike the one generated by its unimodal counterpart. <ref type="table" target="#tab_9">Table 4c</ref> depicts a dialogue where MAF-TAV B 's explanation better captures the satire than BART. We further dissect the models based on different modalities in Appendix A.3.</p><p>Human Evaluation. Since the proposed SED task is a generative task, it is imperative to man-ually inspect the generated results. Consequently, we perform a human evaluation for a sample of 30 instances from our test set with the help of 25 evaluators 6 . We ask the evaluators to judge the generated explanation, given the transcripts of the sarcastic dialogues along with a small video clip with audio as well. Each evaluator has to see the video clips and then rate the generated explanations on a scale of 0 to 5 based on the following factors 7 :</p><p>? Coherence: Measures how well the explanations are organized and structured. ? Related to dialogue: Measures whether the generated explanation adheres to the topic of the dialogue. ? Related to sarcasm: Measures whether the explanation is talking about something related to the sarcasm present in the dialogue. <ref type="table" target="#tab_12">Table 6</ref> presents the human evaluation analysis with average scores for each of the aforementioned categories. Our scrutiny suggests that MAF-TAV B generates more syntactically coherent explanations when compared with its textual and bimodal counterparts. Also, MAF-TAV B and MAF-TV B generate explanations that are more focused on the conversation's topic, as we see an increase of 0.55 points in the related to the dialogue category. Thus, we reestablish that these models are able to incorporate information that is explicitly absent from the dialogue, such as scene description, facial fea-  tures, and looks of the characters. Furthermore, we establish that MAF-TAV B is better able to grasp sarcasm and its normalization, as it shows about 0.6 points improvement over BART in the related to sarcasm category. Lastly, as none of the metrics in <ref type="table" target="#tab_12">Table 6</ref> exhibit high scores (3.5+), we feel there is still much scope for improvement in terms of the generation performance and human evaluation. The research community can further explore the task with our proposed dataset, WITS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we proposed the new task of Sarcasm Explanation in Dialogue (SED), which aims to generate a natural language explanation for sarcastic conversations. We curated WITS, a novel multimodal, multiparty, code-mixed, dialogue dataset to support the SED task. We experimented with multiple text and multimodal baselines, which give promising results on the task at hand. Furthermore, we designed a unique multimodal fusion scheme to merge the textual, acoustic, and visual features via Multimodal Context-Aware Attention (MCA2) and Global Information Fusion (GIF) mechanisms. As hypothesized, the results show that acoustic and visual features support our task and thus, generate better explanations. We show extensive qualitative analysis of the explanations obtained from different models and highlight their advantages as well as pitfalls. We also perform a thorough human evaluation to compare the performance of the models with that of human understanding. Though the models augmented with the proposed fusion strategy perform better than the rest, the human evaluation suggested there is still room for improvement which can be further explored in future studies. We compared various text based unimodal methods for our task. Although BART is performing the best for SED, it is important to note that BART is pre-trained on English datasets (GLUE <ref type="bibr" target="#b35">(Wang et al., 2018)</ref> and SQUAD <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>).</p><p>In order to explore how the representation learning is being transferred to a code-mixed setting, we analyse the embedding space learnt by the model before and after fine-tuning it for our task. We considered three random utterances from WITS and created three copies of them-one in English, one in Hindi (romanised), and one without modification i.e. code-mixed. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the PCA plot for the embeddings obtained for these nine utterance representations obtained by BART before and after fine-tuning on our task. It is interesting to note that even before any fine-tuning the Hindi, English, and code-mixed representations lie closer to each other and they shift further closer when we fine-tune our model. This phenomenon can be justified as out input is of romanised codemixed format and thus we can assume that representations are already being captured by the pretrained model. Fine-tuning helps us understand the Hindi part of the input. <ref type="table" target="#tab_15">Table 7</ref> shows the cosine distance between the representations obtained for English-Hindi, English-Code mixed, and Code mixed-Hindi utterances for the sample utterances. It can be clearly seen that the distance is decreasing after fine-tuning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Fusion at Different Layers</head><p>We fuse the multimodal information of audio and video in the BART encoder using the proposed fusion mechanism before different layers of the BART encoder. <ref type="table" target="#tab_17">Table 8</ref> shows the results we obtain when the fusion happens at different layers. We obtain the best results when the fusion happens before layer 6 i.e. the last layer of the encoder. This can be attributed to the fact that since there is only one layer of encoder after the fusion, the multimodal information is being retained efficiently and thus being decoded more accurately.  A.3 More Qualitative Analysis <ref type="table" target="#tab_19">Table 9</ref> highlights one of many cases where BART is able to capture the essence of sarcasm in a better way when compared to mBART. While mBART gives us an incorrect and incoherent explanation, BART generates an explanation which essentially means the same as the ground truth explanation. The inclusion of audio modality in the unimodal system often helps in generating preferable explanations, as shown in <ref type="table" target="#tab_1">Table 10</ref>. AVII-TA is able to capture the essense of sarcasm in the dialogue while the unimodal systems were not able to do so. Furthermore, video modality facilitates even better understanding of sarcasm as illustrated in <ref type="table" target="#tab_1">Table 11</ref>. AVII-TV is able to generate the best results while audio may act as noise in this particular example.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(2017) collected a Hindi corpus of 2000 sarcastic tweets and employed rule-based approaches to detect sarcasm. Swami et al. (2018) curated a dataset of 5000 satirical Hindi-English code-mixed tweets and used n-gram feature vectors with various ML models for sarcasm detection. Other notable studies include Arabic (Abu Farha and Magdy, 2020), Spanish (Ortega-Bueno et al., 2019), and Italian (Cignarella et al., 2018) languages. Sarcasm and Multimodality: In the conversational setting, MUStARD, a multimodal, multispeaker dataset compiled by Castro et al. (2019) is considered the benchmark for multimodal sarcasm identification. Chauhan et al. (2020) leveraged the intrinsic interdependency between emotions and sarcasm and devised a multi-task framework for multimodal sarcasm detection. Currently, Hasan et al. (2021) performed the best on this dataset with their humour knowledge enriched transformer model. Recently, Bedi et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of attributes in WITS. The number of utterances in a dialog lies between 2 and 27. Maximum number of speakers in a dialogue are 6. The speaker 'Maya' is the most common common sarcasm source while the speaker 'Monisha' is the most prominent sarcasm target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Model architecture for MAF-TAV B . The proposed Multimodal Fusion Block captures audio-visual cues using Multimodal Context Aware Attention (MCA2) which are further fused with textual representations using Global Information Fusion (GIF) block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Embedding space for BART before and after fine-tuning on sarcasm explanation in dialogues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of dialogs present in WITS.</figDesc><table><row><cell>In another direction, Mishra et al. (2019) devised a</cell></row><row><cell>modular unsupervised technique for sarcasm gen-</cell></row><row><cell>eration by introducing context incongruity through</cell></row><row><cell>fact removal and incongruous phrase insertion. Fol-</cell></row><row><cell>lowing this, Chakrabarty et al. (2020) proposed a</cell></row><row><cell>retrieve-and-edit-based unsupervised framework</cell></row><row><cell>for sarcasm generation. Their proposed model</cell></row><row><cell>leverages the valence reversal and semantic incon-</cell></row><row><cell>gruity to generate sarcastic sentences from their</cell></row><row><cell>non-sarcastic counterparts.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 ,</head><label>2</label><figDesc>BART performs the best across all the metrics for the textual modality, showing an improvement of almost 2-3% on the METEOR and ROUGE scores when compared with the next best baseline. PGN, RNN, and Transformers demonstrate admissible performance considering that they have been trained from scratch. However, it is surprising to see mBART not performing better than BART as it is trained on multilingual data. We elaborate more on this in Appendix A.1. 23.54 71.90 mBART 33.66 11.02 31.50 22.92 10.56 6.07 3.39 21.03 73.83 BART 36.88 11.91 33.49 27.44 12.23 5.96 2.89 26.65 76.03 Multimodality MAF-TAM 39.02 15.90 36.83 31.26 16.94 11.54 7.72 29.05 77.06 MAF-TVM 39.47 16.78 37.38 32.44 17.91 12.02 7.36 29.74 77.47 MAF-TAVM 38.52 14.13 36.60 30.50 15.20 9.78 5.74 27.42 76.70 MAF-TAB 38.21 14.53 35.97 30.58 15.36 9.63 5.96 27.71 77.08 MAF-TVB 37.48 15.38 35.64 30.28 16.89 10.33 6.55 28.24 76.95 MAF-TAVB 39.69 17.10 37.37 33.20 18.69 12.37 8.58 30.40 77.67</figDesc><table><row><cell cols="2">Mode Model</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>M</cell><cell>BS</cell></row><row><cell></cell><cell>RNN</cell><cell>29.22</cell><cell>7.85</cell><cell cols="2">27.59 22.06</cell><cell>8.22</cell><cell>4.76</cell><cell cols="3">2.88 18.45 73.24</cell></row><row><cell>Textual</cell><cell cols="2">Transformers 29.17 PGN 23.37</cell><cell>6.35 4.83</cell><cell cols="2">27.97 17.79 17.46 17.32</cell><cell>5.63 6.68</cell><cell>2.61 1.58</cell><cell cols="3">0.88 15.65 72.21 0.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Experimental results. (Abbreviation: R1/2/L:</cell></row><row><cell>ROUGE1/2/L; B1/2/3/4: BLEU1/2/3/4; M: METEOR;</cell></row><row><cell>BS: BERT Score; PGN: Pointer Generator Network).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>TAVM  38.52 14.13 36.60 30.50 15.20 9.78 5.74 27.42  76.70 -MCA2 + CONCAT1 37.56 14.85 34.90 30.16 15.76 10.12 6.82 28.59 76.59 -MAF + CONCAT2 17.22 1.70 14.12 13.11 2.11 0.00 0.00 9.34 66.64 -MCA2 + DPA 36.43 13.04 33.75 28.73 14.02 8.00 4.89 25.60 75.58 -GIF 36.37 13.85 34.92 28.49 14.34 9.00 6.16 25.75 76.86 MAF-TAVB 39.69 17.10 37.37 33.20 18.69 12.37 8.58 30.40 77.67 -MCA2 + CONCAT1 36.88 13.21 34.39 29.63 14.56 8.43 4.84 26.15 76.08 -MAF + CONCAT2 21.11 2.31 19.68 12.44 2.44 0.73 0.31 9.51 69.54 -MCA2 + DPA 38.84 14.76 36.96 30.23 15.95 9.88 5.83 28.04 77.20 -GIF 39.45 14.85 37.18 31.85 15.97 9.62 5.47 28.87 77.54</figDesc><table><row><cell>Model</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>M</cell><cell>BS</cell></row><row><cell>MAF-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation results on MAF-TAV M and MAF-TAV</figDesc><table /><note>B (DPA: Dot Product Attention).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Ab tumne ghar ki itni saaf safai ki hai and secondly us Karan Verma ke liye pasta, lasagne, caramel custard banaya. Now you have cleaned the house so much and secondly made pasta, lasagne, caramel custard for that Karan Verma. MONISHA: Walnut brownie bhi. And walnut brownie too. Ladki ka naam Ajanta Kyon Rakha? Why did they named the girl Ajanta? INDRAVARDHAN: Kyunki uski maa ajanta caves dekh rahi thi Jab vo Paida Hui haha. Because her mother must be watching the Ajanta caves when she was born haha.</figDesc><table><row><cell cols="2">INDRAVARDHAN: Accha suno Monisha tumhaare ghar mein been ya aisa kuuch hain? Listen Monisha, do you have a flute or something similar? MAYA: Kaise hogi? Monisha aapne ghar pe dustbin mushkil se rakhti hain to snake charmer waali been kaha se rakhegi? How will it be there? Monisha hardly keeps a dustbin in her home so how will she has a snake charmer's flute? Gold Maya Monisha ko tana marti hai safai ka dhyan</cell><cell cols="2">SAHIL: SAHIL: Walnut brownie, matlab wo khane wali? You mean edible walnut brownie? Gold Sahil monisha ki cooking ka mazak udata hai Sahil</cell><cell cols="2">MONISHA: Gold Indravadan Ajanta ke naam ka mazak udata hai</cell></row><row><cell></cell><cell>na rakhne ke liye Maya taunts Monisha for not</cell><cell></cell><cell>makes fun of Monisha's cooking.</cell><cell></cell><cell>Indravardhan makes fun of Ajanta's name</cell></row><row><cell></cell><cell>keeping a check of cleanliness</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BART Maya Monisha ko tumaari burayi nahi karta. Maya</cell><cell cols="2">BART Monisha sahil ko walnut brownie ki matlab wo</cell><cell cols="2">BART Indravardhan Monisha ko taunt maarta hai ki uski</cell></row><row><cell></cell><cell>doesn't blame you for Monisha</cell><cell></cell><cell>khane wali. Walnut Brownie to Monisha Sahil</cell><cell></cell><cell>maa ajanta caves dekh rahi thi Jab vo Paida Hui</cell></row><row><cell></cell><cell></cell><cell></cell><cell>means she eats</cell><cell></cell><cell>Indravardhan taunts Monisha as her mother was</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>watching Ajanta Caves when she was born.</cell></row><row><cell>MAF-</cell><cell>Maya implies ki Monisha bohot ghar mein bahar</cell><cell>MAF-</cell><cell>Sahil monisha ki cooking ka mazak udata hai Sahil</cell><cell>MAF-</cell><cell>Indravadan ajanta ke naam ka mazak udata hai</cell></row><row><cell>TAVB</cell><cell>nahi kar sakati. Maya implies that Monisha very in</cell><cell>TAVB</cell><cell>makes fun of Monisha's cooking.</cell><cell>TAVB</cell><cell>Indravardhan makes fun of Ajanta's name</cell></row><row><cell></cell><cell>home cannot do outside.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a) Incoherent explanation</cell><cell cols="2">(b) Explanation related to dialogue</cell><cell cols="2">(c) Explanation related to sarcasm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Actual and generated explanations for sample dialogues from test set. The last utterance is the sarcastic utterance for each dialogue.</figDesc><table><row><cell></cell><cell cols="5">mBART BART MAF-TA B MAF-TV B MAF-TAV B</cell></row><row><cell>Source</cell><cell>75.00</cell><cell>77.23</cell><cell>87.94</cell><cell>85.71</cell><cell>91.07</cell></row><row><cell>Target</cell><cell>45.53</cell><cell>52.67</cell><cell>43.75</cell><cell>43.75</cell><cell>46.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Source-target accuracy of the generated explanations for BART-based systems.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation statistics -comparing different models. Multimodal models are BART based.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Cosine distance between three random samples from the dataset before and after fine-tuning. (PT: pre-trained; FT: fine-tuned)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: ROUGE scores for fusion before different lay-</cell></row><row><cell>ers (R1/2/L: ROUGE1/2/L).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Maya monisha ke speech ka mazaak udati hai Maya says that make fun of Monisha.MAF-TV B Maya mocks monisha kyunki wo rhe theek haiMaya mocks Monisha because she is okay.Maya kehti hai ki uske speech bure hai Maya says that she didn't like the speech.</figDesc><table><row><cell cols="2">MAYA: Sahil, beta tum bhi soche ho ki maine Monisha ki speech</cell></row><row><cell cols="2">churai? Sahil, do you also think that I stole Monisha's speech?</cell></row><row><cell cols="2">INDRAVARDHAN: Haan.Yes.</cell></row><row><cell cols="2">MAYA: Are darling maine to speech ko chua bhi nahin. chhoti to</cell></row><row><cell cols="2">germs nahin lag jaate? Kyunki Monisha ne mithaai box ki wrapper</cell></row><row><cell cols="2">per likhi thi apni speech hath mein uthati to makkhiya bhanbhana</cell></row><row><cell cols="2">ne lagti. Darling, I didn't even touch the speech. Would I not have</cell></row><row><cell cols="2">got germs by touching it? Monisha used sweets wrapper to write</cell></row><row><cell cols="2">her speech, if I would have picked it up, there would've been flies</cell></row><row><cell cols="2">buzzing around me.</cell></row><row><cell>Gold</cell><cell>Maya ne Monisha ke speech ka mazak udaya.</cell></row><row><cell></cell><cell>Maya makes fun of Monisha's speech.</cell></row><row><cell>mBART</cell><cell>Maya kehti hai ki Monisha ka mazak udata hai</cell></row><row><cell></cell><cell>Maya says that make fun of Monisha.</cell></row><row><cell>BART</cell><cell>Maya monisha ke speech ka mazak udati hai Maya</cell></row><row><cell></cell><cell>makes fun of Monisha's speech.</cell></row><row><cell>MAF-TA B MAF-</cell><cell></cell></row><row><cell>TAV B</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 9 :</head><label>9</label><figDesc>BART v/s mBART: An example where explanation generated by BART is better than mBART.Ek  minute, kya hai maa ji, humaare naatak mein ek bhi stree patra nahi hai, sare ladke hai. One minute, what is it ma'am, we don't have any female parts in our play, all are male PRABHAVATI: To uss mein bhi kaunsi badi baat hai, mai ladka ban jaungi. Mere paas pant shirt to hai, moonche aapki de dena! So what is the big deal in it, I'll play a male. I have pant shirt, you give me your mustache. INDRAVARDHAN: Cancel! Naatak cancel! Maa ji huaa aisa ki humaari jo bahu hai, uska ek chota sa accident ho gaya, to iss liye natak cancel! Monisha le jaao inhe. Cancel! Play cancel! Ma'am, what happened is, that our daughter in law had a small accident, that is why the play is cancelled. Monisha take her. SAHIL: Aur aate aate apna ek chota sa accident bhi kara ke aao! And when you come, have a small accident too! Gold Sahil Monisha pe gussa hai as usne Prabhavati as an actress le aya. Sahil is angry on Monisha that she hired Prabhavati as an actress. mBART Sahil ko Prabhavati ko role offer karne par taunt maarta hai. Sahil taunts because the role is being offered to Prabhavati. BART Indravardhan Monisha ko taunt maarta hai ki uska ek chota sa accident bhi kara ke aao. Indravardhan taunts Monisha that she should have a small accident. MAF-TA B Sahil ko Prabhavati ko role offer nahi karna. Sahil does not want Prabhavati to have this role. MAF-TV B Sahil Indravardhan ko ek accident keh ke uska majaak udaata hai. Calls Indravardhan an accident and makes fun of him. Sahil ko Prabhavati ko role offer nahi karna. Sahil does not want Prabhavati to have this role.</figDesc><table><row><cell>MAF-</cell></row><row><cell>TAV B</cell></row></table><note>SAHIL:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 :</head><label>10</label><figDesc>Audio helps: An example where audio modality helps in generating more fitting explanation.MAYA: Kshama? You mean Sahil Kshama ko pyaar karta hai!? Kshama? You mean Sahil loves Kshama? SAHIL: Nahi, nahi! Ek minute, ek minute, mai kshama chahata hu. No no, One minute, one minute, I want forgiveness (kshama in hindi). INDRAVARDHAN: Dekha, Kshama chahata hai! Chahata ka matlab pyaar karna hi hua na!? See, wants forgiveness! Wants means love only, no!? Gold Indravardhan Sahil ko tease karta hai ki vo Kshama se pyaar karta hai.Indravardhan teases Sahil by implying that he loves kshama (name of a girl in hindi meaning forgiveness) mBART Indravardhan implies ki Sahil ek kshama chahata hai. Indravardhan implies that Sahil wants forgiveness. BART Maya ko kshama chahata hai Maya wants forgiveness. MAF-TA B Indravardhan Kshama ko pyaar karne par taunt maarta hai. Indravardhan taunts that he loves Kshama. MAF-TV B Indravardhan majaak mein kehta hai ki Sahil Kshama ko pyaar karta hai. Indravardhan Rosesh ko Kshama ki matlab pyaar karne par taunt maarta hai. Indravardhan taunts Rosesh for loving the meaning of forgiveness.</figDesc><table><row><cell>Indravardhan jokes</cell></row><row><cell>that Sahil loves Kshama</cell></row><row><cell>MAF-</cell></row><row><cell>TAV B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Video helps: An example where video modality helps in generating more fitting explanation. MAYA: And this time I thought lets have a theme party! animals! Hum log sab animals banenge! And this time I thought lets have a theme party! animals! We will all be animals! MONISHA: Walnut brownie bhi. And walnut brownie too. MAYA: Mai hiran, Sahil horse, and Monisha chhipakalee! I'll be a deer, Sahil horse, and Monisha lizard! Gold Maya Monisha ko chhipakalee keha kar uska majaak udaati hai.Maya makes fun of Monisha by comparing her with a lizard. mBART Maya Monisha ko taunt maarti hai ki use animal themed party Maya taunts Monisha for her animal themed party. BART Maya Monisha ko taunt maarti hai. Maya taunts Monisha. MAF-TA B Maya implies ki vo animal mein theme party ke baare mein nahi banenge. Maya implies that she won't be in regarding animal themed party. MAF-TV B Maya Monisha ke animal ke behaviour par taunt maarti hai. Maya taunts Monisha for her animal behaviour. Maya Monisha ko animal kaha ke taunt maarti hai. Maya taunts Monisha by calling her an animal.</figDesc><table><row><cell>MAF-</cell></row><row><cell>TAV B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Audio and video helps: An example where audio and video modality together helps in generating better explanation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.imdb.com/title/tt1518542/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://audeering.github.io/ opensmile-python/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/OpenNMT/OpenNMT-py 5 https://huggingface.co/facebook/ mbart-large-50-many-to-many-mmt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Evaluators are the experts in linguistics and NLP and their age ranges in 20-28 years. 7 0 denoting poor performance while 5 signifies perfect performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to acknowledge the support of the Ramanujan Fellowship (SERB, India), Infosys Centre for AI (CAI) at IIIT-Delhi, and ihub-Anubhuti-iiitd Foundation set up under the NM-ICPS scheme of the Department of Science and Technology, India.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Embedding Space for BART and mBART</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From Arabic sentiment analysis to sarcasm detection: The Ar-Sarcasm dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</title>
		<meeting>the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
	<note>European Language Resource Association</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salting a wound or sugaring a pill: The pragmatic functions of ironic criticism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Colston</surname></persName>
		</author>
		<idno type="DOI">10.1080/01638539709544980</idno>
	</analytic>
	<monogr>
		<title level="m">Discourse Processes</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="25" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You&apos;ll never believe this: Irony and hyperbole in expressing surprise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shauna</forename><forename type="middle">B</forename><surname>Colston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1023229304509</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of psycholinguistic research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="513" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3348</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep models for converting sarcastic utterances into their non sarcastic interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297001.3297043</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM India Joint International Conference on Data Science and Management of Data, CoDS-COMAD &apos;19</title>
		<meeting>the ACM India Joint International Conference on Data Science and Management of Data, CoDS-COMAD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">Y</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petri</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khiet</forename><forename type="middle">P</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2015.2457417</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sarcasm analysis using conversation context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00336</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="755" to="792" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The role of conversation context for sarcasm detection in online interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Richard</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbr?cken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="186" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Humor knowledge enriched transformer for understanding multimodal humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwu</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12972" to="12980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context incongruity and irony processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penny</forename><forename type="middle">M</forename><surname>Ivanko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pexman</surname></persName>
		</author>
		<idno type="DOI">10.1207/S15326950DP3503_2</idno>
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="279" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic sarcasm detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="DOI">10.1145/3124420</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Carman</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Harnessing context incongruity for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinita</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="757" to="762" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexical influences on the perception of sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina</forename><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Figurative Language</title>
		<meeting>the Workshop on Computational Approaches to Figurative Language<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A modular architecture for unsupervised sarcasm generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Tater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1636</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6144" to="6154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Individual differences in the processing of written sarcasm and metaphor: Evidence from eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Olkoniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Ranta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><forename type="middle">K</forename><surname>Kaakinen</surname></persName>
		</author>
		<idno type="DOI">https:/psycnet.apa.org/doi/10.1037/xlm0000176</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">433</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Are you serious?: Rhetorical questions and sarcasm in social media dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vrindavan</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5537</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbr?cken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="310" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overview of the task on irony detection in spanish variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reynier</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Hern?ndez Far?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos? E Medina</forename><surname>Montes-Y G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pagola</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">co-located with 34th conference of the Spanish Society for natural language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2421</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of the Iberian languages evaluation forum</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling intra and intermodality incongruity for multi-modal sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hongliang Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.124</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sarcasm SIGN: Interpreting sarcasm with sentiment based monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lotem</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1690" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Why do people use figurative language?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kreuz</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.1994.tb00653.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="163" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vocal features of conversational sarcasm: A comparison of methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rockwell</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10936-006-9049-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of psycholinguistic research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="361" to="369" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel hierarchical BERT architecture for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surabhi</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.figlang-1.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Figurative Language Processing</title>
		<meeting>the Second Workshop on Figurative Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="93" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A corpus of english-hindi code-mixed tweets for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahil</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11869</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Syed Sarfaraz Akhtar, and Manish Shrivastava</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Raised eyebrows as gestural triggers in humour: The case of sarcasm and hyper-understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabina</forename><surname>Tabacaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Lemmens</surname></persName>
		</author>
		<idno type="DOI">10.7592/EJHR2014.2.2.tabacaru</idno>
	</analytic>
	<monogr>
		<title level="j">The European Journal of Humour Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="31" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reasoning with sarcasm by reading inbetween</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1010" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Icwsm -a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Making minds: How theory of mind develops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henry M Wellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sarcasm detection with self-matching networks and low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313735</idno>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2115" to="2124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reasoning with multimodal sarcastic tweets via modeling cross-modality contrast and semantic association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenji</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.349</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3777" to="3786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context-aware self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.3301387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
