<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OBJECT COUNTING: YOU ONLY NEED TO LOOK AT ONE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
							<email>hongxiaopeng@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Wang</surname></persName>
							<email>iamwangyabin@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OBJECT COUNTING: YOU ONLY NEED TO LOOK AT ONE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object Counting</term>
					<term>One-Shot Learning</term>
					<term>At- tention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims to tackle the challenging task of oneshot object counting. Given an image containing novel, previously unseen category objects, the goal of the task is to count all instances in the desired category with only one supporting bounding box example. To this end, we propose a counting model by which you only need to Look At One instance (LaoNet). First, a feature correlation module combines the Self-Attention and Correlative-Attention modules to learn both inner-relations and inter-relations. It enables the network to be robust to the inconsistency of rotations and sizes among different instances. Second, a Scale Aggregation mechanism is designed to help extract features with different scale information. Compared with existing few-shot counting methods, LaoNet achieves state-of-the-art results while learning with a high convergence speed. The code will be available soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Object counting has become increasingly important due to its wide range of applications such as crowd surveillance, traffic monitoring, wildlife conservation and inventory management. Most of the existing counting methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> focus on a particular, single category. However, when applying them into new categories, their performances will drop catastrophically. Meanwhile, it is extremely difficult and costly to collect all categories and label them for training.</p><p>For humans, the generalization ability allows them to learn and deal with various vision tasks without much prior knowledge and experience. We are amazed by this remarkable ability and in this work, we focus on this learning paradigm and design a network to efficiently recognize and count new categories given only one example. We follow the few-shot setting in <ref type="bibr" target="#b3">[4]</ref> and modify it to one-shot object counting. That is, the model takes an image with unseen novel categories and a supporting bounding box containing an example instance of desired category as input, and then predicts the object count in the image.</p><p>However, there are two main challenges. First, the object counting task includes many different categories, and even several categories exist within a same image. Moreover in few-shot setting, these categories will not overlap between training and inference. This means that the model needs to have a strong distinguishing ability between features of different categories, and meanwhile, an effective associating ability among instances sharing the same category. Second, in one-shot counting, the model learns from only one supporting instance. Much of the difficulty results from the fact that the supporting sample may differ from other instances in, for example, sizes and poses. Hence, the model is required to be invariant towards these variations without seeing the commonalities across different instances. Therefore, in this paper, we propose an effective network named LaoNet for one-shot object counting. It consists of three main parts: feature extraction, feature correlation and the density regressor, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The feature correlation model and the feature extraction model are elaborately designed to address the above two challenges.</p><p>We propose the feature correlation based on Self-Attention and Correlative-Attention modules to learn innerrelations and inter-relations respectively. The Self-Attention encourages the model to focus more on important features and their correlations, improving the efficiency of information refinement. Previous few-shot counting methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> usually leverage on a convolution operation to match the similarities between image features and supporting features. However, as the kernel is derived from supporting features with the default size and rotation angle, the convolution operation will greatly depend on the quality of supporting features and the consistency of physical properties among different instances. Instead, our designed feature correlation model benefits from two kinds of attention modules and addresses the above problem by considering all correlations.</p><p>We further propose a Scale Aggregation mechanism in scale extraction to deal with scale variations among different categories and different instances. By learning features from multi-subspace, the model aggregates various scale information while maintaining a spatial consistency.</p><p>To summarize, our contribution is threefold.</p><p>? We design a novel network named LaoNet (A network by which you only need to Look At One instance) for one-shot object counting. By combining Self-Attention and Correlative-Attention modules, LaoNet exploits the correlation among novel category objects with high accuracy and efficiency.</p><p>? We propose a Scale Aggregation mechanism to extract more comprehensive features and fuse multi-scale information from the supporting box.</p><p>? The experimental results show that our model achieves state-of-the-art results with significant improvements on FSC-147 <ref type="bibr" target="#b3">[4]</ref> and COCO <ref type="bibr" target="#b5">[6]</ref> datasets under the oneshot setting without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Object counting methods can be briefly divided into two types. Detection based methods <ref type="bibr" target="#b6">[7]</ref> count the number of objects by exhaustively detecting every target in images. But they rely on the complex labels such as bounding boxes. Regression based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> learn to count by predicting a density map, in which each value represents the density of target objects at the corresponding location. The count prediction equals to the total sum of density map. Nevertheless, most of the counting methods are category specifically, e.g. for human crowd <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, for cars <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>, for plants <ref type="bibr" target="#b12">[13]</ref> or for cells <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. They focus on only one category and will loss the original satisfied performance when transferring to other categories. Moreover, most traditional approaches usually rely on tens of thousands of instances to train a counting model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>To reduce considerably the number of samples needed to train a counting model for a particular category, recently, fewshot counting task has been developed. The key lies in the generalization ability of the model to deal with novel categories from few labeled examples. The study <ref type="bibr" target="#b15">[16]</ref> proposes a Generic Matching Network (GMN) for class-agnostic counting. However it still needs several dozens to hundreds examples of a novel category for adaptation and good performance. CFOCNet is introduced to match and utilize the similarity between objects within the same category <ref type="bibr" target="#b4">[5]</ref>. The work <ref type="bibr" target="#b3">[4]</ref> presents a Few Shot Adaptation and Matching Network (Fam-Net) to learn feature correlations and few-shot adaptation and also introduces a few-shot counting dataset named FSC-147.</p><p>When the number of labeled example decreases to one, the task evolves into one-shot counting. In other visual tasks, researchers develop methods for one-shot segmentation <ref type="bibr" target="#b16">[17]</ref> and one-shot object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Compared to the fewshot setting which usually uses at least three instances for each object <ref type="bibr" target="#b3">[4]</ref>, the one-shot setting, where only one instance is available, is clearly more challenging.</p><p>It is worth mentioning that detection based approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> are inferior for the tasks of few-shot and one-shot counting. One main reason is that it requires extra and costly bounding-box annotations of all instances in the training stage while one-shot counting approach which we focus on depends on dot annotations and only one supporting box. To illustrate this point further, we perform experiments in Section 4.3 to compare with detection based approaches and validate the proposed network for one-shot counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>One-shot object counting consists of a training set (I t , s t , y t ) ? T and a query set (I q , s q ) ? Q, in which categories are mutually exclusive. Each input for the model contains an image I and a supporting bounding box s annotating one object of the desired category. In training set, abundant point annotations y t are available to supervise the model. In inference stage, we aim the model to learn to count the novel objects in I q with a supporting category instance sampled by s q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Correlation</head><p>As the model is required to learn to count from only one supporting object, seizing the correlation between features with high efficiency is quite important. Therefore, we build the feature correlation model in our one-shot network based on Self-Attention and Correlative-Attention modules, for learning the inner-relations and inter-relations respectively.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (violet block), our Self-Attention module consists of a Multi-head Attention (MA) and a layer normalization (LN). We first introduce the definition of attention <ref type="bibr" target="#b22">[23]</ref>, given the query Q, key K and value vector V :</p><formula xml:id="formula_0">A(Q, K, V | W ) = S( (QW Q )(KW K ) T ? d + P E)(V W V ),</formula><p>(1) where S is the softmax function and 1 ? d is a scaling factor based on the vector dimension d. W : W Q , W K , W V ? R d?d are weight matrices for projections and P E is the position embedding.</p><p>To leverage on more representation subspaces, we adopt the extending form with multi attention heads:</p><formula xml:id="formula_1">M A(Q, K, V ) = Concat(head 1 , .., head h )W O where head i = A(Q, K, V | W i ).<label>(2)</label></formula><p>The representation dimensions are divided by parallel attention heads, where parameter matrices W i :</p><formula xml:id="formula_2">W Q i , W K i , W V i ? R d?d/h and W O ? R d?d .</formula><p>One challenging problem in counting task is the existence of many complex interfering things. To efficiently weaken the negative influence by those irrelevant background, we apply Multi-head Self-Attention in image features to learn innerrelations and encourage the model to focus more on repetitive objects that can be counted.</p><p>We denote the feature sequences of the query image and the supporting box region as X and S, with sizes X ? R HW ?C and S ? R hw?C . And the refined query feature is calculated by:</p><formula xml:id="formula_3">X = LN (M A(X Q , X K , X V ) + X).<label>(3)</label></formula><p>A layer normalization (LN) is adopted to balance the value scales.</p><p>Meanwhile, as there is only one supporting object in oneshot counting problem, refining the salient features within the object is necessary and helpful for counting efficiency and accuracy. Therefore we apply another Self-Attention module to supporting feature and get refinedS.</p><p>Previous few-shot counting methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> usually adopt a convolution operation where the supporting features act as kernels to match the similarities for target category. However, the results will greatly depend on the quality of supporting features and the consistency of objects' properties, including rotations and scales.</p><p>To this end, we propose a Correlative-Attention module to learn inter-relations between query and supporting features and alleviate the constraints of irrelevant properties.</p><p>Specifically, we extend the MA by learning correlations between different feature sequences and add a feed-forward network (FFN) to fuse the features, i.e.,</p><formula xml:id="formula_4">X * = Corr(X,S) = G(M A(X Q ,S K ,S V ) +X). (4)</formula><p>G includes two LNs and a FFN in the form of residual (light blue block in <ref type="figure" target="#fig_0">Figure 1</ref>). Finally, X * andS will be fed into the cycle as new feature sequences where each cycle consists of two Self-Attention modules and a Correlative-Attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction and Scale Aggregation</head><p>To extract feature sequences from images, we use VGG-19 as our backbone. For query image, the output of the final level is directly flattened and transmitted into Self-Attention module. For the supporting box, as there are uncontrollable scale variations among instances due to the perspective, we propose a Scale Aggregation mechanism to fuse different scale information.</p><p>Given l as the number of layers in CNN, we aggregate the feature maps among different scales:</p><formula xml:id="formula_5">S = Concat(F l (s), F l?1 (s), ..., F l+1?? (s)),<label>(5)</label></formula><p>where F i represents a feature map at i th level and ? ? [1, l] decides the number of layers taken for aggregation. Meanwhile, we leverage on identifying position embedding to help the model distinguish the integrated scale information in attention model. By adopting the fixed sinusoidal absolute position embedding <ref type="bibr" target="#b22">[23]</ref>, feature sequences from different scales can still maintain the consistency between positions, i.e., P E (posj ,2i) = sin(pos j /10000 2i/d ), P E (posj ,2i+1) = cos(pos j /10000 2i/d ).</p><p>i is the dimension and pos j is the position for j th feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss</head><p>We use Euclidean distance to measure the difference between estimated density map and ground truth density map, which is generated based on annotated points following <ref type="bibr" target="#b0">[1]</ref>. The loss is defined as follows:</p><formula xml:id="formula_7">L E = ||D gt ? D|| 2 2 ,<label>(7)</label></formula><p>where D is the estimated density map and D gt is the ground truth density map. To improve the local pattern consistency, we also adopt a SSIM loss followed the calculation in <ref type="bibr" target="#b7">[8]</ref>. By integrating the above two loss functions, we have</p><formula xml:id="formula_8">L = L E + ?L SSIM ,<label>(8)</label></formula><p>where ? is the balanced weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implement Details and Evaluation Metrics</head><p>We design the density regressor by an upsampling layer and three convolution layers with ReLU activation. The kernel sizes of first two layers are 3 ? 3 and that of last is 1 ? 1. Random scaling and flipping are adopted for each training image. Adam <ref type="bibr" target="#b23">[24]</ref> with a learning rate 0.5 ? 10 ?5 is used to optimize the parameters. We set the number of attention heads h as 4, the correlation cycle T as 2, the number of aggregated layers ? as 2, and the loss balanced parameter ? as 10 ?4 . Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to measure the performance of our methods. They are defined by:</p><formula xml:id="formula_9">M AE = 1 M M i=1 N gt i ? N i , RM SE = 1 M M i=1 (N gt i ? N i ) 2 ),<label>(9)</label></formula><p>where M and N gt are the number of images and the groundtruth count, respectively. The predicted count N is calculated by integrating the estimated density map D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datesets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Few-Shot Approaches</head><p>We hold experiments on above two few-shot counting datasets to evaluate the proposed network. As there are few existing  <ref type="figure">Fig. 2</ref>. Visualizations of one-shot counting inputs and corresponding predicted density maps. The model can perform great counting accuracy even it has never seen strawberry, hot air balloon or cashew before.</p><p>methods specifically designed for one-shot counting, for comprehensive evaluation, we modify FamNet <ref type="bibr" target="#b3">[4]</ref> and CFOC-Net <ref type="bibr" target="#b4">[5]</ref> for this setting and also compare with other few-shot counting approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref>. First, quantitative results on FSC-147 are shown in Table 1. We list seven results of previous few-shot detection and counting methods in 3-shot setting and two results of stateof-the-art counting methods in 1-shot setting for comparison. The result of FamNet <ref type="bibr" target="#b3">[4]</ref> uses the adaptation strategy during testing.</p><p>It is worth noticing that our one-shot LaoNet outperforms all of previous few-shot methods, even those in 3 shot set-   <ref type="table" target="#tab_2">Table 2</ref> shows the results on each of four folds of COCO val2017. Methods with ? in the upper part of the table follow the experiment setting in <ref type="bibr" target="#b4">[5]</ref>. That is, the supporting examples are chosen from all instances in the dataset during training and testing, which is laborious and costly under the need of all instances annotated by bounding boxes. While our setting allows only one fixed instance for each image, we reconduct the experiment of CFOCNet <ref type="bibr" target="#b4">[5]</ref>. As the result shows, our method maintains a great performance on COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussions</head><p>Contribution of Different Terms. We study the accuracy contributions of different terms in FSC-147. The result is shown in <ref type="table" target="#tab_3">Table 3</ref>, each row whereof reports the results after removing one component or one term from LaoNet. The Self-Attention modules for the two feature sequences to learn inner-relations increase the accuracy in testing set by 19.9% and 15.7% for MAE, 9.5% and 13.1% for RMSE, respectively. Compared to other two terms, the Self-Attention modules contribute most to the performance of our model.</p><p>The Scale Aggregation mechanism helps more on RMSE. We hold experiments to measure the convergence speed and the performance stability. We pick FamNet <ref type="bibr" target="#b3">[4]</ref> as the baseline for LaoNet with a pre-trained CNN backbone and an Adam optimizer. We train both two models on FSC-147 and report the validation MAE for 100 epochs. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our model has faster convergence speed and better stability than FamNet. With just 2 epoches, our method achieves a low counting error which FamNet has to reach after 40 epochs. Meanwhile, the convergence of our method is smooth and stable, while that of Famet is jagged, with multiple sharp peaks and the highest error of 70. Comparison with Object Detectors. Object detectors can be used for counting task with the number of predicted detections. However, even these detectors work with categories which they are trained on instead of one-shot setting, their counting performances are still limited. We select images of FSC-147-COCO subset from FSC147 Val and Test sets which share categories with MS-COCO dataset and conduct quantitative experiments.</p><p>As the results shown in <ref type="table" target="#tab_4">Table 4</ref>, we compare LaoNet with several object detectors which are well pre-trained with thou- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper targets one-shot object counting, which requires the counting model to count objects of new categories by looking at only one instance. We propose an efficient network named LaoNet to address this challenge. LaoNet includes a feature correlation module to learn both inner-relations and inter-relations and a scale aggregation module to extract multi-scale information for improving robustness. Without any fine-tuning in inference, our LaoNet outperforms previous state-of-the-art few-shot counting methods with a high convergence speed. In the future, we consider applying our model to a wider range of one-shot vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall architecture of the proposed LaoNet for one-shot object counting. Both the query image and the supporting box are fed into CNN to extract features. Supporting features are aggregated among scales. Then the flatten features with unique position embedding are transmitted into feature correlation model with Self-Attentions and Correlative Attentions. Finally, a density regressor is adopted to predict the final density map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Comparisons of validation MAE during training. The blue line represents our proposed LaoNet. With just one epoch, it can perform a great accuracy which FamNet needs to train for about 20 epochs. sands of annotated examples on MS-COCO. Nevertheless, our method, which counts unseen categories, still outperforms the detection based methods which have met those categories in training, by a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on each of four folds of COCO val2017. Methods with ? follow the experiment setting in<ref type="bibr" target="#b4">[5]</ref>. Our method achieves great accuracy without any fine-tuning on testing categories.</figDesc><table><row><cell>Methods</cell><cell cols="2">MAE</cell><cell cols="2">Fold 0 RMSE</cell><cell>MAE</cell><cell>Fold 1 RMSE</cell><cell>MAE</cell><cell>Fold 2 RMSE</cell><cell>MAE</cell><cell>Fold 3 RMSE</cell><cell>Average MAE RMSE</cell></row><row><cell>Segment [17]  ?</cell><cell cols="2">2.91</cell><cell></cell><cell>4.20</cell><cell>2.47</cell><cell>3.67</cell><cell>2.64</cell><cell>3.79</cell><cell>2.82</cell><cell>4.09</cell><cell>2.71</cell><cell>3.94</cell></row><row><cell>GMN [16]  ?</cell><cell cols="2">2.97</cell><cell></cell><cell>4.02</cell><cell>3.39</cell><cell>4.56</cell><cell>3.00</cell><cell>3.94</cell><cell>3.30</cell><cell>4.40</cell><cell>3.17</cell><cell>4.23</cell></row><row><cell>CFOCNet [5]  ?</cell><cell cols="2">2.24</cell><cell></cell><cell>3.50</cell><cell>1.78</cell><cell>2.90</cell><cell>2.66</cell><cell>3.82</cell><cell>2.16</cell><cell>3.27</cell><cell>2.21</cell><cell>3.37</cell></row><row><cell>FamNet [4]</cell><cell cols="2">2.34</cell><cell></cell><cell>3.78</cell><cell>1.41</cell><cell>2.85</cell><cell>2.40</cell><cell>2.75</cell><cell>2.27</cell><cell>3.66</cell><cell>2.11</cell><cell>3.26</cell></row><row><cell>CFOCNet [5]</cell><cell cols="2">2.23</cell><cell></cell><cell>4.04</cell><cell>1.62</cell><cell>2.72</cell><cell>1.83</cell><cell>3.02</cell><cell>2.13</cell><cell>3.03</cell><cell>1.95</cell><cell>3.20</cell></row><row><cell>LaoNet (Ours)</cell><cell cols="2">2.20</cell><cell></cell><cell>3.78</cell><cell>1.32</cell><cell>2.66</cell><cell>1.58</cell><cell>2.19</cell><cell>1.84</cell><cell>2.90</cell><cell>1.73</cell><cell>2.93</cell></row><row><cell>Methods</cell><cell></cell><cell cols="5">Val MAE RMSE MAE RMSE Test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LaoNet</cell><cell></cell><cell cols="2">17.11</cell><cell>56.81</cell><cell>15.78</cell><cell>97.15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">? Self-Attention (X) 19.83</cell><cell>64.84</cell><cell cols="2">19.71 107.32</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">? Self-Attention (S)</cell><cell cols="2">19.67</cell><cell>63.79</cell><cell cols="2">18.71 111.83</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">? Scale Aggregation</cell><cell cols="2">18.82</cell><cell>63.74</cell><cell cols="2">17.16 106.40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? SSIM</cell><cell></cell><cell cols="2">17.82</cell><cell>57.66</cell><cell cols="2">16.11 100.59</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for different terms. X stands for feature sequences of query image and S stands for that of supporting box region. Experiments are performed in FSC-147 val and test. ting, without any fine-tuning strategy. We have generated new records by reducing the error of FamNet from 26.55 to 17.11 for MAE and from 77.01 to 56.81 for RMSE in validation set, from 26.76 to 15.78 for MAE and from 110.95 to 97.15 for RMSE in testing set. Second,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with pre-trained object detectors on FSC147-COCO splits of FSC147 which contain images with COCO categories. Even pre-trained with thousands of annotated examples on MS-COCO dataset, these object detectors still perform unsatisfied accuracy on counting task. The result demonstrates a robustness contribution under the multi-scale aggregation. Finally, the SSIM loss further improves the counting accuracy by both lower MAE and RMSE.</figDesc><table><row><cell>Methods</cell><cell cols="4">FSC147-COCO Val FSC147-COCO Test MAE RMSE MAE RMSE</cell></row><row><cell>RetinaNet [20]</cell><cell>63.57</cell><cell>174.36</cell><cell>52.67</cell><cell>85.86</cell></row><row><cell cols="2">Faster R-CNN [21] 52.79</cell><cell>172.46</cell><cell>36.20</cell><cell>79.59</cell></row><row><cell cols="2">Mask R-CNN [22] 52.51</cell><cell>172.21</cell><cell>35.56</cell><cell>80.00</cell></row><row><cell>FamNet [4]</cell><cell>39.82</cell><cell>108.13</cell><cell>22.76</cell><cell>45.92</cell></row><row><cell>LaoNet (Ours)</cell><cell>31.12</cell><cell>97.15</cell><cell>12.89</cell><cell>26.64</cell></row></table><note>Convergence Speed.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An automatic car counting system using overfeat framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Blankenship</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Stevanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to count everything</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udbhav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Class-agnostic few-shot object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo-Diao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counting everyday objects in everyday scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distribution matching for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Hoai</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Direct measure matching for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic car counting method for unmanned aerial vehicle images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Moranduzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn refitting strategy for plant counting and sizing in uav imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?lissande</forename><surname>Machefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Lemarchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginie</forename><surname>Bonnefond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Hitchins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>?zg?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoe</forename><surname>Deubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>J?ckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seiwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classagnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">One-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander S</forename><surname>Ecker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One-shot object detection with coattention and co-excitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">One-shot object detection without fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Yau Pun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: Amethod for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multirelation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
