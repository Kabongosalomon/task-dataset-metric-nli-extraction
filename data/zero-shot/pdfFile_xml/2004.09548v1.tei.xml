<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AANet: Adaptive Aggregation Network for Efficient Stereo Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
							<email>juyong@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AANet: Adaptive Aggregation Network for Efficient Stereo Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., 41? than GC-Net, 4? than PSMNet and 38? than GA-Net), but also improve the performance of fast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our full framework is available at https: //github.com/haofeixu/aanet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating depth from stereo pairs is one of the most fundamental problems in computer vision <ref type="bibr" target="#b28">[29]</ref>. The key task is to find spatial pixel correspondences, i.e., stereo matching, then depth can be recovered by triangulation. Efficient and accurate stereo matching algorithms are crucial for many real-world applications that require fast and reliable responses, such as robot navigation, augmented reality and autonomous driving.</p><p>Traditional stereo matching algorithms generally per- * Corresponding author form a four-step pipeline: matching cost computation, cost aggregation, disparity computation and refinement, and they can be broadly classified into global and local methods <ref type="bibr" target="#b28">[29]</ref>. Global methods usually solve an optimization problem by minimizing a global objective function that contains data and smoothness terms <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17]</ref>, while local methods only consider neighbor information <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref>, making themselves much faster than global methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>. Although significant progress has been made by traditional methods, they still suffer in challenging situations like textureless regions, repetitive patterns and thin structures. Learning based methods make use of deep neural networks to learn strong representations from data, achieving promising results even in those challenging situations. DispNetC <ref type="bibr" target="#b19">[20]</ref> builds the first end-to-end trainable framework for disparity estimation, where a correlation layer is used to measure the similarity of left and right image features. GC-Net <ref type="bibr" target="#b13">[14]</ref> takes a different approach by directly concatenating left and right features, and thus 3D convolutions are required to aggregate the resulting 4D cost volume. PSMNet <ref type="bibr" target="#b3">[4]</ref> further improves GC-Net by introducing more 3D convolutions for cost aggregation and accordingly obtains better accuracy. Although state-of-the-art performance can be achieved with 3D convolutions, the high computational cost and memory consumption make it quite expensive to deploy in practice (for example, PSMNet costs about 4G memory and 410ms to predict a KITTI stereo pair even on high-end GPUs). The recent work, GA-Net <ref type="bibr" target="#b42">[43]</ref>, also notices the drawbacks of 3D convolutions and tries to replace them with two guided aggregation layers. However, their final model still uses fifteen 3D convolutions.</p><p>To this end, a motivating question arises: How to achieve state-of-the-art results without any 3D convolutions while being significantly faster? Answering this question is especially challenging due to the strong regularization provided by 3D convolutions. In this paper, we show that by designing two effective and efficient modules for cost aggregation, competitive performance can be obtained on both Scene Flow and KITTI datasets even with simple feature correlation <ref type="bibr" target="#b19">[20]</ref> instead of concatenation <ref type="bibr" target="#b13">[14]</ref>.</p><p>Specifically, we first propose a new sparse points based representation for intra-scale cost aggregation. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, a set of sparse points are adaptively sampled to locate themselves in regions with similar disparities, alleviating the well-known edge-fattening issue at disparity discontinuities <ref type="bibr" target="#b28">[29]</ref>. Moreover, such representation is flexible to sample from a large context while being much more efficient than sampling from a large window, an essential requirement for traditional local methods to obtain highquality results <ref type="bibr" target="#b22">[23]</ref>. We additionally learn content-adaptive weights to achieve position-specific weighting for cost aggregation, aiming to overcome the inherent drawback of spatial sharing nature in regular convolutions. We implement the above ideas with deformable convolution <ref type="bibr" target="#b44">[45]</ref>.</p><p>We further approximate traditional cross-scale cost aggregation algorithm <ref type="bibr" target="#b43">[44]</ref> with neural network layers by constructing multi-scale cost volumes in parallel and allowing adaptive multi-scale interactions, producing accurate disparity predictions even in low-texture or textureless regions.</p><p>These two modules are simple, lightweight, and complementary, leading to an efficient architecture for cost aggregation. We also make extensive use of the key ideas in the feature extraction stage, resulting in our highly efficient and accurate Adaptive Aggregation Network (AANet). For instance, we can outperform existing top-performing models on Scene Flow dataset, while being significantly faster, e.g., 41? than GC-Net <ref type="bibr" target="#b13">[14]</ref>, 4? than PSMNet <ref type="bibr" target="#b3">[4]</ref> and 38? than GA-Net <ref type="bibr" target="#b42">[43]</ref>. Our method can also be a valuable way to improve the performance of fast stereo models, e.g., Stere-oNet <ref type="bibr" target="#b14">[15]</ref>, which are usually based on a very low-resolution cost volume to achieve fast speed, while at the cost of sacrificing accuracy. We also achieve competitive performance on KITTI dataset while running at 62ms, demonstrating the versatility and high efficiency of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews the most relevant work to ours.</p><p>Local Cost Aggregation. Local stereo methods (either traditional <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref> or 2D/3D convolution based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>) usually perform window based cost aggregation:</p><formula xml:id="formula_0">C(d, p) = q?N (p) w(p, q)C(d, q),<label>(1)</label></formula><p>whereC(d, p) denotes the aggregated cost at pixel p for disparity candidate d, pixel q belongs to the neighbors N (p) of p, w(p, q) is the aggregation weight and C(d, q) is the raw matching cost at q for disparity d. Despite the widespread and successful applications of local methods, they still have several important limitations. First and foremost, the fundamental assumption made by local methods is that all the pixels in the matching window have similar disparities. However, this assumption does not hold at disparity discontinuities, causing the well-known edge-fattening issue in object boundaries and thin structures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23]</ref>. As a consequence, the weighting function w needs to be designed carefully to eliminate the influence of pixels that violate the smoothness assumption <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>. While learning based methods automatically learn the aggregation weights from data, they still suffer from the inherent drawback of regular convolutions: weights are spatially shared, thus making themselves content-agnostic. Moreover, a large window size is often required to obtain high-quality results <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>, leading to high computational cost. Some works have been proposed to address the limitations of fixed rectangular window, e.g., using varying window size <ref type="bibr" target="#b25">[26]</ref>, multiple windows <ref type="bibr" target="#b10">[11]</ref>, or unconstrained shapes <ref type="bibr" target="#b1">[2]</ref>. Different from existing methods, we propose a new sparse points based representation for cost aggregation. This representation is also different from <ref type="bibr" target="#b22">[23]</ref>, in which sparse points inside the matching window are regularly sampled to reduce the computational complexity. In contrast, our proposed sampling mechanism is completely unconstrained and adaptive, providing more flexibility than the regular sampling in <ref type="bibr" target="#b22">[23]</ref>. We also learn additional content-adaptive weights to enable position-specific weighting in contrast to the spatial sharing nature of regular convolutions.</p><p>Cross-Scale Cost Aggregation. Traditional cross-scale cost aggregation algorithm <ref type="bibr" target="#b43">[44]</ref> reformulates local cost aggregation from a unified optimization perspective, and shows that by enforcing multi-scale consistency on cost volumes, the final cost volume is obtained through the adaptive combination of the results of cost aggregation performed at different scales. Details are provided in the supplementary material. We approximate this conclusion with neural network layers in an end-to-end manner. Different from existing coarse-to-fine approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref>, we build multiscale cost volumes in parallel and allow adaptive multiscale interactions. Our cross-scale aggregation architecture is also different from the very recent work <ref type="bibr" target="#b34">[35]</ref>, in which multi-scale cost volumes are also constructed. However, <ref type="bibr" target="#b34">[35]</ref> fuses the cost volumes from the lowest level to the higher ones hierarchically, while ours aggregates all scale cost volumes simultaneously based on the analysis in <ref type="bibr" target="#b43">[44]</ref>. Stereo Matching Networks. Existing end-to-end stereo matching networks can be broadly classified into two categories: 2D and 3D convolution based methods. They mainly differ in the way that cost volume is constructed. 2D methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> generally adopt a correlation layer <ref type="bibr" target="#b19">[20]</ref> while 3D methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3]</ref> mostly use direct feature concatenation <ref type="bibr" target="#b13">[14]</ref>. An exception to concatenation based 3D methods is <ref type="bibr" target="#b7">[8]</ref>, in which group-wise correlation is proposed to reduce the information loss of full correlation <ref type="bibr" target="#b19">[20]</ref>. In terms of performance, 3D methods usually outperform 2D methods by a large margin on popular benchmarks (e.g., Scene Flow <ref type="bibr" target="#b19">[20]</ref> and KITTI <ref type="bibr" target="#b21">[22]</ref>), but the running speed is considerably slower. In this paper, we aim at significantly speeding up existing top-performing methods while maintaining comparable performance. The very recent work, DeepPruner <ref type="bibr" target="#b5">[6]</ref>, shares a similar goal with us to build efficient stereo models. They propose to reduce the disparity search range by a differentiable PatchMatch <ref type="bibr" target="#b0">[1]</ref> module, and thus a compact cost volume is constructed. In contrast, we aim at reducing the sampling complexity and improving the sampling flexibility in cost aggregation, which works on different aspects, and both methods can be complementary to each other.</p><p>Deformable Convolution. Deformable convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45]</ref> is initially designed to enhance standard convolution's capability of modeling geometric transformations, and commonly used as backbone for object detection and semantic/instance segmentation tasks. We instead take a new perspective of traditional stereo methods and propose an adaptive sampling scheme for efficient and flexible cost aggregation. Since the resulting formulation is similar to deformable convolution, we adopt it in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a rectified image pair I l and I r , we first extract downsampled feature pyramid {F s l } S s=1 and {F s r } S s=1 with a shared feature extractor, where S denotes the number of scales, s is the scale index, and s = 1 represents the highest scale. Then multi-scale 3D cost volumes {C s } S s=1 are constructed by correlating left and right image features at corresponding scales, similar to DispNetC [20]:</p><formula xml:id="formula_1">C s (d, h, w) = 1 N F s l (h, w), F s r (h, w ? d) ,<label>(2)</label></formula><p>where ?, ? denotes the inner product of two feature vectors and N is the channel number of extracted features. C s (d, h, w) is the matching cost at location (h, w) for disparity candidate d. The raw cost volumes {C s } S s=1 are then aggregated with several stacked Adaptive Aggregation Modules (AAModules), where an AAModule consists of S adaptive Intra-Scale Aggregation (ISA) modules and an adaptive Cross-Scale Aggregation (CSA) module for S pyramid levels. Finally, the predicted low-resolution disparity is hierarchically upsampled to the original resolution with the refinement modules. All disparity predictions are supervised with ground truth when training, while only the last disparity prediction is required for inference. <ref type="figure" target="#fig_1">Fig. 2</ref> provides an overview of our proposed Adaptive Aggregation Network (AANet). In the following, we introduce the ISA and CSA modules in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adaptive Intra-Scale Aggregation</head><p>To alleviate the well-known edge-fattening issue at disparity discontinuities, we propose a sparse points based representation for efficient and flexible cost aggregation. Since the resulting formulation is similar to deformable convolution, we adopt it in our implementation.</p><p>Specifically, for cost volume C ? R D?H?W at a certain scale, where D, H, W represents the maximum disparity, height and width, respectively, the proposed cost aggregation strategy is defined as</p><formula xml:id="formula_2">C(d, p) = K 2 k=1 w k ? C(d, p + p k + ?p k ),<label>(3)</label></formula><p>whereC(d, p) denotes the aggregated cost at pixel p for disparity candidate d, K 2 is the number of sampling points (K = 3 in our paper), w k is the aggregation weight for kth point, p k is the fixed offset to p in window based cost aggregation approaches. Our key difference from previous stereo works is that we learn additional offset ?p k to regular sampling location p + p k , thus enabling adaptive sampling for efficient and flexible cost aggregation, leading to high-quality results in object boundaries and thin structures. However, in the context of learning, the spatial sharing nature of regular convolution weights {w k } K 2 k=1 makes themselves content-agnostic. We further learn positionspecific weights {m k } K 2 k=1 (i.e., modulation in <ref type="bibr" target="#b44">[45]</ref>, they also have effects of controlling the relative influence of the sampling points) for each pixel location p to achieve content-adaptive cost aggregation:</p><formula xml:id="formula_3">C(d, p) = K 2 k=1 w k ? C(d, p + p k + ?p k ) ? m k .<label>(4)</label></formula><p>We implement Eq. (4) with deformable convolution <ref type="bibr" target="#b44">[45]</ref>, both ?p k and m k are obtained by a separate convolution layer applied over the input cost volume C. The original formulation of deformable convolution assumes the offsets ?p k and weights m k are shared by each channel (i.e., disparity candidate d in this paper), we further evenly divide all disparity candidates into G groups, and share ?p k and m k within each group. Dilated convolution <ref type="bibr" target="#b40">[41]</ref> is also used for deformable convolution to introduce more flexibility. We set G = 2 and the dilation rate to 2 in this paper. We build an Intra-Scale Aggregation (ISA) module with a stack of 3 layers and a residual connection <ref type="bibr" target="#b8">[9]</ref>. The three layers are 1 ? 1, 3 ? 3 and 1 ? 1 convolutions, where the 3 ? 3 convolution is a deformable convolution. This design is similar to the bottleneck in <ref type="bibr" target="#b8">[9]</ref>, but we always keep the channels constant (equals to the number of disparity candidates). That is, we keep reasoning about disparity candidates, similar to traditional cost aggregation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Cross-Scale Aggregation</head><p>In low-texture or textureless regions, searching the correspondence at the coarse scale can be beneficial <ref type="bibr" target="#b20">[21]</ref>, as the texture information will be more discriminative under the same patch size when an image is downsampled. A similar observation has also been made in <ref type="bibr" target="#b35">[36]</ref>. Therefore, multiscale interactions are introduced in traditional cross-scale cost aggregation algorithm <ref type="bibr" target="#b43">[44]</ref>.</p><p>The analysis in <ref type="bibr" target="#b43">[44]</ref> shows that the final cost volume is obtained through the adaptive combination of the results of cost aggregation performed at different scales (details are given in the supplementary material). We thus approximate this algorithm wit?</p><formula xml:id="formula_4">C s = S k=1 f k (C k ), s = 1, 2, ? ? ? , S,<label>(5)</label></formula><p>where? is the resulting cost volume after cross-scale cost aggregation,C k is the intra-scale aggregated cost volume at scale k, for example, with the algorithm in Sec. 3.1, and f k is a general function to enable the adaptive combination of cost volumes at each scale. We adopt the definition of f k from HRNet <ref type="bibr" target="#b31">[32]</ref>, a recent work for human pose estimation, which depends on the resolutions of cost volumesC k and C s . Concretely, for cost volume? s ,</p><formula xml:id="formula_5">f k = ? ? ? ? ? I, k = s, (s ? k) stride?2 3 ? 3 convs, k &lt; s, upsampling 1 ? 1 conv, k &gt; s,<label>(6)</label></formula><p>where I denotes the identity function, s ? k stride-2 3 ? 3 convolutions are used for 2 s?k times downsampling to make the resolution consistent, and means bilinear upsampling to the same resolution first, then following a 1 ? 1 convolution to align the number of channels. We denote this architecture as Cross-Scale Aggregation (CSA) module.</p><p>Although our CSA module is similar to HRNet[32], they have two major differences. First, we are inspired by traditional cross-scale cost aggregation algorithm <ref type="bibr" target="#b43">[44]</ref> and aiming at approximating the geometric conclusion with neural network layers, while HRNet is designed for learning rich feature representations. Moreover, the channel number (corresponding to the disparity dimension) of lower scale cost volume is halved in our approach due to the smaller search range in coarser scales, while HRNet doubles, indicating our architecture is more efficient than HRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Aggregation Network</head><p>The proposed ISA and CSA modules are complementary and can be integrated, resulting in our final Adaptive Aggregation Module (AAModule, see <ref type="figure" target="#fig_1">Fig. 2</ref>). We stack six AAModules for cost aggregation, while for the first three AAModules, we simply use regular 2D convolutions for intra-scale aggregation, thus a total of nine deformable convolutions are used for cost aggregation in this paper.</p><p>Our feature extractor adopts a ResNet-like <ref type="bibr" target="#b8">[9]</ref> architecture (40 layers in total), in which six regular 2D convolutions are replaced with their deformable counterparts. We use Feature Pyramid Network <ref type="bibr" target="#b18">[19]</ref> to construct feature pyramid at 1/3, 1/6 and 1/12 resolutions. Two refinement modules proposed in StereoDRNet <ref type="bibr" target="#b2">[3]</ref> are used to hierarchically upsample the 1/3 disparity prediction to the original resolution (i.e., upsample to 1/2 resolution first, then to original resolution). Combining all these components leads to our final Adaptive Aggregation Network (AANet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Disparity Regression</head><p>For each pixel, we adopt the soft argmin mechanism <ref type="bibr" target="#b13">[14]</ref> to obtain the disparity predictiond:</p><formula xml:id="formula_6">d = Dmax?1 d=0 d ? ?(c d ),<label>(7)</label></formula><p>where D max is the maximum disparity range, ? is the softmax function, and c d is the aggregated matching cost for disparity candidate d. ?(c d ) can be seen as the probability of disparity being d. This regression based formulation can produce sub-pixel precision and thus is used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>Our AANet is trained end-to-end with ground truth disparities as supervision. While for KITTI dataset, the high sparsity of disparity ground truth may not be very effective to drive our learning process. Inspired by the knowledge distillation in <ref type="bibr" target="#b9">[10]</ref>, we propose to leverage the prediction results from a pre-trained stereo model as pseudo ground truth supervision. Specifically, we employ a pre-trained model to predict the disparity maps on the training set, and use the prediction results as pseudo labels in pixels where ground truth disparities are not available. We take the pre-trained GA-Net <ref type="bibr" target="#b42">[43]</ref> model as an example to validate the effectiveness of this strategy.</p><p>For disparity prediction D i pred , i = 1, 2, ? ? ? , N , it is first bilinearly upsampled to the original resolution. The corresponding loss function is defined as</p><formula xml:id="formula_7">L i = p V (p) ? L(D i pred (p), D gt (p)) + (1 ? V (p)) ? L(D i pred (p), D pseudo (p)),<label>(8)</label></formula><p>where V (p) is a binary mask to denote whether the ground truth disparity for pixel p is available, L is the smooth L1 loss <ref type="bibr" target="#b3">[4]</ref>, D gt is the ground truth disparity and D pseudo is the pseudo ground truth. The final loss function is a combination of losses over all disparity predictions</p><formula xml:id="formula_8">L = N i=1 ? i ? L i ,<label>(9)</label></formula><p>where ? i is a scalar for balancing different terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We conduct extensive experiments on three popular stereo datasets: Scene Flow, KITTI 2012 and KITTI 2015.</p><p>The Scene Flow dataset <ref type="bibr" target="#b19">[20]</ref> is a large scale synthetic dataset and provides dense ground truth disparity maps. The end-point error (EPE) and 1-pixel error are reported on this dataset, where EPE is the mean disparity error in pixels and 1-pixel error is the average percentage of pixel whose EPE is bigger than 1 pixel. The KITTI 2012 <ref type="bibr" target="#b6">[7]</ref> and KITTI 2015 <ref type="bibr" target="#b21">[22]</ref> are real-world datasets in the outdoor scenario, where only sparse ground truth is provided. The official metrics (e.g., D1-all) in the online leader board are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement our approach in PyTorch <ref type="bibr" target="#b26">[27]</ref> and using Adam <ref type="bibr" target="#b15">[16]</ref> (? 1 = 0.9, ? 2 = 0.999) as optimizer. For Scene Flow dataset, we use all training set (35454 stereo pairs) for training and evaluate on the standard test set (4370 stereo pairs). The raw images are randomly cropped to 288 ? 576 as input. We train our model on 4 NVIDIA V100 GPUs for 64 epochs with a batch size of 64. The learning rate starts at 0.001 and is decreased by half every 10 epochs after 20th epoch. For KITTI dataset, we use 336 ? 960 crop size, and first fine-tune the pre-trained Scene Flow model on mixed KITTI 2012 and 2015 training sets for 1000 epochs. The initial learning rate is 0.001 and decreased by half at 400th, 600th, 800th and 900th epochs. Then another 1000 epochs are trained on the separate KITTI 2012/2015 training set for benchmarking, with an initial learning rate of 0.0001 and same schedule as before. But only the last disparity prediction is supervised with ground truth following a similar strategy in <ref type="bibr" target="#b12">[13]</ref>. For all datasets, the input images are normalized with ImageNet mean and standard deviation  statistics. We use random color augmentation and vertical flipping, and set the maximum disparity as 192 pixels. From highest scale to lowest, the loss weights in Eq. 8 are set to ? 1 = ? 2 = ? 3 = 1.0, ? 4 = 2/3, ? 5 = 1/3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>To validate the effectiveness of each component proposed in this paper, we conduct controlled experiments on Scene Flow test set and KITTI 2015 validation set (the KITTI 2015 training set is split into 160 pairs for training and 40 pairs for validation).</p><p>Ablation Study. As shown in Tab. 1, removing the proposed ISA or CSA module leads to clear performance drop. The best performance is obtained by integrating these two modules, which are designed to be complementary in principle. <ref type="figure" target="#fig_2">Fig. 3</ref> further shows the visual comparison results. Our full model produces better disparity predictions in thin structures and textureless regions, demonstrating the effectiveness of the proposed method.</p><p>Sampling Points Visualization. To better understand our proposed adaptive intra-scale cost aggregation algo- rithm, we visualize the sampling locations in two challenging regions. As illustrated in <ref type="figure">Fig. 4</ref>, for pixel in object boundary <ref type="figure">(Fig. 4a)</ref>, the sampling points tend to focus on similar disparity regions. While for large textureless region <ref type="figure">(Fig. 4b)</ref>, a large context is usually required to obtain reliable matching due to lots of local ambiguities. Our method can successfully adapt the sampling locations to these regions, validating that the proposed adaptive aggregation method can not only dynamically adjust the sampling locations, but also enables sampling from a large context. Pseudo Ground Truth Supervision. <ref type="figure">Fig. 5</ref> shows the visual results on KITTI 2015 validation set. We empirically find that leveraging the prediction results from a pretrained GA-Net <ref type="bibr" target="#b42">[43]</ref> model helps reduce the artifacts in regions where ground truth disparities are not available, e.g., the sky region. Quantitatively, the D1-all error metric decreases from 2.29 to 2.15, while the EPE increases from 0.68 to 0.69. The possible reason might be that the validation set is too small to make the results unstable. Similar phenomenon has also been noticed in <ref type="bibr" target="#b7">[8]</ref>. However, the qualitative results indicate that our proposed strategy can be  <ref type="table">Table 2</ref>: Comparisons with four representative stereo models: StereoNet, GC-Net, PSMNet and GA-Net. We replace the 3D convolutions in cost aggregation stage with our proposed architectures and denote the resulting model with suffix AA. Our method not only obtains clear performance improvements (except GA-Net has lower EPE), but also shows fewer parameters, less computational cost and memory consumption, while being significantly faster than top-performing models (41? than GC-Net, 4? than PSMNet and 38? than GA-Net). The comparison with StereoNet indicates that our method can also be a valuable way to improve the performance of existing fast stereo models. "DConvs" is short for deformable convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>PSMNet AANet an effective way to handle highly sparse ground truth data.</p><p>Generalization. We further test the generalization ability of our method on Middlebury 2014 dataset <ref type="bibr" target="#b27">[28]</ref>. Specifically, we directly use our KITTI fine-tuned model to predict the disparity map, no additional training is done on Middlebury. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the results. Compared with the popular PSMNet <ref type="bibr" target="#b3">[4]</ref> model, our AANet produces sharper object boundaries and better preserves the overall structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with 3D Convolutions</head><p>To demonstrate the superiority of our proposed cost aggregation method over commonly used 3D convolutions, we conduct extensive experiments on the large scale Scene Flow dataset.</p><p>Settings. We mainly compare with four representative stereo models: the first 3D convolution based model GC-Net <ref type="bibr" target="#b13">[14]</ref>, real-time model StereoNet <ref type="bibr" target="#b14">[15]</ref>, previous and current state-of-the-art models PSMNet <ref type="bibr" target="#b3">[4]</ref> and GA-Net <ref type="bibr" target="#b42">[43]</ref>.</p><p>For fair comparisons, we use similar feature extractors with them. Specifically, StereoNet uses 8? downsampling for fast speed while we use 4?; five regular 2D convolutions in GA-Net are replaced with their deformable counterparts; for GC-Net and PSMNet, the feature extractors are exactly the same. We replace the 3D convolutions in cost aggregation stage with our proposed AAModules, and denote the resulting model with suffix AA. We integrate all these models in a same framework and measure the inference time with 576 ? 960 resolution on a single NVIDIA V100 GPU.</p><p>Results. Tab. 2 shows the comprehensive comparison metrics/statistics. To achieve fast speed, StereoNet <ref type="bibr" target="#b14">[15]</ref> uses 8? downsampling to build a very low-resolution cost volume, while at the cost of sacrificing accuracy. But thanks to our efficient adaptive aggregation architecture, we are able to directly aggregate the 1/4 cost volume with even less computation while being more accurate and faster, indicating that our method can be a valuable way to improve the performance of existing fast stereo models. Compared with top-performing stereo models GC-Net <ref type="bibr" target="#b13">[14]</ref>, PSMNet <ref type="bibr" target="#b3">[4]</ref> and GA-Net <ref type="bibr" target="#b42">[43]</ref>, we not only obtain clear performance improvements (except GA-Net has lower EPE than ours), but also show fewer parameters, less computational cost and memory consumption, while being significantly faster (41? than GC-Net, 4? than PSMNet and 38? than GA-Net), demonstrating the high efficiency of our method compared with commonly used 3D convolutions.</p><p>Complexity Analysis. 2D stereo methods use simple feature correlation to build a 3D cost volume (D ? H ? W ) while 3D methods use concatenation thus a 4D cost volume is built (C ? D ? H ? W ), where C, D, H, W denotes channels after feature concatenation, maximum disparity, height and width, respectively. C usually equals to 64 for Method GC-Net <ref type="bibr" target="#b13">[14]</ref> PSMNet <ref type="bibr" target="#b3">[4]</ref> GA-Net <ref type="bibr" target="#b42">[43]</ref> DeepPruner-Best <ref type="bibr" target="#b5">[6]</ref> DispNetC <ref type="bibr" target="#b19">[20]</ref> StereoNet <ref type="bibr" target="#b14">[15]</ref>    3D convolutions based methods and D = 64 for 1/3 resolution cost volume. Supposing the output cost volume has the same size as input and the kernel size of a convolution layer is K (K = 3 usually), then the computational complexity of a 3D convolution layer is O(K 3 C 2 DHW ). In contrast, the complexity of a deformable convolution layer is O(K 2 D 2 HW + 3K 4 DHW + 3K 2 DHW ). Therefore, the computational complexity of a deformable convolution layer is less than 1/130 of a 3D convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Benchmark Results</head><p>For benchmarking, we build another model variant AANet+. Specifically, the AANet+ model is built by replacing the refinement modules in the GA-Net-AA (see Tab. 2) model with hourglass networks, and five regular 2D convolutions in each refinement module are replaced with their deformable counterparts. We note that our AANet+ has more parameters than AANet (8.4M vs. 3.9M), but it still enjoys fast speed. Tab. 3 shows the evaluation results on Scene Flow test set. Our method not only achieves state-ofthe-art results, but also runs significantly faster than existing top-performing methods. The evaluation results on KITTI 2012 and KITTI 2015 benchmarks are shown in Tab. 4. Compared with other fast models, our AANet is much more accurate. The AANet+ model achieves competitive results among existing top-performing methods while being considerably faster. We also note that HD 3 <ref type="bibr" target="#b38">[39]</ref> has more than 4? parameters than our AANet+ (39.1M vs. 8.4M), and our AANet+ performs much better than previous robust vision challenge winner 1 , iResNet-i2 <ref type="bibr" target="#b17">[18]</ref>, demonstrating that our method achieves a better balance between accuracy and speed. <ref type="figure" target="#fig_5">Fig. 7</ref> further visualizes the disparity prediction error on KITTI 2015 test set. Our AANet produces better results in object boundaries, validating the effectiveness of our proposed adaptive aggregation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented an efficient architecture for cost aggregation, and demonstrated its superiority over commonly used 3D convolutions by high efficiency and competitive performance on both Scene Flow and KITTI datasets. Extensive experiments also validate the generic applicability of the proposed method. An interesting future direction would be extending our method to other cost volume based tasks, e.g., high-resolution stereo matching <ref type="bibr" target="#b36">[37]</ref>, multi-view stereo <ref type="bibr" target="#b37">[38]</ref> and optical flow estimation <ref type="bibr" target="#b29">[30]</ref>. We also hope our lightweight design can be beneficial for downstream tasks, e.g., stereo based 3D object detection <ref type="bibr" target="#b33">[34]</ref>. w(p, q) z ? C(d, q) 2 , (10) whereC(d, p) denotes the aggregated cost at pixel p for disparity candidate d, pixel q belongs to the neighbors N (p) of p, and w is the weighting function to measure the similarity of pixel p and q. The solution of this weighted least square problem <ref type="formula" target="#formula_0">(10)</ref>  </p><p>where p s and d s denote pixel and disparity at scale s, respectively, and p s+1 = p s /2, d s+1 = d s /2, p 1 = p and d 1 = d. The aggregated cost at each scale is denoted as v = [C 1 (d 1 , p 1 ),C 2 (d 2 , p 2 ), ? ? ? ,C S (d S , p S )] T . <ref type="formula" target="#formula_0">(13)</ref> The solution of Eq. (12) is obtained by performing cost aggregation at each scale independently: </p><p>By enforcing the inter-scale consistency on the cost volume, we can obtain the following optimization problem: </p><p>where ? is a parameter to control the regularization strength, andv is denoted a? v = [? 1 (d 1 , p 1 ),? 2 (d 2 , p 2 ), ? ? ? ,? S (d S , p S )] T . <ref type="formula" target="#formula_0">(16)</ref> The optimization problem <ref type="bibr" target="#b14">(15)</ref> is convex and can be solved analytically (see details in <ref type="bibr" target="#b43">[44]</ref>). The solution can be expressed asv = P?,</p><p>where P is an S ? S matrix. That is, the final cost volume is obtained through the adaptive combination of the results of cost aggregation performed at different scales. Inspired by this conclusion, we design our cross-scale cost aggregation architecture a?</p><formula xml:id="formula_13">C s = S k=1 f k (C k ), s = 1, 2, ? ? ? , S,<label>(18)</label></formula><p>where f k is defined by neural network layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the sampling locations in regular convolution based cost aggregation methods and our proposed approach, where the yellow and red points represent the locations for aggregation. (a) left image of a stereo pair. (b) fixed sampling locations in regular convolutions, also the aggregation weights are spatially shared. (c) adaptive sampling locations and position-specific aggregation weights in our approach. The background in (b) and (c) is ground truth disparity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our proposed Adaptive Aggregation Network (AANet). Given a stereo pair, we first extract downsampled feature pyramid at 1/3, 1/6 and 1/12 resolutions with a shared feature extractor. Then multi-scale cost volumes are constructed by correlating left and right features at corresponding scales. The raw cost volumes are aggregated by six stacked Adaptive Aggregation Modules (AAModules), where an AAModule consists of three Intra-Scale Aggregation (ISA, Sec. 3.1) modules and a Cross-Scale Aggregation (CSA, Sec. 3.2) module for three pyramid levels. Next multi-scale disparity predictions are regressed. Note that the dashed arrows are only required for training and can be removed for inference. Finally the disparity prediction at 1/3 resolution is hierarchically upsampled/refined to the original resolution. For clarity, the refinement modules are omitted in this figure, see Sec. 3.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparisons of ablation study on SceneFlow test set. Our AANet produces sharper results in thin structures and better predictions in textureless regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visualization of sampling points (red points) in two challenging regions (green points). In object boundary (a), the sampling points tend to focus on similar disparity regions. While for large textureless region (b), they are more discretely distributed to sample from a large context. Image w/o pseudo gt w/ pseudo gt Visualization of disparity prediction results on KITTI 2015 validation set. Leveraging pseudo ground truth as additional supervision helps reduce the artifacts in regions where ground truth disparities are not available, e.g., the sky region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Generalization on Middlebury 2014 dataset. Our AANet produces sharper object boundaries and better preserves the overall structures than PSMNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of disparity prediction error on KITTI 2015 test set (red and yellow denote large errors). Our method produces better results in object boundaries. Best viewed enlarged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>is C(d, p) = q?N (p) w(p, q)C(d, q).(11)Thus, different local cost aggregation methods can be reformulated within a unified framework. Without considering multi-scale interactions, the multiscale version of Eq.(10) can be expressed as v = arg min {z s } S s=1 S s=1 q s ?N (p s ) w(p s , q s ) z s ? C s (d s , q s ) 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>C</head><label></label><figDesc>s (d s , p s ) = q s ?N (p s ) w(p s , q s )C s (d s , q s ), s = 1, 2, ? ? ? , S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>v</head><label></label><figDesc>= arg min {z s } S s=1 ? ? S s=1 q s ?N (p s ) w(p s , q s ) z s ? C s (d s , q s ) 2 +? S s=2 z s ? z s?1 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of ISA and CSA modules. The best performance is obtained by integrating these two modules.</figDesc><table><row><cell>Method</cell><cell cols="2">Scene Flow</cell><cell cols="2">KITTI 2015</cell></row><row><cell></cell><cell cols="4">EPE &gt; 1px EPE D1-all</cell></row><row><cell cols="2">w/o ISA &amp; CSA 1.10</cell><cell>10.9</cell><cell>0.75</cell><cell>2.63</cell></row><row><cell>w/o ISA</cell><cell>0.97</cell><cell>10.1</cell><cell>0.70</cell><cell>2.22</cell></row><row><cell>w/o CSA</cell><cell>0.99</cell><cell>10.1</cell><cell>0.69</cell><cell>2.31</cell></row><row><cell>AANet</cell><cell>0.87</cell><cell>9.3</cell><cell>0.68</cell><cell>2.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on Scene Flow test set. Our method not only achieves state-of-the-art performance but also runs significantly faster than existing top-performing methods.</figDesc><table><row><cell>Method</cell><cell cols="2">KITTI 2012</cell><cell cols="2">KITTI 2015</cell><cell>Time</cell></row><row><cell></cell><cell cols="4">Out-Noc Out-All D1-bg D1-all</cell><cell>(s)</cell></row><row><cell>MC-CNN [42]</cell><cell>2.43</cell><cell>3.63</cell><cell>2.89</cell><cell>3.89</cell><cell>67</cell></row><row><cell>GC-Net [14]</cell><cell>1.77</cell><cell>2.30</cell><cell>2.21</cell><cell>2.87</cell><cell>0.9</cell></row><row><cell>PSMNet [4]</cell><cell>1.49</cell><cell>1.89</cell><cell>1.86</cell><cell>2.32</cell><cell>0.41</cell></row><row><cell>DeepPruner-Best [6]</cell><cell>-</cell><cell>-</cell><cell>1.87</cell><cell>2.15</cell><cell>0.182</cell></row><row><cell>iResNet-i2 [18]</cell><cell>1.71</cell><cell>2.16</cell><cell>2.25</cell><cell>2.44</cell><cell>0.12</cell></row><row><cell>HD 3 [39]</cell><cell>1.40</cell><cell>1.80</cell><cell>1.70</cell><cell>2.02</cell><cell>0.14</cell></row><row><cell>GwcNet [8]</cell><cell>1.32</cell><cell>1.70</cell><cell>1.74</cell><cell>2.11</cell><cell>0.32</cell></row><row><cell>GA-Net [43]</cell><cell>1.36</cell><cell>1.80</cell><cell>1.55</cell><cell>1.93</cell><cell>1.5</cell></row><row><cell>AANet+</cell><cell>1.55</cell><cell>2.04</cell><cell>1.65</cell><cell>2.03</cell><cell>0.06</cell></row><row><cell>StereoNet [15]</cell><cell>4.91</cell><cell>6.02</cell><cell>4.30</cell><cell>4.83</cell><cell>0.015</cell></row><row><cell>MADNet [33]</cell><cell>-</cell><cell>-</cell><cell>3.75</cell><cell>4.66</cell><cell>0.02</cell></row><row><cell>DispNetC [20]</cell><cell>4.11</cell><cell>4.65</cell><cell>4.32</cell><cell>4.34</cell><cell>0.06</cell></row><row><cell>DeepPruner-Fast [6]</cell><cell>-</cell><cell>-</cell><cell>2.32</cell><cell>2.59</cell><cell>0.061</cell></row><row><cell>AANet</cell><cell>1.91</cell><cell>2.42</cell><cell>1.99</cell><cell>2.55</cell><cell>0.062</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Benchmark results on KITTI 2012 and KITTI 2015 test sets. Our AANet+ model achieves competitive results among existing top-performing methods while being considerably faster. Compared with other fast models, our AANet is much more accurate.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.robustvision.net/rvc2018.php</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank anonymous reviewers for their constructive comments. This work was supported by the National Natural Science Foundation of China (No. 61672481) and Youth Innovation Promotion Association CAS (No. 2018495).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We briefly review traditional cross-scale cost aggregation algorithm <ref type="bibr" target="#b43">[44]</ref> to make this paper self-contained.</p><p>For cost volume C ? R D?H?W , <ref type="bibr" target="#b43">[44]</ref> reformulates the local cost aggregation from an optimization perspective:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A variable window approach to early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1283" to="1294" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stereodrnet: Dilated residual stereonet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11786" to="11795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeppruner: Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4384" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time correlation-based stereo vision with reduced border errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Peter R Innocent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="229" to="246" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmaa</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="511" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="573" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computing visual correspondence with occlusions using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="508" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereoscopic depth processing in the visual cortex: a coarse-to-fine mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph D</forename><surname>Menz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A revisit to cost aggregation in stereo matching: How far can we reduce its computational redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1567" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cost aggregation and occlusion handling with wls in stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1431" to="1442" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-level context ultra-aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Yu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3283" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A locally adaptive window for signal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="162" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo matching using belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan-Ning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="787" to="800" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic stereo matching with pyramid cost volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7484" to="7493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale geometric consistency guided multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5483" to="5492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical deep stereo matching on highresolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Manela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive support-weight approach for correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Kuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis &amp; machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="650" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1592" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-scale cost aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1590" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
