<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gu</surname></persName>
							<email>li.gu@huawei.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
							<email>zhixiang.chi@huawei.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huan.liu3@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Yu</surname></persName>
							<email>yuanhao.yu@huawei.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>yang.wang3@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">McMaster University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Manitoba</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* denotes equal contribution.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present the winning solution for OR-BIT Few-Shot Video Object Recognition Challenge 2022. Built upon the ProtoNet baseline, the performance of our method is improved with three effective techniques. These techniques include the embedding adaptation, the uniform video clip sampler and the invalid frame detection. In addition, we re-factor and re-implement the official codebase to encourage modularity, compatibility and improved performance. Our implementation accelerates the data loading in both training and testing. The code can be found: ORBIT-2022-winner-method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, Few-shot Learning has received increasing attention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> as it allows models to recognize novel objects from only a few examples. This will enable computer vision systems adapt to dynamic real-world environments where users can provide a few training examples themselves. Few-shot concept is adapted and extended to various real-world settings, such as few-shot continual learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>, test-time adaptation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref> and leveraging domain shift <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Existing datasets in few-shot learning undergo a lack of high variation in both the number of examples per object and the quality of those examples. The resulting trained object recognizers are not robust to the noisy input data (e.g., video frames) streamed from real-world systems. To drive further innovation in few-shot learning, ORBIT dataset <ref type="bibr" target="#b10">[11]</ref> captures the high variations inherent in real-world applications via the collection of thousands of videos recorded by vision-impaired people. ORBIT Few-Shot Object Recognition Challenge, newly introduced in 2022, invited teams to build a teachable object recognizer using the ORBIT dataset. Unlike a generic object recognizer, a user can 'teach' a teachable object recognizer to recognize their spe-cific personal objects by providing just a few video sequences. Specifically, to register the object categories to be classified, a few clean video sequences with several usercentric objects are provided for the recognizer at the personalization stage. Then, the different video sequences from the same user are used to evaluate the recognizer at the recognition stage. Considering the fact that visionimpaired people cannot localize the target object accurately and quickly, video frames at the recognition stage are collected from cluttered scenes containing multiple objects.</p><p>[11] establishes the baseline on the ORBIT benchmark by extending 4 mainstream meta-learning based methods from image to videos <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. LITE <ref type="bibr" target="#b0">[1]</ref> scales up the input image resolution with a general and memory-efficient episodic training scheme and thus achieves state-of-the-art accuracy. However, no algorithmic design for handling noisy examples and sampling video frames is considered in the existing methods. As observed in the real-world dataset, video frames are quite noisy, especially for those taken by vision-impaired people. In addition, the video frame sampling method used in the prior works is inefficient and ineffective. Since the video lengths are variable, random sampling may cause over-sampling or under-sampling. Both factors hamper the performance of the trained object recognizer.</p><p>We build our solution on top of the state-of-the-art method, ProtoNet <ref type="bibr" target="#b12">[13]</ref> with LITE. In the ORBIT benchmark, ProtoNet firstly uses clean video clips as the support data to produce category-level prototypes at the personalization stage. Then, the clutter video clips as the query data can be classified by directly comparing their embeddings with the prototypes using a similarity metric at the recognition stage. To generate high-quality prototypes at the personalization stage, we add three techniques: First, inspired by <ref type="bibr" target="#b19">[20]</ref>, we incorporate one transformer encoder block to leverage the relationship among prototypes to enable the adaptation to the specific episode, and thus highlight their discriminative representation for a specific user. Also, we replace the random video clip sampler by the uniform sam- pler to enable higher temporal coverage during testing and to reduce down-sampling and over-sampling in long and short video sequences respectively. Last, we apply an edge detector on each sampled video frame and set an empirical threshold to determine and remove the frame that contains nothing. Our approach improve the accuracy significantly by 8% and was selected as the winner from 12 submissions in the ORBIT Few-Shot Object Recognition Challenge.</p><p>In addition to our innovation on algorithm design, we also refactor and re-implement the data pipeline in the original ORBIT codebase to encourage modularity, compatibility and performance improvement. Specifically, our implementation achieves more than 2.7x acceleration in data loading during both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we present the details of our solution. We begin by introducing the entire pipeline of the baseline method using ProtoNet in Few-shot Video Object Recognition. Then, we introduce several improvements to each module, including a few-shot learner, video clip sampler, and frame edge detector, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline pipeline</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the pipeline consists of three modules: video clip sampler, few-shot learner and casual sliding window. At the personalization stage, the few-shot learner, ProtoNet, aims to extract semantic information from a few of clean video sequences and generates the prototypes of object categories. Since the few-shot learner cannot process  all video frames simultaneously due to the limited computational resources, a video clip sampler is introduced to randomly select multiple video clips from each sequence. Thus, the few-shot learner takes a few of clean video clips as the support set to generate the prototypes. At the recognition stage, in order to classify every frame in a clutter video sequence, a causal sliding window converts the entire video sequence into a series of overlapped video clips as the query set, where each video frame corresponds to a video clip. Furthermore, each video clip is fed into the few-shot learner and generates the prediction via comparing their embeddings with the prototypes using a similarity metric. However, there are several reasons that hinder the generation of high-quality prototypes. First, due to the distri-Transformer (Embedding Adaptation)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Nearest Neighbor Classification Score</head><p>EfficientNet ours original support clips query clip bution shift between the clean support and the clutter query video sequences, using the same backbone to extract video clip features from both sets is sub-optimal. Second, each user's video sequences are collected from limited scenes, resulting in a similar background or multiple target userspecific object issues. Third, there are dramatic appearance changes across each support video sequence, and some frames suffer from an "object not present issue". Thus, randomly sampled clips from support video sequences would not provide comprehensive information for building prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Improvements</head><p>To make the few-shot learner, ProtoNet, build highquality prototypes at the personalization stage, we develop three techniques on top of the baseline pipeline, shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Embedding Adaptation. Since the support video clips are from the same user, the few-shot learner aims to generate prototypes that can adapt to the specific user. Therefore, inspired by <ref type="bibr" target="#b19">[20]</ref>, we add a set-to-set function, one transformer encoder block, to refine the original prototypes by leveraging their relationship, shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As a result, the most discriminative representations for a specific user can be highlighted. Also, the transformer encoder block can map the feature embeddings of the clean support video clips to the space close to those of the clutter query video clips and thus help alleviate the distribution shift. Uniform Clip Sampler. In the random video clip sampler, the number of frames in each clip is fixed. However, both the number of clips and the starting position of each clip are randomly chosen across different video sequences, regardless of their duration. In consequence, the long and short video sequences may suffer from under-sampling or over-sampling respectively. Thus, to achieve higher temporal coverage and constant sampling rate, we replace the random video clip sampler with the uniform one <ref type="bibr" target="#b8">[9]</ref>: First, we split each support video sequence into multiple fix-sized and non-overlapped clip candidates. Then, we evenly split clip candidates into non-overlapped chunks and ensure each chunk has the same number of clip candidates. Last, we sample one clip from each chunk. <ref type="figure" target="#fig_1">Fig. 2</ref> demonstrates the details. Invalid frame detection. Due to the dramatic changes across the video sequence, a few sampled support video clips may not contain the target object, thus, failing to contribute informative features to generate high-quality prototypes. Therefore, we apply an edge detector to each frame in the sampled video clips and determine whether the frame contains objects via an empirical threshold. If more than half of the frames in one video clip are identified with "object not present issue", that clip will be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Contributions on Code Quality</head><p>To improve the code efficiency, we refactor and reimplement the data pipeline of the original official codebase to encourage modularity, compatibility and performance. Modularity. We decouple the object category sampling, video sequence (instance) sampling, video frame image loading and tensor preparing from one deeper class into multiple independent shallow classes. These components can be used in plug-and-play and mix-and-match manners. Compatibility. It is designed to be interoperable with other Pytorch standard domain-specific libraries (torchvision), and their highly-optimized modules and functions can be used in processing the ORBIT dataset. Performance. To optimize I/O, we introduce multithreading to reduce the latency of loading images from disk in each episode. <ref type="table" target="#tab_1">Table 1</ref> shows the comparison of data loading speed, where our re-implemented codebase accelerates the data loading speed by 2.7 and 2.77 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and implementation details</head><p>Dataset. For this challenge, we evaluate our method on the ORBIT dataset which is designed for real-world fewshot object recognition <ref type="bibr" target="#b10">[11]</ref>. It contains 3,822 videos of 486 objects collected by 67 users. Each user is asked to collect videos with the target object in isolation which is referred as clean videos. They are also asked to take videos where the target object is mixed with multiple other objects. These videos are referred as clutter videos. The goal of this challenge is to train a teachable object recognizer such that the model is personalized for each user using their clean videos. The personalized model is then evaluated on the clutter videos. Please refer <ref type="bibr" target="#b10">[11]</ref> for more details. In the concept of meta-learning scenario, the clean videos are analogy as support set while the clutter videos are query set.</p><p>Network. We follow <ref type="bibr" target="#b0">[1]</ref> to use EfficientNet-B0 <ref type="bibr" target="#b15">[16]</ref> pretrained on ImageNet <ref type="bibr" target="#b5">[6]</ref> as the feature extractor. A single layer transformer encoder as in FEAT <ref type="bibr" target="#b19">[20]</ref> is used to further adapt the computed prototypes.</p><p>Training and evaluation protocol. We leverage the concept of prototype-based meta-learning to train our network. A comprehensive survey on bi-level optimization can be found <ref type="bibr" target="#b1">[2]</ref>. The embedding of the query videos corresponding to the same class is averaged as its prototypes. The query videos are classified by comparing the cosine similarity between the prototypes. We follow the episodic learning as in LITE <ref type="bibr" target="#b0">[1]</ref> to utilize large resolution frame patches to train the network. For a fair comparison, we use the same hyper-parameters and evaluation protocol as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Quantitative results. <ref type="table" target="#tab_2">Table 2</ref> shows the main results and the contributions of each component. We re-run the codebase of <ref type="bibr" target="#b10">[11]</ref> for typical ProtoNet with default hyperparameters, but its per frame accuracy dropped by 3%. We re-implemented the codebase, and ours achieves similar results as reported by <ref type="bibr" target="#b0">[1]</ref>. With Embedding adaptation method, the generated prototypes for all categories are further enhanced by examining the relationships among them. The prototypes are pushed away from others and become more discriminative. Therefore, the accuracy increased by 2.87%. Uniform clip sampler ensures the constant sampling rate and the higher temporal coverage for all videos to reduce the randomness of information gathering. In addition, due to the variable lengths of the videos, a uniform clip sampler ensures that the short videos are not over-sampled. Therefore, it improves an additional 1.52%. Furthermore, due to the high correlation between consecutive video frames, some sampled video clips may convey repetitive information, especially with minor movements. Thus, the overall amount of discriminative data points is re-  duced. Enriching the data information by augmenting the video clip is an effective method. Hence, Data augmentation improves additional 0.88%. To filter out the frames with less information (background only), the Invalid frame detection by edge detection and threshold is able to reduce such effect. Therefore, it further improves by 0.12%. Overall, with full proposed components, our method outperforms the baseline by 5.39%. Qualitative results. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the per user performance accuracy. It is worth noting that the embedding adaptation and uniform sampling provide significant improvement for most of the users. With the integration of all proposed components, our method achieves the best accuracy for 11 users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>In this work, we proposed several improvements for the few-shot video object recognition task. The proposed components consist of embedding adaptation, uniform video sampling, and invalid frame detection. Our unified solution achieves the winner of the first challenge on the ORBIT dataset. Furthermore, we also contribute to refactoring and optimizing the original codebase to improve the productivity of other researchers. Future work includes the solution to tackle the domain shift between support and query videos which is quite common in real-world scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our method. At the personalization stage, the support video frames are uniformly sampled and then filtered by an edge detector. The resulting clips are used to generate the adaptive prototypes. At the recognition stage, the frames are classified by measuring the distance with the prototypes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of uniform sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of embedding adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Per user performance comparison. Among 17 users, our method achieves the highest accuracy for the 11 users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Video A Video B Random Clip Sampler: Low Sampling Rate High Sampling Rate Video A Video B Uniform Clip Sampler --Ours: Fixed Sampling Rate Fixed Sampling Rate Video Sequence Sampled Clip</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Training and testing speed for data loaders. The baseline is the original ORBIT codebase. Testing speed is measured by preparing 300 videos from 17 users. Training speed is measured by preparing 100 episodes.</figDesc><table><row><cell cols="5">Method # workers # threads Test speed Train speed</cell></row><row><cell>Baseline</cell><cell>4</cell><cell>1</cell><cell>233</cell><cell>2.61</cell></row><row><cell></cell><cell>4</cell><cell>4</cell><cell cols="2">201 (1.15x) 2.2 (1.18x)</cell></row><row><cell>Ours</cell><cell>4</cell><cell>16</cell><cell cols="2">152 (1.53x) 1.08 (2.41x)</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>86 (2.7x)</cell><cell>0.94 (2.77x)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Main results and contribution from each component.</figDesc><table><row><cell>Method</cell><cell cols="2">Frame accuracy Improvement</cell></row><row><cell>ProtoNet (copy from [1])</cell><cell>66.30</cell><cell>-</cell></row><row><cell>ProtoNet (ORBIT code base)</cell><cell>63.27</cell><cell>-3.03</cell></row><row><cell>ProtoNet (Ours)</cell><cell>66.27</cell><cell>-0.03</cell></row><row><cell>+ Embedding adaptation</cell><cell>69.17</cell><cell>+2.87</cell></row><row><cell>+ Uniform sampler</cell><cell>70.69</cell><cell>+4.39</cell></row><row><cell>+ Data augmentation</cell><cell>71.57</cell><cell>+5.27</cell></row><row><cell>+ Invalid frame detection</cell><cell>71.69</cell><cell>+5.39</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory efficient meta-learning with large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gradient-based bi-level optimization for deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11719,2022.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bidirectional learning for offline infinite-width model-based optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07507</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metafscil: A meta-learning approach for fewshot class incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Testtime fast adaptation for dynamic scene deblurring via metaauxiliary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nitesh Bharadwaj Gundavarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonglin</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal representation learning by exploiting video continuity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niamul</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Few-shot class-incremental learning via entropy-regularized data-free replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11213</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orbit: A real-world few-shot dataset for teachable object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Tobias</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecily</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Test-time training with selfsupervision for generalization under distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Few-shot classincremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive risk minimization: Learning to adapt to domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23664" to="23678" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
