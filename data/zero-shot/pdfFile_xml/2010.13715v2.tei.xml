<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Birkbeck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balu</forename><surname>Adsumilli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
						</author>
						<title level="a" type="main">ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of conducting frame rate dependent video quality assessment (VQA) on videos of diverse frame rates, including high frame rate (HFR) videos. More generally, we study how perceptual quality is affected by frame rate, and how frame rate and compression combine to affect perceived quality. We devise an objective VQA model called Space-Time GeneRalized Entropic Difference (GREED) which analyzes the statistics of spatial and temporal band-pass video coefficients. A generalized Gaussian distribution (GGD) is used to model band-pass responses, while entropy variations between reference and distorted videos under the GGD model are used to capture video quality variations arising from frame rate changes. The entropic differences are calculated across multiple temporal and spatial subbands, and merged using a learned regressor. We show through extensive experiments that GREED achieves state-of-the-art performance on the LIVE-YT-HFR Database when compared with existing VQA models. The features used in GREED are highly generalizable and obtain competitive performance even on standard, non-HFR VQA databases. The implementation of GREED has been made available online: https://github.com/pavancm/GREED. Index Terms-high frame rate, objective algorithm evaluations, video quality assessment, full reference, entropy, natural video statistics, generalized Gaussian distribution arXiv:2010.13715v2 [cs.MM]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ROVIDING immersive visual experiences for consumers is of principal importance for entertainment, streaming and social video service providers. In recent years considerable effort has been expended on improving video quality by extending current video parameter spaces along spatial and temporal resolutions, color gamut, dynamic range and multiview formats. However there has been less attention directed to high frame rate (HFR) videos, and existing major television, cinema and other video streaming applications currently only deliver videos at 60 frames per second (fps) or less.</p><p>The impact of frame rate on perceptual video quality is a less studied topic. Although there exists a belief that HFR videos generally possess superior perceptual quality due to implied reductions in temporal artifacts such as aliasing (flicker, judder etc.) and motion blur, there have been few systematic studies validating these notions. As interest in HFR video delivery has begun to increase, owing to a plethora of high P. C. <ref type="bibr">Madhusudana</ref>  motion and sports and live action content, the question naturally arises whether effective HFR video quality assessment (VQA) databases and measurement tools can be successfully developed and put into practice.</p><p>The problem of HFR-VQA has been previously attempted by analyzing databases like Waterloo HFR <ref type="bibr" target="#b0">[1]</ref> and BVI-HFR <ref type="bibr" target="#b1">[2]</ref>, which primarily address HFR content quality. Although these databases try to address the problem of frame rate dependent video quality prediction, they suffer from some fundamental limitations: only a few frame rates are considered, or the combined effects of compression distortions are not considered. Fortunately, a new HFR-VQA database, called LIVE-YT-HFR <ref type="bibr" target="#b2">[3]</ref> has been published, spanning 6 different frame rates ranging from 24 to 120 fps, combined with diverse levels of compression distortions. This publicly available database provides a new and valuable tool to enable the modeling of the complex relationships between frame rate, compression and perceptual video quality.</p><p>Existing generic VQA models are of limited use in this application space, since the reference and distorted videos that they compare are required to have the same frame rates. Although existing VQA methods can be extended to HFR scenarios by suitable preprocessing methods, such as temporal downsampling of the reference videos, or upsampling of distorted videos, these operations often result in new distortions and poor correlations against human judgments of video quality <ref type="bibr" target="#b2">[3]</ref>. Moreover, the performances of these "standard" models can be sensitive to the choice of preprocessing method employed.</p><p>Here we propose a new HFR-VQA model that we call GeneRalizEd Entropic Difference (GREED), which analyzes the statistics of spatial and temporal band-pass filtered video coefficients against statistical and perceptual models of distortion. Important characteristics of the proposed design are as follows:</p><p>1) The band-pass coefficients are modeled as following a Generalized Gaussian Distribution (GGD). We show that local space-time block entropies can effectively quantify perceptual artifacts that arise from changes in frame rate or compression or both. 2) GREED is composed of separate spatial and temporal features. Spatial GREED (SGREED) features are calculated on spatial band-pass coefficients and can only capture spatial distortions, while temporal GREED (TGREED) features are obtained from temporal bandpass responses, which can capture both spatial and temporal impairments. We also show that SGREED and TGREED features account for complementary perceptual quality information. 3) GREED features are calculated over multiple spatial and temporal subbands, and then combined using a learned regressor to predict quality. The parameters of the regressor are obtained in a data driven manner, whereby a mapping is learned from GREED features to quality scores using a suitable VQA dataset. 4) GREED is highly generalizable, and a family of algorithms is designed that vary in the choice of temporal band-pass filter employed. For example choosing a single level Haar filter is equivalent to a simple frame differencing operation, which has been successfully used in various prior VQA models <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>. 5) Models that employ GREED features achieve stateof-the-art performance on the new LIVE-YT-HFR Database. Moreover, these features achieve competitive performance, even on standard VQA databases showing their generalizability to non-HFR scenarios. The rest of the paper is organized as follows: In Section II we discuss prior work on the objective VQA problem. In Section III we provide a detailed description of our proposed VQA model. In Section IV we compare and analyze various experimental results comparing GREED against existing VQA models, and we conclude with thoughts for future work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Objective VQA models are broadly categorized into three groups <ref type="bibr" target="#b7">[8]</ref>: Full-Reference (FR), Reduced-Reference (RR) and No-Reference (NR). FR VQA models require access to an entire pristine undistorted video along with its degraded version, while RR models operate with limited reference information. NR models predict quality without any knowledge about a reference. This work addresses the problem of quality evaluation when pristine (reference) and distorted sequences may possibly have different frame rates, thus our primarily focus will be on FR and RR VQA methods.</p><p>The literature concerning FR-VQA has significantly matured as a multitude of approaches have been proposed over the last decade. One can trivially extend FR Image Quality Assessment (IQA) indices <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> for application on videos, by predicting the quality of every video frame and using a suitable temporal pooling scheme. Although this procedure can be computationally inexpensive, performance is limited since useful temporal quality information is not effectively employed. The Video Quality Metric (VQM) <ref type="bibr" target="#b11">[12]</ref> is an early VQA method, which employs losses in the spatial gradients of luminance, along with features based on the product of luminance contrast and motion. The later VQM-VFD <ref type="bibr" target="#b12">[13]</ref> model is particularly successful at capturing frame delays, and achieves competitive performance on the LIVE-mobile database <ref type="bibr" target="#b13">[14]</ref>. The MOVIE index <ref type="bibr" target="#b14">[15]</ref> and a SSIM-based precursor <ref type="bibr" target="#b15">[16]</ref> use the idea of motion tuning by tracking perceptually relevant artifacts along motion trajectories to measure video quality. ST-MAD <ref type="bibr" target="#b16">[17]</ref> index uses spatio-temporal video slices and applies the idea of "most apparent distortion" <ref type="bibr" target="#b17">[18]</ref> to assess quality. In <ref type="bibr" target="#b18">[19]</ref>, a contrast sensitivity function derived from an attention-driven foveation mechanism is integrated into a wavelet-distortion visibility measure, yielding a full reference video quality model. The proposed models in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> use optical flow characteristics to measure video quality. Spatio-temporal Natural Scene Statistics (NSS) based models such as ST-RRED <ref type="bibr" target="#b3">[4]</ref> and SpEED-VQA <ref type="bibr" target="#b5">[6]</ref> compute spatial and temporal entropic differences in a band-pass domain to measure quality deviation. A common principle underlying in these NSS based approaches is that pristine video frames can be well modeled as following Gaussian Scale Mixture (GSM) statistical model and that the presence of distortions results in departures from the GSM model. These statistical deviations can be used to quantify and predict video quality. Recently, data driven approaches have become increasingly popular because of the performances they are able to deliver. For example, Video Multi-method Fusion (VMAF) <ref type="bibr" target="#b4">[5]</ref> model developed by Netflix is a widely used quality predictor built on existing IQA/VQA models on features which are combined using a Support Vector Regressor (SVR). ST-VMAF <ref type="bibr" target="#b6">[7]</ref> improves upon VMAF by affixing additional perceptually relevant features that better express temporal aspects of video quality. The recent popularity of deep learning has led to a variety of CNN based models <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> that achieve competitive performance on existing VQA databases. Note that all of the above models require the reference and distorted videos to have the same number of frames in temporal synchrony as well as same spatial resolution. Because of this additional nontrivial processing and undesirable processing steps are required when either of these two conditions are violated.</p><p>Research pertaining to HFR quality prediction is nascent and the associated literature is very sparse. One of the earliest HFR-VQA models was proposed by Nasiri et al. <ref type="bibr" target="#b24">[25]</ref>, where they measure the amount of aliasing in the temporal frequency spectrum to evaluate quality. In <ref type="bibr" target="#b25">[26]</ref> motion smoothness is measured by examining the local phase correlation of complex wavelet coefficients. This model achieved good performance in the presence of global motion but falls short in presence of local motions or high spatial variations of motion. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> proposed the wavelet domain based Frame Rate Quality Metric (FRQM), which uses absolute differences between the temporal wavelet filtered sequences of the reference video and the temporally upsampled distorted video to quantify quality. Although FRQM achieves competitive performance on the BVI-HFR database <ref type="bibr" target="#b1">[2]</ref>, it cannot be used when the reference and distorted videos have the same frame rate. Moreover, FRQM does not account for the combined effects of compression and frame rate, thus limiting its generalizability. In previous work <ref type="bibr" target="#b27">[28]</ref>, a temporal Haar filter based GGD model was proposed to measure HFR video quality without including any temporal pre-processing stage, which achieved good performance on the LIVE-YT-HFR Database. Here we substantially extend and generalize this concept by using very general classes of temporal filters, which are deployed over multiple scales, and by using data-driven learning to achieve significantly improved perceptual quality prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Here we introduce a novel FR-VQA called GREED, which can be employed when the reference and the distorted videos have either the same or different frame rates. Our model is inspired by prior NSS based models which measure statistical deviations in a bandpass transform domain to quantify quality. These methods rely on two specific principles: One is the strongly decorrelating property of spatial band-pass image decompositions such as the DCT <ref type="bibr" target="#b28">[29]</ref>, or wavelets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> the responses of which strongly tend to follow well-modeled heavy-tailed distributions. The second is subsequent divisive normalization of the bandpass coefficients, the perceptually relevant GSM natural image model and known functional processing by retino-cortical neurons, which relates to contrast masking. Divisive normalization, or equivalently, conditioning on the bandpass variance field <ref type="bibr" target="#b29">[30]</ref>, applied on on the bandpass coefficients of images/videos further decorrelates and strongly Gaussianizes them <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. However, these statistical consistencies tend to be disrupted by distortions which is particularly useful when capturing quality variations. Although there exists a multitude of NSS inspired VQA models, these primarily use spatial NSS models or very simple temporal extensions of them. There has been much less attention directed towards designing temporal NSS models to address temporal artifacts. Current models account for temporal distortions by employing basic operations, such as frame differences <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b28">[29]</ref> or that perform expensive motion estimation computations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Although these methods perform well under generic conditions, they can only be used when the reference and distorted videos have the same frame rate, i.e.the same number of frames which are in temporal correspondence. Here we aim to go beyond the scope of these prior methods, by removing the frame rate limitation, while addressing the measurement of quality disruptions arising from frame rate variations as well as combined frame rate and compression effects.</p><p>Consider a bank of K temporal band-pass filters denoted by b k for k ? {1, . . . K}. The temporal band-pass response to a video V (x, t) (x = (x, y) represents spatial co-ordinates and t denotes temporal dimension) is denoted by</p><formula xml:id="formula_0">B k (x, t) = V (x, t) * b k (t) ?k ? {1, . . . K},<label>(1)</label></formula><p>where * and B k are the convolution operation and band-pass response of the k th filter respectively. Note that these are 1D filters applied only along the temporal dimension. We also note that frame differences are a special case of (1), where the band-pass filter is the high pass component of a Haar wavelet filter. We have empirically observed that the distributions of the coefficients of B k varies with frame rate. This is illustrated in <ref type="figure">Fig. 1a</ref> where the empirical distributions (histograms) of bandpass videos having different frame rates are shown following temporal filtering using a 4-level Haar wavelet filter. It may be observed that, as the frame rates increase, the distribution becomes peakier since the correlations between neighborhood frames increase with frame rate, making band-pass responses more sparse. An interesting phenomenon is illustrated in <ref type="figure">Fig.  1b</ref>, where the distributions of divisively normalized bandpass coefficients under a GSM model are plotted; any differences between the histograms at are very small. This observation is rather unconventional, given that in many prior models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> it is assumed that divisive normalization tends to capture underlying distortions because they also predict the empirical distributions of the processed image or video signals. One possible explanation behind this observation could be that successful normalization relies on spatial local neighborhood responses, which frame rate changes may not significantly alter. Note that so far, the observations that we are making are in regards to videos having different frame rates, but no compression. Certainly, the presence of compression artifacts can further impact the shape of the distributions. Although this implies that bandpass normalization may not be strongly predictive of frame rate variations, the band-pass coefficients without normalization are still well modeled as obeying a Generalized Gaussian Distribution (GGD). GGD models have previously been employed to model band-pass coefficients in many applications, such as image denoising <ref type="bibr" target="#b34">[35]</ref>, texture retrieval <ref type="bibr" target="#b35">[36]</ref>, blind VQA model <ref type="bibr" target="#b28">[29]</ref> and so on. Our work is primarily motivated by the successful ST-RRED model <ref type="bibr" target="#b30">[31]</ref> whereby entropic differences calculated under a GSM model are used to measure deviations in bandpass coefficient distributions caused by distortion. We alter this idea by designing a statistical model based on the GGD, rather than the GSM to capture frame rate variations. In the next subsection, we explain our GGD based model of bandpass coefficients in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GGD Based Statistical Model</head><p>Let the reference and distorted videos be denoted by R and D respectively, with R t , D t representing corresponding frames at time t. Note that R and D can possibly have different frame rates, although we will require them to have the same spatial resolution. Let the responses of the k th band pass filter b k , k ? {1, 2, . . . K}, on the reference and distorted videos be denoted by B R kt and B D kt , respectively. Assume that every</p><formula xml:id="formula_1">frame of B R kt , B D kt follows a GGD model i.e. B R kt ? GGD(? R kt , ? R kt , ? R kt ), B D kt ? GGD(? D kt , ? D kt , ? D kt )<label>(2)</label></formula><p>where ? is a location parameter which is the mean of the distribution, ? is a scale parameter and ? is the shape parameter. Note that these parameters are time-varying, depending on the dynamics of the video under consideration. Since the bandpass coefficients have zero-mean, we only consider the two parameter GGD model:</p><formula xml:id="formula_2">? R kt = ? D kt = 0 ?k, t.</formula><p>The probability density of a zero mean GGD(?, ?) is given by:</p><formula xml:id="formula_3">f (x; ?, ?) = ? 2??(1/?) exp ? |x| ? ? where ?(.)</formula><p>is the gamma function:</p><formula xml:id="formula_4">?(a) = ? 0 x a?1 e ?x dx.</formula><p>The shape parameter ? controls the shape of the distribution (tail weight and peakiness) while ? affects the variance. Special cases of GGD include the Gaussian distribution (? = 2) and Laplacian distribution (? = 1). Let the band pass coefficients at frame t be partitioned into non-overlapping patches/blocks of size</p><formula xml:id="formula_5">? M ? ? M , which are indexed by p ? {1, 2, . . . P }. Let B R</formula><p>kpt and B D kpt denote vectors of band pass coefficients in patch p for subband k and frame t of the reference and distorted videos, respectively.</p><p>Neural Noise Model: We model the band-pass coefficients within each patch as having passed through a Gaussian channel, to model perceptual imperfections such as neural noise <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Let B R kpt , B D kpt represent coefficients which undergo channel imperfections yieldingB R kpt ,B D kpt . This model is expressed as:</p><formula xml:id="formula_6">B R kpt = B R kpt + W R kpt ,B D kpt = B D kpt + W D kpt (3) where B R kpt is independent of W R kpt , B D kpt is independent of W D kpt , W R kpt ? N (0, ? 2 W I M ) and W D kpt ? N (0, ? 2 W I M ) and I M denotes the identity matrix of dimensions M ? M . It may be inferred from (3) thatB R kpt ,B D kpt need not necessarily</formula><p>follow a GGD law, since it is a sum of GGD and Gaussian random variables. However, prior authors <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> have shown that such a sum can be well approximated as GGD under the independence assumption. We hypothesize that the sample entropies ofB R kpt ,B D kpt contain information potentially pertaining to quality, thus measuring their differences may reflect observed quality differences between the reference and distorted videos. The entropy of a GGD random variable X ? GGD(0, ?, ?) has a closed form expression given by:</p><formula xml:id="formula_7">h(X) = 1 ? ? log ? 2??(1/?) .<label>(4)</label></formula><p>Entropy computation requires knowledge of the GGD parameters ofB R kpt andB D kpt . However we only have access to observed band-pass responses, B R kpt and B D kpt . In order to estimate the parameters ofB R kpt andB D kpt we follow the kurtosis matching procedure detailed in <ref type="bibr" target="#b37">[38]</ref>. The first step involves the estimation of variance, which can be directly calculated from (3) using the independence assumption:</p><formula xml:id="formula_8">? 2 (B R kpt ) = ? 2 (B R kpt ) + ? 2 W , ? 2 (B D kpt ) = ? 2 (B D kpt ) + ? 2 W .</formula><p>(5) The next step involves calculation of the kurtosis ?:</p><formula xml:id="formula_9">?(B R kpt ) = ?(B R kpt ) ? 2 (B R kpt ) ? 2 (B R kpt ) 2 ?(B D kpt ) = ?(B D kpt ) ? 2 (B D kpt ) ? 2 (B D kpt ) 2 .<label>(6)</label></formula><p>Interested readers can refer to <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> for a detailed derivation of <ref type="bibr" target="#b5">(6)</ref>. The sample variance and kurtosis values of B R kpt , B D kpt are employed in <ref type="bibr" target="#b5">(6)</ref> to calculate the kurtosis ofB R kpt andB D kpt , respectively. In the last step, the bijective mapping between the GGD parameters and kurtosis <ref type="bibr" target="#b37">[38]</ref> is applied to estimate the GGD parameters. The expression for the kurtosis of a GGD random variable in terms of its parameters is given by:</p><formula xml:id="formula_10">?(X) = ?(5/?)?(1/?) ?(3/?) 2 .<label>(7)</label></formula><p>A simple grid search can be used to estimate the shape parameter ? from the kurtosis value obtained from <ref type="bibr" target="#b5">(6)</ref>. The other parameter ? is calculated using the relation</p><formula xml:id="formula_11">? = ? ?(1/?) ?(3/?) .<label>(8)</label></formula><p>In our implementation we only calculate ? over each entire frame using <ref type="bibr" target="#b6">(7)</ref> rather than on every patch, as we have empirically observed the local predictions to be particularly noisy. However, the scale parameter ? is still computed on every patch as it depends on the local variance ? along with the shape parameter ?. The entropies h(B R kpt ) and h(B D kpt ) are computed by simply substituting the GGD parameters obtained from <ref type="formula" target="#formula_10">(7)</ref> and <ref type="formula" target="#formula_11">(8)</ref>, into (4). In the next section we show how these entropies can be effectively used for quality prediction.  Illustration of frame rate bias of entropy for bouncyball sequence from the LIVE-YT-HFR database. Note that in the above plot, only compressed versions are included, while lossless (CRF = 0) videos are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Temporal Measure</head><p>We define entropy scaling factors similar to the ones used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b30">[31]</ref> as:</p><formula xml:id="formula_12">? R kpt = log(1 + ? 2 (B R kpt )), ? D kpt = log(1 + ? 2 (B D kpt ))</formula><p>The notion behind using these scaling factors is to lend more locality to the model. They also have the additional advantage of providing numerical stability on regions having low variance values, as entropy estimates can be noisy/inconsistent in these places. Scaled entropies are obtained by premultiplying these scaling factors:</p><formula xml:id="formula_13">R kpt = ? R kpt h(B R kpt ), D kpt = ? D kpt h(B D kpt )<label>(9)</label></formula><p>Frame Rate Bias of Entropy: Although absolute differences between the scaled entropies in (9) can represent quality differences, there exists a frame rate bias associated with the entropy values, since different frame rates have entropies at different scales. This is illustrated in <ref type="figure" target="#fig_3">Fig. 2</ref>, where the entropy values remain nearly constant at a given frame rate, even with different levels of compression. Thus, simple subtraction can only measure the difference in frame rates between R and D. Although this is desirable, it can be ineffective when analyzing videos which only differ by compression artifacts, for example, R and D of the same frame rate but different compression levels. Moreover, quality is jointly dependent on frame rate and compression, while the existence of frame rate bias makes the entropy difference insufficient to capture compression distortions.</p><p>To remove this bias, we employ an additional video sequence which we call Pseudo Reference (P R) signal, which is obtained by temporally subsampling the reference to match the frame rate of the distorted video. In our experiments, we used the frame dropping technique in FFmpeg <ref type="bibr" target="#b39">[40]</ref> to achieve this. A similar subsampling scheme was also employed to obtain reduced frame rate videos in the LIVE-YT-HFR database. Note that when a distorted sequence has the same frame rate as the reference, P R is identical to the reference R. Similar to B R kpt and B D kpt , we calculate the band-pass response B P R kpt and its corresponding scaled entropy P R kpt . Given these, we define the Temporal-GREED (TGREED) index as:</p><formula xml:id="formula_14">TGREED kt = 1 P P p=1 1 + | D kpt ? P R kpt | R kpt + 1 P R kpt + 1 ? 1<label>(10)</label></formula><p>The expression in <ref type="formula" target="#formula_0">(10)</ref> can be interpreted by breaking down into two components: an absolute difference term and a ratio term. Absolute difference term removes the frame rate bias, while accounting for quality variations as if R and D had the same frame rate. The ratio term acts as a weighting function, where the weights only depend on the frame rates of R and D.</p><p>We can derive some very important properties from <ref type="bibr" target="#b9">(10)</ref>. First, when R and D the have same frame rate, the expression depends only on the absolute difference term since the ratio term reduces to 1. The added unit terms within the absolute values ensure that TGREED does not become zero when D = P R = R, which occurs when the distorted video is a temporally subsampled version of the reference. Note that TGREED = 0 only when D = P R = R. The added unit terms in the ratio ensure that indeterminate values of the ratio will not occur in regions having small entropy values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Measure</head><p>TGREED primarily addresses temporal artifacts by analyzing the statistics of temporal band-pass responses. Although TGREED is calculated in a spatial block based manner, it is mainly influenced by temporal filtering. In order to measure artifacts that arise only or primarily from spatial inconsistencies, we employ spatial band-pass filters applied on every frame of the reference and distorted sequences. To obtain the spatial band-pass responses, we use a simple local Mean Subtracted (MS) filtering scheme similar to <ref type="bibr" target="#b5">[6]</ref></p><formula xml:id="formula_15">. Let R M S t = R t ? ? R t and D M S t = D t ? ? D</formula><p>t be the reference and distorted MS coefficients at frame t, where the local mean is calculated as</p><formula xml:id="formula_16">? R t (i, j) = G g=?G H h=?H ? g,h R t (i + g, j + h), ? D t (i, j) = G g=?G H h=?H ? g,h D t (i + g, j + h),<label>(11)</label></formula><p>where ? = {? g,h |g = ?G, . . . G, h = ?H, . . . H} is a 2D circularly symmetric Gaussian weighting function sampled out to 3 standard deviations and rescaled to unit volume, and where R t and D t are single frames at time t. In our implementation, we use G = H = 7. Similar to the temporal case, the spatial band-pass coefficients R M S </p><formula xml:id="formula_17">R M S pt = R M S pt + Z R pt ,D M S pt = D M S pt + Z D pt<label>(12)</label></formula><p>where Calculate SGREED from <ref type="formula" target="#formula_0">(14)</ref> 5:</p><formula xml:id="formula_18">R M S pt is independent of Z R pt and R M S pt is independent of Z D pt and where Z R pt ? N (0, ? 2 Z I M ) and Z D pt ? N (0, ? 2 Z I M ).</formula><p>for each b k do 6:</p><p>Calculate TGREED k from <ref type="formula" target="#formula_0">(10)</ref> 7:</p><p>end for 8: end for 9: Concatenate SGREED, TGREED from all scales in <ref type="bibr" target="#b14">(15)</ref> to obtain GREED.</p><p>The spatial entropies h(R M S t ) and h(D M S t ) are calculated using the procedure detailed in subsection III-A, but replacing temporal band-pass responses with MS coefficients. Similarly, we define the scaling factors and modified entropies:</p><formula xml:id="formula_19">? R pt = log(1 + ? 2 (R M S pt )), ? D pt = log(1 + ? 2 (D M S pt )) ? R pt = ? R pt h(R M S pt ), ? D pt = ? D pt h(D M S pt ).<label>(13)</label></formula><p>Since spatial entropies are computed using only information from single frames, the obtained values are frame rate agnostic. Thus there does not arise any scale bias due to frame rate as was observed in the temporal case. We define the Spatial-GREED (SGREED) index as:</p><formula xml:id="formula_20">SGREED t = 1 P P p=1 |? D pt ? ? R pt |.<label>(14)</label></formula><p>D. Spatio-Temporal Measure</p><p>The expressions in <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_0">(14)</ref> calculate entropic differences at the frame level. We combine these frame level differences by average pooling over all temporal coordinates, to obtain video level differences:</p><formula xml:id="formula_21">TGREED k = 1 T T t=1 TGREED kt , SGREED = 1 T T t=1 SGREED t .</formula><p>The factors SGREED and TGREED operate individually on data obtained by separately processing spatial and temporal band-pass responses. Interestingly, while SGREED is obtained in a purely spatial manner, TGREED has both spatial and temporal information embedded in it (since the entropies are obtained in a spatial blockwise manner). Thus temporal artifacts such as judder/strobing only influence TGREED, while spatial artifacts affect both TGREED and SGREED. The combined spatio-temporal GREED index is obtained as a function of both SGREED and TGREED:</p><formula xml:id="formula_22">GREED = f (SGREED, TGREED),<label>(15)</label></formula><p>where f is a function which takes SGREED and TGREED as input features and predict quality scores. The mapping f may be learned from a suitable VQA database containing human quality judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Regression</head><p>We employed a Support Vector Regressor (SVR) <ref type="bibr" target="#b40">[41]</ref> that was trained on the LIVE-YT-HFR database, using Difference Mean Opinion Scores (DMOS) to obtain the mapping function f described in <ref type="bibr" target="#b14">(15)</ref>. The SVR is a well known modeling technique which is widely used in many prior IQA/VQA methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref> and is known for obtaining effective nonlinear mappings on high-dimensional data with high accuracy.</p><p>During training, SGREED and TGREED features were calculated on each video and then fed as input to SVR along with the corresponding DMOS labels. In our implementation, we used the LIBSVM <ref type="bibr" target="#b41">[42]</ref> package with radial basis function (RBF) kernel to train and test GREED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details</head><p>For simplicity, we implemented our method only in the luminance domain. To accomplish the temporal band-pass filtering, we experiment with 3 wavelet filters: Haar, Daubechies-2 (db2) and Biorthogonal-2.2 (bior2.2). We used wavelet packet (constant linear bandwidth) filter bank <ref type="bibr" target="#b42">[43]</ref> as we found it to be more effective than using constant octave bandwidth filters. The choice of linear bandwidth was also beneficial when analyzing the impacts of individual frequency bands on perceived quality. We employed 3 levels of wavelet decomposition for all wavelet filters b k , k ? {1, . . . 7} (ignoring the low pass response), where higher values of k correspond to filter with higher center frequencies. When calculating entropies we used spatial patches of size 5 ? 5 (i.e. ? M = 5). The neural noise variance was fixed at ? 2 W = ? 2 Z = 0.1 in <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_0">(12)</ref>, matching those employed in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b5">[6]</ref>.</p><p>In our experiments we found that our algorithm is most effective when the SGREED and TGREED features are calculated over multiple spatial resolutions. In particular, scales s = 4 and s = 5 were observed to provide superior performance, where the spatial resolution was downsampled 2 s times along both dimensions. The importance of using features from lower scales is likely attributable to the motion downshifting phenomenon, which posits that in the presence of motion, humans tend to be more sensitive to coarser scales than finer ones. Note that similar observations were made in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Downsampling delivers the additional advantage of significantly reducing the computational complexity. Since each scale results in an 8-dimensional feature vector (one value of SGREED, and TGREED values from each of seven subbands), employing two scales leads to a 16-dimensional vector as the input in <ref type="bibr" target="#b14">(15)</ref>.</p><p>Since reference and distorted sequences can have different frame rates, the reference entropy terms R kpt , ? R pt will generally have a different number of frames when compared to their counterpart distorted entropy terms D kpt , ? D pt . Thus we average reference entropy terms over all temporal indices: The above procedure is equivalent to dividing the entropy terms into subsequences of length F along the temporal dimension, and averaging each subsequence <ref type="bibr" target="#b1">[2]</ref>. This results in an equal number of entropy terms from the reference and distorted videos, which can then be used to calculate SGREED and TGREED in <ref type="formula" target="#formula_0">(14)</ref> and <ref type="bibr" target="#b9">(10)</ref>. The entire GREED flow is summarized in Algorithm 1.</p><formula xml:id="formula_23">R kpt ? 1 F F n=1 R kpt where F = F P Sref F P Sdist , t = (t ? 1)F + n ? R pt ? 1 F F n=1 ? R pt</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>We conduct a series of experiments to evaluate the performance of GREED. We will first describe the experimental settings, comparison methods and basic evaluation criteria. Then we explain how we evaluated GREED against existing state-of-the-art VQA models on the LIVE-YT-HFR database. We conduct a variety of ablation studies to analyze the significance and generalizabiliy of each conceptual feature present in GREED. Additionally, we demonstrate the generalizability of the GREED features by testing them on generic VQA databases. We also report the time complexity associated with GREED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Compared Methods. Since our proposed framework is an FR/RR model, we selected 4 popular and widely used FR-IQA methods: PSNR, SSIM <ref type="bibr" target="#b8">[9]</ref>, MS-SSIM <ref type="bibr" target="#b9">[10]</ref> and FSIM <ref type="bibr" target="#b10">[11]</ref> for comparison. Since these are IQA models, they do not take into account any temporal information. They were computed on every frame and averaged across all frames to obtain final video scores. In addition to the above IQA metrics, we also included 5 popular FR-VQA models: ST-RRED <ref type="bibr" target="#b3">[4]</ref>, SpEED <ref type="bibr" target="#b5">[6]</ref>, FRQM <ref type="bibr" target="#b26">[27]</ref>, VMAF 1 <ref type="bibr" target="#b4">[5]</ref> and deepVQA <ref type="bibr" target="#b21">[22]</ref>. When evaluating deepVQA, we only used stage-1 of the pretrained model (trained on the LIVE-VQA <ref type="bibr" target="#b43">[44]</ref> database) obtained from the code released by the authors. All of the above methods other than FRQM require the reference and corresponding distorted sequences to have the same frame rate. When there were differences in frame rates, we performed naive temporal upsampling by frame duplication to match the reference frame rate. Another way of matching the frame rates is to downsample the reference video, however we did not use this method since it often introduces temporal artifacts in the reference video, such as stutter/judder which is undesirable. We also ignored highly specific temporal upsampling methods (e.g. motion compensated temporal interpolation), as the performances of these can be highly susceptible to the type of content considered and to the choice of interpolation method.</p><p>Evaluation Criteria. We employed Spearman's rank order correlation coefficient (SROCC), Kendall's rank order correlation coefficient (KROCC), Pearson's linear correlation coefficient (PLCC), and root mean squared error (RMSE) to evaluate the VQA models. All of the above metrics are calculated using DMOS values of the VQA dataset as ground truth. Before computing PLCC and RMSE, the predicted scores were passed through a four-parameter logistic nonlinearity as described in <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_24">Q(x) = ? 2 + ? 1 ? ? 2 1 + exp ? x??3 |?4| .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Correlation Against Human Judgments</head><p>Since GREED requires training on the quality prediction problem, we randomly divide the LIVE-YT-HFR dataset into 70%,15%, and 15% subsets corresponding to training, validation and test sets, respectively. We observed that this choice of train-validation-test splits yielded stable model learning without overfitting/underfitting the training data. We also ensured that there was never any overlap between contents in each set. Since LIVE-YT-HFR comprises 16 contents with 30 videos per content, this implies splits of about 300/90/90 videos in each set. The validation set was used to determine the hyperparameters of the SVR using grid search. We repeated this random train-test sequence over 200 times, and report the median performance.</p><p>We compared the performance of GREED against other FR models in <ref type="table" target="#tab_2">Table I</ref>. It may be observed from the Table that the family of GREED based models significantly outperformed the compared VQA models by a large margin, with GREED-bior2.2 achieving top performance. <ref type="figure" target="#fig_5">Fig. 3</ref> shows the spreads of  SROCC values for each FR model over 200 iterations. The plot indicates that GREED-bior2.2 has a much tighter confidence interval than the other indices, highlighting the robustness of the algorithm. In order to individually analyze the performance of GREED against each frame rate we divided the LIVE-YT-HFR database into sets containing videos having the same frame rates. The SROCC and PLCC performance comparison is shown in <ref type="table" target="#tab_2">Table II</ref>. The KROCC and RMSE were observed to follow the similar trends as in <ref type="table" target="#tab_2">Table II</ref>. Here as well, the GREED-bior2.2 was among the top two performing models at every frame rate. Note that FRQM requires compared videos to have different frame rates, thus for 120 fps videos correlation values are not reported in <ref type="table" target="#tab_2">Table II</ref>. We also observed an interesting trend among GREED models whereby the correlation values roughly follow a monotonically increasing behavior with increasing frame rate values. This behavior is expected and desirable, since higher frame rate videos offer more information about the distorted video, thus resulting in better quality judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Significance of Spatial and Temporal Measures</head><p>We conduct an ablation study to evaluate the importance of SGREED and TGREED when employed in isolation. In this experiment we separately trained SGREED and TGREED, and the performance values reported in <ref type="table" target="#tab_2">Table III</ref>. We can infer from the Table that SGREED and TGREED capture complementary perceptual information, since SGREED and TGREED yield lower correlation when used separately, while the combined GREED model obtained much higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contribution of Temporal Subbands</head><p>To obtain TGREED values, we employed a bank of temporal filters as described by <ref type="bibr" target="#b0">(1)</ref>. In our implementation we used a 3-level wavelet decompositions resulting in 7 filters (a low pass filter is not used). In this experiment, we investigated the contribution of each subband by training and testing them individually. The results are shown in <ref type="figure" target="#fig_6">Fig. 4</ref> where SROCC is plotted against the center frequencies (normalized to [0, ?] range) for each subband. From the plots we can infer that the middle frequency regions tended to have higher individual correlation as compared to other parts of the frequency spectrum. This behavior can be explained in terms of the temporal contrast sensitivity function (CSF) <ref type="bibr" target="#b45">[46]</ref> of human vision, according to which sensitivity to the visual signal is band-pass, resulting in reduced sensitivity to lower and higher frequencies. Note that the correlation values plotted along the y-axis in <ref type="figure" target="#fig_6">Fig. 4</ref> are sensitive to the presence of temporal frequencies present in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance Analysis on BVI-HFR Database</head><p>BVI-HFR <ref type="bibr" target="#b1">[2]</ref> is another HFR-VQA database available in the public domain, consisting of 22 source sequences spanning 4 frame rates: 15, 30, 60 and 120 fps. BVI-HFR and LIVE-YT-HFR differ in the downsampling scheme employed to obtain lower frame rate videos: the creators of BVI-HFR used temporal frame averaging, while in LIVE-YT-HFR, frame dropping was used. The choice of downsampling method can have a significant impact on the quality of lower frame rate videos: temporal frame averaging often introduces motion blur, while frame dropping may result in judder/strobing artifacts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIVE-VQA</head><p>LIVE-mobile CSIQ-VQA PSNR 0.711 0.788 0.601 SSIM <ref type="bibr" target="#b8">[9]</ref> 0.802 0.798 0.705 MS-SSIM <ref type="bibr" target="#b9">[10]</ref> 0.830 0.800 0.757 FSIM <ref type="bibr" target="#b10">[11]</ref> 0.806 0.868 0.752 ST-RRED <ref type="bibr" target="#b3">[4]</ref> 0.826 0.882 0.813 SpEED <ref type="bibr" target="#b5">[6]</ref> 0.801 0.886 0.743 VMAF <ref type="bibr" target="#b4">[5]</ref> 0.794 0.897 0.618 deepVQA <ref type="bibr" target="#b21">[22]</ref> -0.826 0.702 GREED-Haar 0.771 0.875 0.785 GREED-db2 0.735 0.850 0.786 GREED-bior2.2 0.750 0.863 0.780</p><p>In this experiment we investigated the performance of GREED on BVI-HFR, specifically its sensitivity to detect motion blur artifacts. Note that the BVI-HFR Database is primarily focused towards frame rate distortions like motion blur, and does not include other impairments such as compression, white noise etc. We again randomly split the dataset into 70%, 15%, and 15% subsets for training, validation and testing respectively, and while ensuring no overlap between the contents across these sets. The above procedure was repeated 200 times and the median SROCC performance computed and compared in <ref type="table">Table V</ref>. Here, the FRQM index achieved the highest correlation against subjective judgments, while GREED-bior2.2 was second best among the compared FR-VQA models. This suggests that GREED is not very sensitive to the choice of downsampling scheme used when obtaining lower frame rate sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance Comparison on other VQA databases</head><p>We further investigated the generalizability of the GREED features by evaluating them on three popular VQA databases: LIVE-VQA <ref type="bibr" target="#b43">[44]</ref>, LIVE-mobile <ref type="bibr" target="#b13">[14]</ref> and CSIQ-VQA <ref type="bibr" target="#b46">[47]</ref>. These databases contain videos of the same frame rate for both reference and distorted sequences, thus the TGREED term in (10) will only depend on the absolute difference term, since the ratio term reduces to unity. On each database, we divided the contents into non-overlapping 80% and 20% subsets for training and testing, respectively. Further, this procedure was repeated for all possible train-test combinations, and median SROCC performance reported in <ref type="table" target="#tab_2">Table VI</ref>. From the <ref type="table">Table,</ref> we observe that GREED models achieved comparable performance to state-of-the-art (SOTA) VQA methods. This also indicates the efficacy of the features employed by GREED is not restricted to HFR content, and generalizes well across other types of artifacts which may arise in non-HFR streaming and social media scenarios.</p><p>To analyze the dependence of GREED on training data, we also performed cross dataset evaluation, whereby we trained on one database and used the remaining datasets for testing. The results of this experiment are given in <ref type="table" target="#tab_2">Table VII</ref> where the correlation remains nearly unchanged regardless of the training data employed, highlighting the robustness of the features used in GREED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Time Complexity</head><p>In  V. HFR-VMAF VMAF video quality prediction framework has demonstrated high prediction performance when the reference and distorted videos have the same frame rate. Because of this, VMAF has been widely employed by Netflix to control the quality of its streaming content. Given that VMAF achieves competitive performance when the videos being compared have the same frame rates, we attempted to leverage this usefulness by combining GREED and VMAF predictions. Specifically, we introduce a variant of VMAF that we dub as HFR-VMAF, defined as the average of VMAF (here we use 100 ? VMAF since we require the score of pristine video to be at zero) and GREED predictions:</p><formula xml:id="formula_25">HFR-VMAF(R, D) = 1 2 (VMAF(P R, D) + GREED(R, D))<label>(17)</label></formula><p>Note that VMAF is computed between the distorted video D and subsampled reference P R sequences, unlike the temporally upsampled distorted video D used in <ref type="table" target="#tab_2">Tables I and  II</ref>. Although subsampling the reference video can result in temporal artifacts like judder, strobing etc., we empirically observed that when combined with GREED, these artifacts tended to have negligible effect on the quality predictions. Moreover, we observed that the quality predictions produced by HFR-VMAF to be highly effective, particularly when sets of videos having fixed frame rates were compared. This is illustrated in <ref type="table" target="#tab_2">Table VIII</ref>, where we observed considerable improvement in correlation values on individual frame rates. Yet, although there is a performance boost when fixed frame rates were considered, there was also considerable correlation degradation when entire database was included, indicating that HFR-VMAF is less efficient at differentiating the perceptual quality of videos over multiple frame rates. This suggests that HFR-VMAF could be beneficial when videos of the same frame rates are compared, while GREED is more suitable when videos across different frame rates are compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>We proposed a new model that accurately predicts frame rate dependent video quality based on measurement of bandpass video statistics. A distinguishing element of the new GREED model is that it can be used to measure video quality when reference and distorted sequences have differing frame rates, with no requirement of any temporal pre-processing. We conducted a comprehensive and holistic evaluation of GREED against human judgments of video quality and found that GREED delivers more accurate and robust predictions of quality than other VQA models on variable frame rate videos. We conducted ablation studies to analyze the significance of the spatial and temporal components of GREED, and demonstrated their complementary nature in capturing relevant perceptual information. We evaluated the generalizability of GREED features on multiple fixed frame rate VQA databases and observed comparable performance to SOTA VQA models. We also proposed HFR-VMAF, an extension of VMAF to HFR videos incorporating the advantages of both the GREED and VMAF methods. HFR-VMAF was observed to enhance prediction performance when videos of fixed frame rates were analyzed. A software release of GREED has been made available online 2 .</p><p>Although GREED achieves high correlations against perceptual judgments, we observed a shortcoming that could affect prediction performance. The differences between the frame rates of reference and distorted videos can influence performance, as observed in <ref type="table" target="#tab_2">Table II</ref> where lower frame rate videos led to worse performance than higher frame rate videos. Although HFR-VMAF addresses this concern, it results in performance degradations when videos across different frame rates are considered. A more careful design addressing the above drawback would be beneficial towards understanding frame rate influences on HFR video quality, and in creating further improved models.</p><p>Neil Birkbeck obtained his Ph.D from the University of Alberta in 2011 working on topics in computer vision, graphics and robotics, with a specific focus on image-based modeling and rendering. He went on to become a Research Scientist at Siemens corporate research working on automatic detection and segmentation of anatomical structures in full body medical images. He is now a software engineer in the Media Algorithms team at YouTube/Google, with research interests in perceptual video processing, video coding, and video quality assessment. Research Scientist at Citrix Online, and from 2011-2016, he was Sr. Manager Advanced Technology at GoPro, at both places developing algorithms for images/video quality enhancement, compression, capture, and streaming. He is an active member of IEEE (and MMSP TC), ACM, SPIE, and VES, and has co-authored more than 120 papers and patents. His fields of research include image/video processing, machine vision, video compression, spherical capture, VR/AR, visual effects, and related areas.</p><p>Alan C. Bovik (F '95) is the Cockrell Family Regents Endowed Chair Professor at The University of Texas at Austin. His research interests include image processing, digital photography, digital television, digital streaming video, social media, and visual perception. For his work in these areas he has been the recipient of the 2019 Progress Medal from The Royal Photographic Society, the 2019 IEEE Fourier Award, the 2017 Edwin H. Land Medal from The Optical Society, a 2015 Primetime Emmy Award for Outstanding Achievement in Engineering Development from the Television Academy, a 2020 Technology and Engineering Emmy Award from the National Academy for Television Arts and Sciences, and the Norbert Wiener Society Award and the Karl Friedrich Gauss Education Award from the IEEE Signal Processing Society. He has also received about 10 'best journal paper' awards, including the 2016 IEEE Signal Processing Society Sustained Impact Award. His books include The Essential Guides to Image and Video Processing. He co-founded and was longestserving Editor-in-Chief of the IEEE Transactions on Image Processing, and also created/Chaired the IEEE International Conference on Image Processing which was first held in Austin, Texas, 1994.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and A. C. Bovik are with the Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA (e-mail: pavancm@utexas.edu; bovik@ece.utexas.edu). Neil Birkbeck, Yilin Wang and Balu Adsumilli are with Google Inc. (e-mail: birk-beck@google.com; yilin@google.com; badsumilli@google.com).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>80</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of frame rate bias of entropy for bouncyball sequence from the LIVE-YT-HFR database. Note that in the above plot, only compressed versions are included, while lossless (CRF = 0) videos are not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>GGD law. Again, we divide each frame into non-overlapping patches of size ? M ? ? M , indexed by p ? {1, 2, . . . P }. The channel imperfections can be similarly modeled as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Boxplot of SROCC distributions of the compared VQA algorithms in Table I over 200 trials on the LIVE-YT-HFR Database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Contribution analysis of each subband present in GREED. Horizontal axis represents center frequencies of each subband normalized to [0, ?] range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Yilin</head><label></label><figDesc>Wang received B.S. and M.S. degrees in Computer Science from Nanjing University, China, in 2005 and 2008 respectively, PhD degree in Computer Science from the University of North Carolina at Chapel Hill in 2014, working on topics in computer vision and image processing. After graduation, he joined the Media Algorithm team in Youtube/Google. His research fields include video processing infrastructure, video quality assessment, and video compression. Balu Adsumilli manages and leads the Media Algorithms group at YouTube/Google. He did his masters in University of Wisconsin Madison in 2002, and his PhD at University of California Santa Barbara in 2005, on watermark-based error resilience in video communications. From 2005 to 2011, he was Sr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Probability Distributions of Bandpass CoefficientsProbability Distributions of Normalized Bandpass Coefficients</head><label></label><figDesc></figDesc><table><row><cell>0.025 0.030 0.035</cell><cell></cell><cell>24 fps 30 fps 60 fps 82 fps 98 fps 120 fps</cell><cell>0.5 0.6 0.7</cell><cell></cell><cell>24 fps 30 fps 60 fps 82 fps 98 fps 120 fps</cell><cell></cell></row><row><cell>0.020</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.015</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.010</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.005</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.000</cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>60</cell><cell>40 Bandpass Luminance Coefficients 20 0 20 40</cell><cell>60</cell><cell>80</cell><cell>4</cell><cell>3 Normalized Bandpass Luminance Coefficients 2 1 0 1 2 3</cell><cell>4</cell></row><row><cell cols="3">(a) Histograms of band-pass video coefficients</cell><cell></cell><cell cols="2">(b) Histograms of normalized bandpass video coefficients</cell><cell></cell></row><row><cell cols="6">Fig. 1. Comparisons of distributions with and without divisive normalization for bobblehead sequence from LIVE-YT-HFR database.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Generalized Entropic Difference (GREED) Input: reference video R, distorted video D Output: GREED score 1: Scale S = {4, 5}, band-pass filter bank b k : k ? {1, . . . 7} 2: Temporal subsample R to get P R 3: for each s ? S do</figDesc><table /><note>4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON OF GREED AGAINST DIFFERENT FR ALGORITHMS ON THE LIVE-YT-HFR DATABASE. IN EACH COLUMN, THE FIRST AND SECOND BEST MODELS ARE BOLDFACED.</figDesc><table><row><cell></cell><cell>SROCC ?</cell><cell>KROCC ?</cell><cell>PLCC ?</cell><cell>RMSE ?</cell></row><row><cell>PSNR</cell><cell>0.7802</cell><cell>0.5934</cell><cell>0.7481</cell><cell>7.75</cell></row><row><cell>SSIM [9]</cell><cell>0.5566</cell><cell>0.4042</cell><cell>0.5418</cell><cell>9.99</cell></row><row><cell>MS-SSIM [10]</cell><cell>0.5742</cell><cell>0.4135</cell><cell>0.5512</cell><cell>10.01</cell></row><row><cell>FSIM [11]</cell><cell>0.6528</cell><cell>0.4881</cell><cell>0.6332</cell><cell>9.34</cell></row><row><cell>ST-RRED [4]</cell><cell>0.6394</cell><cell>0.4516</cell><cell>0.6073</cell><cell>9.58</cell></row><row><cell>SpEED [6]</cell><cell>0.6051</cell><cell>0.4437</cell><cell>0.5206</cell><cell>10.28</cell></row><row><cell>FRQM [27]</cell><cell>0.5133</cell><cell>0.3701</cell><cell>0.5017</cell><cell>10.38</cell></row><row><cell>VMAF [5]</cell><cell>0.7782</cell><cell>0.5918</cell><cell>0.7419</cell><cell>8.10</cell></row><row><cell>deepVQA [22]</cell><cell>0.4331</cell><cell>0.3082</cell><cell>0.3996</cell><cell>10.87</cell></row><row><cell>GREED-Haar</cell><cell>0.8305</cell><cell>0.6389</cell><cell>0.8467</cell><cell>6.22</cell></row><row><cell>GREED-db2</cell><cell>0.8347</cell><cell>0.6447</cell><cell>0.8478</cell><cell>6.21</cell></row><row><cell>GREED-bior2.2</cell><cell>0.8822</cell><cell>0.7046</cell><cell>0.8869</cell><cell>5.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON OF GREED AGAINST VARIOUS FR METHODS FOR INDIVIDUAL FRAME RATES ON THE LIVE-YT-HFR DATABASE.</figDesc><table><row><cell></cell><cell cols="2">24 fps</cell><cell cols="2">30 fps</cell><cell cols="2">60 fps</cell><cell cols="2">82 fps</cell><cell cols="2">98 fps</cell><cell cols="2">120 fps</cell><cell cols="2">Overall</cell></row><row><cell></cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell><cell>SROCC?</cell><cell>PLCC?</cell></row><row><cell>PSNR</cell><cell>0.5411</cell><cell>0.4873</cell><cell>0.5643</cell><cell>0.5352</cell><cell>0.7536</cell><cell>0.6945</cell><cell>0.7714</cell><cell>0.7714</cell><cell>0.8214</cell><cell>0.7834</cell><cell>0.7413</cell><cell>0.7364</cell><cell>0.7802</cell><cell>0.7481</cell></row><row><cell>SSIM [9]</cell><cell>0.2661</cell><cell>0.2228</cell><cell>0.2839</cell><cell>0.1892</cell><cell>0.3821</cell><cell>0.3027</cell><cell>0.3714</cell><cell>0.3621</cell><cell>0.5375</cell><cell>0.4975</cell><cell>0.8671</cell><cell>0.8337</cell><cell>0.5566</cell><cell>0.5418</cell></row><row><cell>MS-SSIM [10]</cell><cell>0.3054</cell><cell>0.2608</cell><cell>0.2964</cell><cell>0.2383</cell><cell>0.4161</cell><cell>0.3382</cell><cell>0.4393</cell><cell>0.3938</cell><cell>0.5786</cell><cell>0.5619</cell><cell>0.7063</cell><cell>0.6963</cell><cell>0.5742</cell><cell>0.5512</cell></row><row><cell>FSIM [11]</cell><cell>0.3107</cell><cell>0.3148</cell><cell>0.3161</cell><cell>0.3279</cell><cell>0.5857</cell><cell>0.5083</cell><cell>0.4571</cell><cell>0.4796</cell><cell>0.6839</cell><cell>0.6774</cell><cell>0.7622</cell><cell>0.7062</cell><cell>0.6528</cell><cell>0.6332</cell></row><row><cell>ST-RRED [4]</cell><cell>0.3054</cell><cell>0.2756</cell><cell>0.2964</cell><cell>0.2066</cell><cell>0.6125</cell><cell>0.6136</cell><cell>0.5839</cell><cell>0.5130</cell><cell>0.6500</cell><cell>0.6042</cell><cell>0.7552</cell><cell>0.6966</cell><cell>0.6394</cell><cell>0.6073</cell></row><row><cell>SpEED [6]</cell><cell>0.4321</cell><cell>0.2729</cell><cell>0.4107</cell><cell>0.2332</cell><cell>0.4393</cell><cell>0.2927</cell><cell>0.5464</cell><cell>0.3901</cell><cell>0.5786</cell><cell>0.4713</cell><cell>0.7587</cell><cell>0.7393</cell><cell>0.6051</cell><cell>0.5206</cell></row><row><cell>FRQM [27]</cell><cell>0.2893</cell><cell>0.1536</cell><cell>0.3196</cell><cell>0.1851</cell><cell>0.2109</cell><cell>0.0999</cell><cell>0.2857</cell><cell>0.0780</cell><cell>0.2750</cell><cell>0.0953</cell><cell>-</cell><cell>-</cell><cell>0.5133</cell><cell>0.5017</cell></row><row><cell>VMAF [5]</cell><cell>0.2500</cell><cell>0.3685</cell><cell>0.3625</cell><cell>0.4716</cell><cell>0.6304</cell><cell>0.6806</cell><cell>0.7339</cell><cell>0.7928</cell><cell>0.8607</cell><cell>0.8685</cell><cell>0.8182</cell><cell>0.8166</cell><cell>0.7782</cell><cell>0.7419</cell></row><row><cell>deepVQA [22]</cell><cell>0.2732</cell><cell>0.1576</cell><cell>0.2893</cell><cell>0.1428</cell><cell>0.4036</cell><cell>0.2071</cell><cell>0.2929</cell><cell>0.2401</cell><cell>0.4107</cell><cell>0.3753</cell><cell>0.7622</cell><cell>0.6702</cell><cell>0.4331</cell><cell>0.3996</cell></row><row><cell>GREED-Haar</cell><cell>0.6196</cell><cell>0.6917</cell><cell>0.5482</cell><cell>0.7646</cell><cell>0.7125</cell><cell>0.8092</cell><cell>0.7464</cell><cell>0.8546</cell><cell>0.8054</cell><cell>0.8491</cell><cell>0.8112</cell><cell>0.8235</cell><cell>0.8305</cell><cell>0.8467</cell></row><row><cell>GREED-db2</cell><cell>0.6696</cell><cell>0.7396</cell><cell>0.6179</cell><cell>0.7983</cell><cell>0.6982</cell><cell>0.8244</cell><cell>0.7250</cell><cell>0.8671</cell><cell>0.7518</cell><cell>0.8449</cell><cell>0.8322</cell><cell>0.8632</cell><cell>0.8347</cell><cell>0.8478</cell></row><row><cell>GREED-bior2.2</cell><cell>0.7268</cell><cell>0.8221</cell><cell>0.7018</cell><cell>0.8431</cell><cell>0.7321</cell><cell>0.8405</cell><cell>0.8179</cell><cell>0.8960</cell><cell>0.8643</cell><cell>0.8915</cell><cell>0.8881</cell><cell>0.8952</cell><cell>0.8822</cell><cell>0.8869</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF SPATIAL AND TEMPORAL MEASURES WHEN EVALUATED IN ISOLATION ON THE LIVE-YT-HFR DATABASE.</figDesc><table><row><cell></cell><cell>SROCC ?</cell><cell>KROCC ?</cell><cell>PLCC ?</cell><cell>RMSE ?</cell></row><row><cell>SGREED</cell><cell>0.7233</cell><cell>0.5269</cell><cell>0.6888</cell><cell>8.51</cell></row><row><cell>TGREED-Haar</cell><cell>0.6637</cell><cell>0.4881</cell><cell>0.6907</cell><cell>8.60</cell></row><row><cell>TGREED-db2</cell><cell>0.5543</cell><cell>0.3988</cell><cell>0.6121</cell><cell>9.49</cell></row><row><cell>TGREED-bior2.2</cell><cell>0.5849</cell><cell>0.4165</cell><cell>0.6180</cell><cell>9.41</cell></row><row><cell>GREED-Haar</cell><cell>0.8305</cell><cell>0.6389</cell><cell>0.8467</cell><cell>6.22</cell></row><row><cell>GREED-db2</cell><cell>0.8347</cell><cell>0.6447</cell><cell>0.8478</cell><cell>6.21</cell></row><row><cell>GREED-bior2.2</cell><cell>0.8822</cell><cell>0.7046</cell><cell>0.8869</cell><cell>5.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF COMPUTE TIMES OF VARIOUS VQA MODELS ON 100 FRAMES OF 1920 ? 1080 RESOLUTION VIDEO.</figDesc><table><row><cell cols="2">PSNR</cell><cell>SSIM</cell><cell>MS-SSIM</cell><cell>FSIM</cell><cell cols="2">ST-RRED</cell><cell>SpEED</cell><cell>FRQM</cell><cell>deepVQA</cell><cell>VMAF</cell><cell>GREED-Haar</cell><cell>GREED-db2</cell><cell>GREED-bior2.2</cell></row><row><cell>Time (sec.)</cell><cell>1.51</cell><cell>17.94</cell><cell>23.79</cell><cell>29.60</cell><cell></cell><cell>777.85</cell><cell>15.40</cell><cell>6.71</cell><cell>44.94</cell><cell>35.60</cell><cell>7.75</cell><cell>7.83</cell><cell>8.17</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE COMPARISON OF DIFFERENT FR ALGORITHMS ON THE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">BVI-HFR DATABASE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>SROCC ?</cell><cell>KROCC ?</cell><cell cols="2">PLCC ?</cell><cell>RMSE ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR</cell><cell></cell><cell>0.2552</cell><cell>0.1818</cell><cell cols="2">0.3155</cell><cell>17.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSIM [9]</cell><cell></cell><cell>0.1958</cell><cell>0.1515</cell><cell cols="2">0.3532</cell><cell>16.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MS-SSIM [10]</cell><cell>0.2063</cell><cell>0.1515</cell><cell cols="2">0.3583</cell><cell>16.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSIM [11]</cell><cell></cell><cell>0.1888</cell><cell>0.1212</cell><cell cols="2">0.3448</cell><cell>17.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ST-RRED [4]</cell><cell>0.2028</cell><cell>0.1515</cell><cell cols="2">0.1699</cell><cell>17.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpEED [6]</cell><cell></cell><cell>0.2657</cell><cell>0.1818</cell><cell cols="2">0.2304</cell><cell>18.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FRQM [27]</cell><cell></cell><cell>0.9021</cell><cell>0.7576</cell><cell cols="2">0.9394</cell><cell>6.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VMAF [5]</cell><cell></cell><cell>0.1888</cell><cell>0.1212</cell><cell cols="2">0.3703</cell><cell>16.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">deepVQA [22]</cell><cell>0.1469</cell><cell>0.1212</cell><cell cols="2">0.2013</cell><cell>17.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GREED-Haar</cell><cell>0.7762</cell><cell>0.5455</cell><cell cols="2">0.8225</cell><cell>10.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GREED-db2</cell><cell>0.7133</cell><cell>0.5152</cell><cell>0.772</cell><cell></cell><cell>11.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GREED-bior2.2</cell><cell>0.8042</cell><cell>0.6061</cell><cell cols="2">0.8312</cell><cell>10.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">SROCC PERFORMANCE COMPARISON ON MULTIPLE VQA DATABASES.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">THE REPORTED NUMBERS ARE MEDIAN VALUES FROM EVERY POSSIBLE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMBINATION OF TRAIN-TEST SPLITS WITH 80% OF CONTENT USED FOR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">TRAINING.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII CROSS</head><label>VII</label><figDesc>DATABASE SROCC PERFORMANCE COMPARISON OF GREED. THE HIGHEST VALUE IN EACH COLUMN IS BOLDFACED.</figDesc><table><row><cell>Test</cell><cell>LIVE-</cell><cell>LIVE</cell><cell>LIVE</cell><cell>CSIQ</cell></row><row><cell>Train</cell><cell>YT-HFR</cell><cell>VQA</cell><cell>mobile</cell><cell>VQA</cell></row><row><cell>LIVE-YT-HFR</cell><cell>-</cell><cell>0.705</cell><cell>0.804</cell><cell>0.626</cell></row><row><cell>LIVE-VQA</cell><cell>0.705</cell><cell>-</cell><cell>0.801</cell><cell>0.622</cell></row><row><cell>LIVE-mobile</cell><cell>0.658</cell><cell>0.692</cell><cell>-</cell><cell>0.636</cell></row><row><cell>CSIQ-VQA</cell><cell>0.719</cell><cell>0.682</cell><cell>0.797</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(a) GREED-Haar</cell><cell></cell><cell></cell></row><row><cell>Test</cell><cell>LIVE-</cell><cell>LIVE</cell><cell>LIVE</cell><cell>CSIQ</cell></row><row><cell>Train</cell><cell>YT-HFR</cell><cell>VQA</cell><cell>mobile</cell><cell>VQA</cell></row><row><cell>LIVE-YT-HFR</cell><cell>-</cell><cell>0.685</cell><cell>0.807</cell><cell>0.634</cell></row><row><cell>LIVE-VQA</cell><cell>0.705</cell><cell>-</cell><cell>0.807</cell><cell>0.621</cell></row><row><cell>LIVE-mobile</cell><cell>0.660</cell><cell>0.664</cell><cell>-</cell><cell>0.617</cell></row><row><cell>CSIQ-VQA</cell><cell>0.721</cell><cell>0.678</cell><cell>0.820</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(b) GREED-db2</cell><cell></cell><cell></cell></row><row><cell>Test</cell><cell>LIVE-</cell><cell>LIVE</cell><cell>LIVE</cell><cell>CSIQ</cell></row><row><cell>Train</cell><cell>YT-HFR</cell><cell>VQA</cell><cell>mobile</cell><cell>VQA</cell></row><row><cell>LIVE-YT-HFR</cell><cell>-</cell><cell>0.697</cell><cell>0.798</cell><cell>0.616</cell></row><row><cell>LIVE-VQA</cell><cell>0.707</cell><cell>-</cell><cell>0.825</cell><cell>0.600</cell></row><row><cell>LIVE-mobile</cell><cell>0.678</cell><cell>0.599</cell><cell>-</cell><cell>0.418</cell></row><row><cell>CSIQ-VQA</cell><cell>0.738</cell><cell>0.716</cell><cell>0.837</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(c) GREED-bior2.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc>IV we compare the compute times (in seconds) of various VQA models on 100 frames of video having 1920 ? 1080 resolution. The compute times were calculated on a Xeon E5 2620 v4 2.1 GHz CPU with 64 GB RAM. As compared to other VQA models, GREED has relatively low</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII SROCC</head><label>VIII</label><figDesc>PERFORMANCE COMPARISON OF HFR-VMAF FOR INDIVIDUAL FRAME RATES ON THE LIVE-YT-HFR DATABASE. IN EACH COLUMN THE BEST VALUE IS MARKED IN BOLDFACE.</figDesc><table><row><cell></cell><cell>24 fps</cell><cell>30 fps</cell><cell>60 fps</cell><cell>82 fps</cell><cell>98 fps</cell><cell>120 fps</cell><cell>Overall</cell></row><row><cell>VMAF [5]</cell><cell>0.2500</cell><cell>0.3625</cell><cell>0.6304</cell><cell>0.7339</cell><cell>0.8607</cell><cell>0.8182</cell><cell>0.7782</cell></row><row><cell>GREED-Haar</cell><cell>0.6196</cell><cell>0.5482</cell><cell>0.7125</cell><cell>0.7464</cell><cell>0.8054</cell><cell>0.8112</cell><cell>0.8305</cell></row><row><cell>GREED-db2</cell><cell>0.6696</cell><cell>0.6179</cell><cell>0.6982</cell><cell>0.7250</cell><cell>0.7518</cell><cell>0.8322</cell><cell>0.8347</cell></row><row><cell>GREED-bior2.2</cell><cell>0.7268</cell><cell>0.7018</cell><cell>0.7321</cell><cell>0.8179</cell><cell>0.8643</cell><cell>0.8881</cell><cell>0.8822</cell></row><row><cell>HFR-VMAF-Haar</cell><cell>0.6946</cell><cell>0.6357</cell><cell>0.7857</cell><cell>0.8161</cell><cell>0.8571</cell><cell>0.8392</cell><cell>0.7608</cell></row><row><cell>HFR-VMAF-db2</cell><cell>0.7107</cell><cell>0.6804</cell><cell>0.7929</cell><cell>0.8286</cell><cell>0.8536</cell><cell>0.8601</cell><cell>0.7773</cell></row><row><cell>HFR-VMAF-bior2.2</cell><cell>0.7714</cell><cell>0.7429</cell><cell>0.7893</cell><cell>0.8536</cell><cell>0.8821</cell><cell>0.8881</cell><cell>0.8160</cell></row><row><cell cols="4">complexity, as indicated by the lower time required for feature</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">computation. The computational complexity of GREED is</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">mainly dependent on the evaluation of TGREED features,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">since TGREED contributes 14 of the 16 features employed</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in GREED.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We used the pretrained VMAF model available at https://github.com/ Netflix/vmaf</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/pavancm/GREED</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of high frame rate video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A study of high frame rate video formats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1499" to="1512" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subjective and objective quality assessment of high frame rate videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video quality assessment by reduced reference spatio-temporal entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="694" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Toward a practical perceptual video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manohara</surname></persName>
		</author>
		<ptr target="http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SpEED-QA: Spatial efficient entropic differencing for image and video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1333" to="1337" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatiotemporal feature integration and model fusion for full reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2256" to="2270" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Objective video quality assessment methods: A classification, review, and performance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reisslein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Broadcast</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conf. Signals Syst</title>
		<imprint>
			<date type="published" when="2003-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new standardized method for objectively measuring video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Broadcast</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal video quality model accounting for variable frame delay distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Broadcast</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="649" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video quality assessment on mobile devices: Subjective, behavioral and objective studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">De</forename><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="652" to="671" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A structural similarity metric for video based on motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">869</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A spatiotemporal mostapparent-distortion model for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process</title>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="2505" to="2508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Most apparent distortion: fullreference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11006</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention driven foveated video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="200" to="213" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A full reference video quality measure based on motion differences and saliency maps evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ortiz-Jaramillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Platisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. on Comp. Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2014-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An optical flow-based full reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2480" to="2492" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vision</title>
		<meeting>European Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural network model of spatial distortion sensitivity for video quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">C3DVQA: Full-reference video quality assessment with 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Acoustics, Speech and Signal Process</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4447" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual aliasing factors and the impact of frame rate on video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3475" to="3479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal motion smoothness and the impact of frame rate variation on video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A frame rate dependent video quality metric based on temporal wavelet decomposition and spatiotemporal pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Process</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="300" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Capturing video frame rate variations via entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Process. Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1809" to="1813" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RRED indices: Reduced reference entropic differencing for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="517" to="526" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Network: computation in neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="517" to="548" />
		</imprint>
	</monogr>
	<note>The statistics of natural images</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Process. Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wavelet-based texture retrieval using generalized gaussian density and Kullback-Leibler distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the sum of generalized gaussian random signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Signal Process</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">New results on the sum of two generalized Gaussian random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Alouini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conf. on Signal and Information Process</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exposing image splicing with inconsistent local noise variances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Encoding for streaming sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffmpeg</surname></persName>
		</author>
		<ptr target="https://trac.ffmpeg.org/wiki" />
		<imprint>
			<date type="published" when="2019-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">New support vector algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1245" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Entropy-based algorithms for best basis selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Wickerhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="713" to="718" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective quality metrics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatial and temporal contrast-sensitivity functions of the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1141" to="1142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vis3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13016</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">He is currently pursuing the Ph.D. degree in Electrical and Computer engineering with The University of Texas at Austin, USA. His research interests include image and video signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016, and the M.Tech. (Research) degree in Electrical and Communication Engineering from the Indian Institute of Science (IISc)</title>
		<meeting><address><addrLine>Surathkal, India; Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Madhusudana received the B.Tech. degree in Electronics and Communication Engineering from The National Institute of Technology Karnataka (NITK)</orgName>
		</respStmt>
	</monogr>
	<note>and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
