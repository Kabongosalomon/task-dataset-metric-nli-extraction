<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holistic 3D Scene Understanding from a Single Image with Implicit Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>3 Google 4 ETH</addrLine>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD &amp; CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>3 Google 4 ETH</addrLine>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<addrLine>3 Google 4 ETH</addrLine>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Holistic 3D Scene Understanding from a Single Image with Implicit Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shapes, object poses, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine the 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-theart methods in terms of object shape, scene layout estimation, and 3D object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D indoor scene understanding is a long-lasting computer vision problem and has tremendous impact on several applications, e.g., robotics, virtual reality. Given a single color image, the goal is to reconstruct the room layout as well as each individual object and estimate its semantic type in the 3D space. Over decades, there are plenty of works consistently improving the performance of such a task over two focal points of the competition. One is the 3D shape representation preserving fine-grained geometry details, evolving from the 3D bounding box, 3D volume, point cloud, to the recent triangulation mesh. The other is the joint inference of multiple objects and layout in the scene leveraging contextual information, such as cooccurring or relative locations among objects of multiple * Equal contribution ? Corresponding author Project webpage: https://chengzhag.github.io/publication/im3d/ categories. However, the cluttered scene is a double-blade sword, which unfortunately increases the complexity of 3D scene understanding by introducing large variations in object poses and scales, and heavy occlusion. Therefore, the overall performance is still far from satisfactory.</p><p>In this work, we propose a deep learning system for holistic 3D scene understanding, which predicts and refines object shapes, object poses, and scene layout jointly with deep implicit representation. At first, similar to previous methods, we exploit standard Convolutional Neural Networks (CNN) to learn an initial estimation of 3D object poses, scene layout as well as 3D shapes. Different from previous methods using explicit 3D representation like volume or mesh, we utilize the local structured implicit representation of shapes motivated by <ref type="bibr" target="#b11">[12]</ref>. Instead of taking depth images as input like <ref type="bibr" target="#b11">[12]</ref>, we design a new local implicit shape embedding network to learn the latent shape code directly from images, which can be further decoded to generate the implicit function for 3D shapes. Due to the power of implicit representation, the 3D shape of each object can be reconstructed with higher accuracy and finer surface details compared to other representations.</p><p>Then, we propose a novel graph-based scene context network to gather information from local objects, i.e., bottomup features extracted from the initial predictions, and learn to refine the initial 3D pose and scene layout via scene context information with the implicit representation. Being one of the core topics studied in scene understanding, the context has been achieved in the era of deep learning mainly from two aspects -the model architecture and the loss function. From the perspective of model design, we exploit the graph-based convolutional neural network (GCN) to learn context since it has shown competitive performance to learn context <ref type="bibr" target="#b59">[60]</ref>. With the deep implicit representation, the learned local shape latent vectors are naturally a compact and informative feature measuring of the object geometries, which results in more effective context models compared to features extracted from other representations such as mesh.</p><p>Not only the architecture, but the deep implicit representation also benefits the context learning on the loss function. One of the most basic contextual information yet still missing in many previous works -objects should not intersect with each other, could be easily applied as supervision by penalizing the existence of 3D locations with negative predicted SDF in more than one objects 1 . We define this constraint as a novel physical violation loss and find it particularly helpful in preventing intersecting objects and producing reasonable object layouts.</p><p>Overall, our contributions are mainly in four aspects. First, we design a two-stage single image-based holistic 3D scene understanding system that could predict object shapes, object poses, and scene layout with deep implicit representation, then optimize the later two. Second, a new image-based local implicit shape embedding network is proposed to extract latent shape information which leads to superior geometry accuracy. Third, we propose a novel GCN-based scene context network to refine the object arrangement which well exploits the latent and implicit features from the initial estimation. Last but not least, we design a physical violation loss, thanks to the implicit representation, to effectively prevent the object intersection. Extensive experiments show that our model achieves the stateof-the-art performance on the standard benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Single Image Scene Reconstruction. As a highly illposed problem, single image scene reconstruction sets a high bar for learning-based algorithms, especially in a cluttered scene with heavy occlusion. The problem can be divided into layout estimation, object detection and pose estimation, and 3D object reconstruction. A simple version of the first problem is to simplify the room layout as a bounding box <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40]</ref>. To detect objects and estimate poses in 3D space, Recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref> try to infer 3D bounding boxes from 2D detection by exploiting relationships among objects with a graph or physical simulation. At the same time, other works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref> further extend the idea to align a CAD model with similar style to each detected object. Still, the results are limited by the size of the CAD model database which results in an inaccurate representation of the scene. To tackle the above limitations of previous works, Total3D <ref type="bibr" target="#b33">[34]</ref> is proposed as an end-to-end solution to jointly estimate the layout box and object poses while reconstructing each object from the detection and utilizing the reconstruction to supervise the pose estimation learning. However, they only exploit relationships among objects with features based on appearance and 2D geometry. Shape Representation. In the field of computer graphics, traditional shape representation methods include mesh, voxel, and point cloud. Some of the learning-based works try to encode the shape prior into a feature vector but stick to the traditional representations by decoding the vector into mesh <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14]</ref>, voxel <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b45">46]</ref> or point cloud <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b58">59]</ref>. Others try to learn structured representations which decompose the shape into simple shapes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37]</ref>. Recently, implicit surface function <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> has been widely used as a new representation method to overcome the disadvantages of traditional methods (i.e. unfriendly data structure to neural network of mesh and point cloud, low resolution and large memory consumption of voxel). Most recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b54">55]</ref> try to combine the structured and implicit representation which provides a physically meaningful feature vector while introducing significant improvement on the details of the decoded shape. Graph Convolutional Networks. Proposed by <ref type="bibr" target="#b14">[15]</ref>, graph neural networks or GCNs have been widely used to learn from graph-structured data. Inspired by convolutional neural networks, convolutional operation has been introduced to graph either on spectral domain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref> or non-spectral domain <ref type="bibr" target="#b17">[18]</ref> which performs convolution with a message passing neural network to gather information from the neighboring nodes. Attention mechanism has also been introduced to GCN and has been proved to be efficient on tasks like node classification <ref type="bibr" target="#b49">[50]</ref>, scene graph generation <ref type="bibr" target="#b59">[60]</ref> and feature matching <ref type="bibr" target="#b42">[43]</ref>. Recently, GCN has been even used on super-resolution <ref type="bibr" target="#b60">[61]</ref> which is usually the territory of CNN. In the 3D world which interests us most, GCN has been used on classification <ref type="bibr" target="#b52">[53]</ref> and segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> on point cloud, which is usually an enemy representation to traditional neural networks. The most related application scenario of GCN with us is 3D object detection on points cloud. Recent work shows the ability of GCN to predict relationship <ref type="bibr" target="#b1">[2]</ref> or 3D object detections <ref type="bibr" target="#b32">[33]</ref> from point cloud data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our method</head><p>As shown in <ref type="figure">Fig. 2</ref>, the proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage. In the initial estimation stage, similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>, a 2D detector is first adopted to extract the 2D bounding <ref type="figure">Figure 2</ref>: Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work <ref type="bibr" target="#b33">[34]</ref>, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder <ref type="bibr" target="#b11">[12]</ref> and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.</p><p>box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry. The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose. In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information. As 2D detector, LEN, ODN has the standard architecture similar to prior works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>, in this section, we will describe the details of the novel SGCN and LIEN in detail. Please refer to our supplementary materials for the details of our 2D detector, LEN, ODN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scene Graph Convolutional Network</head><p>As shown in <ref type="figure">Fig. 2</ref>, motivated by Graph R-CNN <ref type="bibr" target="#b59">[60]</ref>, we model the whole 3D scene as a graph G, in which the nodes represent the objects, the scene layout, and their relationships. The graph is constructed starting from a complete graph with undirected edges between all objects and layout nodes, which allows information to flow among objects and the scene layout. Then, we add relation nodes to each pair of neighboring object/layout nodes. Considering the nature of directional relation <ref type="bibr" target="#b25">[26]</ref>, we add two relation nodes between each pair of neighbors in different directions.</p><p>It is well known that the input features are the key to an effective GCN <ref type="bibr" target="#b51">[52]</ref>. For different types of nodes, we design features carefully from different sources as follows. For each node, features from different sources are flattened and concatenated into a vector, then embedded into a node representation vector with the same length using MLP.</p><p>Layout Node. We use the feature from the image encoder of LEN, which encodes the appearance of layout, and the parameterized output of layout bounding box and camera pose from LEN, as layout node features. We also concatenate the camera intrinsic parameters normalized by the image height into the feature to add camera priors. Object Node. We collect the appearance-relationship feature <ref type="bibr" target="#b33">[34]</ref> from ODN, and the parameterized output of object bounding box from ODN, along with the element centers in the world coordinate and analytic code from LIEN (which we will further describe in the next section). We also use the one-hot category label from the 2D detector to introduce semantic information to SGCN. Relationship Node. For nodes connecting two different objects, the geometry feature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49]</ref> of 2D object bounding boxes and the box corner coordinates of both connected objects normalized by the image height and width are used as features. The coordinates are flattened and concatenated in the order of source-destination, which differentiate the relationships of different directions. For nodes connecting objects and layouts, since the relationship is presumably different from object-object relationship, we initialize the representations with constant values, leaving the job of inferring reasonable relationship representation to SGCN.</p><p>For a graph with N objects and 1 layout, object-layout nodes and relationship nodes can then be put into two matrices Z o ? R d?(N +1) and Z r ? R d?(N +1) <ref type="bibr" target="#b1">2</ref> . Since the graph is modeled with different types of nodes, which makes a difference in the information needed from different sources to destinations, we define independent message passing weights for each of the source-destination types. We denote the linear transformation and the adjacent matrix from source node to destination node with type a and b as W ab and ? ab , in which node types can be source object (or layout) s, destination object (or layout) d, and relationships r. Thus, the representation of object and layout nodes can be updated as</p><formula xml:id="formula_0">Z o = ?(Z o + Message from Layout or Objects W sd Z o + Messages from Neighboring Relationships W rs Z r ? rs + W rd Z r ? rd ),</formula><p>(1) and the relationship node representations can be updated as</p><formula xml:id="formula_1">Z r = ?(Z r + W sr Z o ? sr + W dr Z o ? dr</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Messages from Layout or Neighboring Objects</head><p>). <ref type="formula">(2)</ref> After four steps of message passing, independent MLPs are used to decode object node representations into residuals for corresponding object bounding box parameters (?, d, s, ?), and layout node representation into residuals for initial layout box C, s l , ? l and camera pose R (?, ?). Please refer to our supplementary or <ref type="bibr" target="#b33">[34]</ref> for the details of the definition. The shape codes can be also refined in the scene graph, while we find that it doesn't improve empirically as much as for the layout and object poses in our pipeline because our local implicit embedding network, which will be introduced in the following, is powerful enough to learn accurate shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Implicit Embedding Network</head><p>With a graph constructed for each scene, we naturally ask what features help SGCN effectively capture contextual information among objects. Intuitively, we expect features that well describe 3D object geometry and their relationship in 3D space. Motivated by Genova et al. <ref type="bibr" target="#b11">[12]</ref>, we propose to utilize the local deep implicit representation as the features embedding object shapes due to its superior performance for single object reconstruction. In their model, the function is a combination of 32 3D elements (16 with symmetry constraints), with each element described with 10 Gaussian function parameters analytic code and 32-dim latent variables (latent code). The Gaussian parameters describe the scale constant, center point, radii, and Euler angle of every Gaussian function, which contains structured information of the 3D geometry. We use analytic code as a feature for object nodes in SGCN, which should provide information on the local object structure. Furthermore, since the centers of the Gaussian functions presumably correspond to centers of different parts of an object, we also transform them from the object coordinate system to the world coordinate system as a feature for every object node in SGCN. The transformation provides global information about the scene, which makes SGCN easier to infer relationships between objects. The above two features make up the implicit features of LIEN.</p><p>As LDIF <ref type="bibr" target="#b11">[12]</ref> is designed for 3D object reconstruction from one or multiple depth images, we design a new image-based Local Implicit Embedding Network (LIEN) to learn the 3D latent shape representation from the image which is obviously a more challenging problem. Our LIEN consists of a Resnet-18 as image encoder, along with a three-layer MLP to get the analytic and latent code. Additionally, in order to learn the latent features effectively, we concatenate the category code with the image feature from the encoder to introduce shape priors to the LIEN, which improves the performance greatly. Please refer to our supplementary material for the detailed architecture of the proposed LIEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>Losses for Initialization Modules. When training LIEN along with LDIF decoder individually, we follow <ref type="bibr" target="#b11">[12]</ref> to use the shape element center loss L c with weight ? c and point sample loss,</p><formula xml:id="formula_2">L p = ? ns L ns + ? us L us ,<label>(3)</label></formula><p>where L ns and L us evaluates L 2 losses for near-surface samples and uniformly sampled points. When training LEN and ODN, we follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> to use classification and regression loss for every output parameter of LEN and ODN,</p><formula xml:id="formula_3">L LEN = y?{?,?,C,s l ,? l } ? y L y ,<label>(4)</label></formula><formula xml:id="formula_4">L ODN = x?{?,d,s,?} ? x L x .<label>(5)</label></formula><p>Joint Refinement with Object Physical Violation Loss.</p><p>For the refinement stage, we aim to optimize the scene layout and object poses using the scene context information by minimizing the following loss function,</p><formula xml:id="formula_5">L j = L LEN + L ODN + ? co L co + ? phy L phy . (6)</formula><p>Besides L LEN , L ODN and cooperative loss L co <ref type="bibr" target="#b33">[34]</ref>, we propose a novel physical violation loss as a part of joint loss for the scene graph convolutional network to make sure that objects will not intersect with each other. The neural SDF representation used by local implicit representation gives us a convenient way to propagate gradient from undesired geometry intersection back to the object pose estimation. To achieve this, we first sample points inside objects. For each object i, we randomly sample points inside the bounding box of each object, along with the center points of Gaussian elements as point candidates. We then queue these candidates into LDIF decoder of the object and filter out points outside object surfaces to get inside point samples S i . Finally, we queue S i into the LDIF decoder of the k-nearest objects N i to verify if they have intersection with other objects (if the predicted label is "inside"). We follow <ref type="bibr" target="#b11">[12]</ref> to compute a L 2 loss between the predicted labels of intersected points with the ground truth surface label (where we use 1, 0, 0.5 for "outside", "inside", "surface" labels). The object physical violation loss can be defined as:</p><formula xml:id="formula_6">L phy = 1 N N i=1 1 |S i | x?Si relu(0.5 ? sig(?LDIF i (x))) ,<label>(7)</label></formula><p>where LDIF i (x) is the LDIF for object i to decode a world coordinate point x into LDIF value. A sigmoid is applied on the LDIF value (scaled by ?) to get the predicted labels, and a ReLU is applied to consider only the intersected points. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the loss punishes intersected sample points thus push both objects away from each other to prevent intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we compare our method with state-ofthe-art 3D scene understanding methods in various aspects and provide an ablation study to highlight the effectiveness of major components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Datasets. We follow <ref type="bibr" target="#b33">[34]</ref> to use two datasets to train each module individually and jointly. We use two datasets for training and evaluation. 1) Pix3D dataset <ref type="bibr" target="#b46">[47]</ref> is presented as a benchmark for shape-related tasks including reconstruction, providing 9 categories of 395 furniture models and 10,069 images with precise alignment. We use the mesh fusion pipeline from Occupancy Network <ref type="bibr" target="#b31">[32]</ref> to get watertight meshes for LIEN training and evaluate LIEN on original meshes. 2) SUN RGB-D dataset <ref type="bibr" target="#b44">[45]</ref> contains 10K RGB-D indoor images captured by four different sensors and is densely annotated with 2D segmentation, semantic labels, 3D room layout, and 3D bounding boxes with object orientations. Follow Total3D <ref type="bibr" target="#b33">[34]</ref>, we use the train/test split from <ref type="bibr" target="#b13">[14]</ref> on the Pix3D dataset and the official train/test split on the SUN RGB-D dataset. The object labels are mapped from NYU-37 to Pix3D as presented by <ref type="bibr" target="#b33">[34]</ref>. Metrics. We adopt the same evaluation metrics with <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>, including average 3D Intersection over Union  <ref type="figure">Figure 4</ref>: Object reconstruction qualitative comparison. We use the implementation from <ref type="bibr" target="#b33">[34]</ref> for AtlasNet <ref type="bibr" target="#b15">[16]</ref>. Our results contain finer details and have more smooth surfaces.</p><p>(IoU) for layout estimation; mean absolute error for camera pose; average precision (AP) for object detection; and chamfer distance for single-object mesh generation from single image. Implementation. We use the outputs of the 2D detector from Total3D as the input of our model. We also adopted the same structure of ODN and LEN from Total3D. LIEN is trained with LDIF decoder on Pix3D with watertight mesh, using Adam optimizer with a batch size of 24 and learning rate decaying from 2e-4 (scaled by 0.5 if the test loss stops decreasing for 50 epochs, 400 epochs in total) and evaluated on the original non-watertight mesh. SGCN is trained on SUN RGB-D, using Adam optimizer with a batch size of 2 and learning rate decaying from 1e-4 (scaled by 0.5 every 5 epochs after epoch 18, 30 epochs in total). We follow <ref type="bibr" target="#b33">[34]</ref> to train each module individually then jointly. When training SGCN individually, we use L j without L phy , and put it into the full model with pre-trained weights of other modules. In joint training, we adopt the observation from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-art</head><p>In this section, we compare to the state-of-the-art methods for holistic scene understand from aspects including object reconstruction, 3D object detection, layout estimation, camera pose prediction, and scene mesh reconstruction. 3D Object Reconstruction. We first compare the performance of LIEN with previous methods, including AtlasNet <ref type="bibr" target="#b15">[16]</ref>, TMN <ref type="bibr" target="#b34">[35]</ref>, and Total3D <ref type="bibr" target="#b33">[34]</ref>, for the accuracy of the predicted geometry on Pix3D dataset. All the methods take as input a crop of image of the object and produce 3D geometry. To make a fair comparison, the one-hot object category code is also concatenated with the appearance feature for AtlasNet <ref type="bibr" target="#b15">[16]</ref> and TMN <ref type="bibr" target="#b34">[35]</ref>. For our method, we run a marching cube algorithm on 256 resolution to reconstruct the mesh. The quantitative comparison is shown in <ref type="table">Table 1</ref>. Our method produces the most accurate 3D shape compared to other methods, yielding the lowest mean Chamfer Distance across all categories. Qualitative results are shown in <ref type="figure">Fig. 4</ref>. AtlasNet produces results in limited topology and thus generates many undesired surfaces. MGN mitigates the issue with the capability of topology modification, which improves the results but still leaves obvious artifacts and unsmooth surface due to the limited representation capacity of the triangular mesh. In contrast, our method produces 3D shape with correct topology, smooth surface, and fine-grained details, which clearly shows the advantage of the deep implicit representation. 3D Object Detection. We then evaluate the 3D object detection performance of our model. Follow <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21]</ref>, we use mean average precision (mAP) with the threshold of 3D bounding box IoU set at 0.15 as the evaluation metric. The quantitative comparison to state-of-the-art methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>   <ref type="table">Table 3</ref>: 3D layout and camera pose estimation comparison. Our method outperforms SOTA by 5.2% in layout estimation while on par with SOTA on camera pose estimation.</p><p>consistently the best over all semantic categories and significantly outperforms the state-of-the-art (i.e. improving AP by 18.83%). <ref type="figure">Figure 5</ref> shows some qualitative comparison. Note how our method produces object layout not only more accurate but also in reasonable context compared to Total3D, e.g. objects are parallel to wall direction. Layout Estimation. We also compare the 3D room layout estimation with Total3D <ref type="bibr" target="#b33">[34]</ref> and other state-of-the-arts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>. The quantitative evaluation is shown in <ref type="table">Table 3</ref> (Layout IoU). Overall, our method outperforms all the baseline methods. This indicates that the GCN is effective in measuring the relation between layout and objects and thus benefits the layout prediction. Camera Pose Estimation. <ref type="table">Table 3</ref> also shows the comparison over camera pose prediction, following the evaluation protocol of Total3D. Our method achieves 5% better camera pitch and slightly worse camera roll. Holistic Scene Reconstruction. To our best knowledge, Total3D <ref type="bibr" target="#b33">[34]</ref> is the only work achieving holistic scene reconstruction from a single RGB, and thus we compare to it. Since no ground truth is presented in SUN RGB-D dataset, we mainly show qualitative comparison in <ref type="figure">Fig. 5</ref>. Compared to Total3D, our model has less intersection and estimates more reasonable object layout and direction. We consider this as a benefit from a better understanding of scene context by GCN. Our proposed physical violation loss L phy also contributes to less intersection.  <ref type="figure">Figure 5</ref>: Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D <ref type="bibr" target="#b33">[34]</ref> and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we verify the effectiveness of the proposed components for holistic scene understanding. As shown in <ref type="table" target="#tab_4">Table 4</ref>, we disable certain components and evaluate the model for 3D layout estimation and 3D object detection, We do not evaluate the 3D object reconstruction since it is highly related to the usage of deep implicit representation, which has been already evaluated in Section 4.2. Does GCN Matter? To show the effectiveness of GCN, we first attach the GCN to the original Total3D to improve the object and scene layout <ref type="figure" target="#fig_1">(Table 4, Total3D+GCN</ref>). For the difference between MGN of Total3D and LIEN of ours, we replace deep implicit features with the feature from image encoder of MGN and use their proposed partial Chamfer loss L g instead of L phy . Both object bounding box and scene layout are improved. We also train a version of our model without the GCN (Ours-GCN), and the performance drops significantly. Both experiments show that GCN is effective in capturing scene context. Does Deep Implicit Feature Matter? As introduced in Section 3.2, the LDIF representation provides informative node features for the GCN. Here we demonstrate the contribution from each component of the latent representation. Particularly, we remove either element centers or analytic code from the GCN node feature (Ours-element, Oursanalytic), and find both hurts the performance. This indicates that the complete latent representation is helpful in pursuing better scene understanding performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does Physical Violation Loss Matter?</head><p>Additionally, we evaluate the effectiveness of the physical violation loss. We train our model without it (Ours-L phy ), and also observe performance drop for both scene layout and object 3D bounding box in <ref type="table" target="#tab_4">Table 4</ref>. We refer to supplementary material for qualitative comparison. Evaluating on Other Metrics. We also test our method in other aspects including supporting relation, geometry accuracy, and room layout as shown in <ref type="table">Table 5</ref>. 1) We calculate the mean distance between the predicted bottom of on-floor objects and the ground truth floor to measure the supporting relationship. As ground truth, an object is considered to be on-floor if its bottom surface is within 15cm to the floor. While GCN significantly improves the metric, L phy slightly hurts possibly because it tends to push objects away. Further qualitative results are shown in the supplementary material. Besides, we also measure the average volume of the collision per scene between objects (Coll Vol), and our full model effectively prevent collision. 2) We follow Total3D <ref type="bibr" target="#b33">[34]</ref> to evaluate the alignment between scene reconstruction and ground truth depth map with global loss L g , and our full model performs the best. 3) We also project the predicted layout onto the image and evaluate with image based metrics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>. Our full model achieves the best on both corner and pixel errors. Overall, the GCN and L phy benefit on all   <ref type="table">Table 5</ref>: Ablation study on other metrics. We compare on supporting error, L g (in units of 10 ?2 ), average collision volume, corner error, and pixel error.  <ref type="figure">Figure 6</ref>: Qualitative results on ObjectNet3D dataset <ref type="bibr" target="#b56">[57]</ref> (row 1) and the layout estimation dataset in <ref type="bibr" target="#b18">[19]</ref> (row 2). these aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalization to other datasets</head><p>We also show qualitative results of our method tested on the 3D detection dataset ObjectNet3D <ref type="bibr" target="#b56">[57]</ref> and the layout estimation dataset in <ref type="bibr" target="#b18">[19]</ref> without fine-tuning in <ref type="figure">Fig. 6</ref>. Our method shows good generalization capability and performs reasonably well on these unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a deep learning model for holistic scene understanding by leveraging deep implicit representation. Our model not only reconstructs accurate 3D object geometry, but also learns better scene context using GCN and a novel physical violation loss, which can deliver accurate scene and object layout. Extensive experiments show that our model improves various tasks in holistic scene understanding over existing methods. A promising future direction could be exploiting object functionalities for better 3D scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide the detailed network architecture, implementation details, 3D detection on all categories, more qualitative results, and discussion of failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture of Our Pipeline</head><p>We show the architecture of LEN, ODN, LIEN, and SCGN in <ref type="figure" target="#fig_6">Fig. 7</ref>. 2D Detector, LEN, ODN. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21]</ref>, we use Faster RCNN <ref type="bibr" target="#b38">[39]</ref> trained on COCO dataset <ref type="bibr" target="#b29">[30]</ref> and finetuned on SUN RGB-D <ref type="bibr" target="#b44">[45]</ref> as 2D detector. The 2D detection results on SUN RGB-D are filtered and matched with the ground-truth 3D object bounding box during the data preparation procedure provided by <ref type="bibr" target="#b33">[34]</ref>. During the initialization stage, we use LEN and ODN architecture shown in <ref type="figure" target="#fig_6">Fig. 7 similar with</ref>  <ref type="bibr" target="#b33">[34]</ref>. LIEN. Our proposed LIEN consists of an image encoder followed by a three-layer MLP to embed a single image into a code. When evaluating on SUN RGB-D, the category labels are mapped to the ones used by Pix3D and concatenated to the image feature following <ref type="bibr" target="#b33">[34]</ref>. To construct the shape elements for LDIF decoder, we follow <ref type="bibr" target="#b11">[12]</ref> to reshape the 1344-dim vector into a 32x42 array, which corresponds to 42-dim (10 for analytic code and 32 for latent code) codes of the 32 shape elements. SGCN. Before being fed into each node, the features from different sources are flattened, concatenated, and embedded into a 512-dim representation using FC layers. The weights of the embedding network for layout, object, and relationship nodes are independent of each other. After updated with four steps of message passing, the representations of layout and object nodes are decoded into parameters with the networks specially designed for each of the node types. The decoding networks follow the design of LEN and ODN, and refine the parameterized initial outputs of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Data Processing. For the training of LIEN, watertight meshes <ref type="bibr" target="#b11">[12]</ref> must be used to retrieve the ground-truth values of inside-outside labels. However, the models of Pix3D <ref type="bibr" target="#b46">[47]</ref> are not that clean with inverted surface normals and holes occasionally, which causes failure with the traditional flood fill algorithm. To get more robust results, we utilize the mesh fusion pipeline <ref type="bibr" target="#b31">[32]</ref> which generates watertight meshes by fusing signed distance fields from several virtual cameras and applying the marching cube algorithm on it. Although the mesh fusion pipeline makes the model thicker and introduces noise to the ground-truth sample points, we evaluate it on the original mesh to directly compare with previous works. SGCN Outputs. As mentioned in main paper Section 3.1, our SGCN predicts residuals to refine the parameters of object bounding boxs, layout box, and camera pose. We follow <ref type="bibr" target="#b33">[34]</ref> to set the origin of the world coordinate frame at the camera center, with the y-axis up and perpendicular to the floor, and the x-axis aligned to the orientation of the camera forward. Thus the camera pose can be parameterized as R (?, ?), where ? is the camera pitch and ? is the camera roll. Also, a bounding box can be parameterized as 3D center C ? R 3 , size s ? R 3 and orientation ? ? [??, ?). Specifically, a layout box can be represented as C, s l , ? l , and a object box can be represented as (?, d, s, ?), where ? ? R 2 is the offset between 2D projection of 3D center and detected 2D object bounding box center, and d is the distance between 3D center and camera center. Hyper Parameters. When training LIEN, we use 1024 near-surface samples and 1024 uniformly samples, and set their loss weight ? ns = 0.1 and ? us = 1. For shape element center loss, we let ? c = 0.2. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>, classification and regression loss is used for parameters of both LEN and ODN, which we denote as L cls,reg Scene Mesh Reconstruction. Since our LIEN is trained on Pix3D with only 9 categories like MGN of Total3D, we suffer from the same problem with them when testing on SUN RGB-D, that our LIEN can not generalize to some of the categories. For the accuracy of the scene reconstruction, we follow <ref type="bibr" target="#b33">[34]</ref> to only consider certain categories of objects (i.e. cabinet, bed, chair, sofa, table, door, bookshelf, desk, shelves, dresser, refrigerator, television, box, whiteboard, nightstand). As a result, the reconstructed scene mesh has fewer objects than 3D detections.</p><formula xml:id="formula_7">x = L cls x + ? reg x L reg x , ?x ? {?, ?, ? l , d,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Detection on all categories</head><p>In this section, we report the average precision of 3D object detection on all categories of SUN RGB-D for a full comparison in <ref type="table" target="#tab_7">Table 6</ref>. Our method achieves the best performance for 27 over 33 categories and a significantly better mean average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Qualitatively Comparison with MGN on Object Mesh Reconstruction</head><p>In this section, we show more results on the object reconstruction in <ref type="figure" target="#fig_0">Fig. 10</ref>  The parameter x 0 initialized by LEN and ODN is then refined with residual x r decoded from updated node representations, ?x ? {?, ?, C, s l , ? l , ?, d, ?, s}. Variables ?, ?, ? l , d, ? are parameterized following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22]</ref>. We set dropout rate to 0.5 for all dropout blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CooP <ref type="bibr" target="#b20">[21]</ref>   method produces more accurate geometry preserving highquality details especially on chairs, bookshelves, and those shapes with relatively more complex topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Qualitatively Comparison on 3D Detection and Scene Reconstruction</head><p>In main paper Section 4.2, we show qualitative results of the 3D object detection and scene reconstruction. Here, we show more results in <ref type="figure" target="#fig_0">Fig. 11, Fig. 12</ref>, and <ref type="figure" target="#fig_0">Fig. 13</ref>. We can observe that compared to the state-of-theart method <ref type="bibr" target="#b33">[34]</ref>, our method produces significantly more accurate object pose estimation with fewer flying objects <ref type="figure" target="#fig_0">(Fig. 11e, Fig. 12a</ref>), fewer objects intersected with each (a) Ours-L phy (b) Full: with L phy <ref type="figure">Figure 8</ref>: Scene reconstruction samples of Ours-L phy and Full. We observe more intersections between objects without physical violation loss in some scenes.</p><p>(a) Total3D (b) Ours (c) GT <ref type="figure">Figure 9</ref>: Qualitative comparison of supporting relation. We take the front view of object 3D detection of main paper <ref type="figure">Figure 5</ref> column 4 as an example. We observe fewer flying objects in our results than Total3D, which shows a better understanding of supporting relation. other <ref type="figure" target="#fig_0">(Fig. 11a, Fig. 12d, Fig. 13e</ref>), and more accurate object orientation estimation <ref type="figure" target="#fig_0">(Fig. 11c, Fig. 12e, Fig. 13c</ref>). We also observe fewer objects intersected with the layout box ( <ref type="figure" target="#fig_0">Fig. 11d, Fig. 12e, Fig. 13a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Comparison of Ablation Study</head><p>In main paper Section 4.3, we quantitatively compare the improvement of our proposed L phy . While exhibiting a small gap from the metric, we show in qualitative results <ref type="figure">(Fig. 8</ref>) that the visual difference is relatively large. Objects are more likely to intersect with each other when trained without L phy , which disobeys physical context severely. On the contrary, training with L phy effectively prevents these errors in the results.</p><p>We also quantitatively compare the supporting relation, in main paper Section 4.3. Here in <ref type="figure">Fig. 9</ref>, we qualitatively compare the understanding of supporting relation in the front view of object 3D detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Failure Cases</head><p>We also show some failure cases in <ref type="figure" target="#fig_0">Fig. 14.</ref> We observe that although our LIEN performs well on Pix3D and is gen-eralized to SUN RGB-D, it still cannot make plausible reconstruction for some objects in rarely seen shapes (i.e. the desks of (a) and (b), the bookshelves of (b) and (c), the bed of (d)). For object detection, our pipeline fails to correctly estimate the pose of the bed in (e), which might result from the clustered scenes. Also, in some extreme cases, heavy occlusion might cause our pipeline to fail like in (f).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed pipeline takes a single image as input, estimates layout and object poses, then reconstructs the scene with Signed Distance Function (SDF) representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Object physical violation loss. Based on the insight that objects should not intersect, we punish points inside neighboring objects (demonstrated as p, which has negative LDIF values in both object A and object B). With error back-propagated through the LDIF decoder, intersected objects should be pushed back from each other, reducing intersection resulting from bad object pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>= 20 .= 30 .</head><label>2030</label><figDesc>?}. Other parameters of LEN and ODN are using only regression loss. For camera parameters, we set ? ? = 0.25, ? reg ? = 40, ? ? = 0.25, and ? reg ? For layout box parameters, we set ? C = 10, ? s l = 10, ? ? l = 0.25, and ? reg ? l = 30. For object box parameters, we set ? ? = 1, ? d = 0.75, ? reg d = 6.7, ? s = 10, ? ? = 0.33, and ? reg ? When training with cooperative loss and object physical violation loss, we set ? co = 150, ? phy = 20, ? = 100, k = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Architecture of LEN, ODN, LIEN, and SGCN. Our pipeline takes features from LEN, ODN, LIEN and other sources and embeds them into node representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :</head><label>1011121314</label><figDesc>More qualitative comparisons on object reconstruction. We compare with MGN from Total3D<ref type="bibr" target="#b33">[34]</ref>. Qualitative comparisons on object detection and scene reconstruction. Qualitative comparisons on object detection and scene reconstruction. Qualitative comparisons on object detection and scene reconstruction. Failure Cases. Possible reasons might be unseen object shapes (a, b, c, d), heavy occlusion (f), cluttered scene (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.93 3.36 14.19 3.12 3.83 26.93 8.36 Ours 4.11 3.96 5.45 7.85 5.61 11.73 2.39 4.31 24.65 6.72Table 1: Object reconstruction comparison. We report the Chamfer distance scaled with the factor of 10 3 . We follow<ref type="bibr" target="#b33">[34]</ref> to align the reconstructed mesh to ground-truth with ICP then sample 10K points from the output and the ground-truth meshes. Although trained on watertight meshes with more noise, our results still shows better results.</figDesc><table><row><cell cols="2">Category</cell><cell cols="9">bed bookcase chair desk sofa table tool wardrobe misc mean</cell><cell></cell></row><row><cell cols="3">AtlasNet [16] 9.03</cell><cell>6.91</cell><cell cols="4">8.37 8.59 6.24 19.46 6.95</cell><cell>4.78</cell><cell cols="2">40.05 12.26</cell><cell></cell></row><row><cell cols="2">TMN [35]</cell><cell>7.78</cell><cell>5.93</cell><cell cols="4">6.86 7.08 4.25 17.42 4.13</cell><cell>4.09</cell><cell cols="2">23.68 9.03</cell><cell></cell></row><row><cell cols="8">MGN [34] 5.32 5Method 5.99 6.56 bed chair sofa table desk dresser nightstand</cell><cell>sink</cell><cell cols="3">cabinet lamp mAP</cell></row><row><cell>3DGP [6]</cell><cell>5.62</cell><cell>2.31</cell><cell>3.24</cell><cell>1.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HoPR [22]</cell><cell cols="5">58.29 13.56 28.37 12.12 4.79</cell><cell>13.71</cell><cell>8.80</cell><cell>2.18</cell><cell>0.48</cell><cell cols="2">2.41 14.47</cell></row><row><cell>CooP [21]</cell><cell cols="5">57.71 15.21 36.67 31.16 19.90</cell><cell>15.98</cell><cell>11.36</cell><cell>15.95</cell><cell>10.47</cell><cell cols="2">3.28 21.77</cell></row><row><cell cols="6">Total3D [34] 60.65 17.55 44.90 36.48 27.93</cell><cell>21.19</cell><cell>17.01</cell><cell>18.50</cell><cell>14.51</cell><cell cols="2">5.04 26.38</cell></row><row><cell>Ours</cell><cell cols="5">89.32 35.14 69.10 57.37 49.03</cell><cell>29.27</cell><cell>41.34</cell><cell>33.81</cell><cell>33.93</cell><cell cols="2">11.90 45.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>3D object detection comparison. For CooP, we report the better results from<ref type="bibr" target="#b33">[34]</ref> trained on NYU-37 object labels. Our method outperforms SOTA, benefiting from a better understanding of the object relationships and the scene context.</figDesc><table /><note>[34] that object reconstruction depends on clean mesh for supervision, to fix the weights of LIEN and LDIF decoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>is shown inTable 2. Our method performs</figDesc><table><row><cell>Method</cell><cell cols="3">Layout IoU Cam pitch Cam roll</cell></row><row><cell>3DGP [6]</cell><cell>19.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Hedau [19]</cell><cell>-</cell><cell>33.85</cell><cell>3.45</cell></row><row><cell>HoPR [22]</cell><cell>54.9</cell><cell>7.60</cell><cell>3.12</cell></row><row><cell>CooP [21]</cell><cell>56.9</cell><cell>3.28</cell><cell>2.19</cell></row><row><cell>Total3D [34]</cell><cell>59.2</cell><cell>3.15</cell><cell>2.09</cell></row><row><cell>Ours</cell><cell>64.4</cell><cell>2.98</cell><cell>2.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study. We evaluate layout estimation with layout IoU and 3D object detection with mAP.</figDesc><table><row><cell>Setting</cell><cell>Sup Err (cm)</cell><cell>L g</cell><cell>Coll Vol (dm 3 /scene)</cell><cell>Corner Err (%)</cell><cell>Pixel Err (%)</cell></row><row><cell>Total3D</cell><cell>26.72</cell><cell>1.43</cell><cell>-</cell><cell>13.29</cell><cell>20.51</cell></row><row><cell>Ours-GCN</cell><cell>24.18</cell><cell>1.41</cell><cell>16.64</cell><cell>13.17</cell><cell>20.05</cell></row><row><cell>Ours-L phy</cell><cell>13.35</cell><cell>1.14</cell><cell>13.65</cell><cell>11.60</cell><cell>17.91</cell></row><row><cell>Full</cell><cell>14.71</cell><cell>1.11</cell><cell>13.55</cell><cell>11.45</cell><cell>17.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. Compared to MGN in Total3D<ref type="bibr" target="#b33">[34]</ref>, our</figDesc><table><row><cell></cell><cell>Input Image</cell><cell></cell><cell></cell><cell></cell><cell>2D detection</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Faster RCNN</cell><cell></cell><cell></cell><cell></cell><cell>Geometry feature ( ? ?64)</cell></row><row><cell>LEN</cell><cell></cell><cell></cell><cell>LIEN</cell><cell></cell><cell>ODN</cell><cell></cell></row><row><cell></cell><cell>Resnet-</cell><cell></cell><cell></cell><cell>Resnet-</cell><cell></cell><cell cols="2">Resnet-</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell>Category code (9)</cell><cell>18</cell><cell></cell><cell>32</cell></row><row><cell></cell><cell>Appearance feature (2048)</cell><cell></cell><cell cols="2">Appearance feature (1536)</cell><cell></cell><cell cols="2">2048 2048 Appearance feature (2048)</cell></row><row><cell></cell><cell>FC-2048</cell><cell></cell><cell cols="2">Concatenate</cell><cell></cell><cell cols="2">Attention sum</cell></row><row><cell></cell><cell>ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dropout</cell><cell></cell><cell cols="2">Appearance feature (1545)</cell><cell></cell><cell cols="2">Relation feature (2048)</cell></row><row><cell>FC-1024 ReLU Dropout FC-8</cell><cell>FC-1024 ReLU Dropout FC-6</cell><cell>FC-1024 ReLU Dropout FC-4</cell><cell cols="2">FC-1536 Leaky ReLU FC-1536 Leaky ReLU FC-1536 Leaky ReLU</cell><cell>FC-128 ReLU</cell><cell cols="2">A-R feature (2048) FC-128 ReLU FC-128 ReLU</cell><cell>FC-128 ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Latent code (1536)</cell><cell>Dropout</cell><cell>Dropout</cell><cell>Dropout</cell><cell>Dropout</cell></row><row><cell></cell><cell>0 , 0 , 0 , 0 , 0</cell><cell></cell><cell></cell><cell></cell><cell>FC-2</cell><cell>FC-12</cell><cell>FC-12</cell><cell>FC-3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Slice [:, :10]</cell><cell></cell><cell></cell></row><row><cell></cell><cell>More features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGCN</cell><cell>Concatenate</cell><cell></cell><cell>Concatenate</cell><cell></cell><cell cols="2">Concatenate</cell></row><row><cell></cell><cell>Layout feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FC-512</cell><cell></cell><cell>FC-512</cell><cell></cell><cell cols="2">FC-512</cell></row><row><cell></cell><cell>ReLU</cell><cell></cell><cell>ReLU</cell><cell></cell><cell cols="2">ReLU</cell></row><row><cell></cell><cell>FC-512</cell><cell></cell><cell>FC-512</cell><cell></cell><cell cols="2">FC-512</cell></row><row><cell></cell><cell cols="2">Layout Representation (512)</cell><cell cols="2">Relation Representation (512) Relation Representation (512) Relation Representation (512)</cell><cell cols="3">Relation Representation (512) Relation Representation (512) Object Representation (512)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Message passing x4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Layout Representation (512)</cell><cell></cell><cell></cell><cell cols="3">Relation Representation (512) Relation Representation (512) Object Representation (512)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC-256</cell><cell>FC-256</cell><cell>FC-256</cell><cell>FC-256</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dropout</cell><cell>Dropout</cell><cell>Dropout</cell><cell>Dropout</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC-2</cell><cell>FC-12</cell><cell>FC-12</cell><cell>FC-3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">, , , , , , , , ,</cell></row><row><cell></cell><cell>, , , ,</cell><cell></cell><cell></cell><cell></cell><cell cols="2">, , , , , , , , , s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Average precision of 3D object detection on all categories. For CooP, we report the better results from<ref type="bibr" target="#b33">[34]</ref> trained on NYU-37 object labels.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The object interior is with negative SDF, and thus no location should be inside of two objects.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Leonidas Guibas</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Denver Dash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Khanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12622</idno>
	</analytic>
	<monogr>
		<title level="m">Angela Dai, and Matthias Nie?ner. Scenecad: Predicting object alignments and layouts in rgb-d scans</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Holistic++ scene understanding: Single-view 3d holistic scene parsing and human pose estimation with human-object interaction and physical commonsense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01507</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delay: Robust spatial layout estimation for cluttered indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to exploit stability for 3d scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Basevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sdm-net: Deep generative network for structured deformable mesh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local deep implicit functions for 3d shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning shape templates with structured implicit functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7154" to="7164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mesh r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A papier-m?ch? approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxue</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Holistic 3d scene parsing and reconstruction from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxue</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Seethrough: finding chairs in heavily occluded indoor scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moos</forename><surname>Hueting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradyumna</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10473</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David C Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grass: Generative recursive autoencoders for shape structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning efficient point cloud generation for dense 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07036</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dops: Learning to detect 3d objects and predict their 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangda</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11913" to="11922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinyu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep mesh reconstruction from single rgb images via topology modification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05103</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Superquadrics revisited: Learning 3d shape parsing beyond cuboids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Despoina</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10344" to="10353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04618</idno>
		<title level="m">Convolutional occupancy networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A coarse-to-fine indoor layout estimation (cfile) method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11461</idno>
		<title level="m">Geometrics: Exploiting geometric structure for graph-encoded objects</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning 3d shape completion from laser scan data with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1955" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rgcnn: Regularized graph cnn for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gusi</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pq-net: A generative part seq2seq network for 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rundi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Objectnet3d: A large scale database for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonhui</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="160" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Disn: Deep implicit surface network for high-quality single-view 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10711</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4541" to="4550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cross-scale internal graph neural network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
