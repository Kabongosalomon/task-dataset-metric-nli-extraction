<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Barua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current natural language processing (NLP) pipelines often make use of transfer learning, where a model is pre-trained on a data-rich task before being fine-tuned on a downstream task of interest <ref type="bibr" target="#b28">(Ruder et al., 2019)</ref>. The success of this paradigm is partially thanks to the release of parameter checkpoints for pre-trained models. These checkpoints allow members of the NLP community to quickly attain strong performance on many tasks without needing to perform expensive pre-training themselves. As one example, the pre-trained checkpoints for the "Text-to-Text Transfer Transformer" (T5) model released by  have been used to achieve state-of-the-art results on many benchmarks <ref type="bibr">(Khashabi et al., 2020;</ref><ref type="bibr">Kale, 2020;</ref><ref type="bibr">Izacard and Grave, 2020;</ref><ref type="bibr">Narang et al., 2020, etc.)</ref>.</p><p>Unfortunately, many of these language models were pre-trained solely on English-language text. This significantly limits their use given that roughly 80% of the world population does not speak <ref type="bibr">English (Crystal, 2008)</ref>. One way the community has addressed this English-centricity has been to release dozens of models, each pre-trained on a single non-English language <ref type="bibr" target="#b2">(Carmo et al., 2020;</ref><ref type="bibr">de Vries et al., 2019;</ref><ref type="bibr" target="#b8">Le et al., 2020;</ref><ref type="bibr" target="#b16">Martin et al., 2020;</ref><ref type="bibr">Delobelle et al., 2020;</ref><ref type="bibr" target="#b16">Malmsten et al., 2020;</ref><ref type="bibr">Nguyen and Tuan Nguyen, 2020;</ref><ref type="bibr">Polignano et al., 2019, etc.)</ref>. A more general solution is to produce multilingual models that have been pre-trained on a mixture of many languages. Popular models of this type are mBERT <ref type="bibr">(Devlin, 2018)</ref>, mBART <ref type="bibr" target="#b12">(Liu et al., 2020a)</ref>, and XLM-R , which are multilingual variants of <ref type="bibr">BERT (Devlin et al., 2019)</ref>, BART <ref type="bibr" target="#b10">(Lewis et al., 2020b)</ref>, and RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>, respectively.</p><p>In this paper, we continue this tradition by releasing mT5, a multilingual variant of T5. Our goal with mT5 is to produce a massively multilingual model that deviates as little as possible from the recipe used to create T5. As such, mT5 inherits all of the benefits of T5 (described in section 2), such as its general-purpose text-to-text format, its design based on insights from a large-scale empirical study, and its scale. To train mT5, we introduce a multilingual variant of the C4 dataset called mC4. mC4 comprises natural text in 101 languages drawn from the public Common Crawl web scrape. To validate the performance of mT5, we include results on several benchmark datasets, showing state-of-the-art results in many cases. Finally, we characterize a problematic behavior of pre-trained generative multilingual language models in the zero-shot setting, where they erroneously translate part of their prediction into the wrong language. To address this "accidental translation", we describe a simple procedure that involves mixing in unlabeled pre-training data during fine-tuning and demonstrate that it dramatically alleviates this issue. We release our pre-trained models and code arXiv:2010.11934v3 [cs.CL] 11 Mar 2021 so that the community can leverage our work. 1 2 Background on T5 and C4</p><p>In this section, we provide a short overview of T5 and the C4 pre-training dataset. Further details are available in .</p><p>T5 is a pre-trained language model whose primary distinction is its use of a unified "text-totext" format for all text-based NLP problems. This approach is natural for generative tasks (such as machine translation or abstractive summarization) where the task format requires the model to generate text conditioned on some input. It is more unusual for classification tasks, where T5 is trained to output the literal text of the label (e.g. "positive" or "negative" for sentiment analysis) instead of a class index. The primary advantage of this approach is that it allows the use of exactly the same training objective (teacher-forced maximumlikelihood) for every task, which in practice means that a single set of hyperparameters can be used for effective fine-tuning on any downstream task. Similar unifying frameworks were proposed by <ref type="bibr">Keskar et al. (2019)</ref> and <ref type="bibr" target="#b18">McCann et al. (2018)</ref>. Given the sequence-to-sequence structure of this task format, T5 uses a basic encoder-decoder Transformer architecture as originally proposed by <ref type="bibr" target="#b30">Vaswani et al. (2017)</ref>. T5 is pre-trained on a masked language modeling "span-corruption" objective, where consecutive spans of input tokens are replaced with a mask token and the model is trained to reconstruct the masked-out tokens.</p><p>An additional distinguishing factor of T5 is its scale, with pre-trained model sizes available from 60 million to 11 billion parameters. These models were pre-trained on around 1 trillion tokens of data. Unlabeled data comes from the C4 dataset, which is a collection of about 750GB of English-language text sourced from the public Common Crawl web scrape. C4 includes heuristics to extract only natural language (as opposed to boilerplate and other gibberish) in addition to extensive deduplication. The pre-training objective, model architecture, scaling strategy, and many other design choices for T5 were chosen based on a large-scale empirical study described in detail in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">mC4 and mT5</head><p>Our goal in this paper is to create a massively multilingual model that follows T5's recipe as closely as possible. Towards this end, we develop an ex-tended version of the C4 pre-training dataset that covers 101 languages and introduce changes to T5 to better suit this multilinguality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">mC4</head><p>The C4 dataset was explicitly designed to be English only: any page that was not given a probability of at least 99% of being English by langdetect 2 was discarded. In contrast, for mC4 we use cld3 3 to identify over 100 languages. Since some of these languages are relatively scarce on the internet, we make use of all of the 71 monthly web scrapes released so far by Common Crawl. This is dramatically more source data than was used for C4, for which the April 2019 web scrape alone was enough to provide plenty of English-language data.</p><p>An important heuristic filtering step in C4 was the removal of lines that did not end in an English terminal punctuation mark. Since many languages do not use English terminal punctuation marks, we instead apply a "line length filter" that requires pages to contain at least three lines of text with 200 or more characters. Otherwise, we follow C4's filtering by deduplicating lines across documents and removing pages containing bad words. 4 Finally, we detect each page's primary language using cld3 and remove those with a confidence below 70%.</p><p>After these filters are applied, we group the remaining pages by language and include in the corpus all languages with 10,000 or more pages. This produces text in 107 "languages" as defined by cld3. However, we note that six of these are just script variants of the same spoken language (e.g. ru is Russian in Cyrillic script and ru-Latn is Russian in Latin script). A histogram of the page counts for each language is shown in <ref type="figure">fig. 1</ref>. Detailed dataset statistics including per-language token counts are shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">mT5</head><p>The model architecture and training procedure that we use for mT5 closely follows that of T5. Specifically, we base mT5 on the "T5.1.1" recipe, 5 which improves upon T5 by using GeGLU nonlinearities <ref type="bibr" target="#b29">(Shazeer, 2020)</ref>, scaling both d model and d ff instead 2 https://pypi.org/project/langdetect/ 3 https://github.com/google/cld3 4 https://github.com/LDNOOBW/ 5 https://github.com/google-research/ text-to-text-transfer-transformer/blob/ master/released_checkpoints.md#t511 <ref type="table">en  ru  es  de  fr  it  pt  pl  nl  tr  ja  vi  id  cs  zh  fa  ar  sv  ro  el  uk  hu  da  fi  no  bg  hi  sk  ko  th  ca  ms  iw  lt  sl  mr  bn  et  lv  az  gl  cy  sq  ta  sr  ne  lb  hy  kk  ka  mt  af  fil  is  mk  ml  mn  ur  be  la  eu  tg  te  fy  kn  ky  sw  so  my  uz</ref>    Encoder-only 270M -550M 100 Common Crawl (CCNet) mBART <ref type="bibr" target="#b10">(Lewis et al., 2020b)</ref> Encoder-decoder 680M 25 Common Crawl (CC25) MARGE  Encoder-decoder <ref type="formula">960M</ref>   of just d ff in the larger models, and pre-training on unlabeled data only with no dropout. We refer to  for further details on T5. A major factor in pre-training multilingual models is how to sample data from each language. Ultimately, this choice is a zero-sum game: If low-resource languages are sampled too often, the model may overfit; if high-resource languages are not trained on enough, the model will underfit. We therefore take the approach used in <ref type="bibr">(Devlin, 2018;</ref><ref type="bibr" target="#b0">Arivazhagan et al., 2019)</ref> and boost lower-resource languages by sampling examples according to the probability p(L) ? |L| ? , where p(L) is the probability of sampling text from a given language during pre-training and |L| is the number of examples in the language. The hyperparameter ? (typically with ? &lt; 1) allows us to control how much to "boost" the probability of training on low-resource languages. Values used by prior work include ? = 0.7 for mBERT <ref type="bibr">(Devlin, 2018)</ref>, ? = 0.3 for XLM-R , and ? = 0.2 for MMNMT <ref type="bibr" target="#b0">(Arivazhagan et al., 2019)</ref>. We tried all three of these values (ablation results in section 4.2) and found ? = 0.3 to give a reasonable compromise between performance on high-and low-resource languages.</p><p>The fact that our model covers over 100 languages necessitates a larger vocabulary. Following XLM-R <ref type="bibr">(Conneau et al., 2018)</ref>, we increase the vocabulary size to 250,000 wordpieces. As in T5, we use SentencePiece (Kudo and Richardson, 2018; Kudo, 2018) models trained with the language sampling rates used during pre-training. To accommodate languages with large character sets like Chinese, we use a character coverage of 0.99999 and enable SentencePiece's "byte-fallback" feature to ensure that any string can be uniquely encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison to related models</head><p>To contextualize our new model, we provide a brief comparison with existing massively multilingual pre-trained language models. For brevity, we focus on models that support more than a few dozen languages. <ref type="table" target="#tab_2">Table 1</ref> gives a high-level comparison of mT5 to the most similar models.</p><p>mBERT (Devlin, 2018) is a multilingual version of <ref type="bibr">BERT (Devlin et al., 2019)</ref>. Similar to our approach with mT5, mBERT follows the BERT recipe as closely as possible (same architecture, objective, etc.). The primary difference is the training set: Instead of training on English Wikipedia and the Toronto Books Corpus, mBERT is trained on up to 104 languages from Wikipedia. XLM (Conneau and Lample, 2019) is also based on BERT but applies improved methods for pre-training multilingual language models including explicitly crosslingual pre-training objectives. Many pre-trained versions of XLM have been released; the most massively-multilingual variant was trained on 100 languages from Wikipedia. XLM-R (Conneau In-language multitask (models fine-tuned on gold data in all target languages)   <ref type="bibr" target="#b15">(Luo et al., 2020)</ref> and <ref type="bibr">RemBERT (Chung et al., 2020)</ref>. For the "translate-train" setting, we include English training data, so as to be comparable with Fang et al.</p><formula xml:id="formula_0">mBERT - - 89.1 - - 77.6 / 68.0 mT5-Small - - 83.4 - - 73.0 / 62.0 mT5-Base - - 85.4 - - 80.8 / 70.0 mT5-Large - - 88.4 - - 85.5 / 75.3 mT5-XL - - 90.9 - - 87.5 / 78.1 mT5-XXL - - 91.2 - - 88.5 / 79.1</formula><p>(2020) and <ref type="bibr" target="#b15">Luo et al. (2020)</ref>. This differs from the XTREME "translate-train" setup of Hu et al. <ref type="bibr">(2020)</ref>. For mT5 results on TyDi QA zero-shot, we report the median across five fine-tuning runs, as we observed high variance across runs. Full results for all languages in all tasks are provided in the appendix. et al., 2020) is an improved version of XLM based on the RoBERTa model <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>. XLM-R is trained with a cross-lingual masked language modeling objective on data in 100 languages from Common Crawl. To improve the pre-training data quality, pages from Common Crawl were filtered by an n-gram language model trained on Wikipedia . mBART <ref type="bibr" target="#b12">(Liu et al., 2020a)</ref> is a multilingual encoder-decoder model that is based on BART <ref type="bibr" target="#b10">(Lewis et al., 2020b)</ref>. mBART is trained with a combination of span masking and sentence shuffling objectives on a subset of 25 languages from the same data as XLM-R. MARGE ) is a multilingual encoderdecoder model that is trained to reconstruct a docu-ment in one language by retrieving documents in other languages. It uses data in 26 languages from Wikipedia and CC-News <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To validate the performance of mT5, we evaluate our models on 6 tasks from the XTREME multilingual benchmark <ref type="bibr">(Hu et al., 2020)</ref>: the XNLI (Conneau et al., 2018) entailment task covering 14 languages; the XQuAD <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref>, MLQA , and TyDi QA <ref type="bibr" target="#b5">(Clark et al., 2020)</ref> reading comprehension benchmarks with 10, 7, and 11 languages respectively; the Named Entity Recognition (NER) dataset of WikiAnn <ref type="bibr" target="#b22">(Pan et al., 2017)</ref> restricted to the 40 languages from XTREME <ref type="bibr">(Hu et al., 2020)</ref>, and the PAWS-X <ref type="bibr" target="#b32">(Yang et al., 2019)</ref> paraphrase identification dataset with 7 languages. We cast all tasks into the text-to-text format, i.e. generating the label text (XNLI and PAWS-X), entity tags and labels (WikiAnn NER), or answer (XQuAD, MLQA, and TyDi QA) directly in a generative fashion. For NER, if there are multiple entities, then they are concatenated in the order they appear, and if there are no entities then the target text is "None". We consider three variants of these tasks: (1) "zero-shot", where the model is fine-tuned only on English data, (2) "translate-train", adding machine translations from English into each target language, and (3) "inlanguage multitask", training on gold data in all target languages. For brevity, we refer to Hu et al.</p><p>(2020) for further details on these benchmarks.</p><p>Following the original T5 recipe, we consider five model sizes: Small (? 300M parameters), Base (580M), Large (1.2B), XL (3.7B), and XXL (13B). The increase in parameter counts compared to the corresponding T5 model variants comes from the larger vocabulary used in mT5. Note that, because mT5 is an encoder-decoder model, it has roughly twice as many parameters as correspondingly-sized encoder-only models such as XLM-R. For example, the "Large" variant of XLM-R has 550 million parameters whereas mT5-Large has around 1 billion. However, the computational cost for text classification is roughly the same: In both cases, the model processes a length-T input sequence with an encoder of approximately equal size. In an encoder-only model like XLM-R, the encoder processes one additional "CLS" token, which is used to generate the representation for classification. In mT5, the decoder typically produces two additional tokens: the class label and an endof-sequence token. Since the decoder has the same architecture (ignoring encoder-decoder attention) as the encoder, the computational cost of classification with mT5 typically amounts to the cost of processing T + 2 tokens compared to T + 1 for an encoder-only model. However, encoder-decoder architectures have the additional benefit of being applicable to generative tasks like abstractive summarization or dialog.</p><p>We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total. This is the same amount of pretraining as T5 and about 1 6 as much as XLM-R.</p><p>We use the same inverse square-root learning rate schedule used by T5 during pre-training, with the learning rate set to 1/ max(n, k) where n is the current training iteration and k = 10 4 is the number of warm-up steps. Following the T5.1.1 recipe, we do not apply dropout during pre-training. We use the same self-supervised objective as T5, with 15% of tokens masked and an average noise span length of 3. We ablate some of these experimental details in section 4.2.</p><p>For fine-tuning, we use a constant learning rate of 0.001 and dropout rate of 0.1 for all tasks. We use batch size 2 17 for most tasks but increased this up to 2 20 in a few cases based on performance on the validation set. For early stopping, we save checkpoints every 200 steps and choose the checkpoint with the highest validation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Table 2 presents our main results, with perlanguage breakdowns for each task given in the appendix. Our largest model mT5-XXL exceeds state-of-the-art on all classification and QA tasks and is near SOTA on NER (69.2 vs. 70.1). Note that unlike our model, InfoXLM <ref type="bibr" target="#b3">(Chi et al., 2020)</ref> and VECO <ref type="bibr" target="#b15">(Luo et al., 2020)</ref> benefit from parallel training data, while X-STILTs <ref type="bibr" target="#b23">(Phang et al., 2020)</ref> leverages labeled data from tasks similar to the target task. Overall, our results highlight the importance of model capacity in cross-lingual representation learning and suggest that scaling up a simple pre-training recipe can be a viable alternative to more complex techniques relying on LM filtering, parallel data, or intermediate tasks.</p><p>In the "translate-train" setting, we exceed stateof-the-art on all XTREME classification and QA tasks. For these tasks, we fine-tune on the combination of the labeled English data and machine translations thereof. 6 This allows direct comparison with both FILTER (Fang et al., 2020) as well as the XLM-R baseline of Fang et al. <ref type="bibr">(2020)</ref>. Note that this setup differs from XTREME "translate-train" <ref type="bibr">(Hu et al., 2020)</ref>, which excludes English. <ref type="figure">Figure 2</ref> shows that model capacity is key to improving performance on variants of the TyDi QA GoldP task in the absence of "gold" multilingual data: For the smallest model, training on gold datasets (in-language multitask) achieves dra-  matically better performance than using weakly supervised data (translate-train) or English-only data (zero-shot), whereas the gap between these three settings is much smaller for the largest model. For our two largest models, zero-shot and translatetrain performance is nearly the same, showing that machine translations of the monolingual dataset bring diminishing returns as model capacity increases. Overall, these trends point to the possibility of avoiding the costly step of annotating data in more than one language when using large models. Massively multilingual models have been observed to underperform on a given language when compared to a similarly-sized "dedicated" model trained specifically for that language <ref type="bibr" target="#b0">(Arivazhagan et al., 2019)</ref>. To quantify this effect, we compare the performance of mT5 and T5 when fine-tuned on the SQuAD reading comprehension benchmark <ref type="bibr" target="#b26">(Rajpurkar et al., 2016)</ref>. The results are shown in table 3, with results for T5 reproduced from . While the Small and Base mT5 models fall short of their English T5 counterparts, we find that the larger models close the gap. This suggests there may be a turning point past which the  model has enough capacity to effectively learn 101 languages without significant interference effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation</head><p>We run six ablations, modifying various settings, using our Large model as a baseline: (i) increase dropout to 0.1 in hopes of mitigating overfitting on low-resource languages, (ii) decrease sequence length to 512 (as was used in T5), (iii) increase the average noise span length in the pre-training objective to 10 since we observe fewer characters per token than T5, (iv) adjust the language sampling exponent ? to {0.2, 0.7} as used in MMNMT (Arivazhagan et al., 2019) and mBERT (Devlin, 2018), respectively, (v) turn off the "line length filter" in the mC4 data pipeline, and (vi) supplement mC4 with Wikipedia data 7 from 103 languages. The effect of these ablations on XNLI zero-shot accuracy is shown in table 4. In each case, the average XNLI score is lower than the mT5-Large baseline, justifying our chosen settings. The line length filter provides a +2 point boost, corroborating the findings of  and  that filtering low-quality pages from Common Crawl is valuable. Increasing the language sampling exponent ? to 0.7 has the expected effect of improving performance in highresource languages (e.g. Russian 81.5 ? 82.8), while hurting low-resource languages (e.g. Swahili 75.4 ? 70.6), with the average effect being negative. Conversely, lowering ? to 0.2 boosts one tail language slightly (Urdu 73.5 ? 73.9) but is harmful elsewhere. Detailed per-language metrics on XNLI and the results of our ablations on zero-shot XQuAD are provided in the appendix, showing similar trends.</p><p>Since mT5 is a generative model, it can output arbitrary text predictions in a free form fashion. This is in contrast to "encoder-only" models like mBERT and XLM(-R) that make a prediction by either extracting it from the input or producing a class label. We found that the lack of constraints during prediction caused mT5 to sometimes have trouble generating a well-formed prediction in a language unseen during fine-tuning. Focusing on XQuAD zero-shot, we find that many of these errors are due to "accidental translation" into the fine-tuning language (English). In this section, we characterize this behavior and demonstrate that it can be counteracted by mixing a small amount of our multilingual pre-training task into the fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Illegal predictions</head><p>In using a generative model for span selection (as in extractive QA tasks), we hope the model learns to generate "legal" spans that are substrings of the provided context. However, unlike encoder-based models like BERT, this is not a hard constraint of the model. Notably, T5 learns to always output legal spans on SQuAD, suggesting this is not a major issue for generative models in simple cases.</p><p>A more challenging case for generative models is zero-shot cross-lingual span selection. Here, a pretrained multilingual model is fine-tuned on English but tested on other languages. We want the model to generate legal non-English predictions despite having only seen English targets in fine-tuning.</p><p>In practice, while mT5 achieves SOTA on the zero-shot variants of XQuAD, MLQA and TyDi QA, illegal predictions are still a problem. For example, on zero-shot XQuAD, a non-trivial portion of mT5 mistakes are in fact illegal spans, for all model sizes (cf. <ref type="figure" target="#fig_2">fig. 4</ref> "Baseline"). Through inspection, we find these illegal predictions mainly fall into three categories: (i) normalization, (ii) grammatical adjustment, and (iii) accidental translation. <ref type="table" target="#tab_10">Table 5</ref> provides examples of each type.</p><p>Normalization indicates predictions that would be legal, except that "equivalent" Unicode characters have been substituted, so a legal span may be recovered through Unicode NFKC normalization. This is particularly common in Thai, Chinese and Hindi, where most mT5-XXL illegal predictions are resolved by normalization, as seen in <ref type="figure" target="#fig_1">fig. 3b</ref>.</p><p>Grammatical adjustment involves minor morphological changes to the original text. We fre-  quently observe these adjustments when the target span cannot stand as a well-formed answer on its own. For example, mT5-XXL's Arabic and Russian predictions in the middle rows of table 5 are judged by native speakers as correct and grammatical answers to the posed XQuAD questions, while the gold targets are judged as ungrammatical answers. This type of illegal prediction is most common in languages with extensive grammatical case marking, such as Russian, Turkish and German.</p><p>Accidental translation involves the model translating part or all of a contextual span into English (the language of all fine-tuning data). On the one hand, it is remarkable that mT5 performs "spontaneous" translation despite never seeing parallel training data. On the other, as practitioners we would ideally be able to control this behavior.</p><p>We observe accidental translation across all model sizes and all XQuAD languages. The problem is most prevalent in mT5-Small and mT5-Base, where from manual inspection, half or more of the illegal predictions within each language exhibit accidental translation, with many of the illegal predictions coming from Greek and Russian, as shown in <ref type="figure" target="#fig_1">fig. 3a</ref>. While we do observe full phrase translations, a more common occurrence is partial translation, where the model outputs a token or two of English before reverting to the correct target language. The transition may even occur mid-word, as in the prediction "chlor??????", where the first half of the target "??????????" (Russian: chloroplast) has been translated to English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Preventing accidental translation</head><p>The most direct solution to avoiding accidental translation on span selection tasks would be to mod- ify our inference procedure. As is common practice with encoder-based models, we could devise a taskspecific fine-tuning mechanism that restricts the model to perform ranking over legal spans, removing the possibility of illegal predictions entirely. While this would likely improve our zero-shot metrics, it is unsatisfying for two reasons: First, it implies taking a step backward from the general text-to-text interface, as different tasks would demand different types of inference. Second, this solution won't extend to more "open-ended" zeroshot generative tasks like summarization, where the legal output space can't be easily delimited.</p><p>For these reasons, we consider a more general solution that remains within the text-to-text framework and can apply to all zero-shot generation tasks. Our motivating intuition is that the reason the model outputs English when given a non-English test input is that it has never observed a non-English target during fine-tuning. As English-only finetuning proceeds, the model's assigned likelihood of non-English tokens presumably decreases, eventually reaching the point where English becomes the most likely answer to any question.</p><p>To prevent the model from "forgetting" how to generate other languages, we use a strategy inspired by domain/task-adaptive pre-training (Howard and <ref type="bibr">Ruder, 2018;</ref><ref type="bibr">Gururangan et al., 2020)</ref>: We simply mix in our unsupervised multilingual pre-training task during fine-tuning. A similar approach was explored by . We use the same mC4 task definition as in pre-training, with two adjustments: First, we remove all "sentinel" tokens (corresponding to non-masked spans in the input text) from the target sequence, as otherwise we observe occasional sentinels in downstream predictions. Second, we reduce the language sampling parameter ? from 0.3 to 0.1. This produces a nearuniform distribution of languages, encouraging the model to treat all languages as equally likely. <ref type="bibr">8</ref> With these changes, we mix a small amount of our unsupervised task (covering 101 languages) into XQuAD fine-tuning, at a ratio of just 1:100. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results on XQuAD zero-shot error rates. The addition of even this small amount of multilingual data has a marked effect on the mT5-Small and mT5-Base models (where accidental translation was most rampant), reducing the illegal prediction rates by more than 70% (relative), and contributing to an overall reduction in errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced mT5 and mC4: massively multilingual variants of the T5 model and C4 dataset. We demonstrated that the T5 recipe is straightforwardly applicable to the multilingual setting, and achieved strong performance on a diverse set of benchmarks. We also characterized illegal predictions that can occur in zero-shot evaluation of multilingual pre-trained generative models, and described a simple technique to avoid this issue. We release all code and pre-trained datasets used in this paper to facilitate future work on multilingual language understanding. 9  <ref type="table">Table 6</ref>: Statistics of the mC4 corpus, totaling 6.6B pages and 6.3T tokens. The "mT5" column indicates the percentage of mT5 training data coming from a given language, using the default exponential smoothing value of ?=0.3. We list 107 "languages" as detected by cld3, but note six of these (marked "Latin") are just Romanized variants of existing languages.         </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2: Results on XTREME sentence-pair classification, structured prediction and question answering tasks. mBERT metrics are from Hu et al. (2020). Metrics for XLM, InfoXLM, X-STILTs and XLM-R are from Fang et al. (2020), though Conneau et al. (2020) report better performance of XLM-R on XNLI (80.9). All other metrics are from the original sources: FILTER (Fang et al., 2020), VECO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Per-language error rates on XQuAD zeroshot, sorted by illegal rate. Incorrect: Not matching the target span. Illegal: Missing from the input context. Illegal after norm: Illegal even after Unicode NFKC normalization is applied to the prediction and context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Error rates of mT5 on XQuAD zero-shot. Baseline: Fine-tuning on XQuAD alone. Domain Preserving Training (DPT): Mixing in the unsupervised mC4 task with fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 1: Page counts per language in mC4 (left axis), and percentage of mT5 training examples coming from each language, for different language sampling exponents ? (right axis). Our final model uses ?=0.3.</figDesc><table><row><cell>Pages of mC4 training text</cell><cell>10 5 10 6 10 7 10 8 10 9</cell><cell></cell><cell></cell><cell>=0.2 =0.3 =0.7</cell><cell>0.008 0.04 0.2 1 5 25 % of mT5 training examples</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>km ru-Latn sd gu hi-Latn jv zu si ja-Latn eo co ga el-Latn zh-Latn pa ceb mg ps sn gd ku hmn su ht ha ny am bg-Latn yi lo mi sm ig haw xh st yo</cell></row><row><cell></cell><cell>Model</cell><cell>Architecture</cell><cell>Parameters</cell><cell># languages Data source</cell></row><row><cell></cell><cell cols="2">mBERT (Devlin, 2018) XLM (Conneau and Lample, 2019) Encoder-only Encoder-only XLM-R</cell><cell>180M 570M</cell><cell>104 100</cell><cell>Wikipedia Wikipedia</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of mT5 to existing massively multilingual pre-trained language models. Multiple versions of XLM and mBERT exist; we refer here to the ones that cover the most languages. Note that XLM-R counts five Romanized variants as separate languages, while we ignore six Romanized variants in the mT5 language count.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison of T5 vs. mT5 on SQuAD question answering (F1/EM).</figDesc><table><row><cell></cell><cell>90</cell></row><row><cell></cell><cell>80</cell></row><row><cell></cell><cell>70</cell></row><row><cell>F1</cell><cell>60</cell></row><row><cell></cell><cell>40 50</cell><cell>Human In-Language Multitask Translate-Train Zero-Shot</cell></row><row><cell></cell><cell></cell><cell>10 9 # Parameters</cell><cell>10 10</cell></row><row><cell cols="3">Figure 2: Average F1 on the TyDi QA GoldP task across languages. Performance improves with increas-ing model capacity. The importance of in-language training data (whether gold In-Lanugage Multitask or synthetic Translate-Train) decreases with model scale, as seen by Zero-Shot closing the quality gap.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Average XNLI zero-shot accuracy of various ablations on our mT5-Large model. Per-language metrics are shown in the appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Illegal mT5-XXL predictions on XQuAD zero-shot, illustrating normalization (top), grammatical ad-justment (middle) and translation (bottom).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium. Association for Computational Linguistics. Mihir Kale. 2020. Text-to-text pre-training for data-totext tasks. arXiv preprint arXiv:2005.10433. Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Unifying question answering and text classification via span extraction. arXiv preprint arXiv:1904.09286.</figDesc><table><row><cell>ISO Code Language</cell><cell cols="2">Tokens Pages mT5 (B) (M) (%) Code Language ISO</cell><cell>Tokens Pages mT5 (B) (M) (%)</cell></row><row><cell cols="2">Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Han-naneh Hajishirzi. 2020. UnifiedQA: Crossing for-mat boundaries with a single QA system. In Find-ings of the Association for Computational Linguis-tics: EMNLP 2020, pages 1896-1907, Online. As-sociation for Computational Linguistics. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple sub-word candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 66-75, Mel-bourne, Australia. Association for Computational Linguistics. Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tok-enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics. en English 2,733 3,067 5.67 ru Russian 713 756 3.71 es Spanish 433 416 3.09 de German 347 397 3.05 fr French 318 333 2.89 it Italian 162 186 2.43 pt Portuguese 146 169 2.36 pl Polish 130 126 2.15 nl Dutch 73 96 1.98 tr Turkish 71 88 1.93 ja Japanese 164 87 1.92 vi Vietnamese 116 79 1.87 id Indonesian 69 70 1.80 cs Czech 63 60 1.72 zh Chinese 39 55 1.67 fa Persian 52 54 1.67 ar Arabic 57 53 1.66 sv Swedish 45 49 1.61 ro Romanian 52 46 1.58 el Greek 43 42 1.54 uk Ukrainian 41 39 1.51 hu Hungarian 39 37 1.48 da Danish 29 29 1.38 fi Finnish 25 27 1.35 no Norwegian 27 25 1.33 bg Bulgarian 22 23 1.29 hi Hindi 24 19 1.21 sk Slovak 18 18 1.19 ko Korean 26 16 1.14 th Thai 11 15 1.14 ca Catalan 13 14 1.12 ms Malay 13 13 1.09 iw Hebrew 17 12 1.06 lt Lithuanian 11 11 1.04 sl Slovenian 8.8 8.5 0.95 mr Marathi 14 7.8 0.93 bn Bengali 7.3 7.4 0.91 et Estonian 6.9 6.9 0.89 lv Latvian 7.0 6.4 0.87 az Azerbaijani 4.4 5.3 0.82 gl Galician 2.4 4.6 0.79 cy Welsh 4.9 4.1 0.76 sq Albanian 4.0 4.1 0.76 ta Tamil 3.4 3.5 0.73 sr Serbian 4.3 3.4 0.72 ne Nepali 3.2 2.9 0.69 lb Luxembourgish 1.0 2.7 0.68 hy Armenian 2.4 2.4 0.65 kk Kazakh 3.1 2.4 0.65 ka Georgian 2.5 2.3 0.64 mt Maltese 5.2 2.3 0.64 af Afrikaans 1.7 2.2 0.63 fil Filipino 2.1 2.1 0.62 is Icelandic 2.6 2.1 0.62</cell><cell cols="2">David Crystal. 2008. Two thousand million? English today, 24(1):3-6. Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. 2019. BERTje: A dutch BERT model. arXiv preprint arXiv:1912.09582. Pieter Delobelle, Thomas Winters, and Bettina Berendt. 2020. RobBERT: a dutch RoBERTa-based language model. arXiv preprint arXiv:2001.06286. Jacob Devlin. 2018. Multilingual BERT README. https://github.com/ google-research/bert/blob/master/ multilingual.md. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. Yuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun, and Jingjing Liu. 2020. FILTER: An enhanced fu-sion method for cross-lingual language understand-ing. arXiv preprint arXiv:2009.05166. Suchin Gururangan, Ana Marasovi?, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics. ization. arXiv preprint arXiv:2003.11080. task benchmark for evaluating cross-lingual general-2020. XTREME: A massively multilingual multi-ham Neubig, Orhan Firat, and Melvin Johnson. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-mk Macedonian 1.8 2.1 0.62 ml Malayalam 1.8 2.1 0.62 mn Mongolian 2.7 2.1 0.62 ur Urdu 2.4 1.9 0.61 be Belarusian 2.0 1.7 0.59 la Latin 1.3 1.7 0.58 eu Basque 1.4 1.6 0.57 tg Tajik 1.4 1.3 0.54 te Telugu 1.3 1.2 0.52 fy West Frisian 0.4 1.1 0.51 kn Kannada 1.1 1.1 0.51 ky Kyrgyz 1.0 1.0 0.50 sw Swahili 1.0 1.0 0.50 so Somali 1.4 0.9 0.48 my Burmese 0.9 0.8 0.47 uz Uzbek 0.9 0.8 0.46 km Khmer 0.6 0.8 0.46 -Russian (Latin) 0.9 0.7 0.46 sd Sindhi 1.6 0.7 0.45 gu Gujarati 0.8 0.6 0.43 -Hindi (Latin) 0.6 0.6 0.43 jv Javanese 0.3 0.6 0.42 zu Zulu 0.2 0.6 0.42 si Sinhala 0.8 0.5 0.41 -Japanese (Latin) 0.3 0.5 0.41 eo Esperanto 0.7 0.5 0.40 co Corsican 0.2 0.5 0.40 ga Irish 0.5 0.5 0.40 -Greek (Latin) 0.4 0.4 0.39 -Chinese (Latin) 0.2 0.4 0.37 pa Punjabi 0.6 0.4 0.37 ceb Cebuano 0.2 0.4 0.36 mg Malagasy 0.2 0.3 0.36 ps Pashto 0.4 0.3 0.36 sn Shona 0.2 0.3 0.35 gd Scottish Gaelic 0.4 0.3 0.35 ku Kurdish 0.4 0.3 0.34 hmn Hmong 0.2 0.3 0.34 su Sundanese 0.1 0.3 0.34 ht Haitian Creole 0.2 0.3 0.33 ha Hausa 0.2 0.2 0.33 ny Chichewa 0.1 0.2 0.29 am Amharic 0.3 0.2 0.29 -Bulgarian (Latin) 0.09 0.2 0.29 yi Yiddish 0.3 0.1 0.28 lo Lao 0.1 0.1 0.28 mi Maori 0.1 0.1 0.25 sm Samoan 0.09 0.1 0.25 ig Igbo 0.09 0.09 0.24 haw Hawaiian 0.09 0.08 0.24 xh Xhosa 0.06 0.07 0.22 st Sotho 0.08 0.07 0.22 yo Yoruba 0.05 0.05 0.20</cell></row><row><cell></cell><cell></cell><cell cols="2">Gautier Izacard and Edouard Grave. 2020. Lever-aging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>XNLI accuracy scores for each language.</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>avg</cell></row><row><cell cols="7">Cross-lingual zero-shot transfer (models fine-tune on English data only)</cell><cell></cell><cell></cell></row><row><cell>mBERT XLM XLM-R mT5-Small mT5-Base mT5-Large mT5-XL mT5-XXL</cell><cell>94.0 94.0 94.7 92.2 95.4 96.1 96.0 96.3</cell><cell>85.7 85.9 89.7 86.2 89.4 91.3 92.8 92.9</cell><cell>87.4 88.3 90.1 86.1 89.6 92.0 92.7 92.6</cell><cell>87.0 87.4 90.4 86.6 91.2 92.7 92.4 92.7</cell><cell>73.0 69.3 78.7 74.7 79.8 82.5 83.6 84.5</cell><cell>69.6 64.8 79.0 73.5 78.5 82.7 83.1 83.9</cell><cell>77.0 76.5 82.3 77.9 81.1 84.7 86.5 87.2</cell><cell>81.9 80.9 86.4 82.4 86.4 88.9 89.6 90.0</cell></row><row><cell cols="9">Translate-train (models fine-tune on English training data plus translations in all target languages)</cell></row><row><cell>mT5-Small mT5-Base mT5-Large mT5-XL mT5-XXL</cell><cell>87.9 95.5 96.4 96.4 96.1</cell><cell>81.4 90.9 92.7 92.5 92.9</cell><cell>83.1 91.4 93.3 93.1 93.6</cell><cell>84.1 92.5 93.6 93.6 94.2</cell><cell>74.2 83.6 86.5 85.5 87.0</cell><cell>71.7 84.8 87.4 86.9 87.9</cell><cell>76.7 86.4 88.4 89.0 89.0</cell><cell>79.9 89.3 91.2 91.0 91.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>PAWS-X accuracy scores for each language.Cross-lingual zero-shot transfer (models fine-tune on English data only) mBERT 77.4 41.1 77.0 70.0 78.0 72.5 85.2 77.4 75.4 66.3 46.2 77.2 79.6 56.6 65.0 76.4 53.5 81.5 29.0 66.4 XLM 74.9 44.8 76.7 70.0 78.1 73.5 82.6 74.8 74.8 62.3 49.2 79.6 78.5 57.7 66.1 76.5 53.1 80.7 23.6 63.0 XLM-R 78.9 53.0 81.4 78.8 78.8 79.5 84.7 79.6 79.1 60.9 61.9 79.2 80.5 56.8 73.0 79.8 53.0 81.3 23.</figDesc><table><row><cell>Model</cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell><cell>ja</cell><cell>jv</cell></row><row><cell cols="21">2 62.5 mT5-Small 67.4 36.6 64.6 60.4 66.1 59.1 80.7 63.6 58.4 42.3 25.3 64.5 74.6 39.6 57.9 61.5 46.7 73.4 28.8 50.6 mT5-Base 73.8 48.4 68.2 67.1 72.5 63.5 83.2 71.7 67.3 49.2 31.9 68.6 78.6 47.4 67.6 64.7 49.7 78.9 35.3 56.9 mT5-Large 74.7 55.0 60.6 64.5 75.2 68.2 84.2 74.2 67.0 48.7 51.4 66.4 82.4 55.8 69.0 67.3 51.1 80.7 43.0 57.1 mT5-XL 79.8 60.2 81.0 78.1 80.6 78.3 86.3 74.7 71.8 52.2 61.5 70.1 86.2 65.5 76.5 71.9 56.8 83.3 48.0 64.5 mT5-XXL 80.4 66.2 85.1 79.3 81.7 79.0 86.7 86.0 73.5 57.6 58.8 70.4 86.8 65.1 77.8 74.2 73.5 85.8 50.7 66.4</cell></row><row><cell></cell><cell>ka</cell><cell>kk</cell><cell>ko</cell><cell>ml</cell><cell>mr</cell><cell>ms</cell><cell>my</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>sw</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>yo</cell><cell>zh</cell><cell>avg</cell></row><row><cell cols="22">mBERT XLM XLM-R mT5-Small 53.2 23.4 26.6 39.4 39.4 70.0 30.1 75.4 70.8 46.5 54.8 37.5 32.6 64.6 45.8 59.6 52.3 58.2 72.7 45.2 81.8 80.8 64.0 67.5 50.7 48.5 67.7 57.2 26.3 59.4 62.4 69.6 47.6 81.2 77.9 63.5 68.4 53.6 49.6 71.6 56.2 60.0 67.8 68.1 57.1 54.3 84.0 81.9 69.1 70.5 59.5 55.8 mT5-Base 50.1 23.4 33.9 48.2 43.8 72.6 37.0 80.1 76.0 55.4 62.4 41.2 42.7 mT5-Large 58.2 23.3 36.2 46.3 46.5 69.4 32.2 82.7 79.6 50.2 72.4 46.4 44.5 10.5 79.0 65.1 44.2 77.1 48.4 44.0 3.6 71.7 71.8 36.9 71.8 44.9 42.7 0.3 78.6 71.0 43.0 70.1 26.5 32.4 1.3 73.2 76.1 56.4 79.4 33.6 33.1 7.2 69.4 56.0 26.4 63.8 58.8 37.9 9.5 74.6 58.4 38.4 73.0 59.3 41.5 mT5-XL 66.0 31.6 38.1 54.1 57.6 74.8 42.6 85.7 85.2 66.9 72.8 49.0 54.7 9.6 84.1 67.7 64.7 79.6 59.9 54.4. 65.7 62.2 61.2 65.4 51.0 56.6 58.8 mT5-XXL 66.0 38.7 43.5 54.5 63.1 77.6 44.7 87.7 86.9 72.0 72.9 56.5 59.5 10.4 85.2 71.4 80.7 84.6 70.0 56.8 69.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>WikiAnn NER F1 scores for each language.</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>ar</cell><cell>de</cell><cell>el</cell><cell>es</cell><cell>hi</cell><cell>ru</cell><cell>th</cell><cell>tr</cell><cell>vi</cell><cell>zh</cell><cell>avg</cell></row><row><cell cols="5">Cross-lingual zero-shot transfer (models fine-tune on English data only)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">mBERT XLM XLM-R mT5-Small 78.5 / 66.1 51.4 / 34.0 63.8 / 45.9 53.8 / 33.4 67.0 / 50.3 47.8 / 34.5 50.5 / 30.1 54.0 / 44.5 55.7 / 38.9 58.1 / 41.3 58.9 / 48.7 58.1 / 42.5 83.5 / 72.2 61.5 / 45.1 70.6 / 54.0 62.6 / 44.9 75.5 / 56.9 59.2 / 46.0 71.3 / 53.3 42.7 / 33.5 55.4 / 40.1 69.5 / 49.6 58.0 / 48.3 64.5 / 49.4 74.2 / 62.1 61.4 / 44.7 66.0 / 49.7 57.5 / 39.1 68.2 / 49.8 56.6 / 40.3 65.3 / 48.2 35.4 / 24.5 57.9 / 41.2 65.8 / 47.6 49.7 / 39.7 59.8 / 44.3 86.5 / 75.7 68.6 / 49.0 80.4 / 63.4 79.8 / 61.7 82.0 / 63.9 76.7 / 59.7 80.1 / 64.3 74.2 / 62.8 75.9 / 59.3 79.1 / 59.0 59.3 / 50.0 76.6 / 60.8 mT5-Base 84.6 / 71.7 63.8 / 44.3 73.8 / 54.5 59.6 / 35.6 74.8 / 56.1 60.3 / 43.4 57.8 / 34.7 57.6 / 45.7 67.9 / 48.2 70.7 / 50.3 66.1 / 54.1 67.0 / 49.0 mT5-Large 88.4 / 77.3 75.2 / 56.7 80.0 / 62.9 77.5 / 57.6 81.8 / 64.2 73.4 / 56.6 74.7 / 56.9 73.4 / 62.0 76.5 / 56.3 79.4 / 60.3 75.9 / 65.5 77.8 / 61.5 mT5-XL 88.8 / 78.1 77.4 / 60.8 80.4 / 63.5 80.4 / 61.2 82.7 / 64.5 76.1 / 60.3 76.2 / 58.8 74.2 / 62.5 77.7 / 58.4 80.5 / 60.8 80.5 / 71.0 79.5 / 63.6 mT5-XXL 90.9 / 80.1 80.3 / 62.6 83.1 / 65.5 83.3 / 65.5 85.1 / 68.1 81.7 / 65.9 79.3 / 63.6 77.8 / 66.1 80.2 / 60.9 83.1 / 63.6 83.1 / 73.4 82.5 / 66.8</cell></row><row><cell cols="7">Translate-train (models fine-tune on English training data plus translations in all target languages)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">mT5-Small 74.0 / 61.2 61.0 / 45.0 66.0 / 50.2 64.1 / 47.2 67.5 / 50.8 60.2 / 43.7 64.4 / 46.7 58.9 / 52.9 59.0 / 39.4 63.5 / 46.0 68.2 / 61.2 64.3 / 49.5 mT5-Base 83.1 / 70.3 72.4 / 55.2 76.9 / 59.7 76.8 / 58.8 79.0 / 61.2 71.4 / 53.4 76.1 / 58.5 67.9 / 62.0 72.5 / 51.4 75.9 / 56.3 76.9 / 69.7 75.3 / 59.7 mT5-Large 87.3 / 75.5 79.4 / 62.7 82.7 / 66.0 81.8 / 63.5 83.8 / 66.1 78.0 / 59.8 81.9 / 66.3 74.7 / 68.2 80.2 / 59.2 80.4 / 60.8 83.2 / 76.9 81.2 / 65.9 mT5-XL 88.5 / 77.1 80.9 / 65.4 83.4 / 66.7 83.6 / 64.9 84.9 / 68.2 79.6 / 63.1 82.7 / 67.1 78.5 / 72.9 82.4 / 63.8 82.4 / 64.1 83.2 / 75.9 82.7 / 68.1 mT5-XXL 91.3 / 80.3 83.4 / 68.2 85.0 / 68.2 85.9 / 68.9 87.4 / 70.8 83.7 / 68.2 85.2 / 70.4 80.2 / 74.5 84.4 / 67.7 85.3 / 67.1 85.7 / 80.0 85.2 / 71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>XQuAD results (F1/EM) for each language.</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>ar</cell><cell>de</cell><cell>es</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell><cell>avg</cell></row><row><cell cols="5">Cross-lingual zero-shot transfer (models fine-tune on English data only)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">mBERT XLM XLM-R mT5-Small 77.2 / 63.0 44.7 / 27.3 53.3 / 35.7 60.1 / 41.5 43.0 / 29.2 52.9 / 33.2 51.3 / 29.7 54.6 / 37.1 80.2 / 67.0 52.3 / 34.6 59.0 / 43.8 67.4 / 49.2 50.2 / 35.3 61.2 / 40.7 59.6 / 38.6 61.4 / 44.2 68.6 / 55.2 42.5 / 25.2 50.8 / 37.2 54.7 / 37.9 34.4 / 21.1 48.3 / 30.2 40.5 / 21.9 48.5 / 32.6 83.5 / 70.6 66.6 / 47.1 70.1 / 54.9 74.1 / 56.6 70.6 / 53.1 74.0 / 52.9 62.1 / 37.0 71.6 / 53.2 mT5-Base 81.7 / 66.9 57.1 / 36.9 62.1 / 43.2 67.1 / 47.2 55.4 / 37.9 65.9 / 44.1 61.6 / 38.6 64.4 / 45.0 mT5-Large 84.9 / 70.7 65.3 / 44.6 68.9 / 51.8 73.5 / 54.1 66.9 / 47.7 72.5 / 50.7 66.2 / 42.0 71.2 / 51.7 mT5-XL 85.5 / 71.9 68.0 / 47.4 70.5 / 54.4 75.2 / 56.3 70.5 / 51.0 74.2 / 52.8 70.5 / 47.2 73.5 / 54.4 mT5-XXL 86.7 / 73.5 70.7 / 50.4 74.0 / 57.8 76.8 / 58.4 75.6 / 57.3 76.4 / 56.0 71.8 / 48.8 76.0 / 57.4</cell></row><row><cell cols="7">Translate-train (models fine-tune on English training data plus translations in all target languages)</cell><cell></cell><cell></cell></row><row><cell cols="9">mT5-Small 70.5 / 56.2 49.3 / 31.0 55.6 / 40.6 60.5 / 43.0 50.4 / 32.9 55.2 / 36.3 54.4 / 31.6 56.6 / 38.8 mT5-Base 80.7 / 66.3 61.1 / 40.7 65.5 / 49.2 70.7 / 52.1 63.6 / 44.3 68.0 / 47.6 63.5 / 39.4 67.6 / 48.5 mT5-Large 85.3 / 72.0 68.5 / 47.7 71.6 / 55.8 75.7 / 57.1 71.8 / 52.6 74.3 / 54.0 70.1 / 47.1 73.9 / 55.2 mT5-XL 86.0 / 73.0 70.0 / 49.8 72.7 / 56.8 76.9 / 58.3 73.4 / 55.0 75.4 / 55.0 71.4 / 48.4 75.1 / 56.6 mT5-XXL 86.5 / 73.5 71.7 / 51.4 74.9 / 58.7 78.8 / 60.3 76.6 / 58.5 77.1 / 56.3 72.5 / 49.8 76.9 / 58.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>MLQA results (F1/EM) for each language.Cross-lingual zero-shot transfer (models fine-tune on English data only)mBERT 75.3 / 63.6 62.2 / 42.8 49.3 / 32.7 59.7 / 45.3 64.8 / 45.8 58.8 / 50.0 60.0 / 38.8 57.5 / 37.9 49.6 / 38.4 59.7 / 43.9 XLM 66.9 / 53.9 59.4 / 41.2 27.2 / 15.0 58.2 / 41.4 62.5 / 45.Small 53.9 / 43.6 41.1 / 26.0 18.9 / 13.3 39.2 / 22.6 44.4 / 31.7 24.9 / 16.3 40.5 / 24.3 34.8 / 21.2 16.9 / 11.5 34.9 / 23.4 mT5-Base 71.8 / 60.9 67.1 / 50.4 40.7 / 22.1 67.0 / 52.2 71.3 / 54.5 49.5 / 37.7 54.9 / 32.6 60.4 / 43.9 40.6 / 31.1 58.1 / 42.8 mT5-Large 71.6 / 58.9 60.5 / 40.4 42.0 / 23.9 64.6 / 48.8 67.0 / 49.2 47.6 / 37.3 58.9 / 36.8 65.7 / 45.3 41.9 / 29.7 57.8 / 41.Translate-train (models fine-tune on English training data plus translations in all target languages) In-language multitask (models fine-tuned on gold data in all target languages) mT5-Small 66.4 / 56.1 80.3 / 68.7 71.7 / 60.2 71.9 / 59.5 78.8 / 67.6 55.5 / 46.7 70.1 / 57.1 77.7 / 68.9 82.7 / 71.6 73.0 / 62.0 mT5-Base 76.6 / 65.2 84.2 / 71.8 80.0 / 69.0 80.1 / 69.3 85.5 / 75.0 70.3 / 61.6 77.5 / 64.4 83.6 / 74.9 88.2 / 78.0 80.8 / 70.0 mT5-Large 82.4 / 70.9 87.1 / 75.1 86.3 / 78.8 85.5 / 73.4 87.3 / 77.9 79.1 / 69.9 84.3 / 71.3 87.4 / 79.6 90.2 / 81.2 85.5 / 75.3 mT5-XL 84.1 / 74.3 88.5 / 76.0 87.7 / 80.5 87.4 / 76.1 89.9 / 81.2 82.8 / 75.4 84.9 / 73.2 90.1 / 82.8 92.0 / 83.7 87.5 / 78.1 mT5-XXL 85.7 / 75.5 88.4 / 76.9 88.7 / 80.5 87.5 / 76.3 90.3 / 81.8 83.7 / 75.7 87.9 / 76.8 91.9 / 84.4 92.6 / 83.9 88.5 / 79.1</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>ar</cell><cell>bn</cell><cell>fi</cell><cell>id</cell><cell>ko</cell><cell>ru</cell><cell>sw</cell><cell>te</cell><cell>avg</cell></row><row><cell cols="11">8 71.5 / 56.8 67.6 / 40.4 64.0 / 47.8 70.5 / 53.2 77.4 / 61.9 31.9 / 10.9 67.0 / 42.1 66.1 / 48.1 70.1 / 43.6 65.1 / 45.0 14.2 / 5.1 49.2 / 30.7 39.4 / 21.6 15.5 / 6.9 43.6 / 29.1 mT5-2 XLM-R mT5-XL 80.3 / 70.9 81.7 / 65.5 74.5 / 57.5 79.4 / 65.3 83.5 / 70.4 70.0 / 60.5 71.6 / 47.8 77.3 / 59.7 77.9 / 55.8 77.4 / 61.5 mT5-XXL 83.7 / 72.5 82.8 / 66.0 80.2 / 63.7 83.3 / 70.2 85.3 / 73.3 76.2 / 64.1 76.6 / 55.8 81.9 / 66.1 79.2 / 58.7 81.0 / 65.6</cell></row><row><cell cols="11">mT5-Small 57.1 / 46.6 56.8 / 39.7 37.2 / 21.2 50.9 / 37.2 60.1 / 45.1 40.4 / 29.3 50.7 / 33.6 51.5 / 35.3 29.3 / 18.1 48.2 / 34.0 mT5-Base 71.1 / 58.9 68.0 / 50.2 57.4 / 35.4 68.8 / 55.2 73.5 / 57.2 56.5 / 43.8 64.0 / 45.8 65.8 / 48.3 51.2 / 34.1 64.0 / 47.7 mT5-Large 75.6 / 62.7 74.8 / 57.9 65.0 / 46.0 72.3 / 57.5 78.7 / 63.5 66.4 / 53.6 70.9 / 50.5 74.0 / 56.7 62.0 / 45.1 71.1 / 54.9 mT5-XL 82.0 / 65.7 79.3 / 65.5 80.4 / 68.9 79.1 / 64.7 84.7 / 71.0 70.5 / 56.2 78.3 / 61.1 83.9 / 70.9 80.9 / 64.0 79.9 / 65.3 mT5-XXL 83.3 / 71.6 83.0 / 66.3 82.3 / 70.8 82.9 / 67.8 86.6 / 72.0 75.0 / 62.3 80.7 / 63.1 86.9 / 75.8 84.6 / 69.2 82.8 / 68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>TyDi QA GoldP results (F1/EM) for each language. -large) 79.8 84.1 83.4 83.2 89.4 84.2 84.1 77.6 81.5 75.4 79.4 80.1 73.5 81.0 80.3 81.1 Dropout 0.1 76.4 82.1 81.7 81.0 88.0 70.8 80.3 74.4 79.0 72.3 75.8 75.9 70.6 78.6 76.5 77.6 Sequence length 512 78.1 83.4 83.1 82.1 88.8 84.5 82.8 77.3 81.2 75.4 78.2 79.6 73.8 80.0 78.9 80.5 Span length 10 77.6 81.5 80.5 81.2 87.2 83.0 81.2 74.7 79.8 73.6 76.7 75.9 71.3 78.6 76.5 78.6 ? = 0.7 79.3 84.1 84.5 83.1 89.4 85.3 84.4 76.4 82.8 70.6 78.7 79.8 71.7 80.3 79.9 80.7 ? = 0.2 78.7 83.8 83.3 82.5 89.3 83.4 83.6 77.3 81.2 75.4 78.6 79.4 73.9 79.9 79.7 80.7 No line length filter 78.4 83.3 81.5 81.4 88.9 83.8 82.5 74.4 80.5 69.4 77.6 76.9 71.3 78.8 78.3 79.1 Add Wikipedia data 79.3 83.1 83.1 82.7 88.6 80.1 83.2 77.3 81.4 75.0 78.9 79.3 73.5 80.2 79.2 80.3</figDesc><table><row><cell>Model</cell><cell>ar</cell><cell>bg</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>hi</cell><cell>ru</cell><cell>sw</cell><cell>th</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>zh</cell><cell>avg</cell></row><row><cell>Baseline (mT5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>XNLI zero-shot accuracy of various ablations on our mT5-Large model. / 76.0 54.9 / 33.9 77.6 / 60.2 64.4 / 40.1 79.2 / 60.6 59.1 / 40.4 59.5 / 38.4 65.7 / 51.0 73.6 / 52.8 75.8 / 55.8 77.0 / 64.5 70.4 / 52.1 Sequence length 512 88.0 / 76.9 77.0 / 59.6 80.2 / 62.4 79.8 / 60.0 81.7 / 64.4 75.1 / 57.5 77.4 / 58.5 72.7 / 59.8 75.3 / 53.9 79.4 / 58.9 78.5 / 67.2 78.6 / 61.7 ? = 0.7 88.4 / 77.1 76.5 / 58.8 78.5 / 59.8 77.2 / 55.5 78.7 / 59.5 74.6 / 56.8 73.1 / 54.5 72.5 / 60.2 75.7 / 55.0 79.2 / 58.3 78.6 / 66.2 77.5 / 60.2 ? = 0.2 87.9 / 76.8 75.5 / 57.3 80.2 / 62.4 76.2 / 54.0 81.6 / 63.7 73.7 / 57.0 70.7 / 50.8 72.2 / 60.4 75.5 / 55.7 79.7 / 59.7 78.3 / 67.5 77.4 / 60.5 No line length filter 88.9 / 77.4 73.8 / 54.0 80.8 / 62.7 74.2 / 51.8 80.9 / 62.8 74.1 / 56.6 75.0 / 56.4 71.7 / 60.3 76.7 / 56.0 78.8 / 58.6 78.5 / 67.1 77.6 / 60.3 Add Wikipedia data 89.3 / 78.4 69.6 / 48.9 79.6 / 61.1 59.5 / 36.0 80.6 / 61.0 73.6 / 55.0 68.7 / 47.0 70.5 / 58.1 76.7 / 56.9 78.6 / 56.4 77.5 / 66.3 74.9 / 56.8</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>ar</cell><cell>de</cell><cell>el</cell><cell>es</cell><cell>hi</cell><cell>ru</cell><cell>th</cell><cell>tr</cell><cell>vi</cell><cell>zh</cell><cell>avg</cell></row><row><cell cols="13">Baseline(mT5-large) 88.4 / 77.3 75.2 / 56.7 80.0 / 62.9 77.5 / 57.6 81.8 / 64.2 73.4 / 56.6 74.7 / 56.9 73.4 / 62.0 76.5 / 56.3 79.4 / 60.3 75.9 / 65.5 77.8 / 61.5 Span length 10 88.1 / 76.3 70.0 / 50.6 78.1 / 60.2 68.8 / 44.0 79.0 / 60.8 67.3 / 48.4 65.4 / 43.3 68.1 / 57.2 74.4 / 53.6 77.9 / 57.7 76.6 / 66.4 74.0 / 56.2 Dropout 0.1 87.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>XQuAD zero-shot F1/EM of various ablations on our mT5-Large model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use the translation data provided byHu et al. (2020)   throughout. On the PAWS-X task, FILTER used translation data from the original task instead. Switching to this data would improve our scores slightly (mT5-XXL 91.5 ? 92.0).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use the 2020 Wikipedia data from TensorFlow Datasets, selecting the same languages as mBERT. https://www.tensorflow.org/datasets/ catalog/wikipedia</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Alternatively, one could mix in unlabeled data only for a single language at a time. However, we believe this is contrary to the spirit of multilingual models and zero-shot evaluation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Melvin Johnson for tips on the translatetrain procedure for XTREME and Itai Rolnick for help with infrastructure.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diedre</forename><surname>Carmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Piau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Campiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Lotufo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09144</idno>
		<title level="m">Pretraining and validating the t5 model on brazilian portuguese data</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07834</idno>
		<title level="m">-foXLM: An information-theoretic framework for cross-lingual language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking embedding coupling in pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00317</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7059" to="7069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FlauBERT: Unsupervised language model pre-training for French</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibril</forename><surname>Frej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Segonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2479" to="2490" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15020</idno>
		<title level="m">Pre-training via paraphrasing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MLQA: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07475</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring fine-tuning techniques for pre-trained cross-lingual models via continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14218</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Veco: Variable encoder-decoder pre-training for cross-lingual understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16046</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Playing with words at the national library of sweden-making a swedish BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Malmsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Love</forename><surname>B?rjeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Haffenden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01658</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">?ric de la Clergerie, Djam? Seddah, and Beno?t Sagot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.645</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7203" to="7219" />
		</imprint>
	</monogr>
	<note>CamemBERT: a tasty French language model</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The natural language decathlon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
	</analytic>
	<monogr>
		<title level="m">Multitask learning as question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14546</idno>
		<title level="m">Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training text-to-text models to explain their predictions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PhoBERT: Pre-trained language models for Vietnamese</title>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<editor>Dat Quoc Nguyen and Anh Tuan Nguyen. 2020</editor>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1037" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">English intermediate-task training improves zeroshot cross-lingual transfer too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13013</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AlBERTo: Italian BERT language understanding model for NLP challenging tasks based on tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLiC-it</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-5004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">GLU variants improve transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3687" to="3692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
