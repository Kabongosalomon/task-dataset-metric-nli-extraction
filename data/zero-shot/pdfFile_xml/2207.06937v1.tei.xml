<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Streaming Video Denoising with Bidirectional Buffers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-10">2022. October 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junming</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Real-time Streaming Video Denoising with Bidirectional Buffers</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Reference Format</title>
						<meeting>the 30th ACM International Conference on Multimedia (MM &apos;22), October MM &apos;22 <address><addrLine>Lisboa, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2022-10-10">2022. October 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3547934</idno>
					<note>10-14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 13 pages. https: //doi.org/10.1145/3503161.3547934 * Both authors contributed equally to this research. 10 2 Runtime (s)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Denoising</term>
					<term>Efficient Inference</term>
					<term>Online Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video streams are delivered continuously to save the cost of storage and device memory. Real-time denoising algorithms are typically adopted on the user device to remove the noise involved during the shooting and transmission of video streams. However, slidingwindow-based methods feed multiple input frames for a single output and lack computation efficiency. Recent multi-output inference works propagate the bidirectional temporal feature with a parallel or recurrent framework, which either suffers from performance drops on the temporal edges of clips or can not achieve online inference. In this paper, we propose a Bidirectional Streaming Video Denoising (BSVD) framework, to achieve high-fidelity real-time denoising for streaming videos with both past and future temporal receptive fields. The bidirectional temporal fusion for online inference is considered not applicable in the MoViNet. However, we introduce a novel Bidirectional Buffer Block as the core module of our BSVD, which makes it possible during our pipeline-style inference. In addition, our method is concise and flexible to be utilized in both non-blind and blind video denoising. We compare our model with various state-of-the-art video denoising models qualitatively and quantitatively on synthetic and real noise. Our method outperforms previous methods in terms of restoration fidelity and runtime. Our source code is publicly available at https://github.com/ChenyangQiQi/BSVD</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Computational photography.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our BSVD-64 outperforms stateof-the-art PaCNet <ref type="bibr" target="#b23">[24]</ref> with 700? speedup. It takes BSVD-64 33.7ms per frame to process a video with resolution of 960 ? 540. Green: CPU method. Blue: Learning-based GPU method. Red: Ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the explosive growth of social media, there is an increasing need for video streaming applications on the user's device, such as live streaming on YouTube, TikTok, and video meetings on Zoom. Since the noise from video capturing and compression degrades the quality of videos, noise reduction plays a vital role in improving video fidelity. Although offline video processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> has been an active research topic in recent years, few works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> are feasible for online streaming video denoising, which is more challenging. Streaming video denoising is featured in the following two points compared with offline video denoising. First, since streaming video denoising is primarily applied to sports games and other live streaming media, a real-time inference speed should be satisfied. Second, the frames should be processed online in a continuous frame-by-frame way because there is no explicit end for streaming video. To fulfill the above requirements, we propose a real-time high-quality streaming denoising framework.</p><p>To the best of our knowledge, the existing works can be categorized into three classes: sliding-window-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> ( <ref type="figure">Fig. 2(a)</ref>), recurrent methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>  <ref type="figure">(Fig. 2(b,c)</ref>) and multi-input multi-output (MIMO) methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>  <ref type="figure">(Fig. 2(d)</ref>). The slidingwindow-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>  <ref type="figure">(Figure 2</ref>(a)) restore each frame by feeding the degraded one with its neighbours. However, such sliding-window strategy has unnecessary, redundant computation: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional Fusion</head><p>Clip-edge Fidelity <ref type="figure">Figure 2</ref>: The comparison of computation graph and complexity for different methods. For an input clip with length , we assume all methods use same convolution blocks for temporal fusion. Blue, green, and red features represent the past, present, and future features from three adjacent frames. Solid features are cached in the GPU memory, while dotted features have been deleted. Compared with sliding-window methods (a), our inference time is shorter. For unidirectional-RNN (b), our framework utilize bidirectional temporal fusion, and achieve better fidelity. In addition, bidirectional-RNN (c) and MIMO (d) framework suffer from ( ) memory and fidelity degradation on the clip edges, which is solved in our inference framework.</p><p>each frame is fed into the network multiple times. For instance, to restore a video with resolution of 960 ? 540, it takes PaCNet <ref type="bibr" target="#b23">[24]</ref> about 24 seconds per frame, which impedes this method to realtime application. Meanwhile, such methods only fuse the temporal information in pixel-level <ref type="bibr" target="#b22">[23]</ref> or neighbor patch-level <ref type="bibr" target="#b23">[24]</ref>, while the intermediate multi-scale features are also important for the denoising performance, as shown in (Sec. 5.2). Different from sliding-window-based methods, recurrent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> use previous reconstructed results as a reference for next restoration. Such recurrent methods can be further divided into unidirectional <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>  <ref type="figure">(Fig. 2(b)</ref>) and bidirectional <ref type="bibr" target="#b2">[3]</ref>  <ref type="figure">(Fig. 2(c)</ref>) propagation. Although unidirectional recurrent methods can exploit past information for streaming processing, their denoising performance degrades by ignoring future information. To utilize both past and future information, bidirectional recurrent methods are proposed to propagate the temporal information in both forward and backward directions. For best results from a bidirectional recurrent network, users must prepare the entire video in advance, which means they cannot realize streaming video denoising <ref type="bibr" target="#b2">[3]</ref>.</p><p>Most recently, MIMO frameworks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref> have been introduced for efficient processing a clip of video frames in one-forwardpass( <ref type="figure">Fig. 2(d)</ref>). The inference schema can be implemented as a channel shift <ref type="bibr" target="#b12">[13]</ref> or a local window <ref type="bibr" target="#b9">[10]</ref> in temporal dimension. However, with the growth of clip size , the inference-time memory consumption also increases linearly in O . Thus, such methods suffer from heavy memory consumption <ref type="bibr" target="#b9">[10]</ref>, and the long input video must be segmented into short clips. Moreover, similar to bidirectional recurrent methods <ref type="bibr" target="#b2">[3]</ref>, MIMO also has performance degradation on the edges of clips (Sec. 5.1).</p><p>In this work, we propose the Bidirectional Streaming Video Denoising framework (BSVD). Bidirectional temporal fusion is critical for low-level video processing since it fully utilizes the information of both past and future frames, which is demonstrated in our experiment (Sec. 5.2). Thus, we train our model as a MIMO framework using bidirectional Temporal Shift Modules (TSM) <ref type="bibr" target="#b12">[13]</ref>, which is also more efficient than sliding-window. To address the clip-edges-drop problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> and enable the bidirectional fusion for streaming videos, which is regarded as not applicable in previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>, we propose a Bidirectional Buffer Block that can cache and reuse the features for both past and future propagation during pipeline-style inference. Thus, our BSVD featured with the pipeline-style buffer block can achieve a constant memory complexity of O (1), in contrast with O in other MIMO methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. Extensive evaluations on datasets with synthetic and real noise show that the proposed method outperforms previous methods in denoising performance and runtime.</p><p>Our contributions can be summarized as follows: (a) We propose a pipeline-style buffer-based video denoising framework, named BSVD, which processes the video streams of resolution 960 ? 540 in real-time.</p><p>(b) The proposed novel Bidirectional Buffer Block and pipelinestyle inference framework enable bidirectional temporal fusion for online streaming video processing.</p><p>(c) In addition, we address the fidelity degradation on the clip edges in the MIMO framework <ref type="bibr" target="#b2">[3]</ref> by utilizing buffered features. Our proposed computation graph is general and applicable for pre-trained checkpoints of the existing method FastDVDNet <ref type="bibr" target="#b22">[23]</ref>.</p><p>(d) Our succinct BSVD framework achieves SOTA performance on Gaussian noise <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, real noise <ref type="bibr" target="#b14">[15]</ref>, and blind denoising <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> provides a comparison of PSNR and runtime on DAVIS test set with noise level = 50. BSVD outperforms the previous most efficient baseline FastDVDnet <ref type="bibr" target="#b22">[23]</ref> with more than 2? speedup and improves average PSNR by 0.57dB on DAVIS dataset. Moreover, it surpasses the best fidelity work PaCNet <ref type="bibr" target="#b23">[24]</ref> while being 700? faster.   <ref type="figure">Figure 4</ref>: An overview of our framework. The backbone of our network is two light-weight U-Nets <ref type="bibr" target="#b19">[20]</ref> with temporal fusion operation inserted between convolution layers. At time step during inference, one noisy frame and its noise map are fed into the neural network. Then, our network outputs another clean frame ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Image denoising. Single image denoising has been a long-standing problem as one of the most fundamental image restoration tasks. Some traditional and representative methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref> typically exploit the texture similarity in non-local image patches to achieve satisfying denoising performance. However, such methods suffer from time-consuming patch searching. Recently, deep learning has made great progress for single image denoising tasks. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> propose a deep CNN using residual learning, which outperforms the traditional methods by a large margin. They present an efficient model using downsampled sub-images and a non-uniform noise level map. Meanwhile, some other works attempt to predict pixel-wise kernels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> firstly and then apply these spatialvariant kernels as convolutional weights in the main denoising network branch. Recently, Transformer-based architectures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref> achieve state-of-the-art performance on single image denoising tasks. While single-image methods can restore each frame online, they also discard the information in adjacent frames, which should be considered in the streaming denoising task. Video denoising. In the video, pixels in adjacent frames can be very similar. The temporal correlation can be utilized by deformable convolution <ref type="bibr" target="#b24">[25]</ref>, kernel prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>, block matching <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>, optical flow <ref type="bibr" target="#b21">[22]</ref> and attention <ref type="bibr" target="#b9">[10]</ref> which typically improve the image fidelity at the cost of expensive computation. Meanwhile, various works focus on the reduction of computation in video denoising <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>. FastDVDnet <ref type="bibr" target="#b22">[23]</ref> is composed of two lightweight U-Nets <ref type="bibr" target="#b19">[20]</ref> without explicit motion estimation. It works in a multiple-to-single manner, while ours adopt an efficient buffer-based inference pipeline without redundancy. More recently, EMVD <ref type="bibr" target="#b14">[15]</ref> presents a unidirectional recurrent solution, which reuses the previous predicted clean frame. Compared with EMVD <ref type="bibr" target="#b14">[15]</ref>, our buffer-based bidirectional fusion fully utilized the feature from neighboring frames. Meanwhile, we solve the problem of high memory consumption and performance drop on the temporal edges of clips, which widely exist in MIMO frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Given a stream of noisy frames without an explicit ending frame { 0 , 1 , . . . , , . . .}, where ? R ? ? with input channel , height and width , we restore the clean frames { 0 , 1 , . . . , , . . .} in a continuous pipeline. Our method is suitable for both non-blind and blind denoising. For non-blind denoising, we set = 4 <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> or = 5 <ref type="bibr" target="#b14">[15]</ref> in input frames following previous works. For blind denoising, we only input the RGB channels and thus = 3.</p><p>We train our model in MIMO framework using Temporal Shift Module(TSM) <ref type="bibr" target="#b12">[13]</ref> for distributed parallelism (Sec. 3.1). During inference, we replace TSM with Bidirectional Buffer Block (Sec. 3.2), and our framework processes the video in a pipeline (Sec. 3.3). Figure 2 demonstrates the difference between our method and previous inference frameworks. <ref type="figure">Figure 4</ref> gives an overview of our inference algorithm with Bidirectional Buffer Block ( <ref type="figure" target="#fig_2">Figure 3</ref>). Specifically, at  <ref type="figure">Figure 5</ref>: Illustration of the difference between a single block of our pipeline-style inference method and TSM. We set the ratio = 4 in visualization. TSM uses zero paddings to fill the slots produced by shift operation, while we use buffered feature without zero padding at the temporal boundaries. time step ? {0, 1, 2, . . .}, we feed a noisy frame . Then, the input is fused with previous features buffered in Bidirectional Buffer Blocks. Finally, the model produces a single clean frame ? for the noisy input at time step ? at the end of the time step .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training with Temporal Shift Module</head><p>During the training stage, we utilize TSM <ref type="bibr" target="#b12">[13]</ref> as the temporal fusion operation in our convolution backbone. We randomly crop -frame clips from videos as input and use the pixel loss (e.g., L1 or L2) between { } =1 and { } =1 as the training objective. Assume a feature clip ? R ? ? ? is the stack of continuous frame features, with feature channel , height and width . Then, the temporal shift can be represented as the concatenation (?) of features channels:</p><formula xml:id="formula_0">0 = ?1 [0: ] ? 0 [ :? ] ? +1 [? :] ,<label>(1)</label></formula><p>where superscript ?1 and +1 represent one-frame forward and backward shift, respectively. = ? / ? is the number of shifted channels, and the ratio of channels shifted in each direction is empirically set as = 8 following TSM <ref type="bibr" target="#b12">[13]</ref>. As shown in <ref type="figure">Figure 5</ref>(b), TSM is a MIMO framework, which generates blank slots and humps on the boundaries of the clip. The original TSM <ref type="bibr" target="#b12">[13]</ref> directly truncates the humps and fills the blank slots with zero paddings, which degrades the image fidelity. Thus, it is not an optimal inference framework for processing video streams. Backbone network. We utilize two enhanced lightweight U-Nets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref> as our base model W-Net <ref type="bibr" target="#b28">[29]</ref>. Such a two-step denoising architecture has proven to be effective in previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. Batch normalization (BN) layer is observed to decrease image fidelity in super-resolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> and deblurring <ref type="bibr" target="#b18">[19]</ref>, but it still exists in current SOTA video denoising methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Thus, we remove BN layers in the W-Net. Besides, we replace ReLU activation with ReLU6 to alleviate artifacts during FP16 inference. In the following, "BSVD-32" denotes our model whose smallest feature channel is "32". We adjust our backbone by multiplying or dividing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference with Bidirectional Buffer Block</head><p>During the inference stage, we replace the temporal shift operation with the designed Bidirectional Buffer Block, while keeping the weights of the convolution backbone unchanged. As shown in Figure 5(a), our buffer blocks serve as a bridge to transfer the feature of neighboring frames, which can enlarge the temporal receptive field as well as make bidirectional temporal fusion possible for streaming video, which is regarded not applicable in previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>. Therefore, we solve the fidelity degradation at two clip boundaries in such a MIMO system.</p><p>The pseudo code shown in Alg. 1 and <ref type="figure" target="#fig_2">Figure 3</ref> delineate the computation steps of a single Bidirectional Propagation Block. Each block is initialized with two buffers, which will be filled with intermediate features. For th block, we denote them as current buffer 0, ? R ? ? and past buffer ?1, ? R ? ? . We denote the relative temporal index as the superscript 0, ?1.</p><p>During the forward inference at time step ? , a future feature ? , ? R ? ? is fed into th buffered block. The superscript index ? denotes latency of frames caused by blocks. More details about latency and the operation for &lt; at will be discussed later in Sec.3.3. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, The forward operation is composed of three steps:</p><p>(a) Fusion of new input and buffered features. The temporal fusion can be represented as</p><formula xml:id="formula_1">? ?1, = ?1, ? 0, [ :? ] ? ? , [? :] ,<label>(2)</label></formula><p>where two buffers are filled with features ?1, = ? ?2, [0: ] , 0, = ? ?1, , which are cached in the last time step ? 1. We will discuss the features in buffers in (c).</p><p>(b) Feature extraction. The feature extraction operation can be any general 2d operator, which is implemented as 2d convolution:  </p><formula xml:id="formula_2">? ?1, +1 = ( ? ?1, ),<label>(3)</label></formula><formula xml:id="formula_3">None 2 2 1,1 , 0,2 None . . . ?1,1 , ?2,2 . . . , 1, ?1 0, = 0 ? ?1,1 , ?2,2 , . . . , ? +1, ?1 ? , = ? where ? ?1,</formula><formula xml:id="formula_4">(4) 0, = ? ,<label>(5)</label></formula><p>After the above three steps, the temporal index of output ( ? ? 1 for ? ?1, +1 ) has moved backward by 1 frame compared with the original input ( ? for ? , ), since most of ? ?1, 's channels are from 0, = ? ?1, .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pipeline-style Inference</head><p>A single Bidirectional Buffer Block can be seen as a sliding window with size 3, which has 1 frame temporal latency. We consider the first Bidirectional Buffer Block for ease of description. At time step = 0, both buffers ( ?1,1 and 0,1 ) in 1 st block are empty, which means this block does not have enough information for temporal fusion. Thus, it caches the input 0,1 to 0,1 , and the whole pipeline exits. At time step = 1, together with the new input, the block is activated by information from two time steps. It fills ?1, with zeros and conduct forward temporal fusion to extract feature 0,1 for 0 . <ref type="table" target="#tab_1">Table 1</ref> shows the operation and intermediate feature at each time step. For time step &lt; , each input frame activates one deeper Bidirectional Buffer Block 0, in our network. Therefore, when the first denoised frame 0 comes out, there are already + 1 frames fed into our inference block. At a general time step ? , the network takes as input. th block takes ? , as input and produce ? ?1, +1 . The whole pipeline generates and caches features. Finally, the network produces a clean frame output ? at the end of the time step . For the last frames in the entire video sequence, we feed dummy zero tensors into the pipeline to get the clean output of the last few frames. Our supplement provides more details about the inference pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>Running time. FastDVDnet <ref type="bibr" target="#b22">[23]</ref> is a sliding-window-based framework with a window size of 5, which is composed of 2 U-Nets with the smallest channel 32. FastDVDnet fuses the three adjacent noisy frames at each U-Net's input layer. Thus, the computation cost for each output is 3 + 1 = 4 times that of a single U-Net. In our work, BSVD-32 uses two similar U-Nets with the same channel setting as the convolution backbone. Instead of feeding a clip of frames, we feed in a single frame at any time step. Therefore, the computation cost of our method for each frame is 1 + 1 = 2 times of U-Net. As shown in <ref type="table" target="#tab_5">Table 2</ref>, our BSVD-32 reduces more than 50% of runtime compared with FastDVDnet.</p><p>Memory. In bidirectional-RNN and MIMO frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>, memory consumption is typically proportional to the clip length at inference time, which impedes the processing of the long video on devices with limited resources. Thus, the whole video is divided into multiple short sequences to fit the memory size on the user device. However, for low-level denoising tasks, this will lead to a quality drop on the boundary frames in a video clip due to the loss of the marginal feature (Sec. 5.1). Unlike MIMO frameworks, our buffers work in a pipeline that consumes frames one by one. Therefore, our method can keep a constant run-time memory, which depends only on the number of Bidirectional Buffer Blocks .</p><p>Temporal receptive field. The temporal receptive field of a single Bidirectional Propagation Block is 3. During the inference of the neural network, the receptive field of temporal shift will be accumulated like 1-D convolution. In our framework, we apply = 16 layers of Bidirectional Propagation Block in the network. Therefore, the accumulated temporal receptive field of our BSVD is 33, which is larger than previous window-based methods (e.g., 7 for PaCNet <ref type="bibr" target="#b23">[24]</ref> and 5 for FastDVDnet <ref type="bibr" target="#b22">[23]</ref>) by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Settings</head><p>To evaluate our method, we use both RGB images with synthetic noise <ref type="bibr" target="#b5">[6]</ref> and raw images with real-world noise <ref type="bibr" target="#b32">[33]</ref>.</p><p>DAVIS and Set8 datasets. For RGB images with synthetic noise, we follow the data preparation of FastDVDnet <ref type="bibr" target="#b22">[23]</ref>. The clean patches are randomly sampled from the training set of the DAVIS <ref type="bibr" target="#b5">[6]</ref>. The noisy patches are generated by adding additional white Gaussian noise (AWGN) of ? [5, 50] to clean patches. For models trained on the DAVIS train set, we evaluate them on both Set8 and DAVIS test sets <ref type="bibr" target="#b22">[23]</ref>. We follow the FastDVDnet to limit all test video sequences to the first 85 frames.</p><p>CRVD dataset. For real-world raw images, we use the dataset collected by Yue et al. <ref type="bibr" target="#b32">[33]</ref>. It is composed of one synthetic dataset (SRVD) generated from MOT <ref type="bibr" target="#b16">[17]</ref> and one captured dataset (CRVD). The real raw noise is often assumed to conform to a heteroskedastic Gaussian distribution with signal-dependent variance:</p><formula xml:id="formula_5">2 (x) = x + ,<label>(6)</label></formula><p>where , ? R are parameters for shot and read noise respectively <ref type="bibr" target="#b14">[15]</ref>. 5 possible pairs of ( , ) map to 5 different ISO settings. Following previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>, we train our model with SRVD and CRVD scenes 1 to 6, and test on CRVD scenes 7 to 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our method with state-of-the-art methods, including raw data baselines <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>, and RGB image baselines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b30">31]</ref>. For evaluation on RGB images with Gaussian noise, we train our model with the same L2 loss as FastDVDnet <ref type="bibr" target="#b22">[23]</ref>, and compare it with the quantitative results of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> in the FastDVDnet paper. For experiments on raw images with real-world noise, we use the evaluation metric from RViDeNet <ref type="bibr" target="#b32">[33]</ref> and train with L1   loss, following EMVD <ref type="bibr" target="#b14">[15]</ref>. In <ref type="table" target="#tab_8">Table 4</ref>, we quantitatively compare with the results of all baselines from the EMVD <ref type="bibr" target="#b14">[15]</ref> paper. Since the code of EMVD is not open-sourced, we test the efficiency of EMVD using an unofficial implementation 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Non-blind denoising on RGB images. <ref type="table" target="#tab_5">Table 2</ref> shows the quantitative comparison of our method with baselines on the DAVIS and Set8 datasets. The inference time of VNLB, V-BM4D is obtained from the FastDVDnet paper. These CPU-based baselines are extremely slower than GPU-based methods. For fairness, we test the inference time of the most related baselines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> and our method in PyTorch framwork on the same NVIDIA RTX3090 GPU with FP16 precision. 1 https://github.com/Baymax-chen/EMVD The results in <ref type="table" target="#tab_5">Table 2</ref> indicates that our model surpasses all previous methods in terms of overall denoising performance with a much lower runtime. Compared with the best fidelity baseline PaC-Net <ref type="bibr" target="#b23">[24]</ref>, our BSVD-64 achieve a 0.28dB improvement in average PSNR on the DAVIS dataset and 0.13dB on Set8 with 700? speedup during inference. In addition, our GPU memory cost (3.09GB) is much lower than the cost of PaCNet (10.58GB). Specifically, our method has significant advantages at = 50 (e.g., 0.71dB improvement on DAVIS, 0.4dB on Set8). PaCNet exploits explicit search and fusion of nearest neighbor patches in the RGB pixel domain, which consumes an extremely long runtime and lacks matching of deep features. In contrast, our method implicitly conducts the alignment and fusion through convolution on multi-scale features, which proves to be more efficient with better fidelity. <ref type="figure" target="#fig_4">Figure 6</ref> also demonstrates that our method restores more high-frequency details on the characters and the roof. Our BSVD-64 processes the video    Furthermore, we half the channels in BSVD-64 to BSVD-32 as a lightweight version. Compared with real-time SOTA FastDVDnet <ref type="bibr" target="#b22">[23]</ref>, our BSVD-32 has more than 60% reduction in runtime, with a 0.57dB increase of average PSNR on the DAVIS test set and 0.15dB on the Set8.</p><p>Blind denoising on DAVIS and Set8 datasets. Our method is also applicable for blind video denoising without the noise map as input. We compare ours with previous blind denoising SOTA UDVD <ref type="bibr" target="#b20">[21]</ref>, and a concurrent work <ref type="bibr" target="#b30">[31]</ref>. As shown in <ref type="table" target="#tab_7">Table 3</ref>, our BSVD-64-blind shows better fidelity.</p><p>Non-blind denoising on CRVD dataset. In <ref type="table" target="#tab_8">Table 4</ref>, we compare with baselines on real-world raw data. Since the real-time baseline EMVD <ref type="bibr" target="#b14">[15]</ref> has a very low computation cost, we shrink the channel of our model and train BSVD-24 and BSVD-16. Compared with FastDvDnet, our BSVD-24 produces better images with 3? speedup. Compared with EMVD, our BSVD-16 has 58.3% shorter inference time, and achieves better image fidelity in both raw and sRGB domains. Our model consists of only 2d convolutions, without special designs for raw data prior, such as attention, depth-wise separable convolution in EMVD. Thus, BSVD-16 has shorter runtime <ref type="table" target="#tab_10">Table 5</ref>: Quantitative ablation study based on our BSVD-64. The "Ave" denotes the average PSNR of ? {10, 20, 30, 40, 50}. Due to the space limit, we omit the results for ? {20, 40}. with similar GFLOPs as EMVD. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, our results have sharper edges on CRVD dataset. More results are shown in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Application of Buffer in FastDVDnet.</head><p>Our buffer-based pipeline-style inference can be applied to the existing method. FastDVDnet is a two-stage sliding-window-based method that conducts temporal fusion at the input layer of each U-Net. We utilize the pre-trained checkpoint and buffer the intermediate feature during each forward inference, which modifies the original computation graph into pipeline style. As a result, we half the runtime from 42ms to 23ms with the same image fidelity as the original implementation. More details are in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDIES 5.1 Pipeline Inference vs MIMO</head><p>We modify our BSVD-64 into a MIMO framework using temporal shift operation <ref type="bibr" target="#b12">[13]</ref> with the same model parameters. We set the clip size = 8, which is close to previous works (e.g., 5 for FastDVDnet <ref type="bibr" target="#b22">[23]</ref> and ReMoNet <ref type="bibr" target="#b30">[31]</ref>, 7 for PaCNet <ref type="bibr" target="#b23">[24]</ref>).</p><p>As shown in   set. To further analyze our image fidelity, we demonstrate the PSNR difference at each frame index with clip size = 4, 8, 32 on the video from Set8 dataset in <ref type="figure">Figure 8(a)</ref>. The MIMO framework suffers significant fidelity degradation (up to 1 dB in PSNR) at the boundary of clips (e.g., 32 nd frame). Although increasing improves the fidelity of the middle frames in the clip, it can not achieve obvious improvement on the two temporal edges of the clip. As shown in <ref type="figure">Figure 8</ref>(b), increasing the of MIMO framework results in linear growth of memory consumption. In contrast, our framework processes the video continuously in the constant memory. <ref type="figure">Figure 9</ref> provides another qualitative comparison with = 8 MIMO framework at 32th frame of the "skate-jump" sequence from DAVIS. Our bidirectional buffer improves the sharpness of the tree, especially in areas close to the moving player.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bidirectional Feature Propagation</head><p>Bidirectional vs unidirectional temporal fusion. MoViNet <ref type="bibr" target="#b7">[8]</ref> propose a buffer-based unidirectional temporal fusion for video steam recognition. In <ref type="table" target="#tab_10">Table 5</ref>, we modify our bidirectional fusion to unidirectional temporal fusion with stream buffer <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_6">, +1 = ( ?1, ? , [2 :] ).<label>(7)</label></formula><p>Then, update the buffer ?1, with , [0:2 ] . The unidirectional version of our method "Ours-Uni" does not need future frames and thus does not have frame latency. However, the results in <ref type="table" target="#tab_10">table 5</ref> show a large gap of 0.64dB between bidirectional and unidirectional fusion. For low-level video tasks, the bidirectional fusion can be more advantageous since the future frames also have similar and helpful features to be utilized by each frame. Number of buffer blocks N. In <ref type="table" target="#tab_11">Table 6</ref>, we add buffer blocks at pixel resolution or remove them at downsampled resolution, using the same backbone. Larger brings a larger receptive field (RF) at the cost of longer latency during online inference. The result shows that pixel-level fusion is not necessary. Thus, we only fuse the downsampled features, which can be more robust against noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Backbone</head><p>Batch normalization. We train a W-Net with BN layers and tune other hyperparameters. BN layers decreases the average PSNR by 0.57dB on the DAVIS test set, as shown in <ref type="table" target="#tab_10">Table 5</ref>.</p><p>W-Net vs U-Net. We replace our backbone with a U-Net with increased channels, such that the new model has similar FLOPs and runtime as W-Net. As shown in <ref type="table" target="#tab_10">Table 5</ref>, W-Net outperforms U-Net by 0.40dB. This result indicates that a second U-Net for refinement may help to remove the remaining noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose a SOTA streaming video denoising method BSVD that outperforms existing methods on videos with synthetic and real noise in both inference speed and image fidelity. Our pipeline-style inference with Bidirectional Buffer Blocks allows bidirectional temporal fusion for online streaming video processing, which is proved to be more effective than unidirectional fusion. In addition, we solve the degradation of clip edges, which exists in MIMO frameworks. Our method is effective for both non-blind and blind denoising, and is also general for similar architectures. Extensive experiments on public datasets have demonstrated the effectiveness of our method. <ref type="table" target="#tab_12">Table 7</ref> shows details of a single U-Net in our BSVD-64 model. TSM denotes Temporal Shift Module during training, while BBB stands for Bidirectional Buffer Block during inference. "3?3, 64" denotes a 2d convolution with kernel size 3 and output channel 64. "ReLU6" denotes ReLU6 activation, whose output is clipped to be between 0 to 6. Building blocks are shown in brackets, with the numbers of blocks stacked. Downsampling is performed using a convolution of stride 2 at the beginning of the downsampling block. After the last convolution in the upsampling block, we upsample the decoder feature using Pixel Shuffling. Then, skip-connections with intermediate features are conducted after upsampling. We stack two U-Nets as a W-Net for our backbone architecture. The output channel of the first U-Net and the input channel of the second U-Net are 64. During the training stage, we insert 16 Temporal Shift Module in our W-Net. During the inference stage, we replace each TSM with a Bidirectional Buffer Block.  <ref type="figure" target="#fig_0">Fig. 10</ref> gives a visualization of our implementation at two edges of a long -frame stream. We denote the temporal fusion equation as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS A.1 Backbone Details</head><formula xml:id="formula_7">? ?1, = ?1, ? 0, [ :? ] ? ? , [? :] ,<label>(8)</label></formula><p>We fill past buffer ?1, with zero paddings at the temporal index 1 ? &lt; ( <ref type="figure" target="#fig_0">Fig. 10(b)</ref>). At the temporal index ? ? + ? 1, we feed dummy zero tensor ? , into the pipeline for the last N frames. <ref type="figure" target="#fig_0">(Fig. 10(d,e)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation of Skip-Connection</head><p>Each skip-connection is conducted between two Bidirectional Buffer Blocks at different layers. Thus, there is a temporal latency between input and output during pipeline-style inference. We implement skip-connection as a first-in-first-out queue. The memory cost is proportional to the number of skipped layers, which is ( ).</p><p>A.4 Implementation of Buffer in FastDVDnet.</p><p>We provide a supplementary description for the Sec. 4.4 in the main paper. FastDVDnet <ref type="bibr" target="#b22">[23]</ref> conducts temporal fusion at the input layer of each U-Net:</p><formula xml:id="formula_8">0 = ?1 ? 0 ? +1 ,<label>(9)</label></formula><p>which is similar to the temporal shift operation Equation.</p><p>(1) in the main paper. We modify the original computation graph into pipeline style using a similar representation in Equation.</p><p>(2-5) of the main paper:</p><formula xml:id="formula_9">? ?1, = ?1, ? 0, ? ? ,<label>(10)</label></formula><formula xml:id="formula_10">? ?1, +1 = U-Net( ? ?1, )<label>(11)</label></formula><formula xml:id="formula_11">?1, = 0,<label>(12)</label></formula><formula xml:id="formula_12">0, = ? , ,<label>(13)</label></formula><p>where ? {1, 2}. As summarized in the <ref type="figure">Fig. 2</ref> of the main paper, Fast-DVDNet is a sliding-window method without clip-edge-drop problem. However, its time complexity is ( ), which is higher than ( ) in our pipeline-style inference. Using a pipeline-syle inference framework, we half the runtime from 42ms to 23ms with the same image fidelity as the original implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Training Details</head><p>The model is trained with clips of batch size 16, temporal length = 11 and spatial patch size 96 ? 96 in each iteration. For each clip, We sample a random noise level from the uniform distribution (5, 55) and augment the data with flipping and rotation. The training objective is optimized for 700, 000 iterations with Adam optimizer <ref type="bibr" target="#b6">[7]</ref> of initial learning rate 1 ? 3. The learning rate is decayed by a factor of 0.7 for every 50, 000 iterations. <ref type="figure" target="#fig_0">Fig. 13</ref> and <ref type="figure" target="#fig_0">Fig. 14</ref> provide more qualitative comparison on DAVIS and CRVD dataset, respectively. Our method reconstructs more realistic high-frequency details (e.g., horsetail, snow on pine branches, spots on pumpkins). The source code and results of EMVD <ref type="bibr" target="#b14">[15]</ref> and the concurrent work ReMoNet <ref type="bibr" target="#b30">[31]</ref> are not available. We only do quantitative comparisons with them. Since the source code of RViDeNet <ref type="bibr" target="#b32">[33]</ref> is not compatible with CUDA version of our RTX 3090 GPU, we do not compare running time with RViDeNet. There is another supplementary video for visualization of our algorithm and more results. <ref type="table" target="#tab_13">Table 8</ref> provides a supplement of all noise levels for the quantitative ablation study <ref type="table" target="#tab_10">(Table 5)</ref>     <ref type="figure" target="#fig_0">Figure 11</ref>: Qualitative ablation of inference method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MORE ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU</head><p>ReLU6 (ours) <ref type="figure" target="#fig_0">Figure 12</ref>: ReLU6 alleviates the artifacts caused by FP16 quantization.</p><p>In <ref type="figure" target="#fig_0">Fig. 15</ref>, we compare our inference method with MIMO of different . We show the result on the clip boundaries. A larger in MIMO framework increases the memory linearly and barely improves edge fidelity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSNR / SSIM</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of PSNR and runtime on DAVIS test set with noise level = 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The forward operation of th Bidirectional Buffer Block at time step . This block aggregates input feature ? , with buffered features ?1, , 0, , to output ? ?1, +1 , which is the input for the ( + 1) th temporal buffer block. After convolution operation, the buffered feature are updated using input ? , .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison on Set8 dataset. Ours reconstructs more high-frequency details in shorter running time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative Comparison of real noisy frames from CRVD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) PSNR difference at each temporal index (b) Average PSNR and memory consumption</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Fidelity and memory comparison between our pipeline framework and MIMO for a denoised video sequence from Set8 at = 50. (a) Our inference method solves the PSNR drop on the two edges of the clips with up to 1dB improvement. (b) Our framework consumes lower memory than = 8 with a 0.45dB improvement at average PSNR. Qualitative ablation study of inference method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>in the main paper. Bidirectional temporal fusion substantially improves the reconstructed quality. Moreover, a Implementation of start and end in our pipeline. For a long stream with frames, we use zero paddings at temporal index &lt; and ? ? + ? 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 :</head><label>15</label><figDesc>Comparison with MIMO of different .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, 2 , . . . , 1 , 2 , . .. , , . . . , -? , . . . , ? 1 Output 1 , 2 , . . . 1 , 2 , . . .</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">(a) Sliding-window</cell><cell>(b) Uni-RNN</cell><cell cols="2">(c) Bi-RNN</cell><cell cols="2">(d) MIMO</cell><cell>(e) Ours</cell></row><row><cell cols="3">Visualization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Deleted Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Memory</cell><cell>Output</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Input 1 Buffered Feature Index ? /2 , . . . , + /2 -? 1 1, 2?</cell></row><row><cell cols="3">Memory Complexity</cell><cell>(</cell><cell>)</cell><cell>( )</cell><cell>(</cell><cell>)</cell><cell>(</cell><cell>)</cell><cell>( )</cell></row><row><cell cols="2">Time Per Frame</cell><cell>(</cell><cell></cell><cell>)</cell><cell>( )</cell><cell>( )</cell><cell></cell><cell>( )</cell><cell>( )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1</head><label>1</label><figDesc>Pseudo code of a single Bidirectional Buffer Block. class BidirectionBufferedBlock(nn.Module): def __init__(self, in_channels, out_channels, r=8): self.conv = nn.Conv2d(in_channels, out_channels) self.f = in_channels//r self. ?1, = torch.zeros(n, self.f, h, w) self. 0, = torch.zeros(n, in_channels, h, w) def forward(self, ? , ): # Activate the block if self. 0, is None: self. 0, = ? , = self. 0, [:, :self.f, :, :] self. 0, = ? , return ? ?1, +1 all channel numbers by a scale. Please refer to the supplement for more details about our implementation.</figDesc><table><row><cell>return None</cell></row><row><cell># Fuse the temporal adjacent features</cell></row><row><cell>fusion = torch.cat([</cell></row><row><cell>self. ?1, ,</cell></row><row><cell>self. 0, [:,self.f:-self.f, :, :]</cell></row><row><cell>? , [:, -self.f:, :, :], dim=1)</cell></row><row><cell># 2D Convolution for output</cell></row><row><cell>? ?1, +1 = self.conv(fusion)</cell></row><row><cell># Update the buffer</cell></row><row><cell>self. ?1,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Pipeline-Style inference at different time step.</figDesc><table><row><cell cols="3">Time step Input Buffered features</cell><cell>Output</cell></row><row><cell>0</cell><cell>0</cell><cell></cell><cell>None</cell></row><row><cell>1</cell><cell>1</cell><cell>0,1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>+1 is the input for the next ( + 1) th Bidirectional Buffer Block.(c) Update the buffered feature in the memory. At the end of timestep for th block, we update the intermediate feature in the fixed buffer to reuse the previous computation.</figDesc><table><row><cell>?1, =</cell><cell>0, [0: ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>DAVIS</cell><cell>Set8</cell></row></table><note>Quantitative comparisons of PSNR (dB) and runtime on the test set of DAVIS and Set8. C and G represent CPU and GPU time cost, respectively. 10, 20, 30, 40, 50 represents the of test data. We show the averaged inference time per frame with the resolution of 960 ? 540 in PyTorch framework with FP16 precision. Our method outperforms previous methods on average PSNR and runtime.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparisons of PSNR for blind denoising.</figDesc><table><row><cell>DAVIS</cell><cell>Set8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparisons of PSNR and SSIM on the CRVD test set. We test the averaged computation cost per frame on RGGB raw data with resolution 960 ? 540.</figDesc><table><row><cell>Model</cell><cell cols="2">Time(ms) GFLOPs</cell><cell>raw</cell><cell>sRGB</cell></row><row><cell>RViDeNet [33]</cell><cell>-</cell><cell cols="3">1965.0 44.08 / 0.9881 40.03 / 0.9802</cell></row><row><cell cols="2">FastDVDnet [23] 44.3</cell><cell cols="3">665.0 44.30 / 0.9891 39.91 / 0.9812</cell></row><row><cell>EMVD [15]</cell><cell>24.0</cell><cell>79.5</cell><cell cols="2">44.05 / 0.9890 39.53 / 0.9796</cell></row><row><cell>BSVD-16 (ours)</cell><cell>9.9</cell><cell cols="3">78.76 44.10 / 0.9884 40.17 / 0.9804</cell></row><row><cell>BSVD-24 (ours)</cell><cell>14.2</cell><cell cols="3">175.46 44.39 / 0.9894 40.48 / 0.9820</cell></row><row><cell cols="5">streams of resolution 960 ? 540 at 30Hz framerate, which achieves</cell></row><row><cell cols="2">real-time performance.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Net [20] 39.54 34.66 32.44 35.30 36.56 31.87 29.76 32.49 Ours-MIMO [13] 39.68 34.83 32.61 35.46 36.68 31.98 29.86 32.60 Ours 39.81 35.09 32.91 35.70 36.74 32.14 30.06 32.75</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Set8</cell><cell></cell></row><row><cell>Model</cell><cell>10</cell><cell>30</cell><cell>50 Ave</cell><cell>10</cell><cell>30</cell><cell>50</cell><cell>Ave</cell></row><row><cell>Ours-Uni [8]</cell><cell cols="7">39.40 34.40 32.14 35.06 36.51 31.70 29.54 32.34</cell></row><row><cell>Ours-BN</cell><cell cols="7">39.14 34.57 32.33 35.13 36.50 31.86 29.72 32.46</cell></row><row><cell>Ours-U-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>table 5</head><label>5</label><figDesc></figDesc><table><row><cell>, our buffer-based pipeline inference frame-</cell></row><row><cell>work brings 0.24dB improvement in average PSNR on DAVIS test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Ablation study for the number of buffer blocks .</figDesc><table><row><cell>Model</cell><cell>RF</cell><cell>Scale</cell><cell cols="2">DAVIS-Ave Set8-Ave</cell></row><row><cell>Ours-w/o fusion 0</cell><cell>1</cell><cell>-</cell><cell>34.01</cell><cell>31.63</cell></row><row><cell>Ours-only Pixel 2</cell><cell>5</cell><cell>pixel</cell><cell>34.03</cell><cell>31.66</cell></row><row><cell cols="4">Ours-with Pixel 24 49 Down + pixel 35.12</cell><cell>32.35</cell></row><row><cell cols="2">BSVD-64 (ours) 16 33</cell><cell>Down</cell><cell>35.70</cell><cell>32.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Architecture of our U-Net backbone in BSVD-64.</figDesc><table><row><cell>Stage</cell><cell>Building Block</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Output Size</cell></row><row><cell>Input Layer</cell><cell>3 ? 3, 64 ReLU6</cell><cell cols="2">? 2</cell><cell></cell><cell cols="2">? ? 64</cell></row><row><cell></cell><cell cols="3">3 ? 3, 128, stride=2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ReLU6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Downsampling Block1</cell><cell cols="3">? ? ? ? ? ? 3 ? 3, 256, stride=2 TSM / BBB 3 ? 3, 128 ReLU6 ? ? ? 2 ? ? ? ?</cell><cell>1 2</cell><cell>? 1 2</cell><cell>? 128</cell></row><row><cell></cell><cell>ReLU6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Downsampling Block2 Upsampling Block1</cell><cell>? ? ? ? ? ? ? ? ? ? ? ? 3 ? 3, 512 TSM / BBB 3 ? 3, 256 ReLU6 TSM / BBB 3 ? 3, 256 ReLU6</cell><cell>? ? ? ? ? ? ? ? ? ? ? ?</cell><cell>? 2 ? 2</cell><cell>1 4 1 2</cell><cell>? 1 4 ? 1 2</cell><cell>? 256 ? 128</cell></row><row><cell></cell><cell cols="2">PixelShuffle 2x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upsampling Block2</cell><cell>? ? ? ? ? ? 3 ? 3, 256 TSM / BBB 3 ? 3, 128 ReLU6</cell><cell>? ? ? ? ? ?</cell><cell>? 2</cell><cell></cell><cell cols="2">? ? 64</cell></row><row><cell></cell><cell cols="2">PixelShuffle 2x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 ? 3, 64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Output Layer</cell><cell>ReLU6</cell><cell></cell><cell></cell><cell cols="3">? ? 64 or ? ? 3</cell></row><row><cell></cell><cell cols="3">3 ? 3, 64 or 3 ? 3, 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">A.2 Edges of the Stream</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Quantitative ablation study of framework</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DAVIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Set8</cell><cell></cell></row><row><cell>Model</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 Average</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 Average</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Quantitative ablation study of shifted ratio . = 8 is the best setting among {4, 6, 8, 16} 36.72 34.97 33.74 32.78 35.59 36.73 33.79 32.09 30.90 30.00 32.70 6 39.65 36.58 34.81 33.56 32.59 35.44 36.66 33.74 32.02 30.83 29.91 32.63 8 39.81 36.82 35.09 33.86 32.91 35.70 36.74 33.83 32.14 30.97 30.06 32.75 16 39.71 36.66 34.90 33.66 32.70 35.52 36.67 33.73 32.03 30.84 29.93 32.64</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DAVIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Set8</cell><cell></cell></row><row><cell>Shifted ratio</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 Average</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50 Average</cell></row><row><cell cols="3">4 39.74 Ground Truth Ours w/o buffer</cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR / SSIM</cell><cell cols="2">34.59 / 0.885</cell><cell></cell><cell>35.87 / 0.908</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Figure 13: Qualitative comparison on DAVIS dataset.</figDesc><table><row><cell>Noisy input, =</cell><cell>FastDVDnet 41.9ms</cell><cell>PaCNet 23750ms</cell><cell>Ours 33.7ms</cell><cell>Ground Truth / Runtime</cell></row><row><cell>PSNR / SSIM</cell><cell>30.09 / 0.825</cell><cell>29.88 / 0.825</cell><cell>30.36 / 0.843</cell><cell></cell></row><row><cell></cell><cell>32.97 / 0.878</cell><cell>32.95 / 0.873</cell><cell>33.87 / 0.897</cell><cell></cell></row><row><cell>Noisy input</cell><cell>FastDVDnet 44.3ms</cell><cell>RViDeNet -</cell><cell>Ours 14.2ms</cell><cell>Ground Truth / Runtime</cell></row><row><cell>PSNR / SSIM</cell><cell>32.35 / 0.936</cell><cell>35.82 / 0.942</cell><cell>35.54 / 0.956</cell><cell></cell></row><row><cell></cell><cell cols="3">Figure 14: Qualitative comparison on CRVD dataset</cell><cell></cell></row><row><cell>Ground Truth</cell><cell>= 2</cell><cell>= 8</cell><cell>= 32</cell><cell>Ours</cell></row><row><cell>PSNR / SSIM</cell><cell>34.21 / 0.871</cell><cell>36.08 / 0.910</cell><cell>36.07 / 0.910</cell><cell>37.34 / 0.928</cell></row><row><cell>Memory consumption</cell><cell>0.96GB</cell><cell>3.76GB</cell><cell>14.98GB</cell><cell>3.09GB</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video Denoising via Empirical Bayesian Estimation of Space-Time Patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-Local Means Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Process. Line</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Non-Local Video Denoising by CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
		<idno>arXiv abs/1811.12758</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video Object Segmentation with Referring Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mingxing Tan, Matthew Brown, and Boqing Gong. 2021. MoViNets: Mobile Video Networks for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Nonlocal Bayesian Image Denoising Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12288</idno>
		<title level="m">VRT: A Video Restoration Transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SwinIR: Image Restoration Using Swin Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video Denoising, Deblocking, and Enhancement Through Separable 4-D Nonlocal Spatiotemporal Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Multi-Stage Video Denoising With Recurrent Spatio-Temporal Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglong</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlocal Transform-Domain Filter for Volumetric Data Denoising and Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MOT16: A Benchmark for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>ArXiv abs/1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Burst Denoising With Kernel Prediction Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Video Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashpal</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyas</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Manzorro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Crozier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dvdnet: A fast network for deep video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FastDVDNet: Towards real-time deep video denoising without flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Patch Craft: Video Denoising by Deep Modeling and Patch Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Vaksman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EDVR: Video Restoration With Enhanced Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ESRGAN: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint Adaptive Sparsity and Low-Rankness on the Fly: An Online Tensor Reconstruction Scheme for Video Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VIDOSAT: High-Dimensional Sparsifying Transform Learning for Online Video Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saiprasad</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">W-Net: A Deep Model for Fully Unsupervised Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<idno>abs/1711.08506</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Basis prediction networks for effective burst denoising with large kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ReMoNet: Recurrent Multi-output Network for Efficient Video Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jundong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient Deep Image Denoising via Class Specific Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanye</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised Raw Video Denoising with a Benchmark Dataset on Dynamic Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanjing</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghe</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Restormer: Efficient Transformer for High-Resolution Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours-Unidirectional</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">In addition, the designed pipeline-style inference also demonstrates better image quality than original MIMO framework. For a TSM or Bidirectional Buffer Block with input feature channel , we shift = ? / ? channels along the temporal dimension. We study the ratio of shifted channels in Table. 9</title>
	</analytic>
	<monogr>
		<title level="m">W-Net without batch normalization contributes to the performance</title>
		<imprint/>
	</monogr>
	<note>We retrain our BSVD-64 with ? {4, 6, 8, 16}. The model of = 8 achieves the best result. Thus, we set = 8 in other experiments. Fig. 12 shows an example of artifacts during the FP16 inference of a model with ReLU activation. To alleviate quantization artifacts, we replace the ReLU with ReLU6, which limits the maximum of activation to 6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
