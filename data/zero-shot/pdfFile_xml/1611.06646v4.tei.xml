<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Video Representation Learning With Odd-One-Out Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ACRV</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<addrLine>ACT 2601</addrLine>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ACRV</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<addrLine>ACT 2601</addrLine>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Video Representation Learning With Odd-One-Out Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called odd-oneout learning. In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition.</p><p>On action classification, our method obtains 60.3% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current stateof-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-ofthe art methods by 12.7% on action classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b26">[27]</ref> have emerged as the new state-of-the-art learning framework for many machine learning problems. The success of CNNs has been largely sustained by the manual annotation of big datasets such as ImageNet <ref type="bibr" target="#b33">[34]</ref> and Sports-1M <ref type="bibr" target="#b22">[23]</ref>. As manual annotations are costly and time consuming supervised learning becomes less appealing, especially when considering tasks involving more complex data (e.g., videos) and concepts (e.g., for human behavior analysis). In this work we focus on learning video representations <ref type="figure">Figure 1</ref>: The proposed odd-one-out network, where it takes several video sequences as input to the multi branched network that share weights. Objective is to identify the odd video sequence in this case it is the second video. To find the odd video-clip, learning machine has to compare all video clips, identify the regularities among them, and pick the one with irregularities. This type of tasks are know as analogical reasoning tasks. from unlabeled data.</p><p>Good video feature learning without using action category labels from videos is crucial for action recognition for two reasons. First, unlike static images, videos are generally open-ended media and one cannot a priori contain a particular action within a particular frame range. Thus, for supervised learning one would need to manually annotate videos frame-by-frame or crop them to a range of frames to ensure consistency, obviously an unrealistic expectation for both. Second, existing large video datasets, e.g., Sports-1M <ref type="bibr" target="#b22">[23]</ref> and the recent YouTube-8M <ref type="bibr" target="#b0">[1]</ref>, rely on noisy, unreliable YouTube tags. As a result, one cannot truly identify whether it is the architecture or the noisy labels that contribute to the observed network behavior. Besides, unlabeled videos are abundant and information rich regarding spatio-temporal structures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Although traditionally unsupervised feature learning (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>) implies no supervisory signals, recently researchers introduced the self-supervised learning paradigm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44]</ref>. Here the structure of the data is used as a supervisory signal so the method is unsupervised in the sense that it does not require human annotation but supervised machine learning techniques can still be used. For instance, one can use relative location of the patches in images <ref type="bibr" target="#b9">[10]</ref> or the order of video frames <ref type="bibr" target="#b43">[44]</ref> as a supervisory signal. Different from the above works we express supervision in the context of the odd-one-out problem. More specifically, each training example is comprised of a question composed of N + 1 elements (such as N + 1 video clips or images). Out of these N + 1 elements N are similar or related (e.g., correctly ordered set of frames coming from a video) and one is different or odd (e.g., wrongly ordered set of frames from a video). Both the odd element and the N coherent elements are presented to the learning machine. The learning machine is then trained to predict the odd element. To avoid a trivial solution, in each of the odd-one-out questions, the odd element is presented to the learning machine at random. In particular, we use a CNN as the learning machine, a multi-branch neural network as illustrated in <ref type="figure">Figure 1</ref>. During training, our method learns features that solves the odd-one-out problem. As the network performs a reasoning task about the validity of elements (e.g. video subsequences), the learned features are useful for many other related yet different tasks. Specifically, in this paper we demonstrate the advantages of oddone-out networks to learn features in a self-supervised manner for video data.</p><p>Exploiting the spatio-temporal coherence in videos has been investigated before for unsupervised learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref>. However, with the exception of few works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>, the focus has been on appearance representations, perceiving videos as a collection of frames. There has also been unsupervised temporal feature encoding methods to capture the structure of videos for action classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. In contrast, we focus on learning the motion patterns within videos. Inspired by the recently proposed family of motion representations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref> that compress arbitrary length video sequences into fixed-dimensional tensors while maintaining their spatio-temporal structure, we propose a new video-segment level representation learning strategy for videos in self-supervised manner.</p><p>In this work we present an alternative methodology for learning video segment representations in an selfsupervised manner. Our contributions are threefold: First, we propose a novel learning task, odd-one-out learning, for optimizing model parameters without relying on any manually collected annotations. Second, we present a neural network architecture suitable for odd-one-out learning. Third, our experimental results indicate that the trained networks learn accurate representations, outperforming considerably other recently proposed self-supervised learning paradigms for video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Unsupervised feature learning is well studied in the literature. The most common techniques studied include autoencoders <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, restricted Boltzmann machines <ref type="bibr" target="#b18">[19]</ref>, convolutional deep belief networks <ref type="bibr" target="#b27">[28]</ref>, LSTMs and recurrent neural networks <ref type="bibr" target="#b35">[36]</ref>.</p><p>A recent emerging line of research for learning representations without manual annotations is self-supervised learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44]</ref>. Self-supervised methods do not require manual annotations, instead they exploit the structure of the data to infer supervisory signals, which can then be used with robust and trustworthy supervised-like learning strategies. In Doersch et al. <ref type="bibr" target="#b9">[10]</ref> spatial consistency of images are exploited as a context prediction task to learn image representations. Video data are also used to learn image representations. For example, Wang et al. <ref type="bibr" target="#b43">[44]</ref> generate pairsof-patches from videos using tracking and use a Siamese triplet network to learn image representations such that the similarity between two matching patches should be larger than similarity between two random patches. The matched patches will have intraclass variability due to changes in illumination, occlusion, viewpoint, pose, and clutter. However, tracking is not always reliable. As shown by Kumar et al. <ref type="bibr" target="#b25">[26]</ref>, training of such triplet networks is not straightforward with the need to estimate stable gradients for tripletbased losses. Agrawal et al. <ref type="bibr" target="#b1">[2]</ref> exploit egomotion as a labelling process to learn representations where they show that egomotion is a useful supervisory signal when learning features. Similar to Wang et al. <ref type="bibr" target="#b43">[44]</ref>, they also train a Siamese network to estimate egomotion from two image frames and compare it to the egomotion measured with odometry sensors. The resulting learned features are somewhat similar.</p><p>Another variant of unsupervised feature learning relies on exemplar CNNs <ref type="bibr" target="#b11">[12]</ref>. Here each image is transformed using a large number of transformations and a CNN is trained to recognize instances of transformed images. A disadvantage of such an approach is that each image becomes a class, hence for a million images one trains a one-million class CNN. Moreover, the learned invariances depend on the types of transformations. However, the approach generate consistent labels, which is important for self-supervised learning based on CNNs.</p><p>The direction of time-flow (forward or backward) in videos was studied in an inspiring work by Pickup et al. <ref type="bibr" target="#b32">[33]</ref>. The authors investigate various motion representations to learn the arrow of time. Unsupervised learning of sequence encoding for video data was proposed by Srivastava et al. <ref type="bibr" target="#b35">[36]</ref>, where an LSTM encoder was used to learn unsupervised video encodings. The LSTM is trained such that the encoding of the forward video is similar to the LSTM encoding of the reverse video. However, this method requires a pre-trained network (with supervision) to extract frame level features and thus it is not a unsupervised feature learning method.</p><p>More recently, a CNN-based unsupervised representation learning method was presented in Misra et al. <ref type="bibr" target="#b30">[31]</ref>. In that work, the learning task is to verify whether a sequence of frames from a video is presented in the correct order or not. This method has two shortcomings: i) the binary learning formulation results in a relatively easy learning problem, ii) despite having to determine the correct temporal ordering of frames, the method does not learn to encode temporal information but only spatial. In contrast, our method exploits the analogical reasoning over sequences and pose the feature learning problem as a N + 1 way multi class classification problem which is much harder than the binary verification problem (see <ref type="figure">Fig. 1</ref>). Our method also able to learn temporal information taking the advantage of recent developments <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref> which leads to a superior performance for action recognition tasks.</p><p>Most of the prior work in action recognition is dedicated to hand-crafted features <ref type="bibr" target="#b17">[18]</ref> such as dense trajectory features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Recently, supervised convolutional features obtain state-of-the-art performance either using very large video collections or using 3D convolutions <ref type="bibr" target="#b38">[39]</ref> or by fine-tuning ImageNet pre-trained models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. Our work differs from these in that we learn a video representation from self-supervision without using external information such as optical flow data or transfer filter weights from ImageNet pre-trained models or use effective cross modality pre-training as done in <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Odd-one-out learning</head><p>Task. The goal of odd-one-out learning is to predict the odd element from a set of otherwise related elements. There are different ways to generate such odd-one-out questions for both video or image data. For example, in the case of video representation learning, the even objects could be correctly ordered video clips of a particular video, and the odd one could be a video clip obtained by wrongly permuting frames of the same video. This is just one example and our framework is quite general and can be applied to other data types, such as RGB image patches, video instances, or generic feature descriptors. The set of multiple related elements and the odd element comprise a question q = {I 1 , . . . , I N +1 }, where I i are the elements (in our case videos). We construct questions in an unsupervised manner. For example, in the context of feature learning for video classification, I 1 , . . . , I N +1 are sets of sub-sequences sampled from a video. Out of these, N number of subvideos have the correct chronological order of frames which comprises the even set. The odd video sub-sequence consist of frames sampled from an invalid order from the same video (see <ref type="figure">Figure 1</ref>). In both cases we know that one out of (N + 1) elements is an odd object.</p><p>In order to prevent a trivial solution, we randomize the position of the odd element by a permutation ? and obtain a question q ? with a respective answer a ? = ?(N + 1) ? {1, 2, . . . , N + 1}. The odd-one-out prediction task thus reduces to an (N + 1)-way classification problem. Note, that given a set of unlabelled videos, we can automatically construct a self-supervised question-answer training set D = {(q ?j j , a ?j j )}, where the permutation ? j is chosen randomly for each question. Given this self-supervised dataset, the learning problem can be solved by standard maximum likelihood estimation, namely,</p><formula xml:id="formula_0">? = arg max ? L(f ? ; D)<label>(1)</label></formula><p>where L is the likelihood function and f ? is our parametrized model. Model. We implement the prediction model f ? as a multibranch Convolutional neural network, which we call an odd-one-out network (O3N ). As illustrated in <ref type="figure">Figure 1</ref>, O3N is composed of N + 1 input branches, each contains five Convolutional layers and weights are shared across the input layers. Configuration of each input branch is identical to AlexNet architecture <ref type="bibr" target="#b23">[24]</ref> up to the first fully connected layer. As odd-one-out task requires a comparison among (N+1) elements of the given question and cannot be solved by only looking at individual elements, we introduce a fusion layer which merges the information from (N+1) branches after the first fully connected layer. These fusion layers help the network to perform reasoning about elements in the question to find the odd one. Ideally, the fusion layer should support the network to compare elements and find regularities and pick the element with irregularities. We experiment with two fusion models, the Concatenation model and sum of difference model leading to two different network architectures as shown in <ref type="figure">Fig. 2</ref>.</p><p>Concatenation model: The first fully connected layers from each branch are concatenated to give a (N + 1) ? d dimensional vector, where d is the dimensionality of the first fully connected layer.</p><p>Sum of difference model: The first fully connected layers from each branch are summed after taking the pair-wise activation difference leading to a d dimensional vector, where d is the dimensionality of the first fully connected layer. The advantage is that this strategy still encodes the structure of the odd-one-out feature activations yet can be represented with lower dimensional activation vector. Mathematically, let v i be the activation vector of the i-th branch of the network. The output of the sum of difference layer is given <ref type="figure">Figure 2</ref>: The odd-one-out networks for video representation learning. Network is presented with one wrong sequence (out of order) and two correct sequences from the same video. Temporal encoder encodes the temporal structure of the subsequence. Odd-one-out network learn the feature to find out of order sequences. In the left figure, we see concatenation of FC6 activations and on the right we see the sum of difference network architecture. The constrained consecutive sampling sample from a constrained part of the original video which is denoted by the green box. The constrained sampling window is 1.5 ? W where W is the length of the sampled sub-sequences (must be viewed in colour).</p><p>by</p><formula xml:id="formula_1">o = ?j&gt;i v j ? v i .<label>(2)</label></formula><p>We feed this fused activation vector through two fully connected layers followed by a softmax classifier with N + 1 outputs. Given a new training question q ? , each input branch receives one of the N + 1 elements and the network must learn to predict the location of the right answer, a ? . We illustrate in <ref type="figure">Figure 1</ref> our proposed O3N together with an example question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning video representations with O3N</head><p>In this section we present a method to learn video representations in self-supervised manner using odd-one-out learning. In odd-one-out learning, we have to decide how to generate questions. Mainly, our objective is not only to solve the odd-one-out problem, but also to learn good features. If the odd-one-out task is generalizable and the generated questions are related to solving other related tasks, one can expect to obtain good representations for the input video data.</p><p>Our aim is to learn features that are useful for video classification. Specifically, we are interested in action recognition from video data. It is important to learn good temporal representations to solve the action recognition problem. As videos are essentially composed of sequences of frames, by nature most of the videos have a strong temporal structure. Hence, a good video representation should be able to capture this temporal structure within a sequence of frames. We propose to employ the odd-one-out self-supervised learning to exploit the structure within video sequences. Therefore, we generate odd-one-out questions by exploiting the structure of the videos.</p><p>Specifically, let us assume we are given a video sequence X = X 1 , X 2 , ? ? ? X n which consist of n number of RGB frames. The t-th RGB frame is denoted by X t . Because videos are sequences, there are order constraints over frames such that X 1 X 2 ? ? ? X n . The general idea for generating odd-one-out questions is to sub-sample W frames from X where W &lt; n. Then we generate elements of the odd-one-out questions by different sampling strategies. Each of these sampling strategy has implications on the learned features. Next we discuss three sampling strategies, the consecutive sampling, random sampling, and constrained consecutive sampling. Consecutive sampling: We sample W number of consecu-tive frames N times from video X to generate N number of even (related) elements. Each sampled even element of the odd-one-out question is a valid video sub-clip consisting of W consecutive frames from the original video. However, the odd video sequence of length W is constructed by random ordering of frames and therefore does not satisfy the order constraints. These random frames could come from any location of the original video (see <ref type="figure" target="#fig_0">figure 3 right)</ref>. Then the objective of the odd-one-out video network is to learn to recognize the odd (wrong video sequence) out of other N correct sequences. Random sampling: We randomly sample W frames N times from the video X to generate N number of even (related) elements. Each of these N elements are sequences that has the correct temporal order and satisfy the original order constraints of X . However, the frames are not consecutive as in the case of consecutive sampling. The odd video sequence of length W is also constructed by randomly sampling frames. An illustration is shown in <ref type="figure" target="#fig_0">Figure 3</ref> middle. Similar to consecutive sampling strategy, the odd sequence does not satisfy the order constraints. Specifically, we randomly shuffled the frames of the odd element (sequence). Constrained consecutive sampling: In the constrained consecutive sampling strategy, first we sub select a video clip of size 1.5 ? W from the original video which we denote byX . We randomly sample W consecutive frames N times fromX to generate N number of even (related) elements. Each of these N elements are subsequences that have the correct temporal order and satisfy the original order constraints of X . At the same time each of the sampled even video clips of size W overlaps more than 50% with each other. The odd video sequence of length W is also constructed by randomly sampling frames fromX . Similar to other sampling strategies, the odd sequence does not satisfy the order constraints. Specifically, we randomly shuffled the frames of the odd element (sequence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Video frame encoding</head><p>In this section we describe the video-clip encoding step of our odd-one-out network architecture. As shown in <ref type="figure">Fig. 1</ref>, each element (video-clip or subsequence) in an oddone-out question is encoded to extract temporal information before presenting to the first convolutional filters of the network. As mentioned in section 4, odd-one-out networks are presented with sub-sequences of videos. These sub-videos can be valid or invalid (wrong) video clips. We want to use odd-one-out networks to learn video representations by exploiting the structure of the sequences. There are several ways to capture the temporal structure of a video sequence. For example, one can use 3D-convolutions <ref type="bibr" target="#b21">[22]</ref>, recurrent encoders <ref type="bibr" target="#b37">[38]</ref>, rank-pooling encoders <ref type="bibr" target="#b14">[15]</ref> or simply concatenate frames. Odd-one-out networks can use any of the above methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b14">15]</ref> to learn video representations in <ref type="figure">Figure 4</ref>: Several video-clip encoder outputs for action drumming. (a) Dynamic image (b) sum of difference of frames (c-g) stack of difference of frames. All method use sequence size of 6 frames.</p><formula xml:id="formula_2">(a) (b) (c) (d) (e) (f) (g)</formula><p>self-supervised manner using video data. A single RGB image usually contains only static appearance at a specific time point and lacks the contextual information about previous and next frames. In contrast, the RGB difference between two consecutive frames describe the appearance change, which may correspond to the motion salient region. This information is also related to the velocity of the RGB data. Next, we discuss three technique that is used in our experiments to encode video-frame-clips using the differences of RGB frames into a single tensor X d . Sum of differences of frames video-clip encoder: In this method we take the difference of frames and then sum the differences to obtain a single image X d . This single image captures the structure of the sequence. Precisely, this is exactly same as the equation 2 but now applied over frames instead of vectors. It is interesting to note that this equation boils down to a weighted average of frames such that X d = w t X t where the weight of frame at index t is given by</p><formula xml:id="formula_3">w t = W + 1 ? 2t.<label>(3)</label></formula><p>If the input sequence has spatial resolution of h ? w and temporal extent of W , then, the output image has the same spatial resolution but the temporal information is summarized into a single image of size h ? w ? 3 for R,G,B channels (see <ref type="figure">Fig. 4 (b)</ref>). Dynamic image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> encoder: This method is similar to the sum of differences of frames method, however the only difference is that now the input sequence is pre-processed to obtain a smoothed sequence M = M 1 , M 2 , ? ? ? M W .</p><p>Smoothing is obtain using the mean at index t. The smoothed frame at index t denoted by M t is given by</p><formula xml:id="formula_4">M t = 1 t t j=1 X j<label>(4)</label></formula><p>where X j is the frame at index j of the sub-video. The dynamic image can be computed very efficiently. In-fact, dynamic image can be computer as a weighted linear combination of original frames where the weight at index t is com-</p><formula xml:id="formula_5">puted by w t = 2(W ? t + 1) ? (W + 1)(H T ? H t?1 ). Here H t = t i=1</formula><p>1 t is the t-th Harmonic number and H 0 = 0. For complete derivation of Dynamic image we refer the reader to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. An example of a Dynamic Image is shown in (see <ref type="figure">Fig. 4 (a)</ref>). Stack of differences of frames video-clip encoder: Inspired by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref>, we also stack the difference of frames instead of summing them. Once again the objective is to capture the motion and dynamics of short video clips. However, now the resulting image is not any more a standard RGB image with three channels. Instead, we obtain (N ? 1) ? 3 channel image ((see the stack in <ref type="figure">Fig. 4 (c-g)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section we explain the experimental set up and the experimental results which validate the effectiveness of our odd-one (O3N ) learning. We evaluate the usefulness of our odd-one-out learned features on the action classification task. Specifically, we use UCF101 and HMDB51 datasets for self-supervised feature learning from video data and then use the features for action classification.</p><p>The UCF101 dataset <ref type="bibr" target="#b34">[35]</ref> is an action recognition dataset of realistic action videos, collected from YouTube, consists of 101 action categories. It has 13,320 videos from 101 diverse action categories. The videos of this dataset is challenging which contains large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background and illumination conditions. It consist of three splits, in which we report the classification performance over all three splits as done in the literature.</p><p>The HMDB51 dataset <ref type="bibr" target="#b24">[25]</ref> is a generic action classification dataset consists of 6,766 video clips divided into 51 action classes. Videos and actions of this dataset are challenging due to various kinds of camera motions, viewpoints, video quality and occlusions. Following the literature, we use a one-vs-all multi-class classification strategy and report the mean classification accuracy over three standard splits provided by Kuehne et al. <ref type="bibr" target="#b24">[25]</ref>.</p><p>In rest of the sections, we perform several experiments to demonstrate different aspects of odd-one-out learning, network design choices, and the performance of different video-clip encoders when used with O3N networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Default odd-one-out training for videos.</head><p>In this section, we explain the default odd-one-out learning process. By default, we use questions that consist of six video sequences, five sequences with frames in the correct order and a sequence with frames in a wrong order. Each sampled video subsequence consists of six frames sampled according the sampling process described in section 4. Unless otherwise specified we use the random sampling as the default sampling process. We rely on the AlexNet architecture, however, the number of activations in the first fully connected layer is reduced to 128 unless otherwise specified. The sum of difference model architecture (see section 3) is our default activation fusion method. By default, we use the Dynamic Image <ref type="bibr" target="#b4">[5]</ref> as the temporal video-clip encoder. Experiments are run for 200 epochs without batch normalization and with a learning rate varying from 0.01 to 0.0001 in logarithmic manner. The batches are composed of 64 questions. Each question consist of six sub-videos and each sub-video has six frames. The self-supervised network is trained with stochastic gradient descent using Mat-ConvNet <ref type="bibr" target="#b39">[40]</ref>. We use the first split of UCF101 datasets for training of the odd-one-out networks and also for validation. Temporal jittering is used to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Fine tuning for action recognition.</head><p>Once we train the odd-one-out network, with default setting, we use that to initialize the supervised training. We initialize the fine-tuning network (AlexNet architecture <ref type="bibr" target="#b10">[11]</ref> with standard 4096 activation at fully connected layers) with the convolutional filter weights obtained from the oddone-out network. The fully connected layers are fine-tuned with a learning rate 10 times larger than the ones used for convolutional layers (10 ?2 to 10 ?4 ) and batches composed of 128 samples. Typically, the network takes sub-sequences of length six (six frames) as input (same size used in the the odd-one-out network). We use temporal jittering and drop out rate of 0.8.</p><p>During final inference, to compute the classification accuracy, we sample all non-overlapping sub-sequences (consists of six frames) and compute the maximum conditional probabilistic estimate per sequence. Mathematically, let us assume that given long video X we have sub-sample m subsequences of size W , denoted by {X i } where i = 1 ? ? ? m. Therefore the CNN returns the conditional probability of action category y for subsequenceX i which is denoted by p(y|X i ). During the final inference, using i.i.d. assumption, the conditional log probability of the class y given video X is obtained by m i=1 log(p(y|X i )). We use the category that returns the maximum log conditional probability as the predicted class for that video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluating sampling types for O3N learning.</head><p>The objective of the first experiment is to evaluate the impact of sampling types used for odd-one-out networks. In this experiment, we use the default setting for odd-oneout (O3N ) training for videos and use the default finetuning process explained earlier. Odd-one-out training is performed only on the training set of the UCF101 datasets first split. Learned features are used to fine-tune all three  <ref type="table">Table 1</ref>: Comparing several odd-one-out sampling strategies with the random initialization for video action classification on UCF101 dataset.</p><p>splits of the UCF101 separately to evaluate the action classification accuracy. We compare the three sampling types explained in section 4, namely a) consecutive sampling, b) random sampling and c) constrained consecutive sampling. We also compare our O3N initialization with the randomly initialized fine-tuning network results. Results are reported in <ref type="table">Table 1</ref>.</p><p>As it can be seen from the results obtained in <ref type="table">Table 1</ref>, all three initialization methods that uses odd-one-out learning perform better than random initialization on the supervised action classification task. Random initialization obtains only 47.0% over three splits where as O3N consecutive sampling obtains 50.6% which is 3.6% better than random initialization. Interestingly, the constrained consecutive sampling process obtains better results compared to consecutive sampling (52.4%). The random sapling process obtains the best results for both supervised and selfsupervused tasks.</p><p>The consecutive sampling is more confusing task and therefore the most difficult for the network to solve. Videos typically have slow motions and in that case it may be difficult to tell apart correct vs incorrect ordering using consecutive sampling. With such a confusing task, the network might learn little. Moreover, learning from small motions means focusing on small subtelties and this may not generalize well to large general motion understading. This might be the reason for the poor performance of constrained consecutive sampling compared to random sampling. After analysing these results, we conclude that odd-one-out learning obtains better features that potentially generalizes for other tasks and applications such as video action classification. Secondly, more general O3N tasks based on random question generations process such as random sampling seems to generate more generalizable features. Therefore, the odd-one-out learning does not need a carefully designed sampling process to learn good video features as apposed to methods such as <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Capacity of fully connected layers.</head><p>We hypothesize that analogical reasoning tasks such as O3N learning generates useful features. However, if one wants to capture such information in the convolutional fil-  ters, perhaps it is better to limit the capacity of the fully connected layers. To attain this objective, we have introduced two design choices. First, we have reduced the number of activations at the fully connected layers to have only 128 instead of 4096. Secondly, we use sum of difference (SOD) as the fusion method instead of simply concatenating (CON) the activations in our multi-branch network architecture. In this experiment, we evaluate the impact of both these design choices. We use the default experiment protocol but now use O3N with random sampling. First, we evaluate the impact of using 128 dimensional activations compared to 4096 using sum of difference model as the fusion method. Results are reported in <ref type="table" target="#tab_2">Table 2</ref>. Interestingly, a reduced capacity of 128 activations obtains better results than 4096 dimensional activations for both supervised learning and self-supervised learning. When the number of activations is reduced to 128, the self-supervised performance increase from 25.7% to 29.6% which is also reflected in supervised task where the supervised action classification performance improve from 51.3% to 53.2%. It is also possible that this is partially, due to lack of over-fitting. Secondly, we compare the impact of multi branch fusion using feature concatenation (CON) with the sum of differences (SOD) fusion. Results are also reported in <ref type="table" target="#tab_2">Table 2</ref>. Now we are comparing O3N -128-SOD with O3N -128-CON. Interestingly, the feature concatenation obtains the good results compared to sum of difference model for the self-supervised task. However, the supervised action classification results for CON is not as good as the sum of difference (SOD) method. Even if sum of difference method (128-SOD) has relatively poor performance on the self-supervised task (29.6 compared to 33.6), intuitively it has the ability to push down the abstractions about analogical reasoning to the convolutional filters. Therefore, the sum of difference model learns a better feature representations in the expense of slight performance degradation on the task that it solves when used with odd-one-out learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">How big the O3N questions should be?</head><p>In this experiment we evaluate the impact of O3N learning using different number of elements in each O3N question. We use the default experimental protocol with random sampling and train network with 2, 4, 6, 8 and 10 elements (subsequences) in each of the question and report the super-  vised and unsupervised performance on the validation set. Note that the self-supervised task is only trained on the split 1 of UCF101 dataset. Self-supervised task evaluated on the validation set of UCF101 split 1. Results are reported on <ref type="table" target="#tab_4">Table 3</ref>. Note that O3N method with two elements reduces to what is similar to sequence verification method <ref type="bibr" target="#b30">[31]</ref>. As it can be seen from the results in <ref type="table" target="#tab_4">Table 3</ref>, as we increase the number of elements in the O3N question, the unsupervised task becomes harder. As a result, the unsupervised classification accuracy decreases. However, it is interesting to see that O3N task with two elements obtains only 49.3% accuracy on the supervised classification task. However, with the increment of elements in each question, it tends to obtain better results for supervised classification task. On average best supervised results are obtained for a O3N question consist of six elements (i.e. five related correct subsequences and one odd wrong subsequence). Results suggests as the task becomes very difficult (8 and 10 elements), the supervised results saturate and starts to decrease. This is because when tackling a very ambiguousand hard tasks the network may learn very little because it is not able to solve it which is also reflected in the poor performance. When the task is too easy to solve, the network might also not learn much ( question size of 2). In <ref type="table" target="#tab_4">Table 3</ref>, we see this effect.</p><p>6.6. Video-clip encoding methods.</p><p>In this section we compare the impact of O3N learning using three video-clip encoding methods discussed in the section 5. We evaluate the sum of difference of frames (Sum-of-diff.) video-clip encoding, with the Dynamic Image <ref type="bibr" target="#b4">[5]</ref> encoding, and the stacking of sum of difference of frames (Stck.-of-diff.) video-clip encoder. We compare the results for action recognition using UCF101 and HMDB51 datasets on <ref type="table">Table 4</ref>.</p><p>For UCF101 ( <ref type="table">Table 4</ref>) the random initialization using the (Sum-of-diff.) video-clip encoding method (in section 5) obtains only 43.4% while for the same video-clip encoder, the O3N initialization obtains 54.3% which is a significant improvement of 10.9%. Random initialization of dynamic images obtains better results than the sum of difference random initialization. However, with O3N learning, the obtained results for dynamic images is 1.1% worse than the sum of difference method. Most interestingly, the stack-  <ref type="table">Table 4</ref>: Impact of several video-clip encoder methods for odd-one-out learning using UCF101 dataset and HMDB51 over three splits.</p><p>ing of difference of frames obtains the best results for both random initialization and O3N initialization. Using O3N learning we improve the random initialization results for all three video-clip encoder methods with improvements of 10.9%, 6.2% and 9.8% indicating the advantage of O3N learning for video representation learning. Similar trend can be seen for HMDB51 dataset as well (see <ref type="table">Table 4</ref>). When the network is intialized with ImageNet pretrained models, we obtain 64.9 %, 67.2 %, 70.1 % for sum-of-difference video-clip encoding, dynamic images and stack of difference methods respectively on UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Visualizing the learned network filters</head><p>In this section we visualize some of the network filters learned from sum of difference video-clip encoder ( <ref type="figure" target="#fig_1">Fig. 5(a)</ref>), Dynamic Image video-clip encoder ( <ref type="figure" target="#fig_1">Fig. 5(b)</ref>) and stack of difference of frames video-clip encoder <ref type="figure" target="#fig_1">(Fig. 5(c)</ref>). It is not surprising that the learned filters for sum of difference video-clip encoder ( <ref type="figure" target="#fig_1">Fig. 5(a)</ref>) and Dynamic Image video-clip encoder ( <ref type="figure" target="#fig_1">Fig. 5(b)</ref>) is some what similar. Interestingly, the stack of difference of frames video-clip encoder ( <ref type="figure" target="#fig_1">Fig. 5(c)</ref>) has totally different set of filters (note we are visualizing only the mean of the first 5 depths of the conv. filters as the Red channel, the next 5 depths of the filters as Green channel and the last 5 as the Blue channel). All filters, are obtained using six frame clips and using six elements per question which is the default 03N setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8.">Comparing with state-of-the-art.</head><p>In this section, we compare our O3N -based self supervised results with the other state-of-the-art self-supervised methods. Specifically, we compare with DrLim <ref type="bibr" target="#b16">[17]</ref>, Tem-pCoh <ref type="bibr" target="#b31">[32]</ref>, Obj. Patch <ref type="bibr" target="#b43">[44]</ref> and Seq.Ver <ref type="bibr" target="#b30">[31]</ref>. Results are reported in <ref type="table">Table 5</ref>. Note that we use only the split 1 of UCF101 and HMDB51 so that we can compare with other published results <ref type="bibr" target="#b30">[31]</ref>. As it can be seen from the results, our O3N learning-based features obtains score almost 10% higher in UCF101 than the second best method reported in the literature <ref type="bibr" target="#b30">[31]</ref> that relies on sequential verification. Similarly, we obtain massive improvement of 12.7% for HMDB51 dataset over <ref type="bibr" target="#b30">[31]</ref>.   <ref type="table">Table 5</ref>: Comparing with other state-of-the-art selfsupervised learning methods for action classification using UCF101 and HMDB51 datasets.</p><p>It should be noted that when relying on deep architectures pretrained on supervised datasets, like Imagenet <ref type="bibr" target="#b8">[9]</ref>, the state-of-the art reaches about 94.2% ( <ref type="bibr" target="#b42">[43]</ref>) using optical flow, improved trajectory features and RGB data on UCF101. These accuracies from the state-of-the art action recognition methods are always obtained with the of inclusion of several other modalities, such as optical flow, as well as on massive supervised datasets like ImageNet <ref type="bibr" target="#b8">[9]</ref>. With the obtained results, we show some promising directions in self-supervised learning for video data, which contribute towards self-supervised deep networks that could be alternatives to fully supervised or semi-supervised networks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present odd-one-out networks (O3N), a new way to learn visual features for videos without using category level annotations. During feature learning, our O3N learns to do analogical reasoning about the input data leading to better generalizable features. Learned features are fine-tuned for action classification and obtained 60% classification accuracy on UCF101 dataset without resorting to external information or models such as pre-trained networks, or optical flow features. Similarly, we outperform previous-state-ofthe-art results on self-supervised learning for action classification on HMDB51 dataset by more than 12%. Our O3N can be applied over different kinds of temporal encoders. We experimented using three video-clip encoders showing consistent improvements across all of them. In future, we aim to use our odd-one-out network to learn features for images and videos jointly in self-supervised manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Three different types of sampling strategies are investigated for odd-one-out learning. The red box shows the odd video sub-sequence from each of the sampling type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Visualizing the learned first convolutional filter weights for (a) sum of difference video-clip encoder (b) Dynamic Image video-clip encoder (c) Stack of difference video-clip encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method superv. acc.(%) self.sup. acc.(%)</figDesc><table><row><cell>Random initialization</cell><cell>47.0</cell><cell>n/a</cell></row><row><cell>O3N -consec. samp.</cell><cell>50.6</cell><cell>27.4</cell></row><row><cell>O3N -const. consec. samp.</cell><cell>52.4</cell><cell>29.0</cell></row><row><cell>O3N -random sampling</cell><cell>53.2</cell><cell>29.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparing the impact of capacity of O3N networks fully connected layers on feature learning and action classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Impact of number of question (Nq.) on O3N learning on UCF101 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Slow, decorrelated features for pretraining complex cell-like networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>abs/1612.00738</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning transformational invariants from natural movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical rank pooling for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning end-to-end video classification with rank-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning and releaming in boltzmann machines. Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action localization by tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vlad3: Encoding dynamics of deep features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01794</idno>
		<title level="m">Videolstm convolves, attends and flows for action recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised learning using sequential verification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08561</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seeing the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<title level="m">Learning spatiotemporal features with 3d convolutional networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploiting image-trained CNN architectures for unconstrained video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
