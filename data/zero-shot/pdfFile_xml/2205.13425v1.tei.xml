<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhao</forename><surname>Du</surname></persName>
							<email>dudazhao20@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Su</surname></persName>
							<email>bingsu@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
							<email>liyu@idea.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
							<email>zhongangqi@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">ARC Lab, Tencent PCG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Si</surname></persName>
							<email>lingyu@iscas.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<email>yingsshan@tencent.com</email>
							<affiliation key="aff5">
								<orgName type="laboratory">ARC Lab, Tencent PCG</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action classification has made great progress, but segmenting and recognizing actions from long untrimmed videos remains a challenging problem. Most state-ofthe-art methods focus on designing temporal convolution-based models, but the limitations on modeling long-term temporal dependencies and inflexibility of temporal convolutions limit the potential of these models. Recently, Transformer-based models with flexible and strong sequence modeling ability have been applied in various tasks. However, the lack of inductive bias and the inefficiency of handling long video sequences limit the application of Transformer in action segmentation. In this paper, we design a pure Transformer-based model without temporal convolutions by incorporating the U-Net architecture. The U-Transformer architecture reduces complexity while introducing an inductive bias that adjacent frames are more likely to belong to the same class, but the introduction of coarse resolutions results in the misclassification of boundaries. We observe that the similarity distribution between a boundary frame and its neighboring frames depends on whether the boundary frame is the start or end of an action segment. Therefore, we further propose a boundary-aware loss based on the distribution of similarity scores between frames from attention modules to enhance the ability to recognize boundaries. Extensive experiments show the effectiveness of our model.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the exponential increase in the number of videos and short videos uploaded to various platforms (e.g., YouTube, Tiktok, etc.), video content understanding has received increasing attention in the past few years. Action recognition from videos is one of the most active tasks for video content understanding, which can be classified into two categories: classifying trimmed videos with a single activity and segmenting activities in untrimmed videos. The latter is also known as action segmentation. Although approaches based on various architectures have been proposed to improve the accuracy of video classification greatly, their performance is limited by the action segmentation task for untrimmed videos.</p><p>Action segmentation can be treated as a frame-wise classification problem. Most of the previous deep learning methods adapt temporal convolutional networks (TCNs) as their backbones <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref>, which utilize 1D convolution to capture the temporal relationships between different frames. However, TCNs need very deep layers to capture long-term dependencies and the optimal receptive field is hard to determine. The most popular TCN-based model, MS-TCN <ref type="bibr" target="#b11">[12]</ref>, adopts the strategy of doubling the dilation factor in 1D dilated convolution than the previous layer so that the receptive field grows exponentially with the number of layers, but Global2Local <ref type="bibr" target="#b13">[14]</ref> has proven that there exist more effective receptive field combinations than this hand-designed pattern. Even different data distributions will result in different optimal receptive field combinations. Therefore, we need more flexible models that extract the dependency between frames from the data itself, instead of dilated convolutional structures with fixed weights and hand-designed patterns.</p><p>Due to the flexible modeling capabilities, Transformer <ref type="bibr" target="#b47">[48]</ref> outperforms other deep models (RNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, TCNs <ref type="bibr" target="#b1">[2]</ref>) on modeling sequential data in various fields <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50]</ref>. However, to our knowledge, there are few works utilizing Transformer to tackle the action segmentation task except ASFormer <ref type="bibr" target="#b54">[55]</ref>. There exist two issues when applying Transformer to action segmentation. On the one hand, Transformer makes fewer assumptions about the structural bias of input data and thus requires larger amounts of data for training. However, limited by the difficulty of frame-wise annotations, most well-annotated datasets in the action segmentation task such as Breakfast <ref type="bibr" target="#b24">[25]</ref> have only thousands of video samples, which are much smaller than the data scale in other fields <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. On the other hand, the time and space complexities increase quadratically with the length of inputs. The untrimmed video samples consisting of thousands of frames are too long to be directly processed by the self-attention layer in Transformer. ASFormer <ref type="bibr" target="#b54">[55]</ref> combines a sparse attention mechanism and 1D convolutions to tackle these two issues, but it is more like incorporating additional attention modules into MS-TCN. Therefore, it is still an open problem whether a pure Transformer-based model without 1D convolutions is suitable for action segmentation and how to make it work.</p><p>To be able to handle long videos, we first replace full attentions in vanilla Transformer <ref type="bibr" target="#b47">[48]</ref> with local attentions, where each frame only attends to frames within the same local window. But local attention will reduce the receptive field so that the model still can not capture long-term dependencies. To this end, we combine the U-Net architecture <ref type="bibr" target="#b39">[40]</ref> and local-attended Transformer to propose a pure Transformer-based model without 1D convolutions, namely Efficient U-Transformer(EUT). Temporal downsampling in the encoder and upsampling in the decoder are exploited to construct the U-Transformer architecture. Temporal sampling in U-Transformer not only increases the receptive field exponentially with the number of layers but also further reduces the complexity. Moreover, we find that the U-Transformer architecture is well suited for dense prediction tasks because it introduces multi-scale information and priors that adjacent frames are likely to belong to the same class, which compensates for the lack of sufficient training data on action segmentation. This has been demonstrated on another dense prediction task, i.e., semantic segmentation <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>. C2F <ref type="bibr" target="#b42">[43]</ref> also leverages the U-Net architecture for action segmentation, but it still uses 1D convolutions to build the model. However, the U-Transformer architecture exacerbates the misclassification of boundaries because coarse-grained features are fed into the decoder. If a frame on the boundary is encoded as an inappropriate coarse-grained feature, the frames upsampled from it in the decoder will be misclassified. To better perceive boundary information, we categorize the boundary frame in the video into two types: the start frame and the end frame, which represent the start and end of an action segment, respectively, regardless of what the action is. Intuitively, the start frame should be more similar to the neighboring frames after it, while the end frame should be more similar to those before it, which corresponds to two different similarity distributions. We define the similarity distribution of a frame with its neighbors as the local-attention distribution of the frame, which can be obtained from the local-attention module. We thereby introduce a boundary-aware loss by minimizing the distance between the local-attention distribution of the boundary frame with pre-defined prior distributions, which serves as a regularization to enforce the model to pay more attention to boundaries.</p><p>In summary, the contributions of the paper include:</p><p>? For the first time, we propose a pure Transformer-based model without 1D convolutions for action segmentation. Our model incorporates local attention and the U-Net architecture into the Transformer, which has reduced complexity compared to the vanilla Transformer and breaks away from the constraints of 1D convolutions.</p><p>? Based on the distribution of inter-frame similarity scores from the attention module and boundary labels, we propose a distribution-based boundary-aware loss to enable our model to classify boundaries more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Action segmentation. Earlier approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b19">20]</ref> apply a sliding window and use non-maximum suppression to obtain the action segments. Some approaches employ linear dynamical systems <ref type="bibr" target="#b2">[3]</ref> or Bayesian non-parametric models <ref type="bibr" target="#b5">[6]</ref> to model the changes in actions. Such models are difficult to capture the long-term dependencies within a large-scale context. To model the long-term temporal dynamics, graphical models and sequential models such as hidden Markov model (HMM) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref>, GRU <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28]</ref>, LSTM <ref type="bibr" target="#b41">[42]</ref>, stochastic context-free grammar <ref type="bibr" target="#b48">[49]</ref>, and spatiotemporal CNN <ref type="bibr" target="#b28">[29]</ref> are also used to perform action classification at each frame. Motivated by the success of WaveNet <ref type="bibr" target="#b46">[47]</ref> in speech synthesis, many recent works are devoted to exploring multi-scale information with temporal convolution networks (TCNs) for action segmentation. In <ref type="bibr" target="#b29">[30]</ref>, a TCN-based encoderdecoder architecture is proposed. In <ref type="bibr" target="#b30">[31]</ref>, deformable convolution is used to replace conventional convolution and residual connections are added to improve TCN. In <ref type="bibr" target="#b9">[10]</ref>, lateral connections are added to TCN. Since these methods depend on temporal pooling to capture long-term dependencies, the fine-grained temporal information may be lost. In <ref type="bibr" target="#b11">[12]</ref>, MS-TCN addresses this issue by a multistage architecture which is a stack of multiple TCNs. MS-TCN uses 1D dilated convolution, residual connections, and the truncated MSE loss to penalize over-segmentations. It gradually alleviates the over-segmentation issue in each refinement stage. In [56], a bilinear pooling operation is combined with MS-TCN. In <ref type="bibr" target="#b16">[17]</ref>, a graph-based temporal reasoning module is added on top of MS-TCN. In <ref type="bibr" target="#b32">[33]</ref>, MS-TCN++ replaces dilated temporal convolution layers in MS-TCN with dual dilated layers that combine dilated convolutions with large and small dilation factors. G2L <ref type="bibr" target="#b13">[14]</ref> proposes a global-local search scheme to search for effective receptive field combinations instead of hand-designed patterns. C2F <ref type="bibr" target="#b42">[43]</ref> combines U-Net architecture and 1D convolution, resulting in a coarse-to-fine structure. ASFormer <ref type="bibr" target="#b54">[55]</ref> firstly introduces attention modules to action segmentation. In this paper, Transformer is integrated into the U-Net architecture, resulting in an efficient pure Transformer model without convolutions.</p><p>Transformer. Transformer <ref type="bibr" target="#b47">[48]</ref> was originally designed for sequence-to-sequence tasks in NLP. Afterward, Transformer-based models have succeeded in many other fields because of their powerful modeling capabilities and flexible architecture. For example, many researchers apply hybrid Transformer models in different video understanding tasks including visual tracking <ref type="bibr" target="#b53">[54]</ref>, video instance segmentation <ref type="bibr" target="#b49">[50]</ref> and video action recognition <ref type="bibr" target="#b14">[15]</ref>. To our best knowledge, the only Transformer-based model for the action segmentation task is ASFormer <ref type="bibr" target="#b54">[55]</ref>. However, ASFormer utilizes 1D dilated convolution in each layer to bring in strong inductive priors, which does not break through the limitations of convolution. In this paper, we propose a pure Transformer model which combines the U-Net architecture and Transformer. Besides, Informer [57] selects dominant queries by measuring the distance between their attention probability distributions and uniform distribution to reduce the complexity of self-attention. Anomaly Transformer <ref type="bibr" target="#b52">[53]</ref> utilizes discrepancy between each time point's prior-association and its series-association to detect anomalies. Differently, we minimize the distance between the prior distribution for boundary frames and the distribution of local attentions to enhance the ability to discriminate boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>In this section, we recap the preliminaries in Transformer and the problem formulation of action segmentation.</p><p>Transformer. Each layer in Transformer consists of two main components: a multi-head selfattention module and a position-wise feed-forward network <ref type="bibr" target="#b47">[48]</ref>. We denote the input of the selfattention module as H ? R T ?d , where T and d are the length and dimension of the input, respectively. For the single-head situation, the input is projected by three matrices </p><formula xml:id="formula_0">W Q ? R d?d Q , W K ? R d?d K and W V ? R d?d V to</formula><formula xml:id="formula_1">{k 1 , . . . , k T } and V = {k 1 , . . . , k T }.</formula><p>Then the attention calculation is given by:</p><formula xml:id="formula_2">Attention(Q, K, V ) = Softmax( QK T ? d K )V = Softmax(A)V (1)</formula><p>where A is the attention matrix consisting of all the similarity scores between any query-key pair. The output of multi-head self-attention is computed by concatenating the outputs of all the single-head modules described above and projecting the concatenation of each frame into a vector.</p><p>Action Segmentation. Given an input long video, we first extract a feature vector from each frame, so that the video is represented by a sequence of frame-wise features:</p><formula xml:id="formula_3">x = [x 1 , ? ? ? , x T ] ? R T ?d , where x t is the feature of the t-th frame, d is the dimensionality of x t ,</formula><p>and T is the total number of frames, i.e., the length of the video. Our goal is to predict the class label? t ? {1, ? ? ? , C} for each frame x t , resulting in the prediction [? 1 , ? ? ? ,? T ], where C is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">U-Transformer Architecture</head><p>In this section, we present our model in detail. EUT contains a prediction generation stage and M refinement stages following the multi-stage architecture in MS-TCN <ref type="bibr" target="#b11">[12]</ref>. The generation stage generates initial segmentation predictions while each refinement stage refines the predictions of the previous stage. These stages have the same architecture. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, each stage can be separated into four components: the input projection, the encoder consisting of N identical encoder layers, the decoder consisting of N identical decoder layers, and the output classifier. Both the input projector and the output classifier are fully connected layers, which reduce the input dimension to feed the encoder and classify the output of the decoder, respectively.</p><p>Local Attention. In the original self-attention module of Transformer, any query q i needs to calculate the similarity scores with all the keys {k 1 , . . . , k T } to generate the attention matrix A, which leads to quadratic complexity, i.e., the complexity is O(T 2 ). Restricting the attention computation to a local window with a fixed size w can reduce the operation to linear complexity O(wT ), which is called local attention. At this point, each query q i only needs to calculate the similarity with those keys in the window centered on its position, i.e., {k s , . . . , k i , . . . , k e }, where s = max{i ? w 2 , 0} and e = min{i + w 2 , T } represent the start and end position respectively. Therefore, the output of the i-th position is:</p><formula xml:id="formula_4">o T i = Softmax( q T i k s ? d k , . . . , q T i k i ? d k , . . . , q T i k e ? d k )(v s , . . . , v i , . . . , v e ) T ,<label>(2)</label></formula><formula xml:id="formula_5">LocalAttention(Q, K, V ) = [o 1 , o 2 , . . . , o T ].<label>(3)</label></formula><p>The local attention does not narrow down the overall receptive field of the model. Due to temporal sampling between layers, the receptive field increases exponentially with the number of layers, which is sufficient to cover the entire video sequence to capture global and local dependencies. Under a common hyper-parameter setting, e,g., w = 51 and N = 5, the receptive field can reach as much as w ? 2 2N ? 50000, thus both local and global receptive fields of our EUT are larger than those of TCN-based models.</p><p>Scale-Shared Positional Encoding. Since the attention mechanism in Transformer can not perceive the positions of elements, many works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref> adopt various strategies to introduce positional information. Since the lengths of untrimmed videos in action segmentation tasks are usually too long and vary drastically, absolute position encoding will influence the performance. Therefore, we employ the learnable relative positional encoding <ref type="bibr" target="#b40">[41]</ref>, the basic idea of which is to embed the relative distances of all query-key pairs as scalars and add them to the attention matrix.</p><p>Considering the distance between any two elements within a local window does not exceed the window size w, we can get the relative position encoding R ij between q i and k j by a learnable embedding matrix W rpe ? R w?h , where i, j represent the position index and h is the number of heads. The resulting positional encoding R will be added to the corresponding positions of the attention matrices in different heads. Layers with the same layer index in different stages process inputs with the same temporal resolution, and their RPEs should be the same. Therefore, we adopt a scale-shared strategy, i.e., corresponding layers with the same scale in different stages share the same W rpe . Besides, we only apply relative positional encoding in encoder layers.</p><p>Fine-to-Abstract Encoder. The encoder is composed of N identical encoder layers. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, it is similar to the encoder in the vanilla Transformer <ref type="bibr" target="#b47">[48]</ref> but there are three differences.</p><p>Firstly, there exists a nearest-neighbor downsampling process at the beginning of each layer, which halves the input temporal dimension. Secondly, the full attention is replaced by the local attention with scale-shared relative position encoding. Thirdly, we utilize the instance normalization <ref type="bibr" target="#b45">[46]</ref> instead of the layer normalization <ref type="bibr" target="#b0">[1]</ref>. In summary, the set of operations at the l-th encoder layer can be formally described as follows: </p><p>Abstract-to-Fine Decoder. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the decoder consisting of N identical layers is symmetric to the encoder. In each decoder layer, the temporal upsampling is utilized to gradually restore the original temporal resolution of input frames. The upsampling process is also implemented by nearest interpolation. We do not concatenate the encoder layer input and the previous layer input as the decoder layer input like the original U-Net <ref type="bibr" target="#b39">[40]</ref>, which will take up more memory. To keep the hidden dimension, we modify the cross-attention in the original Transformer to leverage the information from the encoder. Specifically, in our local cross attention, the query and key both come from the output of the previous decoder layer, while the value is generated by the output of the corresponding encoder layer having the same temporal dimension as the key. Therefore, the l-th decoder layer generates the representation H l de from H l?1 de of the (l?1)-th layer as follows:</p><formula xml:id="formula_7">H l,1 de = UpSample(H l?1 de ), H l,2 de = InstanceNorm(LocalAttention(H l,1 de W l Q,de , H l,1 de W l K,de , H N ?l en W l V,de ) + H l,1 de ), H l de = InstanceNorm(FeedForward(H l,2 de ) + H l,2 de ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Boundary-aware Loss</head><p>During the training phase, we combine three different losses: the frame-wise classification loss L CE , the smoothing loss L T M SE in <ref type="bibr" target="#b11">[12]</ref>, and our proposed boundary-aware loss L BA . Since the loss function of each stage is exactly the same, we only analyze the loss of the s-th stage L s . Following <ref type="bibr" target="#b11">[12]</ref>, we use a cross-entropy loss as L s CE , and a truncated mean squared error over the frame-wise log-probabilities as L s T M SE in the s-th stage:</p><formula xml:id="formula_8">L s CE = 1 T T t=1 ?log(y s t (c t )),<label>(6)</label></formula><formula xml:id="formula_9">L s T M SE = 1 T C T t=1 C c=1 (max(|log(y s t (c)) ? log(y s t?1 (c))|, ?)) 2 ,<label>(7)</label></formula><p>where y s t (c t ) is the predicted probability that x t belongs to the c t -th class, and ? = 4 is a pre-set threshold. In L s T M SE , gradients are not calculated w.r.t. y s t?1 (c). Action boundaries are vital for video action segmentation. Frame-wise classification treats boundary frames and intermediate frames equally, which causes the features learned to be not sensitive to boundaries. We propose a novel boundary-aware loss to enhance the ability to discriminate boundaries. The boundary-aware loss regularizes feature learning by imposing additional constraints on the attention matrix in the local attention module.</p><p>Prior Distribution. Intuitively, if a frame has small similarities with the neighboring frames in its backward direction and large similarities with those in its forward direction, it has a high probability to be a start frame; if a frame is similar to the neighboring frames in its forward direction but different to those on its backward direction, it is probably an end frame. Therefore, the similarity distribution between a boundary frame j (anchor) and its neighbors should exhibit two different patterns, depending on whether the anchor is the start frame or the end frame. We use the adapted sign function as the two prior distributions corresponding to the above two patterns:</p><formula xml:id="formula_10">P = Rescale [(?Sgn(?(i ? j)) + 1] , j ? w 2 ? i ? j + w 2 ,<label>(8)</label></formula><p>where (i ? j) means the distance between frame i and anchor j, ? = 0.8 represents the scale factor. The sign ? is positive or negative, corresponding to the start frame and end frame, respectively. Sgn(x) is 1 when x is greater than or equal to 0 and -1 when x is less than 0. Further, we use Rescale[?] to transform the sum of probabilities to 1.</p><p>Local-Attention Distribution. The attention matrix in the local attention module consists of all the similarity scores between query-key pairs. We can extract the similarity distribution of the anchor and its neighboring frames from the attention matrix A, named as local-attention distribution of the anchor j:</p><formula xml:id="formula_11">D = A j, (j ? w 2 ) : (j + w 2 ) , w 2 ? j ? T ? w 2 .<label>(9)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we introduce a boundary-aware loss to approximate the local-attention distribution of the boundary to the corresponding prior distribution, which can be implemented by minimizing the symmetrized KL divergence between the distributions:</p><formula xml:id="formula_12">L s BA = KL(P s ||D s ).<label>(10)</label></formula><p>We can freely obtain boundary labels from class labels and the similarity distribution of each boundary from the local attention module. Therefore, we can calculate the boundary-aware loss without additional modules and annotations. Considering that the temporal downsampling blurs high-level boundaries, we only calculate it in low-level layers. Therefore, the final loss for the s-th stage is the weighted sum of the three losses:</p><formula xml:id="formula_13">L s = L s CE + ?L s T M SE + ?L s BA .<label>(11)</label></formula><p>Following <ref type="bibr" target="#b32">[33]</ref>, we set ? = 0.15. And ? is an adjustable hyper-parameter.</p><p>The overall loss function of all stages in the training phase is L = s L s . In the testing phase, we use the frame classification results of the last refinement stage as our segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Datasets. We empirically evaluate the performance of the proposed method on three datasets: the 50Salads dataset <ref type="bibr" target="#b43">[44]</ref>, the Georgia Tech Egocentric Activities (GTEA) dataset <ref type="bibr" target="#b12">[13]</ref>, and the Breakfast dataset <ref type="bibr" target="#b24">[25]</ref>. The 50Salads dataset consists of 50 top-view videos with 17 action classes. On average, each video lasts for about 6.4 minutes and contains about 20 action instances. Following <ref type="bibr" target="#b43">[44]</ref>, we perform five-fold cross-validation and report the average performances. The GTEA dataset consists of 28 egocentric videos with 11 action classes including the background class from 7 activities. On average, there are about 20 action instances in each video. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>, we perform four-fold cross-validation by leaving one subject out for each fold and report the average performances. The Breakfast dataset consists of 1,712 third-person view videos with 48 action classes. On average, each video contains about 6 action instances. Following <ref type="bibr" target="#b24">[25]</ref>, we perform four-fold cross-validation on the standard 4 splits and report the average performances.</p><p>Evaluation metrics. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>, we use the segmental F1 score at overlapping thresholds 10%, 25%, 50% (F1@{10,25,50}), segmental edit distance (Edit) measuring the difference between predicted segment paths and ground-truth segment paths, and frame-wise accuracy (Acc) as evaluation metrics. Larger is better for all metrics.</p><p>Implementation details. On all datasets, we represent each video as a sequence of visual features. We employ the I3D <ref type="bibr" target="#b4">[5]</ref> frame-wise features provided in <ref type="bibr" target="#b32">[33]</ref> as inputs. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>, the temporal video resolution is fixed to 15 fps on all datasets. We employ the ADAM <ref type="bibr" target="#b23">[24]</ref> optimizer for a maximum number of 150 epochs to train our model on all datasets. The batch size is set to 1. Detailed training parameters and model settings can be found in Section 8.2. All experiments are implemented with PyTorch <ref type="bibr" target="#b35">[36]</ref> and conducted on one NVIDIA TITAN RTX 24GB GPU. All ablation studies about hyper-parameters and model structures are performed on the 50Salads dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we compare our proposed model with the state-of-the-art methods on three datasets. For a fair comparison of all models, we list the results of training our model without the additional boundary-aware loss, which corresponds to EUT ? . To demonstrate the effectiveness of our proposed boundary-aware loss, we also report the results of our model jointly trained with the additional boundary-aware loss, called EUT.</p><p>On the smallest dataset, GTEA, our performance is slightly behind the top methods, e.g. C2F <ref type="bibr" target="#b42">[43]</ref>. This is reasonable since GTEA has only 21 training samples per split, which are not adequate to train our pure Transformer model. But our model is still better than some other models such as MS-TCN <ref type="bibr" target="#b11">[12]</ref>, which also proves the superiority of the attention mechanism. On two larger datasets, 50Salads and Breakfast, EUT significantly outperforms the previous TCN-based backbone models on all metrics by a large margin, which proves the advantages of Transformer-based models in processing large-scale sequence data. On the 50Salads dataset, our backbone EUT ? beats all previous models on all metrics. Regardless of whether our model is trained with or without the boundary-aware loss, the On the Breakfast dataset, our model achieves the best performance on the F1 metrics, while the performance on Edit and Acc ranks in the top two of all methods. We also visualize some prediction results of our model in Section 9.1.</p><p>Both ASRF <ref type="bibr" target="#b17">[18]</ref> and BCN <ref type="bibr" target="#b50">[51]</ref> use TCN-based models as the feature enhancement backbone. Similar to our boundary-aware loss, they utilize additional auxiliary tasks and training loss to improve performance, which can also be used in our Transformer-based models to expect further performance improvements. Besides, EUT outperforms EUT ? on all the datasets, which demonstrates the effectiveness of the proposed boundary-aware loss. Models trained with the boundary-aware loss are better able to recognize boundary frames, which leads to improvements in all metrics. An intuitive demonstration of the effect of boundary-aware loss is presented in Section 5.3. Comparison of different positional encoding. To verify the effectiveness of relative positional encoding (RPE) in EUT, we compare different positional encoding (PE) methods in <ref type="table" target="#tab_2">Table 2</ref>. Since the lengths of video samples vary over a large span, we observe that inflexible absolute positional encoding (APE) leads to performance degradation. The overall performance of RPE is better than that without PE, which illustrates the importance of position information. We also compare three different sharing ways of applying RPE. Stage-Shared has the worst performance since it applies the same RPE to layers that deal with inputs of different resolutions. Therefore, layers of different scales can not share RPE. Scale-Shared performs best, which means that the attention patterns of layers with the same scale are similar, even if they belong to different stages. Ablations of the architecture and attention patterns. To explore the impact of the model architecture and attention methods, we compare two model architectures and three attention patterns, resulting in a total of 6 combinations. We remove all the temporal sampling in EUT to get the standard architecture. Attention patterns include full attention in the original Transformer <ref type="bibr" target="#b47">[48]</ref>, local attention, and LogSparse attention proposed in <ref type="bibr" target="#b31">[32]</ref>, where each cell only attends to those cells whose distance from it increases exponentially. The reason we compare LogSparse is that its attention pattern is somewhat similar to the pattern in MS-TCN where the convolutional dilation factor exponentially increases with layers. In the standard architecture, long video inputs will cause out-of-memory. To avoid this, we downsample video samples below 5000 frames in all experiments in <ref type="table" target="#tab_3">Table 3</ref>. For a fair comparison, we control all model configurations and training parameters to be consistent. Considering that RPE introduces additional parameters, we do not use RPE in these ablation experiments. The U-Transformer architecture achieves better performance than the standard architecture with less GPU memory consumption. Full attention fails regardless of the architecture, showing that training on small data requires sparser attention patterns. Since adjacent frames usually have a stronger correlation in action segmentation, local attention performs much better than LogSparse attention. Effect of the boundary-aware loss. The quantitative impact of the boundary-aware loss on performance can already be observed in <ref type="table" target="#tab_1">Table 1</ref>. Therefore, in this section, we present visualization results in <ref type="figure" target="#fig_2">Figure 3</ref>. We observe that the positions of the two boundary frames which are predicted by the EUT model trained with the boundary-aware loss are closer to the positions of the boundary frames in the ground truth. We also plot the local-attention distributions of these two boundary frames. When the model is trained without the boundary-aware loss, the local-attention distribution extracted from its attention matrix is irregular. This proves that the model does not impose constraints on the boundaries, resulting in inaccurate boundaries. However, when the model is trained with the boundary-aware loss, the local-attention distributions are very similar to the prior distributions in <ref type="figure" target="#fig_1">Figure 2</ref>, which correspond to two patterns, i.e., the pattern of the start frame and the pattern of the end frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>More ablation studies about hyper-parameters are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In action segmentation tasks, most popular deep learning methods use 1D convolutional networks as their backbones. For the first time, we propose a pure Transformer model which combines the U-Net architecture and Transformer. The temporal downsampling and local attention modules together enable our model to efficiently process long videos. Furthermore, we propose a novel boundary-aware loss based on the local-attention distributions of boundary frames, which serves as a regularization term to train the model and can further enhance the ability of discriminating boundaries.   In this section, we conduct more ablation studies about more hyper-parameters in the multi-head local attention: the window size w and the number of heads h. Since the parameter amount of the embedding matrix W rpe ? R w?h in RPE depends on w and h, we do not use RPE in these ablations for a fair comparison. If w is small, the receptive field is too small to capture long-term dependencies between frames; and if w is too large, the model is difficult to be trained adequately on small-scale data. Therefore, it is best to set w to a number that is neither too large nor too small. In <ref type="table" target="#tab_5">Table 4</ref>, the model performs best when w = 51.</p><p>We observe that the model performs better as the number of heads h increases in <ref type="table" target="#tab_6">Table 5</ref>. Since each head maintains an attention matrix, the memory consumption also increases with the number of heads. Considering that the performance gain brought by the additional memory consumption is not obvious, we set h = 4 by default on the 50Salads dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Impact of ?</head><p>Since the effect of the boundary-aware loss is controlled by the hyper-parameter ?, we study how it affects the performance of the proposed model in <ref type="table">Table 6</ref>. According to <ref type="table">Table 6</ref>, we set ? = 0.15 on the 50Salads dataset. When reducing the value of ? to 0.05, the performance improvement is not obvious. Larger values of ? cause performance degradation, the reason of which is that the transitional focus on boundary frames reduces the classification accuracy of intermediate frames.</p><p>8 Experiment Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Details of Datasets</head><p>We list the details of three action segmentation datasets in <ref type="table">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Details of Settings</head><p>For all experiments, we employ a learning rate decay strategy, which reduces the learning rate by half when the training loss increases three times. In <ref type="table" target="#tab_8">Table 8</ref>, we report the model configurations and training hyper-parameters corresponding to the experimental results in <ref type="table" target="#tab_1">Table 1</ref>. Following MS-TCN and ASFormer, we select the epoch number that can achieve the best average result for all splits to report the results on every dataset. On 50Salads, the sampling rate is reduced from 30 fps to 15 fps, and the output is up-sampled to 30 fps. Weight Decay 1e-5 1e-5 1e-5 9 Qualitative Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Comparison with ASFormer</head><p>ASFormer is the best backbone model before, so we compare its predictions and ours in <ref type="figure">Figure 4</ref>. Our purpose is to fairly compare the performance of backbone models, so we show results of our model which is trained without the boundary-aware loss. The action segments in the green circle are examples that our model classifies correctly but ASFormer misclassifies, while the action segments in the red circle are the opposite. Overall, our model works much better. For those very short action segments (compared to the entire video length), our model may ignore them. And for other cases, our model performs reasonably well. <ref type="figure">Figure 4</ref>: Visualization of the segmentation results of some videos, where the GT row represents the ground truth, the ASFormer row represents the predictions of the ASFormer, and Ours represents the predictions of our model. The same color represents the same action class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">The Effect of Boundary-Aware Loss</head><p>We show more visual comparisons of our model which is trained with or without the boundary-aware loss in <ref type="figure">Figure 5</ref>. Comparing the results in the green circle, we observe that the boundary frames predicted by the model trained with boundary-aware loss are closer to the real boundary frames. <ref type="figure">Figure 5</ref>: Visualization of the segmentation results of some videos, where the GT row represents the ground truth, the with row and without row represent the predictions of our EUT which is trained with and without the boundary-aware loss, respectively. The same color represents the same action class.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>obtain the query, key and value matrices: Q = {q 1 , . . . , q T }, K = Overview of a stage in our EUT model. Since all stages have the same architecture, we only present the picture of one stage. It consists of two main parts. The encoder stacks several encoder layers to capture temporal dependencies to generate high-level features. These features are passed into the decoder, where several decoder layers are utilized to generate frame-wise features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic diagram of the boundary-aware loss. (1) An example of misclassification of boundaries, where GT and EUT represent the ground truth and model output, respectively. Frames of the same color belong to the same class. (2) A sub-matrix taken from the attention matrix, which contains the start and end frames of an action segment. (3) The local-attention distributions of the rows in the attention matrix corresponding to the start frame and the end frame, respectively. (4) The boundary-aware loss aims to minimize the KL divergence between the local-attention distribution and the prior distribution. (5) Model output after training with the boundary-aware loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The segmentation results of two video clips and the local-attention distributions of the boundary frames, where the GT row represents the ground truth, the with row and without row represent the predictions of our EUT which is trained with and without the boundary-aware loss, respectively. Frames of the same class are shown in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state-of-the-art results on the 50Salads, GTEA, and Breakfast datasets. EUT ? is the proposed EUT model trained without the boundary-aware loss. Bold and underlined denote the best and second-best results in each column, respectively.</figDesc><table><row><cell>Dataset</cell><cell>50Salads</cell><cell></cell><cell>GTEA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Breakfast</cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="10">F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc F1@{10,25,50} Edit Acc</cell></row><row><cell>IDT+LM [37]</cell><cell>44.4 38.9 27.8 45.8 48.7 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Bi-LSTM [42]</cell><cell cols="6">62.6 58.3 47.0 55.6 55.7 66.5 59.0 43.6 -55.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Dilated TCN [30] 52.2 47.6 37.4 43.1 59.3 58.8 52.2 42.2 -58.3 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-CNN [29]</cell><cell cols="6">55.9 49.6 37.1 45.9 59.4 58.7 54.4 41.9 -60.6 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ED-TCN [30]</cell><cell cols="6">68.0 63.9 52.6 52.6 64.7 72.2 69.3 56.0 -64.0 -</cell><cell>-</cell><cell>-</cell><cell cols="2">-43.3</cell></row><row><cell>TDRN [31]</cell><cell cols="6">72.9 68.5 57.2 66.0 68.1 79.2 74.4 62.7 74.1 70.1 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS-TCN [12]</cell><cell cols="10">76.3 74.0 64.5 67.9 80.7 85.8 83.4 69.8 79.0 76.3 52.6 48.1 37.9 61.7 66.3</cell></row><row><cell cols="11">MS-TCN++ [33] 80.7 78.5 70.1 74.3 83.7 88.8 85.7 76.0 83.5 80.1 64.1 58.6 45.9 65.6 67.6</cell></row><row><cell>BCN [51]</cell><cell cols="10">82.3 81.3 74.0 74.3 84.4 88.5 87.1 77.3 84.4 79.8 68.7 65.5 55.0 66.2 70.4</cell></row><row><cell cols="11">Global2Local [14] 80.3 78.0 69.8 73.4 82.2 89.9 87.3 75.8 84.6 78.5 74.9 69.0 55.2 73.3 70.7</cell></row><row><cell>ASRF [18]</cell><cell cols="10">84.9 83.5 77.3 79.3 84.5 89.4 87.8 79.8 83.7 77.3 74.3 68.9 56.1 72.4 67.6</cell></row><row><cell>C2F [43]</cell><cell cols="10">84.3 81.8 72.6 76.4 84.9 90.3 88.8 77.7 86.4 80.8 72.2 68.7 57.6 69.6 76.0</cell></row><row><cell>ASFormer [55]</cell><cell cols="10">85.1 83.4 76.0 79.6 85.6 90.1 88.8 79.2 84.6 79.7 76.0 70.6 57.4 75.0 73.5</cell></row><row><cell>EUT  ?</cell><cell cols="10">88.7 87.2 81.3 82.5 87.1 87.2 85.2 72.2 83.1 76.3 76.0 71.4 59.7 73.9 74.7</cell></row><row><cell>EUT</cell><cell cols="10">89.2 87.5 81.0 82.9 87.4 88.2 87.2 74.0 83.9 77.0 76.2 71.8 59.8 74.6 75.0</cell></row></table><note>results of our model are better than ASFormer [55]. Specifically, compared to ASFormer, EUT ? gives 3.6% (85.1?88.7), 3.8% (83.4?87.2), 5.3% (76.0?81.3) improvements on three F1 scores, 2.9% (79.6? 82.5) improvement on the segmental edit distance, and 1.5% improvement on the accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different positional encoding. APE and RPE refer to the absolute positional encoding and relative positional encoding, respectively. We also explore three ways of applying RPE: all layers do not share W rpe (Without Share), all layers in each stage share W rpe (Stage-Shared), and corresponding layers with the same scale in different stages share W rpe (Scale-Shared).</figDesc><table><row><cell>Positional Encoding</cell><cell>Strategy</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>Without PE</cell><cell></cell><cell cols="2">86.4 85.4 78.6 80.5 85.4</cell></row><row><cell></cell><cell>Sinusoidal</cell><cell cols="2">82.8 80.6 71.3 77.0 82.0</cell></row><row><cell>APE</cell><cell>Learnable</cell><cell cols="2">80.5 77.7 69.1 73.5 82.0</cell></row><row><cell></cell><cell cols="3">Without Share 87.1 86.6 79.4 81.9 86.0</cell></row><row><cell>RPE</cell><cell cols="3">Stage-Shared 85.8 85.3 78.6 80.9 85.4</cell></row><row><cell></cell><cell cols="3">Scale-Shared 88.7 87.2 81.3 82.5 87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different architectures and attention patterns in EUT. Standard refers to an architecture where the temporal resolution of each layer is unchanged, while U-Transformer refers to our proposed architecture. Full, LogSparse, and Local are three attention patterns.</figDesc><table><row><cell>Architure</cell><cell>Attention Pattern</cell><cell cols="3">F1@{10,25,50}</cell><cell>Edit Acc GPU Mem.</cell></row><row><cell></cell><cell>Full [48]</cell><cell>4.6</cell><cell>2.8</cell><cell>1.4</cell><cell>3.3 62.8~18.7G</cell></row><row><cell>Standard</cell><cell>LogSparse [32]</cell><cell cols="4">56.2 51.7 41.2 45.3 69.0~18.7G</cell></row><row><cell></cell><cell>Local</cell><cell cols="4">74.6 72.2 63.1 64.8 81.0~4.6G</cell></row><row><cell></cell><cell>Full [48]</cell><cell cols="4">35.1 25.4 9.8 31.9 43.8~9.9G</cell></row><row><cell>U-Transformer</cell><cell>LogSparse [32]</cell><cell cols="4">73.3 71.9 63.7 65.1 80.3~9.9G</cell></row><row><cell></cell><cell>Local</cell><cell cols="4">86.5 85.3 76.9 80.6 84.4~2.8G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>[56] Y.Zhang, K. Muandet, Q. Ma, H. Neumann, and S. Tang. Low-rank random tensor for bilinear pooling. arXiv, pages arXiv-1906, 2019. [57] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021.</figDesc><table><row><cell>7 More Ablations</cell></row><row><cell>7.1 Impact of w and h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of the window size w in local attention.</figDesc><table><row><cell>w</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell cols="3">11 78.9 77.0 70.1 71.0 84.0</cell></row><row><cell cols="3">31 84.3 83.0 75.6 77.3 85.8</cell></row><row><cell cols="3">51 86.4 85.4 78.6 80.5 85.4</cell></row><row><cell cols="3">71 85.7 84.2 78.2 79.5 84.7</cell></row><row><cell cols="3">91 85.2 83.5 76.7 79.3 84.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Effect of the number of heads h in multi-head local attention.</figDesc><table><row><cell>h</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc GPU Mem.</cell></row><row><cell cols="3">1 85.1 83.2 74.3 78.0 83.3~3.0G</cell></row><row><cell cols="3">2 84.7 83.5 75.6 78.1 84.3~3.1G</cell></row><row><cell cols="3">4 86.4 85.4 78.6 80.5 85.4~3.5G</cell></row><row><cell cols="3">8 87.0 86.1 79.2 80.9 85.7~5.0G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :Table 7 :</head><label>67</label><figDesc>Effect of ? which is the weight of the proposed boundary-aware loss. 87.2 81.3 82.5 87.1 0.05 88.9 87.0 81.2 82.7 87.3 0.15 89.2 87.5 81.0 82.9 87.4 0.25 87.6 86.6 80.6 81.5 86.0 0.35 86.9 85.3 79.4 80.3 85.5 Details of three datasets. #Sample and #Class are the numbers of video samples and classes, respectively. #Split is the numer of splits. Length is the average frames of videos.</figDesc><table><row><cell></cell><cell>?</cell><cell cols="2">F1@{10,25,50}</cell><cell cols="2">Edit Acc</cell></row><row><cell cols="5">0 88.7 Datasets #Sample Length #Class #Split</cell><cell>Scene</cell></row><row><cell>50Salads [44]</cell><cell>50</cell><cell>11552</cell><cell>17</cell><cell>5</cell><cell>preparing salads</cell></row><row><cell>GTEA [13]</cell><cell>28</cell><cell>1115</cell><cell>11</cell><cell>4</cell><cell>daily activities</cell></row><row><cell>Breakfast [25]</cell><cell>1712</cell><cell>2097</cell><cell>48</cell><cell>4</cell><cell>cooking breakfast</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Model configurations of EUT and training hyper-parameters on three datasets.</figDesc><table><row><cell></cell><cell cols="3">50Salads GTEA Breakfast</cell></row><row><cell>#Refinement Stages M</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>#Layers N</cell><cell>5</cell><cell>4</cell><cell>5</cell></row><row><cell>Window Size w</cell><cell>51</cell><cell>11</cell><cell>31</cell></row><row><cell>Hidden Dimension in the Prediction stage</cell><cell>128</cell><cell>64</cell><cell>192</cell></row><row><cell>FFN Inner Dimension in the Prediction stage</cell><cell>128</cell><cell>64</cell><cell>192</cell></row><row><cell>Hidden Dimension in Refinement stages</cell><cell>64</cell><cell>64</cell><cell>96</cell></row><row><cell>FFN Inner Dimension in Refinement stages</cell><cell>64</cell><cell>64</cell><cell>96</cell></row><row><cell>#Attention Heads h</cell><cell>4</cell><cell>4</cell><cell>6</cell></row><row><cell>Input Dropout</cell><cell>0.4</cell><cell>0.5</cell><cell>0.4</cell></row><row><cell>FFN Dropout</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>Attention Dropout</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Learning Rate</cell><cell>5e-4</cell><cell>5e-4</cell><cell>2e-4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition of complex events: Exploiting temporal dynamics between underlying concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2235" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<title level="m">Swin-unet: Unet-like pure transformer for medical image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent networks and narma modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global2local: Efficient structure search for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16805" to="16814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14024" to="14034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Alleviating over-segmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2322" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-compound transformer for accurate biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="326" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><forename type="middle">C</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hybrid rnn-hmm approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ms-tcn++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Coarse to fine multi-resolution temporal convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10859</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Wavenet: A generative model for raw</title>
		<imprint>
			<publisher>SSW</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Anomaly transformer: Time series anomaly detection with association discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02642</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal transformer for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17154</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Asformer: Transformer for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
