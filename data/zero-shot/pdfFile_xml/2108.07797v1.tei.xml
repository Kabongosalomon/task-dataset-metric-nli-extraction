<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group-aware Contrastive Regression for Action Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
							<email>raoyongming95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country>China, China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Group-aware Contrastive Regression for Action Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of CoRe, we conduct extensive experiments on three mainstream AQA datasets including AQA-7, MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action quality assessment (AQA), which aims to evaluate how well a specific action is performed, has attracted growing attention in recent years since it plays a crucial role in many real world applications including sports <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21]</ref>, healthcare <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">42]</ref> and others <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Unlike conventional action recognition tasks * Equal contribution. ? Corresponding author.  <ref type="figure">Figure 1</ref>: Our Contrastive Regression (CoRe) framework for action quality assessment. Inspired by contrastive learning that learns representation by encouraging the distances of samples (e.g., d A and d B ) to reflect their semantic relationship, we learn an AQA model to regress the relative scores (e.g., ? A and ? B ) to reflect the differences of action quality among videos. By comparing two videos with different scores, CoRe encourage the model to learn from differences between videos for assessment.</p><p>that focus on action classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref> and detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b17">18]</ref>, AQA is more challenging as it requires the model to predict fine-grained scores from videos that describe the same action. Considering the differences between videos and large variations in scores, we argue that a key to addressing this problem is to discover the differences among the videos and predict scores based on the differences. Many efforts have been made to tackle this problem over the past few years <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20]</ref>. Most of them formulate the AQA as a regression problem, where the scores are directly predicted from a single video. While some promis-ing results have been achieved, AQA still faces three challenges. First, since the score labels are usually annotated by human judges (e.g., the score of the diving game is calculated by summarizing scores from different judges, then multiplied by the degree of difficulty), the subjective appraisal of judges makes accurate score prediction quite difficult. Second, the difference between videos for AQA is very subtle, since actors are usually performing the same action in a similar environment. Last, most current models are evaluated based on Spearman's Rank, which may not faithfully reflect the prediction performance (see our discussions in Section 4.1).</p><p>Towards a better AQA framework that can utilize the differences among the videos to predict the final rating, we borrow the merits from the concept of contrastive learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>. Contrastive learning <ref type="figure">(Figure 1</ref>, top-left) aims to learn a better representation space where the distance d A between two similar samples X, X A is enforced to be small while the distance d B between the dissimilar ones X, X B is encouraged to be large. Therefore, the distance in the representation space can already reflect the semantic relationship between two samples (i.e., if they are from the same category). Analogically, in the context of AQA, we aim to learn a model that can map the input video into the score space where the differences between the action qualities can be measured by the relative scores (? A , ? B ). Motivated by this, we propose a Contrastive Regression (CoRe) framework for the AQA task. Unlike previous works which aim to predict the scores directly, we propose to regress the relative scores between an input video and several exemplar videos as references.</p><p>Moreover, as a step towards more accurate score prediction, we devise a group-aware regression tree (GART) to convert the relative score regression into two easier subproblems: (1) coarse-to-fine classification. We first divide the range of the relative score into several non-overlapping intervals (i.e., groups) and then use a binary tree to allocate the relative score to a certain group by performing classification progressively; (2) regression in a small interval. We perform regression inside the group where the relative score lies and predict the final score. As another contribution, we design a new metric, called relative L2-distance (R-2 ) to more precisely measure the performance of action quality assessment by considering the intra-class variance.</p><p>To verify the effectiveness of our method, we conduct extensive experiments on three mainstream AQA datasets containing both Olympic and surgical actions, namely AQA-7 <ref type="bibr" target="#b19">[20]</ref>, MTL-AQA <ref type="bibr" target="#b21">[22]</ref> and JIGSAWS <ref type="bibr" target="#b7">[8]</ref>. Experiments results demonstrate our method largely outperforms the state-of-the-art on the three benchmarks under the Spearman's Rank Correlation (81.0% to 84.0% on AQA-7, 92.7% to 95.1% on MTL-AQA and 70% to 85% on JIGSAWS) and new proposed R-2 metric, which clearly shows the advantages of our proposed contrastive regression framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The past few years have witnessed the rapid development of AQA. The mainstreams of AQA methods formulate AQA as a regression task based on reliable scores labels given by expert judges. For example, Gordan et al. <ref type="bibr" target="#b8">[9]</ref> propose to use the trajectory of the skeleton to solve the problem of gymnastic vault action quality assessment in their pioneer work. Pirsiavash et al. <ref type="bibr" target="#b23">[24]</ref> use DCT to encode body pose as input features. SVR <ref type="bibr" target="#b0">[1]</ref> is also used to build the mapping from the features to the final score. Thanks to the great success of deep learning in action recognition tasks, Parmar et al. <ref type="bibr" target="#b20">[21]</ref> show that the spatio-temporal features from C3D <ref type="bibr" target="#b29">[30]</ref> can better encode the video data and significantly improve the performance. They also propose a large-scale AQA dataset and explore all-action models to further enhance the scoring performance. Following <ref type="bibr" target="#b20">[21]</ref>, Xu et al. <ref type="bibr" target="#b34">[35]</ref> propose a model containing two LSTM to learn the multi-scale features of videos. Pan et al. <ref type="bibr" target="#b18">[19]</ref> propose to use spatial and temporal relation graphs to model the interaction among the joints. In addition, they also propose to use I3D <ref type="bibr" target="#b2">[3]</ref> as a stronger backbone network to extract spatio-temporal features. Parmar et al. <ref type="bibr" target="#b21">[22]</ref> propose a larger AQA dataset with more annotations for various tasks. The idea of multi-task learning is also introduced to improve the model capacity for AQA. Recently, Tang et al. <ref type="bibr" target="#b28">[29]</ref> propose a new uncertainty-aware score distribution learning (USDL) to reduce the underlying ambiguity of the action score labels from human judges. Different from this line of works, several methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref> formulate AQA as a pair-wise ranking task. However, they mainly focus on longer and more ambiguous tasks and only predict an overall ranking, which might limit the application of AQA where some quantitative comparisons are required. In this work, we present a new contrastive regression framework to simultaneously rank videos and predict accurate scores, which makes our method distinguished from previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The overall framework of our method is illustrated in <ref type="figure">Figure 2</ref>. We will describe our method in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Regression</head><p>Problem Formulation. Most existing works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref> formulate AQA as a regression task, where the input is a video containing the target action and the output is the predicted quality score of the action. Note that in some AQA tasks (e.g., diving), each video is associated with a degree of difficulty for each video (which is a known constant). The final score is the multiplication of the action quality score (i.e., raw score) and the degree of diffi-  <ref type="figure">Figure 2</ref>: The pipeline of our proposed group-aware contrastive regression method. We first sample an exemplar video for each input video according to the category and degree of difficulty of the action. We then feed the video pair into a shared I3D backbone to extract spatio-temporal features and combine these two features with the reference score of the exemplar video. Finally, we pass the combined feature to the group-aware regression tree and obtain the score difference between the two videos. During inference, the final score can be computed by averaging the results from multiple different exemplars.</p><p>culty. Since the degree of difficulty is already known, we only need to predict the action quality score following <ref type="bibr" target="#b28">[29]</ref>. Formally, given the input video v = {F i } L i=1 with action quality label s, the regression problem is to predict the action quality? based on the input video:</p><formula xml:id="formula_0">s = R?(FW (v)),<label>(1)</label></formula><p>where R ? and F W are the regressor model and the feature extractor parameterized by ? and W, respectively. The regression problem is usually solved by minimize the meansquare error between the predicted score and the groundtruth score:</p><formula xml:id="formula_1">LAQA(?, W|v) = MSE(?, s),<label>(2)</label></formula><p>where ? and W are the parameters of regression model and feature extractor. However, since the action videos are usually captured in similar environments (e.g., diving competitions often take place in aquatics centers), it is difficult for the model to learn the diverse scores based on videos with subtle differences. To this end, we propose to reformulate the problem as regressing relative score between the input and an exemplar. Let v m = {F i m } Lm i=1 denotes the input video, and v n = {F i n } Ln i=1 denotes the exemplar video with score label s n . The regression problem can be re-written as:</p><formula xml:id="formula_2">sm = R?(FW (vm), FW (vn)) + sn.<label>(3)</label></formula><p>This formulation can be also viewed as a form of residual learning <ref type="bibr" target="#b10">[11]</ref>, where we aim to regress the difference of the scores between the input video and a reference video.</p><p>Exemplar-Based Score Regression. We now describe how to implement the CoRe framework for the AQA problem. Since we aim to regress the relative score between the input and the exemplar, how to select the exemplar becomes critical. To make the input and the exemplar comparable, we tend to select the video that shares some certain attributes (e.g., category and degree of difficulty) with the input video as the exemplar. Formally, given an input video v m and the corresponding exemplar v n , we first use an I3D <ref type="bibr" target="#b2">[3]</ref> to extract the features {f n , f m } following <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22]</ref>, and then aggregate them with the score of the exemplar s n :</p><formula xml:id="formula_3">f (n,m) = concat([fn, fm, sn/ ]),<label>(4)</label></formula><p>where is a normalizing constant to make sure s n / ? [0, 1]. We then predict the score difference of the pair through a regressor R ? as ?s = R ? (f (n,m) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group-Aware Regression Tree</head><p>Although the contrastive regression framework can predict the relative score ?s, ?s usually takes values from a wide range (e.g., for diving, ?s ? [?30, 30]). Therefore, predicting ?s directly is still of great difficulty. To this end, we devise a group-aware regression tree (GART) to solve the problem in a divide-and-conquer manner. Specifically, we first divide the range of ?s into 2 d non-overlapping intervals (namely "groups"). We then construct a binary regression tree with d?1 layers, of which the leaves represent the 2 d groups, as is illustrated in <ref type="figure" target="#fig_7">Figure 3</ref>. The decision process of group-aware regression tree follows a coarse-to-fine manner: in the first layer, we determine whether the input video is better or worse than the exemplar video; in the following layers, we gradually make a more accurate prediction about how much the input video is better/worse than the exemplar. Once we have reached the leaf nodes, we can know which group the input video should be classified and we can then perform regression in the corresponding small interval.  <ref type="figure" target="#fig_7">Figure 3</ref>: The architecture of the proposed group-aware regression tree. Given the video features and the reference score, the regression tree determines the score difference in a coarse-to-fine manner, where a sequence of binary classification tasks is performed at first (purple nodes) and the regression modules in the leaf layer then give the final prediction (white nodes).</p><p>Tree Architecture. We adopt the binary tree architecture to perform the regression task. To begin with, we perform an MLP to f (n,m) and use the output as an initialization of the root node feature. We then perform the regression in a top-down manner. Each node takes the output feature from its parent node as input and produces the binary probability together with the updated feature. The probability of each leaf node can be computed by multiplying all the probabilities along the path to the root. We use the Sigmoid to map the output of each leaf node to [0, 1], which is the predicted score difference w.r.t. the corresponding group. We then describe our partition strategy to define the boundary of each group. First, we collect the list of score differences of all possible training video pairs ? = [? 1 , ..., ? T ]. Then, we sort the list in an ascending order to obtain ? * = [? * 1 , ..., ? * T ]. Given the group number R, the partitioning algorithm gives the bounds of each interval I r = (? r left , ? r right ) as:</p><formula xml:id="formula_4">? r left = ? * (T ? 1) ? (r ? 1) R , ? r right = ? * (T ? 1) ? r R ] , ?i = 1, 2, . . . , R,<label>(5)</label></formula><p>where we use ? * (i) to represent the i?th element of ?. It is worth noting that the partition strategy is non-trivial. If we simply uniformly divide the whole range into multiple groups, the pairs of videos in the training set of which the differences of scores lie in some certain group may be unbalanced (see <ref type="figure" target="#fig_3">Figure 4</ref> for details).</p><p>Optimization. We train the regression tree by imposing a classification task on the leaf probabilities and a regression task on the ground-truth interval. Specifically, when the ground-truth score difference of the input pairs ? is in</p><formula xml:id="formula_5">i-th group, i.e. ? ? (? i left , ? i right )</formula><p>, the one-hot label classification L = {l r } is defined by assigning 1 to the i-th node and the regression label is set as</p><formula xml:id="formula_6">? i = ??? i left ? i right ?? i left .</formula><p>For each video pair in the training data with classification label {l r } R r=1 and regression label {? r } R r=1 , the objective function for the classification task and regression task can be written as:</p><formula xml:id="formula_7">J cls = ? R r=1 (lr log(Pr) + (1 ? lr) log(1 ? Pr)) Jreg = R r=0 I(lr = 1)(?r ? ?r) 2 ,</formula><p>where {P r } R r=1 and {? r } R r=1 are the predicted leaf probabilities and regression results. The final objective function for the video-pair is:</p><formula xml:id="formula_8">J = J cls + Jreg.<label>(6)</label></formula><p>Inference. The overall regression process of the proposed group-aware regression tree can be written as:</p><formula xml:id="formula_9">R?(f (n,m) ) =?r * (? r * right ? ? r * left ) + ? r * left ,<label>(7)</label></formula><p>where r * is the group with the highest probability. In our implementation, we also adopt a multi-exemplar voting strategy. Given an input video v test , we select M exemplars from training data to construct M pairs using these M different exemplars {v m train } M m=1 whose scores are {s train } M m=1 . The process of multi-exemplar voting can be summarized as: Datasets. We perform experiments on three widely used AQA benchmarks including AQA-7 <ref type="bibr" target="#b19">[20]</ref>, MTL-AQA <ref type="bibr" target="#b21">[22]</ref> and JIGSAWS <ref type="bibr" target="#b7">[8]</ref>. For more details about the datasets, please refer to the Supplementary. Evaluation Protocols. In order to compare with the previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref> in AQA, we adopt Spearman's rank correlation as an evaluation metric. Spearman's correlation is defined as:</p><formula xml:id="formula_10">s m test = R?(FW (vtest, v m train )) + s m train ,<label>(8)</label></formula><formula xml:id="formula_11">stest = 1 M M m=1? m test , m = 1, 2, ..., M.<label>(9)</label></formula><formula xml:id="formula_12">? = i (p i ? p)(q i ? q) i (p i ?p) 2 i (q i ?q) 2 ,<label>(10)</label></formula><p>were p and q represent the ranking for each sample of two series respectively. We also follow the previous work to use Fisher's z-value <ref type="bibr" target="#b19">[20]</ref> when measure the average performance across actions. We also propose a stricter metric to measure the performance of AQA models more precisely, called relative L2distance (R-2 ). Given the highest and lowest scores for an action s max and s min , R-2 is defined as:</p><formula xml:id="formula_13">R-2 (?) = 1 K K k=1 |s k ?? k | s max ? s min 2 ,<label>(11)</label></formula><p>where s k and? k represent ground-truth score and prediction for k-th sample, respectively. We use R-2 instead of traditional L2-distance because different actions have different scoring intervals. Comparing and averaging 2 distance among different classes of actions is meaningless and confusing. Our proposed R-2 is different from Spearman's correlation: Spearman's correlation focuses more on the ranks of the predict scores while our R-2 focuses on the numerical values. Implementation Details:</p><p>We adopt the I3D model pretrained on Kinetics <ref type="bibr" target="#b2">[3]</ref> dataset as the feature extractor F W . For all the experiments, we set the depth of GART to d = 5 and the node feature dimension as 256. The initial learning rate is 1e-3 for the regression tree and 1e-4 for the I3D backbone. We use Adam <ref type="bibr" target="#b13">[14]</ref> optimizer, and weight decay is set to zero. we select 10 exemplars for an input test video during inference and vote for the final score using the multiexemplar voting strategy. In experiments on AQA-7 and MTL-AQA, we follow <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> to extract 103 frames for each video clip, and segment them into 10 overlapping snippets, each containing 16 continuous frames. In JIG-SAWS, we follow <ref type="bibr" target="#b28">[29]</ref> to evenly sampled out 160 frames to form 10 non-overlapping 16-frame snippets. In AQA-7 and JIGSAWS, we select the exemplar video only according to the coarse category of the video. For example, if the input video is from single diving-10m platform in AQA-7, we randomly select an exemplar video from the training set of single diving-10m platform in AQA-7. In MTL-AQA dataset, since there are annotations about the degree of difficulty (DD) for diving sports, we select the exemplar based on both the category and the degree of difficulty. Note that this implementation is consistent with the real-world scenario since DD is known to all judges before the action is completed.</p><p>We report the performance of the following methods in experiments including the baseline method and different versions of our methods 1 :</p><p>? I3D + MLP and I3D + MLP * (Baseline) : Most existing works adopt this strategy. We use I3D <ref type="bibr" target="#b2">[3]</ref> to en- <ref type="bibr" target="#b0">1</ref> We use * to indicate that we did not use DD in both training and test. code a single input video, and predict the score based on the feature with a 3-layer MLP. MSE loss between the prediction and the ground-truth is used to optimize the model.</p><p>? CoRe + MLP and CoRe + MLP * : We reformulate the regression problem as mentioned in Sec. 3.1. We choose exemplar videos from the training set to construct the video pairs and also use MSE loss for optimization.</p><p>? I3D + GART and I3D + GART * : We replace the regression sub-network (MLP) with our group-aware regression tree in the baseline method. We use the loss defined in Equation <ref type="formula" target="#formula_8">(6)</ref> ? CoRe + GART and CoRe + GART * : The proposed method in Section. 3.</p><p>Note that we did not evaluate some of them on the AQA-7 and JIGSAWS datasets due to the absence of degree of difficulty annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on AQA-7 dataset</head><p>The experiment results of our method and other AQA approaches on AQA-7 are shown in <ref type="table" target="#tab_0">Table 1</ref>. The state-of-theart method USDL <ref type="bibr" target="#b28">[29]</ref> uses Gaussian distribution to create a soft distribution label for each video, which can reduce the subjective factor from human judges on original labels. We achieve the same goal with contrastive regression. We also provide the results of the baseline I3D + MLP * on this dataset, which clearly show the performance improvement obtained by our method. We reach the best results on almost all classes in AQA-7 under both Spearman's correlation and R-2 . Our method achieves 8.95%, 2.32%, 8.83%, -6.82%, 3.01% and 2.25% performance improvement for each sports class compared with USDL under Spearman's We also conduct several analysis experiments to study the effects of the depth of the regression tree and the vote number M in the multi-exemplar voting on Diving class of AQA-7 dataset. Effects of the depth of regression tree. In the regression tree module, the depth of the tree is a significant hyperparameter determining the architecture of the regression tree. We conduct several experiments on Diving class of AQA-7 dataset with different values of depth, ranging from 2 to 7, and set the M to 10. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, our model performs better when depth is 5 and 6, where the total number of groups is 32 and 64. However, there is a little drop in performance when depth is smaller than 4 or bigger than 7. In general, our model is robust to different depths. Effects of the number of exemplars for voting.</p><p>The number of exemplars used in the inference phase is another important hyper-parameter. A larger number for M means the model can refer to more exemplars while leading to larger computational cost. We conduct experiments on Diving class to study the impact of M. <ref type="figure" target="#fig_4">Figure 5</ref> shows the result when the depth of regression tree is set to 5. We observe that with M increasing, the performance becomes better and the variance is lower. The improvement on Sp. Corr. becomes less significant when M exceeds 10. We can also find the same trend for R-2 . <ref type="table" target="#tab_1">Table 2</ref> shows the performance of existing methods and our method on MTL-AQA dataset. Since the degree of dif- Pose+DCT <ref type="bibr" target="#b23">[24]</ref> 0.2682 -2014 C3D-SVR <ref type="bibr" target="#b20">[21]</ref> 0.7716 -2017 C3D-LSTM <ref type="bibr" target="#b20">[21]</ref> 0.8489 -2017 MSCADC-STL <ref type="bibr" target="#b21">[22]</ref> 0.8472 -2019 C3D-AVG-STL <ref type="bibr" target="#b21">[22]</ref> 0.8960 -2019 MSCADC-MTL <ref type="bibr" target="#b21">[22]</ref> 0.8612 -2019 C3D-AVG-MTL <ref type="bibr" target="#b21">[22]</ref> 0.9044 -2019 I3D + MLP * <ref type="bibr" target="#b28">[29]</ref> 0.8921 0.707 2020 USDL <ref type="bibr" target="#b28">[29]</ref> 0.9066 0.654 2020 MUSDL * <ref type="bibr" target="#b28">[29]</ref> 0   difficulty, our method becomes even better, achieving 2.6% and 0.191 improvements compared to MUSDL under the two metrics. We conjecture that there are two reasons: one is that we can select more suitable exemplars, the other reason is that our method can exploit more information about the action from the degree of difficulty. To have an intuitive understanding of the differences between our method and baseline methods, we visualize the prediction results in form of a scatter plot in <ref type="figure" target="#fig_5">Figure 6</ref>. We see our method is much more accurate compared to the baseline. By using the degree of difficulty information, the performance of our method can be further improved, where almost all the points are near the red line in the middle of the picture. In <ref type="figure">Figure 7</ref>, we show the cumulative score curves of our methods and SOTA method MUSDL <ref type="bibr" target="#b28">[29]</ref>. Given the error threshold , the samples whose absolute differences between their prediction and ground-truth are less than will be regarded as positive samples. It can be observed that under any error threshold, CoRe + GART (red line) shows a stronger ability to predict accurate scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on MTL-AQA dataset</head><p>Ablation Study. We further conduct an ablation study for our method. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Comparing I3D + MLP and I3D + GART, we see when replacing MLP with our group-aware regression tree, the performance is improved by 0.0022 and 0.028 under Spearman's rank metric and R-2 metric, which demonstrates the effectiveness of the designs of GART. The performance is further improved when replace the I3D baseline with our proposed CoRe framework. The above results demonstrate the effectiveness of the two components of our method.</p><p>Case Study. In order to have a deeper understanding of the behavior of our model, we present a case study in <ref type="figure">Figure 11</ref>. Based on the comparison between input and exemplar, the regression tree determines the relative score from coarse to fine. The first layer of the regression tree tries to determine which video is better, and the following layers try to make . We show the probability output for each layer of the regression tree and the regression value for each leaf on the right. We take the regression value of the leaf node with the highest probability as the final regression result. The very small errors between our prediction results with ground-truths demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input I3D + MLP CoRe + GART Case A</head><p>Case B Case C <ref type="figure">Figure 9</ref>: Visualization. We show the visualization result on MTL-AQA using Grad-CAM <ref type="bibr" target="#b24">[25]</ref>. Our method can focus on the regions that are critical to assess the action quality.</p><p>the prediction more accurate. The first case in the figure shows the behavior when the difference between input and exemplar is large, and the second case shows the behavior when the difference is small. In both situations, our model can give satisfactory predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on JIGSAWS</head><p>We also conduct experiments on this surgical action dataset JIGSAWS. Four-fold cross-validation is used following previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="table" target="#tab_4">Table 4</ref> shows the experiment results. CoRe + GART * largely improves the previous state-of-the-arts. Our method also obtains a more balanced performance in different action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>To further prove the effectiveness of our method, we visualize the baseline model (I3D + MLP) and our best model (CoRe + GART) using Grad-CAM <ref type="bibr" target="#b24">[25]</ref> on MTL-AQA, as is shown in <ref type="figure">Figure 9</ref>. We observe that our method can focus on certain regions (hands, body, etc.), which indicates our contrastive regression framework can alleviate the influence caused by the background and pay more attention to the discriminative parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed the CoRe framework for action quality assessment, which learns the relative scores based on the exemplars. We have also devised a groupaware regression tree to convert the conventional score regression into a coarse-to-fine classification task and a regression task in small intervals. The experiments on three AQA datasets have demonstrated the effectiveness of our approach. We expect the introduction of CoRe provides a new and generic solution for various AQA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We now describe the datasets we used in our experiments in detail. AQA-7 <ref type="bibr" target="#b19">[20]</ref>: AQA-7 contains 1,189 samples from seven different actions collected from winter and summer Olympic Games. It contains two dataset released before: UNLV-Dive <ref type="bibr" target="#b20">[21]</ref> is named single diving-10m platform in AQA-7, contains 370 samples. UNLV-Vault <ref type="bibr" target="#b20">[21]</ref> is named gymnastic vault in AQA-7, contains 176 samples; The other action classes are newly collected in this dataset: synchronous diving-3m springboard contains 88 samples and synchronous diving-10m platform contains 91 samples. big air skiing cantains 175 samples and big air snowboarding contains 206 samples. <ref type="bibr" target="#b21">[22]</ref>: The MTL-AQA dataset contains all kinds of diving actions, which is the largest AQA dataset up to date. There are 1,412 samples collected from 16 difference world events. The annotations in this dataset are various, including the degree of difficulty (DD), scores from each judge (totally 7 judges), type of diver's action, and the final score. We adopt the evaluation protocol suggested in <ref type="bibr" target="#b21">[22]</ref> in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTL-AQA</head><p>JIGSAWS <ref type="bibr" target="#b7">[8]</ref>: JIGSAWS is a surgical actions dataset containing 3 type of surgical task: "Suture(S)", "NeedlePassing(NP)" and "Knotted(KT)". For each task, each video sample is annotated with multiple annotation scores assessing different aspects of surgical actions, and the final score is the sum of those sub-scores. We adopt a similar four-fold cross validation strategy as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Discussions</head><p>More analysis on the regression tree. To better understand the prediction process of the regression tree, we also investigate the prediction accuracy of each layer in the regression tree on the MTL-AQA dataset, as shown in <ref type="figure">Figure 10</ref>. We also compare the results with two baseline methods. Comparing CoRe + GART and GART, we can see CoRe + GART performs better in each layer under all values of K, which indicates measuring relative score between input and exemplar is more effective than predicting the final score directly. Comparing two CoRe-based methods, we see the group-aware regression tree measures relative score more accurately. <ref type="figure">Figure 10</ref>: Classification accuracy for each layer of the group-aware regression tree. CoRe + GART is our final method, combining contrastive regression and group-aware regression tree together. CoRe + MLP uses an MLP to replace the regression tree and the GART method only keeps the regression tree without using the contrastive regression framework. K is a tolerance threshold, which indicates classifying a pair into the nearest-K groups is still regarded as a correct classification.</p><p>More analysis on CoRe. Another advantage of our proposed CoRe is that CoRe could alleviate the subjectiveness from human judges by predicting the difference, despite the fact that the exemplar video is also annotated by human judges. Formally, we can assume a score x = x + n can be decompose as the actual value x and a subjectiveness term n that subjects to normal distribution N (0, ? 2 ). If we directly predict x, the variance of subjectiveness term is ? 2 . By introducing M exemplar videos with scores {x 1 , ..., x M }, our goal is to predict the difference</p><formula xml:id="formula_14">? = 1 M i (x ? x i ),<label>(12)</label></formula><p>which also subjects to a normal distribution:</p><formula xml:id="formula_15">? ? N 1 M i (x ? x i ), 2 M ? 2<label>(13)</label></formula><p>We see the prediction becomes closer to the actual value when M &gt; 2. The empirical results in <ref type="figure" target="#fig_4">Figure 5</ref>(b) in the original paper also support our assumption.</p><p>More analysis on R-2 . To more precisely measure the AQA performance, we propose a stricter metric, called relative L2-distance (R-2 ), to measure the performance of the score prediction model. We use R-2 instead of traditional L2-distance because different actions may have different scoring intervals. Comparing and averaging 2 distance among different classes of actions is may be confusing in some cases. Given the highest and lowest scores for an action s max and s min , R-2 is defined as:</p><formula xml:id="formula_16">R-2(?) = 1 K K k=1 ( max(|s k ?? k | ? ?, 0) smax ? smin ) 2 .<label>(14)</label></formula><p>s k and? k represent ground-truth score and prediction for k th sample. ? is a tolerance threshold. If error between Leaf " : [4.5, 5.5] <ref type="figure">Figure 11</ref>: Case study. The videos marked with E and I in the upper left corner are the exemplar and the input video, respectively. Each pair of exemplar and input videos have the same degree of difficulty (DD). We show the probability output for each layer of the regression tree and the regression value for each leaf on the right. We take the regression value of the leaf node with the highest probability as the final regression result.</p><p>prediction and ground-truth is less than the threshold, the error will be ignored. K is the size of dataset. Compared to previous metrics like Spearman's correlation, the proposed R-2 metric has two key advantages: 1) our metric can judge a single prediction while Spearman's correlation requires the whole test set, which makes our metric more flexible; 2) our metric is stricter and more reasonable especially when the test set is relatively small. For example, diver A and diver B get score of 95 and 65 respectively by human professional judges. If the predictions of these two actions are 80 and 30, it is a prefect prediction under the Spearman's correlation metric, while our metric can clearly reflect the prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Case study</head><p>We conduct two more case studies here, as shown in <ref type="figure">Figure 11</ref>. Based on the comparison between the input and the exemplar, the regression tree determines the relative score from coarse to fine. The first layer of the regression tree tries to determine which video is better, and the following layers try to make this determination more accurate. The first case in the figure shows the behavior when the difference between the pair is small, while the second case shows the behavior when this difference is large. When the difference between the two videos is large, it is easy to make the prediction. While the difference is small, the classification task is more difficult, but our method can still give a relatively accurate judgment. We see the proposed contrastive regression framework and the regression tree are two key techniques to achieve accurate score prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The distribution of the differences of scores in the training set under different partition strategy. (a) Uniform partition. We can observe a large variation of frequency among different groups. (b) The proposed grouping strategy in Equation<ref type="bibr" target="#b4">(5)</ref>. The training pairs belonging to each group are balanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Effects of the depth of regression tree (a) and the number of exemplars for voting (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>A comparison of different methods in scatter plot. Each point in the figure represents a video in the test set. The red line indicates the prefect predictions. rank. Meanwhile, we achieve 0.15, 0.31, 1.15, 1.07, 0.24, -0.21 performance improvement under R-2 . For the average correlation and average R-2 performance, we have nearly 3.7% and 0.45 improvements compared to USDL model, clearly showing the effectiveness of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?= - 3 Figure 8 :</head><label>38</label><figDesc>Case study. The videos marked with E and I in the upper left corner are the exemplar and the input video, respectively. Each pair of exemplar and input videos have the same degree of difficulty (DD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 a b c d e f g h i j k l m n o p probability a b c d e f g h i j k l m n o</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of Spearman's Correlation and R-2 Distance on the AQA-7 dataset. indicts our implementation.</figDesc><table><row><cell>Sp. Corr</cell><cell>Diving</cell><cell>Gym Vault</cell><cell>BigSki.</cell><cell>BigSnow.</cell><cell>Sync. 3m</cell><cell>Sync. 10m</cell><cell>Avg. Corr.</cell><cell>Year</cell></row><row><cell>Pose+DCT [24]</cell><cell>0.5300</cell><cell>0.1000</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2014</cell></row><row><cell>ST-GCN [36]</cell><cell>0.3286</cell><cell>0.5770</cell><cell>0.1681</cell><cell>0.1234</cell><cell>0.6600</cell><cell>0.6483</cell><cell>0.4433</cell><cell>2018</cell></row><row><cell>C3D-LSTM [21]</cell><cell>0.6047</cell><cell>0.5636</cell><cell>0.4593</cell><cell>0.5029</cell><cell>0.7912</cell><cell>0.6927</cell><cell>0.6165</cell><cell>2017</cell></row><row><cell>C3D-SVR [21]</cell><cell>0.7902</cell><cell>0.6824</cell><cell>0.5209</cell><cell>0.4006</cell><cell>0.5937</cell><cell>0.9120</cell><cell>0.6937</cell><cell>2017</cell></row><row><cell>JRG [19]</cell><cell>0.7630</cell><cell>0.7358</cell><cell>0.6006</cell><cell>0.5405</cell><cell>0.9013</cell><cell>0.9254</cell><cell>0.7849</cell><cell>2019</cell></row><row><cell>I3D+MLP  *  [29]</cell><cell>0.7438</cell><cell>0.7342</cell><cell>0.5190</cell><cell>0.5103</cell><cell>0.8915</cell><cell>0.8703</cell><cell>0.7472</cell><cell>2020</cell></row><row><cell>USDL [29]</cell><cell>0.8099</cell><cell>0.7570</cell><cell>0.6538</cell><cell>0.7109</cell><cell>0.9166</cell><cell>0.8878</cell><cell>0.8102</cell><cell>2020</cell></row><row><cell>I3D + MLP  *</cell><cell>0.8685</cell><cell>0.6939</cell><cell>0.5391</cell><cell>0.5180</cell><cell>0.8782</cell><cell>0.8486</cell><cell>0.7601</cell><cell></cell></row><row><cell>CoRe + GART  *</cell><cell>0.8824</cell><cell>0.7746</cell><cell>0.7115</cell><cell>0.6624</cell><cell>0.9442</cell><cell>0.9078</cell><cell>0.8401</cell><cell></cell></row><row><cell>R-2(?100)</cell><cell>Diving</cell><cell>Gym Vault</cell><cell>BigSki.</cell><cell>BigSnow.</cell><cell>Sync. 3m</cell><cell>Sync. 10m</cell><cell>Avg. R-2</cell><cell>Year</cell></row><row><cell>C3D-SVR [21]</cell><cell>1.53</cell><cell>3.12</cell><cell>6.79</cell><cell>7.03</cell><cell>17.84</cell><cell>4.83</cell><cell>6.86</cell><cell>2017</cell></row><row><cell>USDL [29]</cell><cell>0.79</cell><cell>2.09</cell><cell>4.82</cell><cell>4.94</cell><cell>0.65</cell><cell>2.14</cell><cell>2.57</cell><cell>2020</cell></row><row><cell>I3D + MLP  *</cell><cell>0.81</cell><cell>2.54</cell><cell>6.06</cell><cell>5.31</cell><cell>1.41</cell><cell>3.08</cell><cell>3.20</cell><cell></cell></row><row><cell>CoRe + GART  *</cell><cell>0.64</cell><cell>1.78</cell><cell>3.67</cell><cell>3.87</cell><cell>0.41</cell><cell>2.35</cell><cell>2.12</cell><cell></cell></row><row><cell>4. Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4.1. Datasets and Experiment Settings</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of performance with existing methods on the MTL-AQA dataset. indicts our implementation.</figDesc><table><row><cell>Method (w/o DD)</cell><cell>Sp. Corr. R-2(?100) Year</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on MTL-AQA dataset AQA, we also verify the effects of DD on this dataset. We divide all methods into two types: some use the DD labels in the training phase (bottom part of the table) and the others (upper part of the table) do not. We see CoRe + GART</figDesc><table><row><cell>Method</cell><cell cols="3">Ablation Sp. Corr. R-2(?100)</cell></row><row><cell>I3D + MLP</cell><cell>Baseline</cell><cell>0.9381</cell><cell>0.394</cell></row><row><cell>I3D + GART</cell><cell>+ GART</cell><cell>0.9403</cell><cell>0.366</cell></row><row><cell>CoRe + GART</cell><cell>+ CoRe</cell><cell>0.9512</cell><cell>0.260</cell></row><row><cell cols="4">Figure 7: Cumulative score curve on MTL-AQA dataset.</cell></row><row><cell cols="4">The larger the area under the curve indicates the better per-</cell></row><row><cell>formance.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ficulty (DD) annotations are available for diving actions in</cell></row><row><cell>MTL-</cell><cell></cell><cell></cell><cell></cell></row></table><note>* achieves respectively 2.0% and 0.244 improvement compared to MUSDL* [29] under Spearman's rank and R-2 metric without DD labels. By training with the degree of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of performance with existing methods on the JIGSAWS dataset.</figDesc><table><row><cell>Sp. Corr.</cell><cell>S</cell><cell>NP</cell><cell>KT</cell><cell>Avg. Corr.</cell></row><row><cell>ST-GCN [36]</cell><cell>0.31</cell><cell>0.39</cell><cell>0.58</cell><cell>0.43</cell></row><row><cell>TSN [21]</cell><cell>0.34</cell><cell>0.23</cell><cell>0.72</cell><cell>0.46</cell></row><row><cell>JRG [19]</cell><cell>0.36</cell><cell>0.54</cell><cell>0.75</cell><cell>0.57</cell></row><row><cell>USDL [29]</cell><cell>0.64</cell><cell>0.63</cell><cell>0.61</cell><cell>0.63</cell></row><row><cell>MUSDL [29]</cell><cell>0.71</cell><cell>0.69</cell><cell>0.71</cell><cell>0.70</cell></row><row><cell>I3D + MLP  *</cell><cell>0.61</cell><cell>0.68</cell><cell>0.66</cell><cell>0.65</cell></row><row><cell>CoRe + GART  *</cell><cell>0.84</cell><cell>0.86</cell><cell>0.86</cell><cell>0.85</cell></row><row><cell>R-2(?100)</cell><cell>S</cell><cell>NP</cell><cell>KT</cell><cell>Avg.</cell></row><row><cell>I3D + MLP  *</cell><cell cols="3">4.795 11.225 6.120</cell><cell>7.373</cell></row><row><cell cols="2">CoRe + GART  *  5.055</cell><cell>5.688</cell><cell>2.927</cell><cell>4.556</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant U1813218, Grant U1713214, and Grant 61822603, in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Srimanta Pal, and Dipak Chandra Patranabis. Support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasish</forename><surname>Basak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Am I a baller? basketball performance assessment from first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2196" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Who&apos;s better, who&apos;s best: Skill determination in video using deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><forename type="middle">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<idno>abs/1703.09913</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pros and cons: Rank-aware temporal attention for skill determination in long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Walterio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7862" to="7871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjam?n</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Bejar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIC-CAIW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automated video assessment of human performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Johannes F?rnkranz and Thorsten Joachims</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trajectory based assessment of coordinated human activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Pers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branko</forename><surname>Dezman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Kovacic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVS</title>
		<editor>James L. Crowley, Justus H. Piater, Markus Vincze, and Lucas Paletta</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="534" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huafeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengmin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ioannis Kompatsiaris, Benoit Huet, Vasileios Mezaris, Cathal Gurrin, Wen-Huang Cheng, and Stefanos Vrochidis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
	<note>ACM MM</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">BMN: boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<idno>abs/1907.09702</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pairwise comparison-based objective score for automated skill assessment of segments in a surgical task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi Chiung Grace</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
		<editor>Danail Stoyanov, D. Louis Collins, Ichiro Sakuma, Purang Abolmaesumi, and Pierre Jannin</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Temporal activity detection in untrimmed videos with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir?-I-Nieto</surname></persName>
		</author>
		<idno>abs/1608.08128</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action assessment by joint relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibin</forename><surname>Jia-Hui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action quality assessment across multiple actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What and how well you performed? A multitask learning approach to action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic evaluation of organized basketball activity using bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Per?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Pers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Kova?i?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="556" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video based assessment of osats using sequential motion textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pl?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roisin</forename><surname>Mcnaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccaskie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">COIN: A large-scale dataset for comprehensive instructional video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dajun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uncertainty-aware score distribution learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zanlin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dynamical regularity for action analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">K</forename><surname>Turaga</surname></persName>
		</author>
		<editor>Xianghua Xie, Mark W. Jones, and Gary K. L. Tam</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to score figure skating sport videos. TCSVT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video-based motion expertise analysis in simulation-based surgical training using hierarchical dirichlet process hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMAR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relative hidden markov models for video-based evaluation of motion skills in surgical training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1218" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated assessment of surgical skills using frequency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nassir Navab, Joachim Hornegger, William M. Wells III, and</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video and accelerometer-based motion analysis for automated surgical skills assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJ-CARS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="455" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
