<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Superpixel Image Classification with Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">H C</forename><surname>Avelar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Science Brigade</orgName>
								<address>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><forename type="middle">R</forename><surname>Tavares</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">L T</forename><surname>Da Silveira</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Federal University of Rio Grande</orgName>
								<address>
									<settlement>Rio Grande</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?udio</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<settlement>Porto Alegre</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Superpixel Image Classification with Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a methodology for image classification using Graph Neural Network (GNN) models. We transform the input images into region adjacency graphs (RAGs), in which regions are superpixels and edges connect neighboring superpixels. Our experiments suggest that Graph Attention Networks (GATs), which combine graph convolutions with selfattention mechanisms, outperforms other GNN models. Although raw image classifiers perform better than GATs due to information loss during the RAG generation, our methodology opens an interesting avenue of research on deep learning beyond rectangular-gridded images, such as 360-degree field of view panoramas. Traditional convolutional kernels of current state-of-the-art methods cannot handle panoramas, whereas the adapted superpixel algorithms and the resulting region adjacency graphs can naturally feed a GNN, without topology issues.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The generic image classification problem consists of determining what object classes (typically from a set of pre-defined categories) are present in an input image. Early approaches followed the traditional pipeline of extracting image features (e.g., colour, texture, etc.) and feeding them to a classifier. The seminal work by Krizhevsky and colleagues <ref type="bibr" target="#b11">[12]</ref> explored deep neural networks for image classification, winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a large margin and setting a turning point for research on image classification. The datasets became more challenging * Corresponding author: phcavelar@inf.ufrgs.br and the networks grew deeper, with the GoogLeNet architecture <ref type="bibr" target="#b27">[27]</ref> winning the ILSVRC2014 challenge and "Squeeze-and-Excitation" layers being introduced in <ref type="bibr" target="#b6">[7]</ref> to win the ILSVRC2017 challenge, with a top-5 error rate of 2.251%.</p><p>Despite the recent advances both in terms of datasets and network architectures, using traditional convolutional kernels limits the applications of these networks in problems that do not present a domain based on rectangular grids. For example, panoramas capture a full 360-degree field of view. Although the equirectangular representation does use a rectangular domain, sampling is highly non-uniform. To handle these issues, some authors proposed networks designed to adapt to the spherical domain have been designed, such as <ref type="bibr" target="#b3">[4]</ref>, while others propose to learn how to adapt convolutional layers to the spherical domain, as <ref type="bibr" target="#b25">[25]</ref>. As another example, we can mention point-cloud classification, in which spatially unstructured data cannot be represented using a rectangular domain. In this context, some authors either explore a voxelized representation of the scene with 3D networks <ref type="bibr" target="#b32">[31]</ref> or directly use 3D points as input to a network.</p><p>Graph-based representations can be used to model a variety of problems and domains. Furthermore, they naturally allow several "multiresolution" representations of the same object. For example, both pixel-level and superpixel-level representations of the same image might be modeled using graphs. In fact, superpixel-based representations have the advantage of reducing the input size, and potentially allowing different domains (e.g. pinhole and spherical images) to be represented as the same (or similar) graph. Furthermore, there are several recent advances toward the development of Graph Neural Networks (GNNs) <ref type="bibr" target="#b33">[32]</ref>, which could bridge the gap between different domains. In this paper, we explore Graph Attention Networks (GATs) <ref type="bibr" target="#b29">[29]</ref> to classify images based on superpixel representations. GATs are a Graph Neural Network model that combine ideas of graph convolutions <ref type="bibr" target="#b9">[10]</ref>, which allows graph nodes to aggregate information from their irregular neighbourhoods, with self-attention mechanisms <ref type="bibr" target="#b28">[28]</ref>, which allows nodes to learn the relative importance of each neighbour during the aggregation process.</p><p>Our methodology comprises the following steps: (i) generate a superpixel representation of the input image; (ii) create a region adjacency graph (RAG) from the superpixel representation, by connecting neighbouring superpixels; (iii) feed the RAG to the GAT, which will predict the class. Experiments on several datasets show that the GAT outperforms other RAG-based GNN classifiers, but the RAG provides much less information than the raw image, so that the GAT's performance is inferior compared to the raw-image classifiers.</p><p>This paper is organised as follows: in Section 2, we present related works and peering approaches. Section 3 revises existing superpixel segmentation techniques and how they are used to represent graphs, exposing the differences with the competing approach. Section 4 describes the proposed method, while Section 5 shows the experimental setup and obtained results. Finally, the conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Monti et al. <ref type="bibr" target="#b14">[15]</ref> provided, to the best of our knowledge, the first application of Graph Neural Networks (GNNs) to image classification, as well as proposing the MoNET framework for dealing with geometric data in general. Their framework works by weighting the neighbourhood aggregation through a learnt scaling factor based on geometric distances.</p><p>Velickovic et al. <ref type="bibr" target="#b29">[29]</ref> proposed a model using selfattention for weighting the neighbourhood aggregation in GNNs, recognising that this model could be seen as a sub-model of the MoNET framework, nonetheless providing extraordinary results on other datasets, namely Cora and Citeseer, two famous citation networks <ref type="bibr" target="#b19">[19]</ref>, and on the FAUST humans dataset <ref type="bibr" target="#b2">[3]</ref>.</p><p>Although graph-based methods can be applied directly to images by considering each pixel a node of the graph, as in the seminal paper of Shi and Malik for image segmentation <ref type="bibr" target="#b20">[20]</ref>, lower-level representations generate smaller graphs. Using each region produced by a segmentation result might be a natural choice, but generating accurate segmentation results is still an open problem. A compromise solution between using individual pixels and object-related regions is superpixels. Superpixels group pixels similar in colour and other low-level properties, like location, into perceptually meaningful representation units (regions or segments) <ref type="bibr" target="#b24">[24]</ref>. These oversegmented, simplified, images can be applied in a number of common tasks in computer vision, including depth estimation, segmentation, and object localization <ref type="bibr" target="#b0">[1]</ref>. A comprehensive survey on superpixels can be found in <ref type="bibr" target="#b24">[24]</ref>.</p><p>The abovementioned work on using GNNs on images, alongside the work on adapting self-attention for GNNs and the works for generating superpixels of images form the pillars on which we based our experiments.</p><p>Two other models later came to our knowledge, which extended or could be seen as sub-models of the MoNET framework, using geometric information to weight neighbourhood aggregation, and provided results for the MNIST dataset. One of those is the SplineCNN model <ref type="bibr" target="#b5">[6]</ref>, which leverages properties of B-spline bases in their neighbourhood aggregation procedure. The other is the Geo-GCN model <ref type="bibr" target="#b23">[23]</ref>, which is a MoNET sub-model with a differently engineered learned distance function performing data augmentation using rotations and conformations.</p><p>Another technique for using GNNs with image data is to use them as a form of semi-supervised augmentation for classification, as in <ref type="bibr" target="#b8">[9]</ref>. The main difference between their method and ours is that while they extract a CNN feature descriptor for each image with a (possibly pretrained) convolutional network, and then build a graph on which their model is used (akin to how Graph Convolutional Networks are used for semi-supervised classification in bag-ofwords in <ref type="bibr" target="#b9">[10]</ref>), we use the GAT as a classifier for a graph representing an image directly. Although their technique is useful for semi-supervised learning, the technique of using the network for superpixel classification has some possible advantages of its own, and they are not directly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Superpixel Graphs</head><p>A number of techniques exist to generate superpixels from images, such as SLIC <ref type="bibr" target="#b0">[1]</ref>, SNIC <ref type="bibr" target="#b1">[2]</ref>, SEEDS <ref type="bibr" target="#b4">[5]</ref>, ETPS <ref type="bibr" target="#b35">[34]</ref>, and the hierarchical approach from <ref type="bibr" target="#b31">[30]</ref>.</p><p>For our experiments, we chose to use SLIC <ref type="bibr" target="#b0">[1]</ref> since it was readily available and had a spatial component s, N ? S(I) s returns the superpixel n ? N of a (x,y) pixel <ref type="bibr">3:</ref> x(n) ? F (I, s, n)?n ? N x(n) is the feature vector of node n 4: return G = (N , E), x 16: end procedure in its superpixel segmentation. SLIC is stable and it is still recommended among other state-of-the-art oversegmentation algorithms <ref type="bibr" target="#b24">[24]</ref>. Nonetheless, we believe that other segmentation techniques with similar characteristics could be used.</p><formula xml:id="formula_0">E ? {} 5: for 1 ? x ? w do 6: for 1 ? y ? h do</formula><p>After applying a superpixel segmentation technique, we generate a Region Adjacency Graph (RAG) by treating each superpixel as a node and adding edges between all directly adjacent superpixels (1neighborhood connection). Note that this differs from the approach adopted in <ref type="bibr" target="#b14">[15]</ref>, since their superpixel graphs have connections that span more than one neighbour level, with edges formed with the K nearest neighbours. Each graph node can have associated features, providing an aggregate information based on characteristics of the superpixel itself. Algorithm 1 describes the adopted procedure, whereas <ref type="figure" target="#fig_0">Fig. 1</ref> depicts the generation of a RAG from an image.</p><p>There are many possibilities for building the features related to each node. For example, statistics about the colour and position of a superpixel, such as the mean, standard deviation, and correlation matrices of its pixels are readily available from the superpixel segmentation. In the case of images defined on rectangular domains, positional information relates to a 2D point. However, this concept can be easily adapted to omnidirectional images or point clouds, so that a single topology based on graphs can be used in different applications. We do not build features for the edges, since we use an attention-based technique, and believe that the edge feature will be learned accordingly from the attention mechanism using both nodes' features.</p><p>In this work in particular, we apply this procedure to the well-known MNIST <ref type="bibr" target="#b12">[13]</ref>, FashionMNIST <ref type="bibr" target="#b34">[33]</ref>, Street View House Numbers (SVHN) <ref type="bibr" target="#b17">[17]</ref> and CIFAR-10 <ref type="bibr" target="#b10">[11]</ref> datasets. The first two datasets contain grayscale images of 28 ? 28 pixels and 10 classes, and the last two contain RGB images of 32?32 pixels, both also having 10 classes.</p><p>Monti and colleagues <ref type="bibr" target="#b14">[15]</ref> used the MNIST dataset and converted it into a graph-based format by using a superpixel-based representation. But whereas they connected nodes through a K-nearest neighbour procedure, we do so using RAGs. Hence, our dataset presents a lower-connectivity graph, which could impair information flow and make the classification problem harder. We also provide results for the RAG representation of the FashionMNIST dataset, since it is a more challenging dataset for which the information loss from the superpixel representation could impact more significantly the model. Since these two datasets contain only grayscale images, we build each superpixel's feature vector as the concatenation of the average luminosity of the pixels in a superpixel and the geometric centroid.</p><p>The SVHN and CIFAR datasets, however, both contain RGB channels in their images, and a natural extension for the feature vector is to use the concatenation of the average value for each colour channel and the geometric centroid. These datasets were used to see how the model would perform both with simple and complex colour images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Model</head><p>We transform the Undirected Graph produced from the oversegmented image's RAG into a Directed Graph G = (N , E), and feed it to a Neural Network model that operates on Graphs. More specifically, we use GAT layers stacked on top of each other using the same adjacency graph on each layer.</p><p>Our model is a version of the GAT model by Velickovic et al. <ref type="bibr" target="#b29">[29]</ref>, roughly based on the implementation by Nathani et al. <ref type="bibr" target="#b16">[16]</ref>. Attention is implemented by scattering the source and target nodes' input fea- return o = (M tgt , y ? )/? sum 14: end procedure tures into their respective edges, making the transition and activation function on both these inputs and then summing them up over each target node through the edges.</p><p>Therefore, for each layer with input dimension d i and output dimension d o we learn two functions. The transition function f : R 2di ? R do , composed of a linear layer followed by a nonlinearity, and the attention function a : R 2di ? R that tells how much the target node of an edge should attend to the source node's information, also composed of a linear layer. The values produced by the attention function are activated using softmax for each target node. On the implementation, we take advantage of the fact that ?(z + c) j = ?(z) j .</p><p>In summary, given a directed graph G = (N , E),</p><p>with edges e = (s e , t e ) ? E and node features x(n)?n ? N , let T (t) be the set of nodes with an edge towards t, the attention model can be summarised by Equations <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> below, where denotes vector/tensor concatenation.</p><formula xml:id="formula_1">?(s, t) = e a(x(s) x(t)) s ?T (t) e a(x(s ) x(t)) (1) o(t) = s?T (t) ?(s, t)f (x(s) x(t))<label>(2)</label></formula><p>The detailed algorithm showing the optimisation can be seen in Algorithm 2. Each of these layers can be arranged in a multi-head model by concatenating their outputs after the forward pass of each layer. That is, given k heads, the joint output of the k-headed layer, where each head has its own transition and attention functions f i and a i (as well as the intermediary ? i ), would be as in Equation <ref type="formula" target="#formula_2">(3)</ref>, where k i=1 a k is the concatenation of all vectors/tensors a k :</p><formula xml:id="formula_2">o(t) = s?T (t) k i=1 ? i (s, t)f i (x(s) x(t))<label>(3)</label></formula><p>The output of the final GAT layer can then be sumpooled, having all the values added, and then passed through a MultiLayer Perceptron (MLP) for the final prediction. The Python/Pytorch implementation in its fullest can be seen in the provided GitHub repository 1 as well. Most operations have been parallelized as much as the authors could fathom, with some operations done in a preprocessing phase to avoid overload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we show the potential of our technique on four datasets: MNIST <ref type="bibr" target="#b12">[13]</ref>, FashionMNIST <ref type="bibr" target="#b34">[33]</ref>, SVHN <ref type="bibr" target="#b17">[17]</ref> and CIFAR-10 <ref type="bibr" target="#b10">[11]</ref>. The superpixel algorithm has a target of 75 regions, so that the analyzed graphs present approximately 75 nodes each.</p><p>All experiments were ran either in a computer with a NVIDIA Quadro P6000 or one with a NVIDIA GTX 1070 Mobile. Both computers have 32GB of RAM. For development, we adopted the Pytorch library, version 1.X, using CUDA.</p><p>For all experiments we set a budget of 100 epochs for optimisation, with a batch size of 32 images, using a 90/10 split for training and validation in the dataset's original training data. We use Adam as the optimiser, with a learning rate of 0.001, ? 1 = 0.9, and ? 2 = 0.999, using the model with the best validation accuracy on the test dataset. If the model failed to leave a baseline accuracy for the first 10 epochs, we restarted the training procedure from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MNIST</head><p>We trained two versions of the GAT model: A singleheaded GAT with 3 layers, with 32, 64 and 64 neurons, and a two-headed GAT, where each head has the same amount of neurons as the single-headed model. Both models used sum-pooling and a MLP with two layers of 32 and d o neurons for the final classification, where d o = 10 is the number of classes in MNIST. All neurons use ReLU activations, except for those at the last layer of the classification MLP, which use softmax activations. We did not use any regularisation technique.</p><p>All dataset images are converted to a corresponding RAG, using SLICO <ref type="bibr" target="#b22">[22]</ref>, a zero-parameter variant the SLIC algorithm. We set the target number of superpixels as 75, but the generated RAGs are not guaranteed to have exactly 75 nodes due to how the SLIC algorithm works. <ref type="table">Table 1</ref> shows that both GAT models performed better than the MoNET model <ref type="bibr" target="#b14">[15]</ref>, showing that a learned representation of the geometric distance can lead to better performance than the fixed one of <ref type="bibr" target="#b14">[15]</ref>. Note that our model has also to deal with graphs that are sparser than the ones used in the baselines <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">23]</ref>, since in their graph edges are formed through K-nearest neighbours and ours use only directly adjacent nodes. Although one could expect worse accuracy, the results suggest that our approach is able to learn relevant geometric information relating the features from all neighbouring superpixels. We also report the performance of the graphbased approaches SplineCNN <ref type="bibr" target="#b5">[6]</ref> and Geo-GCN <ref type="bibr" target="#b23">[23]</ref>, as well as recent alternative approaches such as the Generative Tensor Network Classifier (GTNC) <ref type="bibr" target="#b26">[26]</ref> and the best performing method based on Support Vector Classification (SVC) as reported in the dataset homepage 2 . Although our approach does not reach state-of-the-art results, it is competitive and the best among graph-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FashionMNIST</head><p>We trained the single-headed and multi-headed GAT models with the same configuration of Section 5.1.</p><p>Since the FashionMNIST dataset also has 10 classes, the number of output neurons is d o = 10 as well. The RAGs were also generated as in Section 5.1.</p><p>Since none of the found graph-based papers present results for both MNIST and FashionMNIST, we provide a performance comparison of our models to GTNC <ref type="bibr" target="#b26">[26]</ref> and the two best classifiers from the Fash-ionMNIST benchmark <ref type="bibr" target="#b34">[33]</ref> in <ref type="table">Table 1</ref>. The gap on the performance between the traditional ML models that used the full features of the dataset and our models, which use a reduced representation based on the oversegmented image, is higher on FashionMNIST. This shows how much harder the FashionMNIST dataset is for oversegmented images, where the information loss is greater with the aggregation of the features of the pixels in the superpixel.</p><p>Another interesting fact to note is that the multiheaded model performed worse than the singleheaded one. This was further confirmed when we tried learning with a 4-headed model, which performed slightly worse than the 2-headed one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Street View House Numbers</head><p>To check the performance of the model with multichannel data, we trained and tested the two-headed model on the Street View House Numbers (SVHN) dataset <ref type="bibr" target="#b17">[17]</ref>, using the 32 ? 32 cropped version and the same parameters for oversegmentation as in the previous sections. We achieved a similar performance with the two-headed GAT model on the FashionM-NIST dataset, with 80.72% test accuracy. This comes to show that our model works even with full colour data, as well as the confounders present in the SVHN dataset. <ref type="table">Table 1</ref>: Test accuracy for the tested models on MNIST and FashionMNIST, processed as RAGs with approximately 75 nodes (called MNIST-75 and FashionMNIST-75), compared to the baseline models. Bold values show the best of the Graph-based models. We also present the mean accuracies of the two best classifiers for the non-oversegmented MNIST and FashionMNIST datasets, available in the Fash-ionMNIST benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST-75 FashionMNIST-75</head><p>MoNET <ref type="bibr" target="#b14">[15]</ref> 91.11% -SplineCNN <ref type="bibr" target="#b5">[6]</ref> 95.22% -GeoGCN <ref type="bibr" target="#b23">[23]</ref> 95.95% -  That is, the classifier has learned both to prioritise the centermost superpixels and to identify which structure it contains by comparing changes in colour tone, instead of simple changes in luminosity, proving that, although it does not reach state-of-the-art performance, the model can achieve relatively good accuracy even working with less expressive data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CIFAR-10</head><p>The CIFAR-10 dataset <ref type="bibr" target="#b10">[11]</ref> contains 50,000 32 ? 32 colour images distributed in 10 classes. We used the same parameters for oversegmentation as in the previous sections. The results we achieved through a network with the same architecture as GAT-2Head was 45.93% accuracy on the test set -very distant from what we achieved on the MNIST and Fashion-MNIST datasets. The training and validation accuracies were not impressive either, being 58.61% and 53.40% respectively.</p><p>As a baseline, we considered the VGG11 <ref type="bibr" target="#b21">[21]</ref> architecture, with and without batch normalisation. We were unable to train the model without batch normalisation, whereas with batch normalisation we achieved 62.86% validation accuracy, which shows the heavy information loss during the RAG transformation procedure. However the comparison of the GAT with the VGG model is still unfair, in terms both of information available to the model and the number of parameters. While the VGG model has access to the oversegmented image, with each segment's pixel having the averaged RGB values for each superpixel -approaching also geometry information -the GAT model only has access to the average colour and the centroid position, not knowing anything about the superpixel's shape. More precisely, while the VGG model has access to the middle images in <ref type="figure" target="#fig_4">Fig. 3</ref>, the GNN model has only access to the graph on the right images, with each node containing the average pixel value and position.</p><p>As for the model size, the VGG11 network has 132,868,840 parameters, while the GAT has only 55,364. The VGG network also consumed almost twice as much VRAM as the GAT-2Head architecture on the CIFAR images. VGG11 consumed 4,109MiB for training and 1,055MiB for testing, while the GAT model expended 1,067MiB for training and 485MiB for testing. For the sake of illustration, the current state-of-the-art result on CIFAR-10 <ref type="bibr" target="#b7">[8]</ref> is reached with an AmoebaNet-B (550M parameters) pre-trained on ImageNet and fine-tuned on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have investigated the application and interplay between Graph Attention Networks (GATs) <ref type="bibr" target="#b29">[29]</ref> and image classification problems. In order to do so, we have used Region Adjacency Graphs (RAGs) computed from an image segmented using a superpixel algorithm, SLICO. We showed that using attention-based graph neural networks on a feature space that contains the geometric information can be improved by weighting the edges of a superpixel graph using a learned function which operates solely on the geometric information.</p><p>However, this approach to image classification has its shortcomings. The information loss in the pixel aggregation for more complex images can result in significant performance degradation when compared to using the full image. Also, graph-based approaches may come with the same limitations intrinsic to the models they use, and in our case the GNN-based architecture imposes some limitations in terms of memory usage for larger graphs due to the batching procedure (and thus finer segmentations), despite the smaller number of parameters the model itself had. Training in small batches lead to an unreliable training pattern, further aggravating such issues.</p><p>These limitations are, however, venues for future work. It has been shown that architectures based on graph convolutional networks, such as the GAT, suffer from an oversmoothing of node-level information, thus acting like low-pass filters <ref type="bibr" target="#b18">[18]</ref>. While GATs might not be subject to the same limitation, this could be investigated to allow deeper GATs with potentially better performance in this domain. Another venue is helping scaling Graph Neural Networks, of which GAT is a representative, to larger graphs (and thus larger images) or to make them work in an online manner, or with smaller batches.</p><p>Also, our models used no regularization whatsoever, and investigating regularization techniques for these models could incur in better performance. Lastly, investigating different node feature vectors could provide the network with richer information and lesser the information loss due to the RAG procedure, possibly with information to recustruct the superpixel's components.</p><p>These graph-based approaches to image classification are also a prime example of application to non-euclidean images, such as omnidirectional images <ref type="bibr" target="#b13">[14]</ref>. The flexibility of a graph-based approach could be more invariant to the domain of the image, possibly allowing pre-training on planar images and transfer to spherical images. torch developers for their library.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Region Adjacency Graph (RAG) generation 1: procedure Superpixel2Graph(Image I of width w, height h and k channels, Superpixel segmentation technique S, and node feature builder F ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 :</head><label>7</label><figDesc>if s(x, y) = s(x + 1, y) then 8: E ? E ? {(s(x, y), s(x + 1, y))} if s(x, y) = s(x, y + 1) then 11: E ? E ? {(s(x, y), s(x, y + 1))}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :Algorithm 2</head><label>12</label><figDesc>From left to right, the image to be converted into a RAG (a), the image with the superpixel segmentation being shown (b), and the image with the superpixel segmentation and the generated region adjacency graph overlayed on top of it (c). Implemented GAT Layer1: procedure GAT-Forward(Directed graph G = (N , E), Node Features x(n)?n ? N ,learnable transition function f and learnable attention function a) 2: M tgt (t e , e) ? 1{e = (s e , t e )}?e ? E 3: h src (s e ) ? x(s e )?e ? E 4: h tgt (t e ) ? x(t e )?e ? E 5: h(e) ? h src (s e ) h tgt (t e )?e = (s e , t e ) ? E 6: y(e) ? f (h(e))?e ? E 7: ?(e) ? a(h(e))?e ? E 8: ? base (e) ? max e?E ?(e) 9: ? norm (e) ? ?(e) ? ? base (e)?e ? E 10: ? exp (e) ? e ?norm(e) ?e ? E 11: ? sum ? (M tgt ? ? exp (e)) + 12: y ? (e) ? y(e)? exp (e)?e ? E 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Training Curve for the Street View House Numbers 32 ? 32 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Examples showing the loss of information in the RAG procedure when using only the average of each channel, which negatively affects the performance of the network.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/machine-reasoning-ufrgs/ spixel-gat</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/zalandoresearch/fashion-mnist</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank NVIDIA Corporation for the Quadro GPU granted to our research group. This work is partly supported by Coordena??o de Aperfei?oamento de Pessoal de N?vel Superior (CAPES) -Finance Code 001 and by the Brazilian Research Council CNPq. We would also like to thank the Py-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to stateof-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Superpixels and polygons using simple noniterative clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4895" to="4904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FAUST: dataset and evaluation for 3d mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="3794" to="3801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10130</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Spherical cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SEEDS: superpixels extracted via energy-driven sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Benjamin De Capitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012 -12th European Conference on Computer Vision</title>
		<editor>Andrew W. Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid</editor>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7578</biblScope>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy-Oukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data representation and learning with graph diffusion-embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10414" to="10423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spherephd: Applying cnns on a spherical polyhedron representation of 360deg images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeon</forename><surname>Kun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Seob Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjune</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9181" to="9189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodol?</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Anna Korhonen, David R. Traum, and Llu?s M?rquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<ptr target="https://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO" />
		<title level="m">SLIC superpixels: SLICO. Available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Geometric graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemyslaw</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Danel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Smieja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Struski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Slowik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Maziarka</surname></persName>
		</author>
		<idno>abs/1909.05310</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Superpixels: An evaluation of the stateof-the-art. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel transformer networks for compact spherical convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9442" to="9451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative tensor network classification model for supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Shi-Ju Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">75135</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Superpixel hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4838" to="4849" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>abs/1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time coarse-to-fine topologically preserving segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2947" to="2955" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
