<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Multimodal Fusion with TupleInfoNCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
							<email>hao.dong@pku.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="department">CS Dept</orgName>
								<orgName type="institution" key="instit1">CFCS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">AIIT</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
							<email>tfunkhouser@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Multimodal Fusion with TupleInfoNCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a method for representation learning of multimodal data using contrastive losses. A traditional approach is to contrast different modalities to learn the information shared among them. However, that approach could fail to learn the complementary synergies between modalities that might be useful for downstream tasks. Another approach is to concatenate all the modalities into a tuple and then contrast positive and negative tuple correspondences. However, that approach could consider only the stronger modalities while ignoring the weaker ones. To address these issues, we propose a novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based not only on positive and negative correspondences, but also by composing new negative tuples using modalities describing different scenes. Training with these additional negatives encourages the learning model to examine the correspondences among modalities in the same tuple, ensuring that weak modalities are not ignored. We provide a theoretical justification based on mutual-information for why this approach works, and we propose a sample optimization algorithm to generate positive and negative samples to maximize training efficacy. We find that TupleInfoNCE significantly outperforms previous state of the arts on three different downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human perception of the world is naturally multimodal. What we see, hear, and feel all contain different kinds of information. Various modalities complement and disambiguate each other, forming a representation of the world. Our goal is to train machines to fuse such multimodal inputs to produce such representations in a self-supervised manner without manual annotations. <ref type="bibr">*</ref>   An increasingly popular self-supervised representation learning paradigm is contrastive learning, which learns feature representations via optimizing a contrastive loss and solving an instance discrimination task <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. Recently several works have explored contrastive learning for multimodal representation learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref>. Among them, the majority <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1]</ref> learn a crossmodal embedding spacethey contrast different modalities to capture the information shared across modalities. However, they do not examine the fused representation of multiple modalities directly, failing to fully leverage multimodal synergies. To cope with this issue, <ref type="bibr" target="#b20">[21]</ref> proposes an RGB-D representation learning framework to directly contrast pairs of point-pixel pairs. However, it is restricted to two modalities only.</p><p>Instead of contrasting different data modalities, we propose to contrast multimodal input tuples, where each tuple element corresponds to one modality. We learn representations so that tuples describing the same scene (set of multimodal observations) are brought together while tuples from different scenes are pushed apart. This is more general than crossmodal contrastive learning. It not only supports extracting the shared information across modalities, but also allows modalities to disambiguate each other and to keep their specific information, producing better-fused representations.</p><p>However, contrasting tuples is not as straightforward as contrasting single elements, especially if we want the learned representation to encode the information from each element in the tuple and to fully explore the synergies among them. The core challenge is: "which tuple samples to contrast?" Previously researchers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b20">21]</ref> have observed that always contrasting tuples containing corresponding elements from the same scene can converge to a lazy suboptimum where the network relies only on the strongest modality for scene discrimination. Therefore to avoid weak modalities being ignored and to facilitate modality fusion, we need to contrast from more challenging negative samples. Moreover, we need to optimize the positive samples as well so that the contrastive learning can keep the shared information between positive and anchor samples while abstracting away nuisance factors. Strong variations between the positive and anchor samples usually result in smaller shared information but a greater degree of invariance against nuisance variables. Thus a proper tradeoff is needed. To handle the above challenges, we propose a novel contrastive learning objective named TupleInfoNCE <ref type="figure" target="#fig_0">(Figure 1</ref>). Unlike the popular InfoNCE loss <ref type="bibr" target="#b23">[24]</ref>, TupleInfoNCE is designed explicitly to facilitate multimodal fusion. TupleIn-foNCE leverages positive samples generated via augmenting anchors and it exploits challenging negative samples whose elements are not necessarily in correspondence. These negative samples encourage a learning model to examine the correspondences among elements in an input tuple, ensuring that weak modalities and the modality synergy are not ignored. To generate such negative samples we present a tuple disturbing strategy with a theoretical basis for why it helps.</p><p>TupleInfoNCE also introduces optimizable hyperparameters to control both the negative sample and the positive sample distributions. This allows optimizing samples through a hyper-parameter optimization process. We define reward functions regarding these hyper-parameters and measure the quality of learned representations via unsupervised feature evaluation. We put unsupervised feature evaluation in an optimization loop that updates these hyper-parameters to find a sample-optimized TupleInfoNCE <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>We evaluate TupleInfoNCE on a wide range of multimodal fusion tasks including multimodal semantic segmentation on NYUv2 <ref type="bibr" target="#b28">[29]</ref>, multimodal object detection on SUN RGB-D <ref type="bibr" target="#b29">[30]</ref> and multimodal sentiment analysis on CMU-MOSI <ref type="bibr" target="#b37">[38]</ref> and CMU-MOSEI <ref type="bibr" target="#b38">[39]</ref>. We demonstrate significant improvements over previous state-of-the-art multimodal self-supervised representation learning methods (+4.7 mIoU on NYUv2, +1.2 mAP@0.25 on SUN RGB-D, +1.0% acc7 on MOSI, and +0.5% acc7 on MOSEI).</p><p>Our key contributions are threefold. First, we present a novel TupleInfoNCE objective for contrastive multimodal fusion with a theoretical justification. Secondly, we pose the problem of optimizing TupleInfoNCE with a self-supervised approach to select the contrastive samples. Finally, we demonstrate state-of-the-art performance on a wide range of multimodal fusion benchmarks and provide ablations to evaluate the key design decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-Supervised Multimodal Learning</head><p>Self-supervised learning (SSL) uses auxiliary tasks to learn data representation from the raw data without using additional labels <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34]</ref>, helping to improve the performance of the downstream tasks. Recently, research on SSL leverages multimodal properties of the data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref>. The common strategy is to explore the natural correspondences among different views and use contrastive learning (CL) to learn representations by pushing views describing the same scene closer, while pushing views of different scenes apart <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref>. We refer to this line of methods as crossmodal embedding, which focuses on extracting the information shared across modalities rather than examining the fused representation directly, failing to fully explore the modality synergy for multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Representation Learning</head><p>CL is a type of SSL that has received increasing attention for it brings tremendous improvements on representation learning. According to the learning method, it can be grouped into Instance-based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref> and Prototypebased CL <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4]</ref>; According to the modality of data, it can be categorized into single-modality based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> and multimodality based CL <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31</ref>]. An underexplored challenge for CL is how to select hard negative samples to build the negative pair <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. Most existing methods either increase batch size or keep large memory banks, leading to large memory requirements <ref type="bibr" target="#b11">[12]</ref>. Recently, several works study CL from the perspective of mutual information (MI). <ref type="bibr" target="#b31">[32]</ref> argues MI between views should be reduced by data augmentation while keeping task-relevant information intact. <ref type="bibr" target="#b35">[36]</ref> shows the family of CL algorithms maximizes a lower bound on MI between multi-"views" where typical views come from image augmentations, and finds the choice of negative samples and views are critical to these algorithms. We build upon this observation with an optimization framework for selecting contrastive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">AutoML</head><p>AutoML is proposed to automatically create models that outperform the manual design. The progress of neural architectural search (NAS) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>, data augmentation strategy search <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> and loss function search <ref type="bibr" target="#b15">[16]</ref> have greatly improved the performance of neural networks. But most of these methods focus on a supervised learning setting. Recently, developing AutoML techniques in an unsupervised/self-supervised learning scenario has drawn more attention <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22]</ref>. UnNAS <ref type="bibr" target="#b18">[19]</ref> shows the potential of searching for better neural architectures with selfsupervision. InfoMin <ref type="bibr" target="#b31">[32]</ref> and SelfAugment <ref type="bibr" target="#b21">[22]</ref> explore how to search better data augmentation for CL on 2D images. In our work, we focus on optimizing two key components of a multimodal CL framework unsupervisedly -data augmentation and negative sampling strategies, none of which has been previously explored for generic multimodal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting InfoNCE</head><p>Before describing our method, we first review the In-foNCE loss widely adopted for contrastive representation learning <ref type="bibr" target="#b23">[24]</ref>, and then discuss its limitations for multimodal inputs. Given an anchor random variable x 1,i ? p(x 1 ), the popular contrastive learning framework aims to differentiate a positive sample x 2,i ? p(x 2 |x 1,i ) from negative samples x 2,j ? p(x 2 ). This is usually done by minimizing the In-foNCE loss:</p><formula xml:id="formula_0">L NCE = ?E log f (x 2,i , x 1,i ) N j=1 f (x 2,j , x 1,i )<label>(1)</label></formula><p>where f (x 2,j , x 1,i ) is a positive scoring function usually chosen as a log-bilinear model. It has been shown that minimizing L NCE is equivalent to maximizing a lower bound of the mutual information I(x 2 ; x 1 ). Many negative samples are required to properly approximate the negative distribution p(x 2 ) and tighten the lower bound.</p><p>In the problem setting of multimodal inputs, an input sample can be represented as a K-tuple t = (v 1 , v 2 , ..., v K ) where each element v k corresponds to one modality and K denotes the total number of modalities being considered. A straightforward way of learning multimodal representations is to draw anchor samples t 1,i ? p(t 1 ), their positive samples t 2,i ? p(t 2 |t 1,i ) and negative samples t 2,j ? p(t 2 ), and then optimize the InfoNCE objective. However, previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b20">21]</ref> observe that even when K = 2 simply drawing negative samples from the marginal distribution p(t 2 ) is insufficient for learning good representations. Weak modalities tend to be largely ignored and synergies among modalities are not fully exploited. The issue becomes more severe when K &gt; 2 when the informativeness of different modalities varies a lot. <ref type="figure" target="#fig_1">Figure 2</ref> provides an intuitive explanation. When one modality v k is particularly informative compared with the rest modalitiesv k in the input tuple t, namely</p><formula xml:id="formula_1">I(v k 2 ; v k 1 ) I(v k 2 ;v k 1 ), maximizing a lower bound of I(t 2 ; t 1 ) = I(v k 2 ,v k 2 ; v k 1 ,v k 1 ) will be largely dominated by the modal- ity specific information I(v k 2 ; v k 1 |v k 2 ,v k 1 )</formula><p>, which is usually not as important as the information shared across modalities</p><formula xml:id="formula_2">I(v k 2 ;v k 2 ; v k 1 ;v k 1 ).</formula><p>Overemphasizing the modality specific information from the strong modality might sacrifice the weak modalities and the modality synergy during learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TupleInfoNCE</head><p>To alleviate the limitations of InfoNCE for overlooking weak modalities and the modality synergy, we present a novel TupleInfoNCE objective. We leverage a tuple disturbing strategy to generate challenging negative samples, which prevents the network from being lazy and only focusing on strong modalities. In addition, we introduce optimizable data augmentations which are applied to anchor samples for positive sample generation. We optimize both the positive and negative samples to balance the information contributed by each modality. All these are incorporated into the proposed TupleInfoNCE objective, designed explicitly to facilitate multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tuple disturbing and augmentation</head><p>Tuple disturbing Generating challenging negative samples is fundamentally important to learning effective representation in contrastive learning, especially in the case of multimodal fusion setting where the strong modalities tend to dominate the learned representation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>. We present a tuple disturbing strategy to generate negative samples where not all modalities are in correspondence and certain modalities exhibit different scenes.</p><p>Given an anchor sample</p><formula xml:id="formula_3">(v 1 1,i , ..., v k 1,i , ..., v K 1,i ) and its positive sample (v 1 2,i , ..., v k 2,i , ..., v K 2,i ), we pro- pose a k-disturbed negative sample represented as (v 1 2,j , ..., v k 2,d(j) , ..., v K 2,j ), where d(?)</formula><p>is a disturbing function producing a random index from the sample set. The negative sample has K ? 1 modalitiesv k 2,j from one scene and one modality v k 2,d(j) from a different scene. Therefore, in order to correctly discriminate the positive sample from k-disturbed negative samples, the learned representation has to encode the information of the k-th modality, since the K-tuple could become negative only due to differences in the k-th modality. k-disturbed negative samples become especially challenging when they are only partially negative, e.g.v k 2,j becomes very similar tov k 2,i . Simply treating v k as an independent modality without considering its correlation with the rest modalities is not able to fully suppress the score of such partially negative samples in a log-bilinear model. Only when the network tells the disturbed modality v k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2,d(j)</head><p>is not in correspondence with the rest modalitiesv k 2,j , can it fully suppress the partially negative samples. Therefore k-disturbed negative samples encourage the correlation between each modality and the rest to be explored.</p><p>We disturb each modality separately and generate K types of negative samples to augment the vanilla InfoNCE objective. This enforces the representation learning of each specific modality in the multimodal inputs. We use ? k to represent the ratio of k-disturbed negative samples. Intuitively, the larger ? k we use, the more emphasis we put on the k-th modality.</p><p>Tuple augmentation Given an anchor sample t 1 , we apply the data augmentation to each modality separately to generate the positive sample t 2 . The data augmentation applied to modality v k will directly influence I(v k 2 ; v k 1 ) <ref type="bibr" target="#b31">[32]</ref>, which roughly measures the information contribution of modality v k in I(t 2 ; t 1 ). To further balance the contribution of each modality in our fused representation, we parameterize these data augmentations with a hyper-parameter ? and make ? optimizable for different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objective function</head><p>The TupleInfoNCE objective is designed for fusing the multimodal input tuple t = (v 1 , v 2 , ..., v K ). Given an anchor sample t 1,i ? p(t 1 ), we draw its positive sample t 2,i ? p ? (t 2 |t 1,i ), and negative sample t 2,j|j =i ? q ? (t 2 ) following a "proposal" distribution where either all modalities are in correspondence yet stem from a different scene, or each modality is disturbed to encourage modality synergy. To be specific, with probability ? 0 we sample negative samples from p(t 2 ), and with probability ? k we sam-</p><formula xml:id="formula_4">ple k-disturbed negative samples from p(v k 2 )p(v k 2 ), where {? k } K</formula><p>k=0 is a set of prior probabilities balancing different types of negative samples which sum to 1. This essentially changes our negative sample distribution to be</p><formula xml:id="formula_5">q ? (t 2 ) = ? 0 p(t 2 ) + K k=1 ? k p(v k 2 )p(v k 2 )</formula><p>. Therefore, the TupleInfoNCE objective is defined as below:</p><formula xml:id="formula_6">L ?? TNCE = ? E t2,i?p ? (t2|t1,i) t 2,j|j =i ?q?(t2) log f (t 2,i , t 1,i ) j f (t 2,j , t 1,i )<label>(2)</label></formula><p>where f (t 2,j , t 1,i ) = exp(g(t 2,j ) ? g(t 1,i )/? ) and g(?) represents a multimodal feature encoder and ? is a temperature parameter. We provide an example for the TupleInfoNCE objective in <ref type="figure" target="#fig_3">Figure 3</ref>. The hyper-parameters ? and ? can be optimized to allow flexible control over the contribution of different modalities as introduced in the next section.</p><p>Connection with Mutual Information estimation To better understand why L ?? TNCE is more suited for multimodal fusion than L NCE , we provide a theoretical analysis from the information theory perspective. As we mentioned in  Section 3, minimizing L NCE is equivalent to maximizing a lower bound of I(t 2 ; t 1 ), which could lead to weak modalities and the modality synergy being ignored. Minimizing L ?? TNCE , instead, is equivalent to maximizing a lower bound of</p><formula xml:id="formula_7">I(t 2 ; t 1 |?) + K k=1 ? k I(v k 2 ;v k 2 )</formula><p>(please see supplementary material for a proof). As is shown in <ref type="figure" target="#fig_1">Figure 2</ref></p><formula xml:id="formula_8">, I(v k 2 ;v k 2 )</formula><p>puts more emphasis on the information shared across modalities to encourage modality synergy and to avoid weak modalities being ignored. The ratio of k-disturbed negative samples ? k plays the role of balancing I(v k 2 ;v k 2 ) and I(t 2 ; t 1 |?). And the data augmentation parameters ? directly influence I(t 2 ; t 1 |?) and further balance the information contribution of each modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sample Optimization</head><p>The hyper-parameters ? and ? designed for tuple disturbing and augmentation play a key role in the TupleInfoNCE objective design. Each set of ? and ? will correspond to one specific objective and fully optimizing L ?? TNCE will result in a multimodal feature encoder g ?? . Manually setting these hyper-parameters is not reliable, motivating us to explore ways to optimize these hyper-parameters. There are mainly two challenges to be addressed. The first is the evaluation challenge: we need a way to evaluate the quality of the multimodal feature encoder g ?? in an unsupervised manner since most existing works have demonstrated that InfoNCE loss itself is not a good evaluator <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref>. The second is the optimization challenge: we need an efficient optimization strategy to avoid exhaustively examining different hyperparameters and training the whole network from scratch repeatedly. We will explain how we handle these challenges to optimize the ratio ? of different types of negative samples in Section 4.3.1, and the hyper-parameter ? of augmented positive samples in Section 4.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Optimizing negative samples</head><p>To evaluate the modality fusion quality in the learned representations unsupervisedly, we propose to use crossmodal discrimination as a surrogate task. To efficiently optimize ?, we adopt a bilevel optimization scheme alternating between optimizing ? and optimizing the main L ?? TNCE objective with a fixed ?. We elaborate on these designs below.</p><p>Crossmodal discrimination TupleInfoNCE differs from the naive InfoNCE in that it emphasizes more on each modality v k as well as its mutual information I(v k ;v k ) with the rest modalitiesv k . In order to learn a good representation that properly covers I(v k ;v k ), we propose a novel surrogate task, crossmodal discrimination, which looks for the correspondingv k only by examining v k in a holdout validation set. Mathematically, we first generate a validation set {t m } M m=1 by drawing M random tuples</p><formula xml:id="formula_9">t m = (v 1 m , v 2 m , ..., v K m ) ? p(t). For each modality v k m , its augmented version is represented as v k m ? p ? k (v k |v k m )</formula><p>following a data augmentation strategy parameterized by ? k . Then the crossmodal discrimination task is defined as, given any v k n sampled from the augmented validation set {v k m } M m=1 , finding its corresponding rest modalitiesv k n in the set {v k m } M m=1 . To solve this surrogate task, for any v k n sampled from the augmented validation set {v k m } M m=1 , we first compute its probability that corresponds tov k l as,</p><formula xml:id="formula_10">p k nl (g ?? ) = exp(g ?? (v k n ) ? g ?? (v k l )/? ) M m=1 exp(g ?? (v k n ) ? g ?? (v k m )/? )<label>(3)</label></formula><p>where g ?? (?) represents our optimal multimodal feature encoder trained via optimizing L ?? TNCE and ? is a temperature parameter. Then the crossmodal discrimination accuracy for the k-th modality can be computed as</p><formula xml:id="formula_11">A k (g ?? ) = M n=1 1(n = arg max l p k nl (g ?? ))/M<label>(4)</label></formula><p>where 1(?) is an indicator function. A k (g ?? ) roughly measures how much I(v k ;v k ) the encoder g ?? has captured and provides cues regarding how we should adjust ? k in the negative samples. We can then leverage the crossmodal discrimination accuracy to optimize ? through maximizing the following reward:</p><formula xml:id="formula_12">R(?) = K k=1 A k (g ?? )<label>(5)</label></formula><p>which properly balances the contribution of different modalities and has a high correlation with downstream semantic inference tasks as shown in Section 5.4. Notice to handle missing modalities in the crossmodal discrimination task, we adopt a dropout training strategy as introduced in the supplemental material.</p><p>Bilevel optimization Now we describe how to efficiently optimize R(?) with one-pass network training. We write our optimization problem as below:</p><formula xml:id="formula_13">maximize R(?) = K k=1 A k (g ?? ) s.t. g ?? = arg min g L ?? TNCE (g)<label>(6)</label></formula><p>This is a standard bilevel optimization problem. Inspired by <ref type="bibr" target="#b15">[16]</ref>, we adopt a hyper-parameter optimization strategy which alternatively optimizes ? and g in a single training pass. Specifically, we relax the constraint that K k=0 ? k = 1 during the optimization and use an independent multivariate Gaussian N (? 0 , ?I) to initialize the distribution of ?. At each training epoch t, we sample B hyper-parameters {? 1 , ...? B } from distribution N (? t , ?I) and train our current feature encoder g t separately to generate B new encoders {g 1 t+1 , ..., g B t+1 }. We evaluate the reward for each of these encoders on the validation set and update the distribution of ? using REINFORCE <ref type="bibr" target="#b34">[35]</ref> as below:</p><formula xml:id="formula_14">? t+1 = ? t + ? 1 B B i=1 R(? i )? ? log(p(? i ; ?, ?)) (7)</formula><p>where p(? i ; ?, ?) represents the PDF of the Gaussian distribution. We then pick up the encoder with the highest reward as our g t+1 and continue with the next epoch. We repeat the above process until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Optimizing positive samples</head><p>Similar to optimizing ?, a reward function is required to evaluate our feature encoder g ?? in an unsupervised manner with respect to ?. A straightforward approach is to adopt the total crossmodal discrimination accuracy defined in Equation 5. Through experiments, we observe two phenomena making this simple adaptation fail to optimize ? effectively. We use ? and ? to represent the data augmentation parameters for training and validation respectively, and they do not have to be the same. 1). If we manually set ? to be fixed, the optimal ? maximizing the total accuracy highly correlates with ? and fails to generate truly good positive samples. 2). If we set ? to be the same as ? and optimize them together, we usually achieve the best total accuracy when no data augmentation is applied, though it has been shown a certain level of data augmentation is important for contrastive learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. Therefore a better reward function is required for ? optimization. We re-write our total crossmodal discrimination accuracy as K k=1 A k (g ?? , ?) to reflect the influence from ?. Instead of manually setting ? which produces a chicken-and-egg problem for hyper-parameter optimization, we set ? = ? and only optimize ?. We follow the conclusion in <ref type="bibr" target="#b31">[32]</ref> and aim to use strong data augmentations, which reduces the information contribution by each modality but make the contributed information more robust to nuanced input noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 :Sample Optimization</head><p>Input: Initialized multimodal feature encoder g 0 , initialized distribution (? ? 0 , ? ? ) and (? ? 0 , ? ? ), total training epochs T , distribution learning rate ? Output: Final multimodal feature encoder g ? * ? *</p><formula xml:id="formula_15">T for t = 1 to T do if t is even then Sample B sampling ratio hyper-parameters {? i } B i=1 via distribution N (? ? t , ? ? I); Train g t for one epoch separately with each ? i and get {g i t+1 } B i=1 ; Calculate rewards {R(? i )} B i=1 using Equation 5</formula><p>; Decide the best model i = arg max j R(? j ); Update ? ? t+1 using Equation 7;</p><formula xml:id="formula_16">Update g t+1 = g i t+1 ; else if t is odd then Sample B data augmentation hyper-parameters {? i } B i=1 via distribution N (? ? t , ? ? I); Train g t for one epoch separately with each ? i and get {g i t+1 } B i=1 Calculate rewards {R(? i ) =} B i=1 using Equation 8</formula><p>; Decide the best model i = arg max j R(? j ); Update ? ? t+1 using Equation 7; Update g t+1 = g i t+1 ; end if end for return g T We observe that the total accuracy will decrease as we use stronger augmentations, and minimizing K k=1 A k (g ?? , ?) with respect to ? will effectively increase the augmentation magnitude. However, as discussed in <ref type="bibr" target="#b31">[32]</ref>, we should not increase the data augmentation without any constraints and there is a sweet spot going beyond which a larger data augmentation could harm the representation learning. We find ? ? ? * (?) 2 providing cues for identifying the sweet spot, where ? * (?) = arg max ? K k=1 A k (g ?? , ?) represents the best ? maximizing the total crossmodal discrimination accuracy K k=1 A k for a feature encoder trained with ?. When ? is weak, we empirically discover that ? * (?) is very close to ?; when ? is too strong, smaller augmentation parameters on the validation set will lead to higher total accuracy, therefore leading to a large difference between ? and ? * (?). We provide empirical studies supporting these findings in Section 5.4. Motivated by the above observations, we design our reward function as:</p><formula xml:id="formula_17">R(?) = 1 ? K k=1 A k (g ?? , ?) K ? ? ? ? ? * (?) 2 ? max 2<label>(8)</label></formula><p>where ? is a balancing parameter and ? max denotes a predefined augmentation parameter upper bound used for the normalization purpose.</p><p>R(?) can be optimized in the same way as how R(?) is optimized, and we alternate between optimizing ? and g in a single training pass. We further combine the optimization of R(?), R(?), and the multimodal encoder g in Algorithm 1, where we update ? when the epoch number is even and update ? otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In this section, we evaluate our method by transfer learning, i.e., fine-tuning on downstream tasks and datasets. Specifically, we first pretrain our backbone on each dataset without any additional data using the proposed TupleIn-foNCE. Then we use the pre-trained weights as initialization and further refine them for target downstream tasks. In this case, good features could directly lead to performance gains in downstream tasks.</p><p>We present results for three popular multi-modality tasks: semantic segmentation on NYUv2 <ref type="bibr" target="#b28">[29]</ref>, 3D object detection on SUN RGB-D <ref type="bibr" target="#b29">[30]</ref>, and sentiment analysis on MOSEI <ref type="bibr" target="#b38">[39]</ref> and MOSI <ref type="bibr" target="#b37">[38]</ref> in Section 5.1, 5.2 and 5.3 respectively. In Section 5.4, extensive ablation studies, analysis and visualization are provided to justify design choices of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">NYUv2 Semantic Segmentation</head><p>Setup. We first conduct experiments on NYUv2 <ref type="bibr" target="#b28">[29]</ref> to see whether our method can help multimodal semantic scene understanding. NYUv2 contains 1,449 indoor RGB-D images, of which 795 are used for training and 654 for testing. We use three modalities in this task: RGB, depth, and normal map. The data augmentation strategies we adopted include random cropping, rotation, and color jittering. We use ESANet <ref type="bibr" target="#b27">[28]</ref>, an efficient ResNet-based encoder, as our backbone. We use the common 40-class label setting and mean IoU(mIoU) as the evaluation metric.</p><p>We compare our method with the train-from-scratch baseline as well as the latest self-supervised multimodal representation learning methods including CMC <ref type="bibr" target="#b30">[31]</ref>, MMV FAC <ref type="bibr" target="#b0">[1]</ref> and MISA <ref type="bibr" target="#b10">[11]</ref>, which are all based upon crossmodal embedding. In addition, we include an InfoNCE <ref type="bibr" target="#b23">[24]</ref> baseline where we directly contrast multimodal input tuples without tuple disturbing and sample optimization. We also include supervised pretraining <ref type="bibr" target="#b27">[28]</ref> methods for completeness.</p><p>Results. <ref type="table" target="#tab_1">Table 1</ref> shows that the previous best performing method MISA <ref type="bibr" target="#b10">[11]</ref> improves the segmentation mIoU by 3.3% over the train-from-scratch baseline. When using InfoNCE <ref type="bibr" target="#b23">[24]</ref>, the improvement drops to 2.0%. Our method achieves 8.0% improvement over the train-fromscratch baseline. The improvement from 40.1% to 48.1% confirms that we can produce better-fused representations to boost the segmentation performance on RGB-D scenes. Notably, our proposed TupleNCE, though only pretrained on NYUv2 self-supervisedly, is only~3% lower than supervised pretraining methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">SUN RGB-D 3D Object Detection</head><p>Setup. Our second experiment investigates how Tuple-InfoNCE can be used for 3D object detection in the SUN RGB-D dataset <ref type="bibr" target="#b29">[30]</ref>. SUN RGB-D contains a training set with~5K single-view RGB-D scans and a test set with~5K scans. The scans are annotated with amodal 3D-oriented bounding boxes for objects from 37 categories. We use three modalities in this experiment: 3D point cloud, RGB color and height. Data augmentation used here is rotation for point cloud, jittering for RGB color, and random noise for height. We use VoteNet <ref type="bibr" target="#b24">[25]</ref> as our backbone, which leverages PointNet++ <ref type="bibr" target="#b25">[26]</ref> to process depth point cloud and supports appending RGB or height information as additional inputs. We compare our method with baseline methods including InfoNCE <ref type="bibr" target="#b23">[24]</ref>, CMC <ref type="bibr" target="#b30">[31]</ref>, and MISA <ref type="bibr" target="#b10">[11]</ref>. We use mAP@0.25 as our evaluation metric. Results. <ref type="table" target="#tab_2">Table 2</ref> shows the object detection results. We find that previous self-supervised methods seem to struggle with 3D tasks: CMC and MISA achieve very limited improvement over the baseline trained from scratch. The improvement of InfoNCE <ref type="bibr" target="#b23">[24]</ref> is also very marginal (0.5%), presumably because overemphasizing the modality-specific information from strong modalities might sacrifice the weak modalities as well as the modality synergy during learning. In contrast, TupleInfoNCE achieves 1.7% mAP improvement over the baseline trained from scratch, which more than triples the improvement InfoNCE achieved. The comparison between our method and InfoNCE directly validates the efficacy of the proposed TupleInfoNCE objective and sample optimization mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multimodal Sentiment Analysis</head><p>Setup. Our third experiment investigates multimodal sentiment analysis with the MOSI <ref type="bibr" target="#b37">[38]</ref> and MOSEI <ref type="bibr" target="#b38">[39]</ref> datasets, both providing word-aligned multimodal signals (language, visual and acoustic) for each utterance. MOSI contains 2198 subjective utterance-video segments. The utterances are manually annotated with a continuous opinion score between <ref type="bibr">[-3,3]</ref>, where -3/+3 represents strongly negative/positive sentiments. MOSEI is an improvement over MOSI with a higher number of utterances, greater variety in samples, speakers, and topics. Following the recent state-ofthe-art multimodal self-supervised representation learning method MISA <ref type="bibr" target="#b10">[11]</ref>, we use features pre-extracted from the original raw data, which does not permit an intuitive way for data augmentation. Therefore we only optimize negative samples in this experiment. We use the same backbone as MISA <ref type="bibr" target="#b10">[11]</ref> to make a fair comparison. We use binary accuracy (Acc-2), 7-class accuracy (Acc-7), and F-Score as our evaluation metrics.</p><p>Results. As shown in <ref type="table">Table 3</ref> and 4, our method consistently outperforms previous methods on these very challenging and competitive datasets -e.g., compared with the previous best performing method MISA, the Acc-7 goes up from 42.3 to 43.3 on MOSI, and from 52.2 to 52.7 on MOSEI. As these two approaches share the same network backbone and only differ in their strategy to learn the fused representation, the improvement provides strong evidence for the effectiveness of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Further Analysis and Discussions</head><p>Efficacy of sample optimization We run ablation studies with and without sample optimization to quantify its efficacy. We find that uniformly setting ? k without optimizing neg- <ref type="figure">Figure 4</ref>. Correlations between the total crossmodal discrimination accuracy and the downstream task performance.</p><p>ative samples results in a 1.7% mIoU drop on the NYUv2 semantic segmentation task, 0.5 mAP drop on the SUN RGB-D 3D object detection task, 0.6 Acc-7 drop on MOSI, and 0.4 Acc-7 drop on MOSEI. Manually designing data augmentation strategies without optimizing positive samples as in <ref type="bibr" target="#b30">[31]</ref> results in a 1.1 mIoU drop on NYUv2 and 0.6 mAP drop on SUN RGB-D. We also examine the optimized negative sampling strategy as well as the data augmentation strategy. On the NYUv2 dataset, we find the best performing negative sampling ratio among RGB, depth and normal is roughly 2 : 1 : 1, showing that RGB is emphasized more in the fused representations. As for the data augmentation strategy, though we use the same types of data augmentations for all the three modalities on NYUv2, the optimal augmentation parameters vary from modality to modality. Considering image rotation with the hyper-parameter representing the rotation angle, we found that 40 degrees is the best hyperparameter for RGB images, while 10 degrees is the best for depth and normal maps. Please refer to the supplementary material for more analysis regarding SUN RGB-D, MOSI, and MOSEI.</p><p>Reward design for negative sample optimization We introduce crossmodal discrimination as a surrogate task for negative sample optimization in Section 4.3.1 and argue that the total crossmodal discrimination accuracy R(?) in Equation 5 is a good reward function. We provide our empirical verification here. We vary the ratio ? k of type-k negative samples while keeping the relative ratio of the rest types unchanged. We train the whole network through with the fixed negative sampling ratio and evaluate both R(?) and the performance of the downstream task. As is shown in <ref type="figure">Figure 4</ref>, adjusting the proportion of different types of negative samples will influence the accuracy R(?) of the surrogate task, which has a high correlation with downstream tasks. Too low and too high proportion for one type of negative samples both lead to low R(?). There is a sweet spot corresponding to the best R(?). Experiments show this sweet spot also corresponds to the best performance on downstream tasks. Reward design for positive sample optimization Our reward function in <ref type="bibr">Equation 8</ref> for positive sample optimization is motivated by two observations: 1). minimizing total crossmodal discrimination accuracy K k=1 A k (g ?? , ?) with respect to ? will increase the augmentation magnitude; 2). ? ? ? * (?) 2 provides cues for identifying the sweet spot beyond which larger augmentation will harm representation learning. We provide empirical studies to verify these observations in <ref type="figure" target="#fig_4">Figure 5</ref>. We train networks from beginning and end with different ? to evaluate how the total crossmodal discrimination accuracy change while varying the data augmentation parameters ? on the validation set. We also evaluate how the performance of downstream tasks varies while changing the training time data augmentation parameters ?. We experiment with two types of data augmentationimage rotation and image crop, and obtain consistent observations.</p><p>K k=1 A k (g ?? , ?) indeed drops while increasing ?. Moreover, ? * (?) corresponds to the peak of each curve in the first row and it is very close to ? when ? is small. Once ? goes beyond a sweet spot, which gives the best performance on downstream tasks, ? * (?) no longer tracks the value of ? and ? ? ? * (?) 2 will give a penalty for further increasing ?. In practice, we find our reward function powerful enough for identifying the best training time data augmentation parameters. Robustness to uninformative modality TupleInfoNCE emphasizes the modality which is easy to be ignored. An obvious question is whether it is robust to uninformative modalities. We conduct experiments on MOSEI multimodal sentiment analysis task and add an uninformative modality named timestamp which denotes the relative time in a sequence. Results show using these four modalities, we achieve 52.6 Acc-7, which is only 0.1% lower than before. The final negative sample ratio among the four modalities is roughly 3(text): 3(video): 4(audio): 1(timestamp), showing our method successfully identifies that "timestamp" is not something worthy of much emphasis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes a new objective for representation learning of multimodal data using contrastive learning, Tu-pleInfoNCE. The key idea is to contrast multimodal anchor tuples with challenging negative samples containing disturbed modalities and better positive samples obtained through an optimizable data augmentation process. We provide a theoretical basis for why TupleInfoNCE works, an algorithm for optimizing TupleInfoNCE with a self-supervised approach to select the contrastive samples, and results of experiments showing ablations and state-of-the-art performance on a wide range of multimodal fusion benchmarks.</p><p>To measure the quality of the learned representation, we consider the task of semantic segmentation on NYUv2. In the 1 modality setting case, TupleInfoNCE using RGB modality coincides with InfoNCE. In 2-3 modalities cases, we sequentially add depth and normal modalities.  As shown in Tab. <ref type="bibr" target="#b5">6</ref>, We see that the performance steadily improves as new modalities are added. This finding is consistent with that of CMC <ref type="bibr" target="#b30">[31]</ref> who learn a cross-modal embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons to InfoMin</head><p>Infomin <ref type="bibr" target="#b31">[32]</ref> proposes to reduce the mutual information between different views while retaining the task-relevant information as much as possible. But how to explore taskrelevant information without labels is a very challenging problem. Infomin utilizes an adversarial training strategy to search for good views in a weakly-supervised manner. When no labels are available, which is the case in a truly self-supervised representation learning setting, the effectiveness of Infomin is greatly reduced. Taking NYUv2 semantic segmentation task as an example, in a truly self-supervised representation learning setting, the optimal rotation parameter Infomin finds is 70 degrees, which leads to 46.91 mIoU after downstream fine-turning, while the optimal rotation parameter TupleInfoNCE finds is 40 degree, corresponding to 47.41 mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Use TupleInfoNCE in four modalities</head><p>We conduct another experiment on NYUv2 to see whether our method can help multi-modal semantic scene understanding in the case of 4 modalities. Following CMC, modalities we used are L, ab, depth, normal. We follow the 3 modalities setting and use random cropping, rotation, and jittering as the augmentation strategy, thus learning a fused representation by contrasting tuples that contain 4 modalities. Using 4 modalities, the best mIoU we obtain is 48.6. This <ref type="figure">Figure 6</ref>. Sample efficiency of TupleInfoNCE is 0.5 higher than our 3 modalities baseline, showing that our method has strong generalization ability. These again validate that our TupleInfoNCE which contrasts multi-modal input tuples can produce better-fused representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sampling efficiency of TupleInfoNCE</head><p>TupleInfoNCE disturbs each modality separately to generate k disturbed negative samples. However, The most direct method is the naive sampling strategy which disturbs all modalities simultaneously to generate 1 disturbed negative sample. We conduct another experiments to compare our method with the naive sampling strategy from the perspective of efficiency. <ref type="figure">Figure 6</ref> shows that our method is more efficient than naive tuple disturbing strategy. Our method with a batch size of 512 already outperforms naive tuple disturbing strategy with a batch size of 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof for the MI lower bound of TupleInfoNCE</head><p>The TupleInfoNCE loss is essentially a categorical crossentropy classifying positive tuples t 2,i ? p ? (t 2 |t 1,i ) from N ?1 negatives views t 2,j ? q ? (t 2 ). We use p ?? (d = i|t 1,i ) to denote the optimal probability for this loss where [d = i] is an indicator showing that t 2,i is the positive view. Then we can derive p ?? (d = i|t 1,i ) as follows:</p><formula xml:id="formula_18">p ?? (d = i|t 1,i ) = p ? (t 2,i |t 1,i ) l =i q ? (t 2,l ) N j=1 p ? (t 2,j |t 1,i ) l =j q ? (t 2,l ) = p ? (t2,i|t1,i) q?(t2,i) N j=1 p ? (t2,j |t1,i) q?(t2,j )</formula><p>It can be seen from above that the optimal value for f (t 2,j , t 1,i ) in L TNCE (?, ?) is proportional to p ? (t2,j |t1,i) q?(t2,j ) . Insert this density ratio back to L TNCE (?, ?) we get:</p><formula xml:id="formula_19">L OPT TNCE (?, ?) = ? E log ? ? p ? (t2,i|t1,i) q?(t2,i) p ? (t2,i|t1,i) q?(t2,i) + j =i p ? (t2,j |t1,i) q?(t2,j ) ? ? = E log ? ? 1 + q ? (t 2,i ) p ? (t 2,i |t 1,i ) j =i p ? (t 2,j |t 1,i ) q ? (t 2,j ) ? ? ? E log 1 + q ? (t 2,i ) p ? (t 2,i |t 1,i ) (N ? 1) E t2,j p ? (t 2,j |t 1,i ) q ? (t 2,j ) = E log 1 + q ? (t 2,i ) p ? (t 2,i |t 1,i ) (N ? 1) ? E log q ? (t 2,i ) p ? (t 2,i |t 1,i ) N We design our negative "proposal" distribution as q ? (t 2 ) = ? 0 p(t 2 ) + K k=1 ? k p(v k 2 )p(v k 2 )</formula><p>. Insert this to the inequality above we obtain:</p><formula xml:id="formula_20">L OPT TNCE (?, ?) ? E log ? 0 p(t 2,i ) + K k=1 ? k p(v k 2,i )p(v k 2,i ) p ? (t 2,i |t 1,i ) N ? E log (p(t 2,i )) ?0 K k=1 (p(v k 2,i )p(v k 2,i )) ? k p ? (t 2,i |t 1,i ) N = E log p(t 2,i ) p ? (t 2,i |t 1,i ) K k=1 ( p(v k 2,i )p(v k 2,i ) p(t 2,i ) ) ? k N ? E log p ? (t 2,i ) p ? (t 2,i |t 1,i ) K k=1 ( p(v k 2,i )p(v k 2,i ) p(t 2,i ) ) ? k N = log(N ) ? I(t 2,i ; t 1,i |?) ? K k=1 ? k I(v k 2,i ;v k 2,i ) Therefore I(t 2,i ; t 1,i |?) + K k=1 ? k I(v k 2,i ;v k 2,i ) ? log(N ) ? L OPT TNCE (?, ?).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Examples for negative sample optimization</head><p>This paper sets out with the aim of assessing the importance of complementary synergies between modalities. What is surprising is that increasing the ratio of weak modalities which is considered to contain less useful information can produce better-fused multi-modal representations. To be specific, the negative sample sampling ratio is roughly 1(RGB): 2(depth): 3(normal) in NYUv2 semantic segmentation task, 1(text): 1(video): 4(audio) in MOSI/MOSEI sentiment analysis task, and 1(point cloud): 3(RGB color): 2(height) in SUNRGB-D 3D Object Detection task. These results further support our idea of contrasting multi-modal input tuples to avoid weak modalities being ignored and to facilitate modality fusion. A possible explanation for this might be that our proposed TupleInfoNCE encourages neural networks to use complement information in weak modalities, which can avoid networks converging to a lazy suboptimum where the network relies only on the strongest modality. This observation may support the hypothesis that mining useful information contained in weak modalities instead of always emphasizing strong modal information can obtain better fusion representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Examples for positive sample optimization</head><p>As mentioned in the main paper, we need to optimize the positive samples so that the contrastive learning can keep the shared information between positive and anchor samples while abstracting away nuisance factors. In the NYUv2 semantic segmentation task, rotation of 40 degrees, cropping with 160 center pixels, and Gaussian noise with a variance of 50 is best for RGB modality, while no augmentation is better for depth and normal modalities. As for the SUNRGB-D 3D Object Detection task, the final augmentation is to rotate the point cloud by 10 degrees, add Gaussian noise with a variance of 30 to the RGB image, and apply Gaussian noise with a variance of 10 to the height modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Dropout training</head><p>Dropout training Note g ?? is originally designed to consume all input modalities, while in the crossmodal discrimination task, g ?? needs to handle missing modalities as shown in Equation. We adopt a simple dropout training strategy to achieve this goal. To be specific, we randomly mask out modalities and fill them with placeholder values in the input. The missing modalities are the same in the positive and negative samples, yet could be different in the anchor tuple. This dropout strategy is only adopted with a probability of 0.6 for each training batch and the rest of the time we feed complete inputs to the feature encoder.</p><p>Robustness to dropout training We found dropout training does not cause a drop in feature quality compared to nondropout training. We first obtain the optimal hyper-parameter by grid searching. When we use a dropout training strategy with fixed optimal hyper-parameter in NYUv2 semantic segmentation task, we achieve 48.3 mIoU performance which only outperforms our strategy by 0.2%, which presumably means dropout training might distill information from other modalities to avoid representation quality degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Implementation Details</head><p>For NYU-Depth-V2 semantic segmentation task, we train the model with one V100 GPU for 500 epochs. The batch size is 20. We use SGD+momentum optimizer with an initial learning rate 0.25. We use onecycle learning rate scheduler. We set the weight decay as 1e-4. In the experiment of SUN RGB-D 3D object detection, our settings are consistent with VoteNet. We train the model on one 2080Ti GPU for 180 epochs. The initial learning rate is 0.001. We sample 20,000 points from each scene and the voxel size is 5cm. As for sentiment analysis on MOSEI and MOSI, We used the same data as MISA, where text features are extracted from BERT. Batch size is 32 for MOSI, and 16 for MOSEI. For sample optimization details, we provide a flow chart <ref type="figure" target="#fig_5">Figure 7</ref> to further support the main paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of sample-optimized TupleInfoNCE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Information diagram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>An example of the TupleInfoNCE objective for RGB, depth and normal map fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Empirical study justifying the reward design for postive sample optimization. In the first row we show the total crossmodal discrimination accuracy on the validation set while varying the augmentation parameter ? and different curves are obtained with different train time data augmentation parameters ?. The second row shows how the performance of downstream tasks vary while changing ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Overview of sample optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Corresponding author</figDesc><table><row><cell></cell><cell cols="2">Tuple Augmentation Hyper-parameter Updates</cell></row><row><cell>Tuple</cell><cell></cell><cell>Multimodal</cell></row><row><cell>Augmentation</cell><cell>Positive Samples</cell><cell>Feature Encoder</cell></row><row><cell></cell><cell></cell><cell>Shared Weights</cell></row><row><cell>Multimodality Inputs</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Anchor Samples</cell><cell>Multimodal Feature Encoder</cell><cell>TupleInfoNCE</cell></row><row><cell></cell><cell></cell><cell>Shared Weights</cell></row><row><cell>Tuple</cell><cell></cell><cell>Multimodal</cell></row><row><cell>Disturbing</cell><cell>Negative Samples</cell><cell>Feature Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Unsupervised</cell></row><row><cell></cell><cell cols="2">Tuple Disturbing Hyper-parameter Updates</cell><cell>Feature Evaluation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Semantic Segmentation results on NYUv2.</figDesc><table><row><cell>Methods</cell><cell>mIoU</cell></row><row><cell>Train from scratch</cell><cell>40.1</cell></row><row><cell>Supervised pretrain on Imagenet</cell><cell>50.3</cell></row><row><cell>Supervised pretrain on Scenenet</cell><cell>51.6</cell></row><row><cell>CMC</cell><cell>41.9</cell></row><row><cell>MMV FAC</cell><cell>42.5</cell></row><row><cell>MISA</cell><cell>43.4</cell></row><row><cell>InfoNCE</cell><cell>42.1</cell></row><row><cell>Ours</cell><cell>48.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>3D Object Detection results on SUN RGB-D.</figDesc><table><row><cell>Methods</cell><cell>mAP@0.25</cell></row><row><cell>Train from scratch</cell><cell>56.3</cell></row><row><cell>InfoNCE</cell><cell>56.8</cell></row><row><cell>CMC</cell><cell>56.5</cell></row><row><cell>MISA</cell><cell>56.7</cell></row><row><cell>Ours</cell><cell>58.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Multimodal sentiment analysis results on MOSI. Multimodal sentiment analysis results on MOSEI.</figDesc><table><row><cell>Methods</cell><cell cols="3">Acc-2 Acc-7 F-Score</cell></row><row><cell>Train from scratch</cell><cell>83.0</cell><cell>40.0</cell><cell>82.8</cell></row><row><cell>CMC</cell><cell>83.3</cell><cell>39.5</cell><cell>83.0</cell></row><row><cell>MMV FAC</cell><cell>83.5</cell><cell>41.5</cell><cell>83.4</cell></row><row><cell>MISA</cell><cell>83.4</cell><cell>42.3</cell><cell>83.6</cell></row><row><cell>InfoNCE</cell><cell>83.1</cell><cell>40.5</cell><cell>82.8</cell></row><row><cell>Ours</cell><cell>83.6</cell><cell>43.3</cell><cell>83.8</cell></row><row><cell>Methods</cell><cell cols="3">Acc-2 Acc-7 F-Score</cell></row><row><cell>Train from scratch</cell><cell>82.5</cell><cell>51.8</cell><cell>82.3</cell></row><row><cell>CMC</cell><cell>83.3</cell><cell>50.8</cell><cell>84.1</cell></row><row><cell>MMV FAC</cell><cell>85.1</cell><cell>52.0</cell><cell>85.0</cell></row><row><cell>MISA</cell><cell>85.5</cell><cell>52.2</cell><cell>85.3</cell></row><row><cell>InfoNCE</cell><cell>83.5</cell><cell>52.0</cell><cell>83.4</cell></row><row><cell>Ours</cell><cell>86.1</cell><cell>52.7</cell><cell>86.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>We show the mean Intersection over Union (mIoU) for</cell></row><row><cell>the NYU-Depth-V2 dataset, as TupleInfoNCE is trained with in-</cell></row><row><cell>creasingly more modalities from 1 to 3. The performance steadily</cell></row><row><cell>improves as new modalities are added.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Does representation quality improve as number of views increases?</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16228</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00224</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Debiased contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Misa: Modality-invariant and-specific representations for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hui</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12050</idno>
		<title level="m">Contrastive learning with adversarial examples</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01028</idno>
		<title level="m">Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Am-lfs: Automl for loss function search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are labels necessary for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="798" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">P4contrast: Contrastive learning with pairs of point-pixel pairs for rgb-d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13089</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Evaluating self-supervised pretraining without using labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07724</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive learning with hard negative samples</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient rgb-d semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06961</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On mutual information in contrastive learning for visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13149</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pretraining for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
