<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eungyeup</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyeon</forename><surname>Lee</surname></persName>
							<email>jihyeonlee@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
							<email>jchoo@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaist</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks often make decisions based on the spurious correlations inherent in the dataset, failing to generalize in an unbiased data distribution. Although previous approaches pre-define the type of dataset bias to prevent the network from learning it, recognizing the bias type in the real dataset is often prohibitive. This paper proposes a novel bias-tailored augmentation-based approach, BiaSwap, for learning debiased representation without requiring supervision on the bias type. Assuming that the bias corresponds to the easy-to-learn attributes, we sort the training images based on how much a biased classifier can exploits them as shortcut and divide them into bias-guiding and bias-contrary samples in an unsupervised manner. Afterwards, we integrate the style-transferring module of the image translation model with the class activation maps of such biased classifier, which enables to primarily transfer the bias attributes learned by the classifier. Therefore, given the pair of bias-guiding and bias-contrary, BiaSwap generates the bias-swapped image which contains the bias attributes from the bias-contrary images, while preserving bias-irrelevant ones in the bias-guiding images. Given such augmented images, BiaSwap demonstrates the superiority in debiasing against the existing baselines over both synthetic and real-world datasets. Even without careful supervision on the bias, BiaSwap achieves a remarkable performance on both unbiased and bias-guiding samples, implying the improved generalization capability of the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent deep neural networks have shown remarkable performances in computer vision tasks including classification and object detection. However, these models often achieve their goals by erroneously relying on the peripheral features that have spurious correlations with their labels, socalled dataset bias <ref type="bibr" target="#b0">[1]</ref>. For instance, imagine a classifier for recognizing a camel, when most of the camels in the training images appear in the desert. This unintended correlation causes the classifier to overly rely on the attributes of the desert, failing to recognize the camel standing on the road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* indicates equal contribution</head><p>In other words, the classifier trained on the biased dataset often shows drastic failures for the images without such bias, which raises a question on its generalization capability in unbiased image classification.</p><p>Existing methods attempt to address this issue by using an explicit definition of the bias type in their debiasing strategies. Some approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> assume the texture bias in the image classification task and propose a hand-crafted module for such bias type. Similarly, textual modality, i.e., question and answer, are pre-defined as the bias <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> in visual question answering task and resolved by leveraging the question-only network.</p><p>However, assuming an already-known bias is quite unrealistic in that bias attributes can vary according to the composition of the training dataset. Moreover, unlike synthetic datasets where bias attributes are manually designated, e.g., color is set as bias in Colored MNIST as shown in <ref type="figure" target="#fig_1">Figure 2</ref>-(a), it is highly demanding to acquire the prior knowledge on the bias that inherently exists in the real-world dataset. Therefore, an unsupervised debiasing with no definition in advance is an appropriate approach for learning generalized representations over various datasets. Moreover, maintaining the classification ability for biased samples as well as unbiased samples needs to be considered crucial for a desirable representation, which has been overlooked by the previous studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This paper mainly focuses on 1) removing the dataset bias without explicit supervision by leveraging the biastailored swapping augmentation, and 2) achieving superior performance on bias-contrary as well as bias-guiding samples against other baselines. We propose an image translation-based augmentation framework, BiaSwap, which transfers the attributes appearing on the regions of the image where the classifier often exploits as a shortcut for prediction. To this end, we first exploit the reasonable observation that the biased classifier often learns to exploit easyto-learn attributes in the early learning phase, proposed in Nam et al. <ref type="bibr" target="#b6">[7]</ref> This enables us to obtain the class activation map (CAM) <ref type="bibr" target="#b7">[8]</ref> which indicates the bias-relevant regions for each image without requiring an explicit definition of the bias type in advance. By integrating the CAM into the image translation framework, we augment the images with their bias attributes being translated by those of another exemplar image. At the same time, we present a simple and intuitive criterion based on the same assumption (i.e., bias is easy-to-learn) for discriminating between the bias-guiding and the bias-contrary samples among the training set. Therefore, given the pairs, BiaSwap mainly translates the bias-guiding image into the bias-contrary one by transferring the specific attributes corresponding to the bias. These augmented images, termed as bias-swapped, make the proportion of bias-guiding images to be less dominant in the training dataset, removing the dataset bias in the end. We provide extensive experiments representing that Bi-aSwap achieves the state-of-the-art debiasing results against the baselines across various datasets from synthetic (i.e., Colored MNIST, Corrupted CIFAR10) to real-world (i.e., BAR, bFFHQ) datasets, even without explicit supervision on the bias type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we first provide the formulation of the dataset bias (Section 2.1). Afterward, we categorize the various existing debiasing approaches in terms of prior assumption on bias type (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Definition of unwanted correlation in dataset</head><p>Consider a training dataset D where each image x ? X has its corresponding class label y ? Y. Each x can be explained by its various visual attributes, such as shape and color, and some of them are exploited by a classifier in the image classification task. Among these attributes, let z g the one that is essential for predicting a target label y, meaning that every image for class y must contain z g . Therefore, a classifier becomes generalized in the unbiased distribution when learning this attribute as a cue. In contrast, let z b denote the attribute which is less essential, but have a strong correlation with target label y. In addition, z b often acts as the bias attribute when it is easier for the classifier to learn compared to z g . Eventually, the model becomes biased by overly exploiting the z b instead of z g when trained in the biased dataset, failing to predict the samples which do not contain the z b . For example, in the Colored MNIST, most of the images in each class are highly correlated with the specific color, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>-(a). On the other hand, an unbiased test set contains the samples whose colors are uniform at random, having no correlation with their target label. In this case, attributes z g corresponds to the digit, while z b indicates the color in each image. Throughout the paper, we term z b bias-guiding attribute and the image containing the z b as bias-guiding image. While most of the samples with the same class in the training distribution share the z b , there might be a small portion of samples that have attributes that are conflicting to z b , which we term z ?b . For example, in the Colored MNIST, while most of the samples in class 0 contain red, a few samples contain non-red color, such as blue or green. As this z ?b attribute is contradictory against z b , the biased network cannot rely on it anymore. We term z ?b bias-contrary attribute, and the image with z ?b as bias-contrary image.</p><p>Since the bias-guiding samples with z b are dominant in the training dataset, it leads the classifier to rely on z b rather than the essential attribute z g . Therefore, removing the dataset bias by increasing the proportion of the biascontrary samples with z ?b can encourage the model to learn z g by preventing it from relying solely on the z b for classification. Our proposed image translation-based augmentation approach generates the images with their visual aspects of z b being transferred into z ?b , while maintaining the essential features z g . We term this augmented sample as a bias-swapped image. This, as a result, leads our classifier to achieve consistent performance in unbiased dataset distribution, where most of the samples are bias-contrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Existing debiasing approaches</head><p>Remove bias with prior knowledge Several approaches with an explicit label on the bias type have been proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Li and Vasconcelos <ref type="bibr" target="#b9">[10]</ref> and Kim et al. <ref type="bibr" target="#b8">[9]</ref> set the particular RGB values to be a bias cue in the Colored MNIST dataset, where a specific color is correlated with each digit. Agarwal et al. <ref type="bibr" target="#b10">[11]</ref> propose to synthesize the data with a generative algorithm by involving manually curated heuristics which selects the objects to remove. Besides, Sagawa et al. <ref type="bibr" target="#b11">[12]</ref> and Goel et al. <ref type="bibr" target="#b12">[13]</ref> utilize the clustering of the bias subgroups which require expensive supervision on the bias type.</p><p>Other approaches pre-define the bias type in advance and build a bias-tailored module for addressing the certain bias type <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Wang et al. <ref type="bibr" target="#b1">[2]</ref> assume the texture bias in the image classification task, and propose a projection method in the latent space to learn the independent features from the texture-biased ones. Geirhos et al. <ref type="bibr" target="#b3">[4]</ref> propose a style transfer-based augmentation method with adaptive instance normalization <ref type="bibr" target="#b16">[17]</ref>, which enhances the robustness against the texture bias. Bahng et al. <ref type="bibr" target="#b2">[3]</ref> introduce the model with limited capacity for capturing the texture bias in image classification or static bias in video action recognition, respectively, and propose the learning of the statistically independent representation against it.</p><p>However, these approaches have limitations in that assuming the certain type of bias does not guarantee the generalized debiasing in the dataset with other types of bias. As the bias-guiding attributes z b is determined by the characteristics of dataset, such as the composition of images and the attribute complexity, learning debiased representation without prior assumption on the certain bias type is essential. Remove bias without explicit supervision Still, learning debiased representation in an unsupervised manner is an ideal but demanding problem. Darlow et al. <ref type="bibr" target="#b17">[18]</ref> utilize the adversarial perturbation in the latent space for synthesizing the images against the bias that the classifier learns. Nam et al. <ref type="bibr" target="#b6">[7]</ref> observe the general aspect of bias as easy-to-learn in the early training phase and adopt the generalized crossentropy loss <ref type="bibr" target="#b18">[19]</ref> to train a biased network. The samples in which such biased network fails to classify are then emphasized through weighted cross-entropy loss in the training of the debiased network.</p><p>A truly debiased classifier learns the generalized attribute z g , which should correctly classify the samples in the unbiased as well as the biased dataset. However, existing baselines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> often suffer from the significant performance degradation in the biased dataset (i.e., bias-guiding samples). This implies that they implicitly learn to avoid the bias-guiding attributes, not fully learning the z g . Learning debiased representation without bias supervision remains challenging, and is thus fairly under-explored. Our proposed bias-tailored augmentation effectively removes the dataset bias, achieving the generalized debiasing capability in both biased and unbiased test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>This section provides a detailed description of distinguishing a bias-guiding and bias-contrary samples (Section 3.1), training a bias-tailored swapping autoencoder (Section 3.2), and training a classifier with debiased representation (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Separation of bias-contrary samples</head><p>We propose a simple, yet effective method that divides the training samples into bias-guiding and bias-contrary groups. Our method assigns the bias label for the images adaptively according to the training dataset, without the explicit supervision on the bias. As mentioned in Section 2.1, bias-guiding samples have the unwanted correlations that are easy-to-learn <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>, while bias-contrary samples are hard-to-learn. Therefore, a biased classifier becomes certain and correct for the bias-guiding samples. In contrast, as the bias-contrary samples do not include the attributes the classifier mainly relies on, the classifier can be either 1) certain and incorrect or 2) uncertain for the bias-contrary samples. Based on these characteristics, we introduce a pseudobias label which determines whether each image belong to bias-guiding or bias-contrary by observing both the classification correctness and the confidence of the model for the image.</p><p>To distinguish the binary category (i.e., bias-contrary or bias-guiding) more certainly, we first train the biased classifier f bias by exploiting the generalized cross-entropy (GCE) loss <ref type="bibr" target="#b18">[19]</ref> in a similar manner to Nam et al. <ref type="bibr" target="#b6">[7]</ref> GCE loss is originally proposed as a noise-robust alternative to the categorical cross-entropy (CE) loss. In our setting, it amplifies the biased representation since its gradient is written as ?GCE(p,y)</p><formula xml:id="formula_0">?? = p q y ?CE(p,y) ??</formula><p>where p y is probability corresponding to the target label y, q ? (0, 1] is a hyperparameter, and ? denotes the network parameters. GCE loss puts greater importance on the samples which are easy to learn compared to the CE loss. As these samples are bias-guiding in our training dataset, our classifier becomes biased.</p><p>Assume the biased classifier f bias outputs the resulting logits z = (z 1 , ..., z K ) ? R K after the last linear layer, where K denotes the number of the target class. We first define the bias score of each sample x by obtaining the absolute difference between the correctness and the max values of probability described as</p><formula xml:id="formula_1">score(x) = 1 argmax k fbias(x)=y ? max e fbias(x) K j=1 e fbias(x)j ,<label>(1)</label></formula><p>where 1 is an indicator function which outputs one when satisfying the given condition and zero in vice versa, max returns the maximum probability value after the softmax operation, and f bias (x) j as the j-th logit values of the biased classifier. For the bias-guiding image which contains z b , the model correctly predicts the target label y (i.e., first term in Eq. 1 becomes 1) with high confidence (i.e., second term in Eq. 1 becomes high), resulting the calculated score to be close to 0. Contrariwise, the score becomes close to 1 when the model makes the wrong prediction (i.e., first term in Eq. 1 becomes 0) with high confidence (i.e., second term in Eq. 1 becomes high), which might be mostly observed for the bias-contrary samples. In addition, for the occasional cases where the classifier correctly predicts the bias-contrary images with low confidence, the score would be placed between 0 and 1. Given such score, we determine the pseudo bias label? bias (x) for each data which is</p><formula xml:id="formula_2">y bias (x) = 1 if score(x) &gt; 1 N N i=0 score(x i ) 0 otherwise ,<label>(2)</label></formula><p>where y denotes a ground-truth target label and N as the total number of training images. We consider the image assigned with? bias = 0 as bias-guiding and that with? bias = 1 as bias-contrary. We adopt the arithmetic mean of the scores over the entire samples as the threshold for determining whether each sample is bias-guiding or bias-contrary. We empirically find that such mean values of the scores can act as a simple and effective threshold for discriminating the bias-guiding images and bias-contrary images. In Section 4.2, we validate this simple criterion reasonably performs over the various datasets utilized in the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bias-tailored swapping autoencoder</head><p>Given the pair of bias-guiding and bias-contrary images using the? bias as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-(a), we leverage the state-of-the-art image-to-image translation method called swapping autoencoder (SwapAE) <ref type="bibr" target="#b20">[21]</ref> as our backbone network for translation. To enable the translation of the bias-aware attributes in the bias-aligned samples to be bias-contrary, we propose a novel variant of patch cooccurrence discriminator, which mainly focuses on bias attributes based on the class activation map (CAM) <ref type="bibr" target="#b7">[8]</ref> of a biased classifier. Swapping autoencoder SwapAE <ref type="bibr" target="#b20">[21]</ref> consists of the encoder E which maps the image into the latent features z, and the generator G which reconstructs the images x from z. Specifically, E encodes the image into its content features z c and style features z s , and G takes them to synthesize the image which can be explained by them. SwapAE first utilizes the reconstruction loss and adversarial loss <ref type="bibr" target="#b21">[22]</ref> for generating both realistic reconstruction of an input image x. Both losses are written as</p><formula xml:id="formula_3">L recon (E, G) = E x?X x ? G(E(x)) 2 2 , L GAN,recon (E, G, D) = E x?X ?logD(G(E(x))) ,<label>(3)</label></formula><p>where X denotes a training dataset distribution and D a discriminator which classifies whether the image is real or fake. In addition, SwapAE learns to translate the style of an image, denoted as x 1 , into that of another one, x 2 , generating the translated image. This can be done by constructing the swapped pair of latent features from these images and decoding them into the image. In other words, each pair of (z 1 c , z 1 s ) and (z 2 c , z 2 s ) are encoded from x 1 and x 2 , respectively, and the swapped pair, i.e., (z 1 c , z 2 s ), are decoded to generate the translated image, which contains the style of x 2 while maintaining the content of x 1 . To ensure the trans-lated images with swapped attributes to contain the same style with x 2 , a patch cooccurrence discriminator D patch is proposed. Such discriminator enforces the styles in the randomly sampled patches from the generated images to be identical to the ones in x 2 . Therefore, the objective function can be written as</p><formula xml:id="formula_4">L CooccurGAN (E, G, D patch ) = E x 1 ,x 2 ?X ?log(D patch (crop u (G(z 1 c , z 2 s )), crops u (x 2 ))) ,<label>(4)</label></formula><p>where crop u and crops u denote the operation of cropping uniformly at random in an image for a single patch and multiple patches, respectively. To make the generated images G(z 1 c , z 2 s ) realistic, the adversarial loss is added as</p><formula xml:id="formula_5">L GAN,swap (E, G, D) = E x 1 ,x 2 ?X ,x 1 =x 2 ?log(D(G(z 1 c , z 2 s )) .<label>(5)</label></formula><p>CAM-based patch sampling As D patch randomly samples the patches from the entire spatial resolution, the style extracted from the patches does not reflect certain attributes. Instead, as we aim to transfer the styles corresponding to the attributes the classifier easily learns as shortcuts, sampling the patches related to these attributes are required. Therefore, we leverage the biased classifier f bias and integrate its CAM <ref type="bibr" target="#b7">[8]</ref>, which identifies the discriminative regions used by such a classifier, into the patch sampling method in D patch . Specifically, given an image, our classifier produces an activation map f bias,k (x, y), where k denotes a channel index and (x, y) the coordinate for the spatial location. Then, following Zhou et al. <ref type="bibr" target="#b7">[8]</ref>, we calculate a logit for each class c as k w c k F k , where F bias,k denotes the result of global average pooling for channel k and w c k indicates a weight which maps the F bias,k into each class probability. The logits for class c can be written as</p><formula xml:id="formula_6">k w c k F bias,k = k w c k x,y f bias,k (x, y) = x,y k w c k f bias,k (x, y).<label>(6)</label></formula><p>Therefore, the importance of the activation map at spatial location (x, y) for classifying the class c by the classifier f bias can be presented as</p><formula xml:id="formula_7">I c (x, y) = k w c k f bias,k (x, y).<label>(7)</label></formula><p>As the classifier is biased, the large value of I c (x, y) demonstrates the location where the bias attributes are highly obtained by the classifier. In this regard, we convert I c (x, y) into the sampling probability P (x, y) for each spatial location of patch (x, y) and utilize such probability in the discriminator D patch for style extraction, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-(b). In other words, instead of random crop operation in Eq. 4, we utilize the cropping of local patches according to the probability described as</p><formula xml:id="formula_8">P (x, y) = e Ic(x,y)</formula><p>x,y e Ic(x,y) .</p><p>This encourages the more frequent sampling of patches corresponding to the bias attribute in the images compared to others, enabling the translation of bias attributes from an image to another. Therefore, the variant of the objective function of Eq. 4 via bias-tailored patch discriminator can be described as</p><formula xml:id="formula_10">L CooccurGAN (E, G, D bias-tailored patch ) = E x 1 ,x 2 ?X ?log(D patch (crop b (G(z 1 c , z 2 s )), crops b (x 2 ))) ,<label>(9)</label></formula><p>where crop b and crops b denote the operation of cropping under the probability of Eq. 8 in an image for a single patch and multiple patches, respectively. In consequence, BiaSwap generates the bias-swapped image, which contains the bias-relevant attributes from the bias-contrary image while preserving the bias-irrelevant features from the bias-guiding image, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training classifier with augmented dataset</head><p>By adding the generated bias-swapped images X bias-swapped , we can obtain our augmented training dataset X aug = X ? X bias-swapped . These reasonable amounts of bias-swapped samples in X alleviates the dataset bias caused by the dominant number of bias-guiding images in the dataset, thus preventing the model from learning biased representation. Finally, we train a classifier f debias with these datasets with the classification loss as</p><formula xml:id="formula_11">L class = E x?Xaug ? c y c logf debias (x) .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Analysis</head><p>In Section 4.1, we first introduce the experimental setup, including the details of the biased datasets and implementation details. Afterward, Section 4.2 and Section 4.3 provide the quantitative and qualitative comparison between our method with existing baselines on synthetic and realworld datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>We evaluate our method as well as the baselines across the synthetic dataset, i.e., Colored MNIST, and Corrupted CIFAR10 <ref type="bibr" target="#b22">[23]</ref>, which are widely used in the previous literature. We also utilize the real-world datasets including BAR <ref type="bibr" target="#b6">[7]</ref> and bFFHQ. Datasets As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, Colored MNIST is an MNIST dataset <ref type="bibr" target="#b23">[24]</ref> which has a correlation with certain colors. To inject the color bias, we select 10 distinct colors and inject each color into the MNIST images with a certain digit label (e.g., red color for images of zero label). Bias-contrary samples have their colors sampled uniformly at random. Corrupted CIFAR10 is the CIFAR10 <ref type="bibr" target="#b24">[25]</ref> dataset with texture corruptions, as proposed in Hendrycks and Dietterich <ref type="bibr" target="#b22">[23]</ref>. Similar to Colored MNIST, each texture corruption has an injurious correlation with each object class. We newly construct the Gender-biased FFHQ dataset (bFFHQ) which has  <ref type="table">Table 1</ref>: Quantitative comparisons of bias-guiding and unbiased test accuracy on two synthetic datasets. Note that each method has a different supervision level. The methods with yellow background assume the bias type in advance, while those with blue do not require such type. We denote the best score with bold and the best score among unsupervised methods with under-lined scores.</p><p>age as a target label and gender as a correlated bias, and the images are from the FFHQ dataset <ref type="bibr" target="#b25">[26]</ref>. The images include the dominant number of young women (i.e., aged 10-29) and old men (i.e., aged 40-59) in the training data. The biased action recognition (BAR) dataset is categorized by six human-action classes that are correlated with the distinct places <ref type="bibr" target="#b6">[7]</ref>. Curated six typical action-place pairs are (Climbing, RockWall), (Diving, Underwater), (Fishing, Wa-terSurface), (Racing, A PavedTrack), (Throwing, Playing-Field), and (Vaulting, Sky). For the experiments on synthetic datasets, we vary the ratio of bias-guiding samples which are 95.0%, 98.0%, 99.0%, and 99.5%. For bFFHQ, we utilize 99.0% of bias-guiding images. For BAR, we utilize typical action-place paired images for training, and biascontrary ones only belong to the evaluation set. Although BAR only contains the bias-guiding training samples, undoubtedly there exist relatively easier samples, i.e., more bias-guiding, than the others i.e., less bias-guiding, in our proposed framework.</p><p>Evaluation sets To measure the generalization capability of the debiasing method, we consider the two types of evaluation sets, the unbiased and bias-guiding sets. The unbiased evaluation set is constructed in a way that the bias attributes are distributed uniformly at random among the data without any correlation with a certain target label, following the evaluation protocol of existing studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>. This set mainly evaluates how the debiasing method correctly classifies the bias-contrary test samples which do not include the strong correlation. Note that for the real-world datasets, we exclude the bias-guiding samples from the unbiased test set, and call the remaining ones as "bias-contrary" test set, as did in LfF <ref type="bibr" target="#b6">[7]</ref>. In contrast, the bias-guiding set is composed of the bias-guiding images from the same distribution of the biased training dataset. Such an evaluation set enables us to evaluate how the debiasing method maintains the classification capability for the bias-aligned test images after learning the debiased representation. We believe that the truely debiased classifier should correctly predict the target labels of images in an unbiased as well as a biased test sets. Implementation details For the biased classifier and debiased classifier, we use MLP with three hidden layers for Colored MNIST, and ResNet-18 <ref type="bibr" target="#b27">[27]</ref> for Corrupted CI-FAR10, bFFHQ, BAR datasets, respectively. For the Swa-pAE, we follow the same network architecture as proposed in Park et al. <ref type="bibr" target="#b20">[21]</ref> To measure the bias score, hyperparameter q = 0.7 is used for the GCE loss, and thresholds are fairly chosen on 50 epochs. We provide a detailed description of experimental details in Section D in the supplementary material.</p><p>Comparison methods We compare BiaSwap with existing methods, ReBias <ref type="bibr" target="#b2">[3]</ref> and LfF <ref type="bibr" target="#b6">[7]</ref>, which address the dataset bias problem in the image classification task. The vanilla classifier which is trained without any debiasing procedure is also included in the comparison. In addition, we add Stylised ImageNet (SIN) <ref type="bibr" target="#b3">[4]</ref> as our baseline in validating the effectiveness of utilizing the realistic augmentation in debiasing. For the fair comparison with baseline models, we re-implement LfF <ref type="bibr" target="#b6">[7]</ref>, ReBias <ref type="bibr" target="#b2">[3]</ref>, and Stylised Im-ageNet (SIN) <ref type="bibr" target="#b3">[4]</ref> with the dataset we evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Evaluation</head><p>Synthetic datasets We verify our method on Colored MNIST and Corrupted CIFAR10. In <ref type="table">Table 1</ref>, we report the classification accuracy on the two datasets with varying ratios of bias, evaluated on both bias-guiding and unbiased test set for each dataset. The more bias becomes severe, the more vanilla model failure to generalize on unbiased data, where the shortcut does not exist. In contrast, our pro-  <ref type="table">Table 2</ref>: Quantitative comparisons of bias-guiding and biascontrary test accuracy on two real-world datasets. We denote the best score with bold and the best score among unsupervised methods with under-lined scores.</p><p>posed method maintains the robust debiasing capability on the unbiased test set, regardless of the bias ratio. In addition, the ReBias is observed to achieve the best score in Colored MNIST, which is mainly due to the network designed for capturing the texture bias. Note that BiaSwap obtains comparable accuracies compared to ReBias, even BiaSwap does not require any assumption on the type of bias in advance. Especially for the bias-guiding samples, our method achieves significant improvement, as success to generalize on the intended direction. Compared to LfF, where no prior knowledge on the bias type is required like BiaSwap, Bi-aSwap outperforms LfF on both bias-guiding and unbiased test accuracies for most of the dataset setup. As for the degraded bias-guiding accuracies of LfF, we assume that the oversampling of the limited number of bias-contrary images makes the network instead under-fitted to the downplayed bias-guiding images during training.</p><p>Real-world datasets To demonstrate the efficacy of our method on a realistic scenario, we provide the quantitative comparisons validated on bFFHQ and BAR datasets that contain complex types of bias in real-world images. <ref type="table">Table 2</ref> demonstrates that BiaSwap achieves superior debiasing performance against the existing baselines on these real-world datasets. ReBias reveals the significant drop of bias-contrary accuracy for those datasets where the texture no more causes the unwanted correlation. Likewise, LfF shows the degraded performance in the bias-guiding images. Therefore, we demonstrate that our method represents the debiasing approach with wide applicability on real-world datasets.</p><p>To validate the proposed methods, the ablation study is  <ref type="table">Table 3</ref>: Quantitative evaluations on the? bias assignment via precision, recall, and F1 score metrics. We report the evaluation scores for 99.0% of each dataset, except BAR where no bias label is accessible. provided in Section A of the supplementary material. Evaluation on? bias assignment We provide the quantitative evaluation on the proposed method of assigning the pseudo-bias labels on the bias-guiding and bias-contrary images via Eqs. 1 and 2, as shown in <ref type="table">Table 3</ref>. The table includes the precision, recall, and F1 Score of the binary classification on the four datasets with varying ratios of bias severity. Note that we consider identifying both the biasguiding and bias-contrary images equally important in our framework, we first calculate each metric for both cases. Afterwards, we add them and divide them by two in order to obtain the overall scores for classifying both bias-guiding and bias-contrary images. As shown in <ref type="table">Table 3</ref>, the proposed method mentioned in Section 3.1 achieves the reasonable performance on dividing the bias-guiding and biascontrary images. Therefore, providing these paired sets of images enables the effective generation of bias-swapped images in the swapping autoencoder described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative analysis</head><p>Generation of bias-swapped image <ref type="figure" target="#fig_2">Figure 3</ref>-(b) depicts a set of bias-guiding, bias-contrary, and the generated biasswapped images for Colored MNIST, Corrupted CIFAR10, and bFFHQ in each row. We observe that the bias-swapped images contain the bias attributes extracted from the bias-  Comparison with stylised imagenet Although existing augmentation-based methods have achieved the improved classification performance <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, they may suffer from generating unrealistic images. Some of the methods often leverage the simple image-level augmentation techniques to combine the two different images, resulting in unrealistic images compared to natural ones. The recently proposed StylisedImageNet (SIN) <ref type="bibr" target="#b3">[4]</ref> utilizes the AdaINbased style transfer to augment the ImageNet images with different textures in order to solve the texture bias. However, as shown in <ref type="figure" target="#fig_2">Fig 3-(a)</ref>, stylized results are much more unrealistic compared to the original images. In contrast, our approach synthesizes realistic images in a more natural way, and we believe that realistic augmented images help debiasing more than unrealistic ones.</p><p>To verify this, we compare the unbiased test accuracy of our method against that of SIN across the Colored MNIST, bFFHQ, and BAR datasets, in <ref type="table" target="#tab_4">Table 4</ref>. We observe that SIN fails to learn the debiased representation on each dataset, and it may be caused by 1) the unrealistic augmented samples and 2) the augmentation without considering the bias attributes. To be specific, in <ref type="figure" target="#fig_2">Figure 3</ref>-(a), stylized pink eight loses the original shape of eight, and the texture of a facial image is changed while the gender attribute remains unchanged. On the other hand, BiaSwap only replaces the bias-relative attributes and generates visually plausible images, as shown in <ref type="figure" target="#fig_2">Figure 3</ref> tailored patch discriminator generates the images considering bias-relevant attributes. For example, the cat in the (e) column contains the same corruption (i.e., saturate) as the one in column (b) while maintaining the overall shape of cat in column (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>In this paper, we propose a novel image translationbased debiasing approach which augments the realistic bias-contrary images for learning debiased representation. Based on the assumption that bias attribute is easy-to-learn, we leverage the patch cooccurence discriminator integrated with CAM and the GCE loss to generate the image with its bias attributes translated from the bias-contrary images while maintaining the other bias-irrelevant visual aspects. Extensive experiments demonstrate that our method successfully generates realistic bias-contrary images, achieving the state-of-the-art debiasing performance across diverse datasets. We acknowledge that the perfect translation of bias in images remains challenging, particularly when the dataset contains a complex combination of bias attributes or the number of training images is limited. However, we believe that our work can be viewed as a cornerstone of future debiasing works. This material complements our paper with additional experimental results and their analysis. First of all, we present the ablation studies on the proposed modules of our framework, presented in Section A. Afterward, Section B provides the qualitative and quantitative analysis on the proposed pseudo-bias labels assignment. In Section C, we provide additional qualitative examples of augmented bias-swapped images generated by our proposed method, along with their class activation map (CAM) <ref type="bibr" target="#b7">[8]</ref> visualization. Section D describes the implementation details, such as the settings for training and the construction of biased-FFHQ (bFFHQ) dataset. Lastly, we provide a detailed explanation of the datasets and baselines we utilized in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation study</head><p>This section demonstrates the effectiveness of our two main contributions, 1) separation of bias-contrary and bias-guiding images and 2) CAM-based patch sampling in the bias-tailored swapping autoencoder (SwapAE). As explained in Sections 3.1 and 3.2 of the main paper, the separation of contrary and guiding images encourages the swapping autoencoder to translate the bias-guiding image into the bias-swapped one by reflecting the bias-contrary attributes. In addition, CAM obtained from the biased classifier enables the sampling of patches based on the highly discriminative (i.e., highly bias-related) regions, enforcing the bias-tailored patch discriminator to translate the visual styles from them. To verify the effectiveness of such methods, we conduct the ablation studies on these two components denoted as c1 and c2 in <ref type="table" target="#tab_6">Table 5</ref> and compare the accuracy of the unbiased test set over Colored MNIST and bFFHQ datasets.</p><p>As the separation is ablated, a pair of two guiding images become more frequently provided in the SwapAE for augmenting the new image, compared to the bias-guiding and bias-contrary pairs. As a result, the translation between these guiding images generates another guiding image, which does not help to remove the dataset bias in the training distribution. It is observed in Table 5 that the model with c1 ablated shows a critically degraded performance in an unbiased test set compared to the highest accuracies in the bias-guiding dataset on both Colored MNIST and bFFHQ. In contrast, our model trained with c1 achieves superior performance both in guiding and unbiased test set, demonstrating that the classifier benefits from the augmented images generated from (bias-guiding, bias-contrary) pairs. When we ablate the second method c2, the patches are randomly sampled as exactly the same as the original patch co-occurrence discriminator proposed in the original paper <ref type="bibr" target="#b20">[21]</ref> does. This simply transfers the overall style of a bias-contrary to a bias-guiding image without considering the regions of bias-attribute. However, utilizing the CAM-based patch sampling enables the further optimized image translation by focusing on transferring the bias-related attributes in the image. <ref type="table" target="#tab_6">Table 5</ref> indicates the proposed method equipped with c2 achieves the best accuracy on the unbiased test set of both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis on pseudo-bias label assignment</head><p>As introduced in Section 3.1 of the main paper, we utilize a bias score as well as the pseudo-bias label ypseudo to divide the entire training dataset into bias-guiding and bias-contrary samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Bias c1 : Separation of bias-contrary and bias-guiding pairs. c2 : CAM-based patch sampling. To provide the qualitative and quantitative verification on the effectiveness of such division, this section consists of two parts, 1) qualitative examples classified as a bias-guiding and bias-contrary by the method and 2) quantitative evaluation of the robustness of ypseudo assignment on the diverse dataset setups, as supplementary to the <ref type="table">Table 3</ref> in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. BAR images separated by pseudo-bias label</head><p>As mentioned in Section 4.1 in the main paper, the training dataset of BAR only contains bias-guiding samples categorized by Nam et al. <ref type="bibr" target="#b6">[7]</ref>. However, even if we assume that all the samples have an unwanted correlation with the target label, there must exist a varying degree of bias between the training samples. In other words, some of the samples can be more bias-guiding compared to the other images. In this regard, the proposed dividing strategy can effectively capture this subtle difference and sorts the samples from easy to hard one. To be specific, the images with the same ground-truth target labels can be sorted by their bias score, which represents how much the image includes the bias attributes. <ref type="figure" target="#fig_4">Figure 5</ref> shows the exemplar images which are assigned to the bias-guiding (left columns) or bias-contrary (right columns) via our bias score and the threshold described in Eqs 1 and 2 in the main paper, respectively. While unwanted correlations shown in the left columns are found in the most of training data, it turns out that some of the samples are relatively hard-to-learn, where there exist less severe correlations between the bias attributes and the target label. For instance, for the images labeled with climbing in the first row, most of the climbers are located on the rock which has an uneven texture and brown color. In this context, the examples with the sky in the most part of their backgrounds or with colors other than brown are classified as a bias-contrary sample. Similarly, for the images labeled with diving in the second row, divers are usually in the deep sea or taking a similar body motion. However, examples on the right side are conflicting with the bias in that they contain a unique diving pose. Some of them also are black-and-white pictures, which is uncommon in the training dataset. For the fishing images on the right side of the third row, the fisher in the lake surrounded by the dense trees or the fisher near the river represents that such places are not the usual cases in the training distribution. This implies that our method empirically well divides the samples based on the relative severity of the bias  <ref type="table">Table 6</ref> demonstrates the quantitative evaluation scores of our proposed dividing method on the Colored MNIST, Corrupted CI-FAR10, and bFFHQ datasets. As our method works as a binary classifier to discriminate whether the images are bias-guiding or bias-contrary, we measure the precision, recall, and F1 score for both bias-guiding and bias-contrary classes on the unbiased test set. Following the same evaluation protocol in Section 4.2 of the main paper, we add the scores of bias-guiding and bias-contrary ones and divide them by two in order to obtain the overall scores.  <ref type="table">Table 6</ref>: Quantitative evaluations on the? bias assignment via precision, recall, and F1 score metrics. We report the evaluation scores for 99.5%, 99%, 98%, and 95% of both Colored MNIST and Corrupted CIFAR10, and 99% of bFFHQ, except BAR where no bias label is accessible. <ref type="table">Table 6</ref> indicates that the dividing method works well on classifying the bias-contrary as well as bias-guiding samples, achieving reasonable results in precision, recall, and F1 score. In consequence, the robustness of the method enables to guarantee of the effective augmentation of bias-swapped images in the bias-tailored swapping autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Quantitative evaluation on pseudo-bias label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional qualitative examples of biasswapped images</head><p>To supplement Section 4.3 in the main paper, this section provides the additional qualitative results of the bias-swapped samples as well as their CAMs in <ref type="figure" target="#fig_5">Figure 6</ref>. In order from left to right, bias-guiding sample, bias-contrary sample, CAM, heatmap of CAM visualized on the bias-contrary image, and the biasswapped image generated from BiaSwap are presented. The first and second rows include the examples of Colored MNIST and Corrupted CIFAR10, respectively. Similar to the ones in the main paper, CAM heatmaps on the Colored MNIST samples show that the biased classifier mainly focuses on the regions where the biascorrelated colors are located. For example, CAM described in the first row follows the region of blue colors in the digit. For the samples of the Corrupted CIFAR10, our model properly transfers the bias attributes of the second column images into the ones in the first column, maintaining the bias-irrelevant visual aspects unchanged. This results in the bias-swapped images shown in the last column. For example, the corruption applied on the car in the second column is transferred into another car in the first column, while the shape of the first column car is maintained in the generated bias-swapped car in the last column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details</head><p>This section provides the specific values of the threshold for separation of bias-guiding and bias-contrary groups over each dataset. Afterward, we provide the detailed architecture of two main networks, bias-tailored swapping autoencoder and debiased classifier. In addition, we provide the training details, such as hyper-parameters for each objective function, over the dataset we utilized. Threshold for division To separate the training samples into biascontrary and bias-guiding sets in an unsupervised manner, we utilize the mean value of confusing scores of the images as our threshold in each dataset, as described in Eq. 2 in the main paper. Such threshold values correspond to 0.0358, 0.0903, 0.0232, and 0.008 for Colored MNIST, Corrupted CIFAR10, BAR, and bFFHQ, respectively. Bias-tailored swapping autoencoder We follow the original network architecture of the encoder, decoder, and discriminator proposed in Park et al. <ref type="bibr" target="#b20">[21]</ref> to maintain its image translation performance. However, to design a CAM-based patch sampling for the co-occurrence discriminator as proposed in Section 3.2 in the main paper, we sample the patches using the patch-wise probability based on the CAM of the biased classifier, instead of random sampling. For the biased classifier, we use a multi-layer per-ceptron (MLP) with three hidden layers for Colored MNIST and ResNet-18 <ref type="bibr" target="#b27">[27]</ref> for the rest of the datasets. The classifier is trained with the GCE loss with hyperparameter q of 0.7. As the parameters of the classifier are not jointly optimized with those of biastailored swapping autoencoder, the classifier is fully trained to be biased. We train and evaluate the biased classifier with the size of 28?28 and 32?32 images for Colored MNIST and Corrupted CI-FAR10, and 128 ? 128 for BAR and bFFHQ datasets. Each channel of the images are normalized with the mean of (0.5, 0.5, 0.5) and the standard deviation of (0.5, 0.5, 0.5). All other details are identical to the training of the debiased classifier. To train the autoencoder, we set the hyper-parameters for each loss functions as ?recon = ?GAN,recon = ?GAN,swap = ?CooccurGAN = 1. To prevent the patch discriminator from only sampling the same single patch due to the high probability close to one, we utilize the temperature scaling with ? = 10, for smoothing the probability. Both the swapping autoencoder and the classifier are trained by using an Adam <ref type="bibr" target="#b31">[31]</ref> optimizer with ?1 = 0 and ?2 = 0.99. Debiased classifier After we fully optimize the bias-tailored swapping autoencoder, we augment the dataset using the pairs of bias-guiding and contrary images given from the threshold. Given these additional images, which we call bias-swapped images in the main paper, we train an MLP with three hidden layers for Color MNIST and ResNet-18 for the rest of the datasets. We train and evaluate the classifier with the size of 28 ? 28, 32 ? 32, 128 ? 128, and 224 ? 224 images for Colored MNIST, Corrupted CIFAR10, bFFHQ, and BAR datasets, respectively. We use Adam optimizer with ?1 = 0.9 and ?2 = 0.999, and learning rate as 0.001. We use a batch size of 256 and train a classifier for 200 epochs for Color MNIST, Corrupted CIFAR10, BAR, and bFFHQ datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Datasets and Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Datasets</head><p>Corrupted CIFAR10 As proposed in Hendrycks and Dietterich <ref type="bibr" target="#b22">[23]</ref>, we apply the certain type of texture corruptions onto the CIFAR10 dataset <ref type="bibr" target="#b24">[25]</ref>. Among 15 types of corruptions, we utilize the Snow, Frost, Fog, Brightness, Contrast, Spatter, Elastic, JPEG, Pixelate, and Saturate in our paper. Such corruptions are applied with the strong correlation with the original classes of CI-FAR10 dataset, which are Plane, Car, Bird, Cat, Deer, Dog, Frog, Horse, Ship, and Truck. In addition, we utilize the corruptions with the highest degree of severity (i.e., 4) in our dataset. bFFHQ We newly construct the biased FFHQ dataset (bFFHQ) which has a strong correlation between the age (target) and gender (bias), based on the Flickr-Faces-HQ (FFHQ) dataset <ref type="bibr" target="#b25">[26]</ref>. FFHQ consists of 70, 000 images at 1024 ? 1024 resolution and contains the considerable variation of human faces in terms of age, ethnicity, and image background. Each face contains different attributes including head pose, gender, age, mustache, glasses, and emotion. Among these attributes, we utilize the age and gender attributes. To be specific, the attribute 'young' (i.e., aged 10 ? 29) is highly correlated with 'women' and 'old' (i.e., aged 40?59) is connected with 'men'. Among the total 70, 000 data, 19, 200 samples are utilized as the training dataset according to the criteria of unwanted correction, and 2,000 unbiased samples that each attribute is uniformly distributed are utilized as an evaluation set. BAR This dataset includes the images which have a correlation be-tween human actions and backgrounds, which is curated by Nam et al. <ref type="bibr" target="#b6">[7]</ref>. It does not have the ground-truth bias label, unlike other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Baselines</head><p>ReBias ReBias <ref type="bibr" target="#b2">[3]</ref> assumes the texture as the unwanted bias type and exploits the biased classifier with a limited kernel size in order to mainly capture the texture attributes from the images. By learning the representations statistically independent from such texture representations, ReBias achieves robust classification accuracies against the texture bias. We train ReBias with the same training protocol as suggested in the original paper including network architecture and the hyper-parameters. Note that for Colored MNIST, ReBias utilizes the convolutional network for capturing the texture cues, while other baselines including ours exploit the MLP with three hidden layers. LfF As mentioned in Sections 1 and 2.2 of the main paper, LfF assumes the general characteristic of bias as "easy-to-learn" and proposes the re-weighting-based debiasing method based on the GCE loss. To the best of our knowledge, this work first learns the debiased representation without any prior assumption on the bias type. We follow the official implementation setups of LfF, except for the network architecture of ResNet-20 for the Corrupted CI-FAR10 dataset. As a fair comparison, we utilize the ResNet-18 architecture for all the baselines including LfF. SIN As mentioned in Section 4.3 of the main paper, we utilize SIN as another baseline for validating the importance of realistic image generation in dataset augmentation. For the augmentation of datasets we utilize, we follow the official implementation of SIN and only replace the original ImageNet dataset with each biased dataset. Style images are identical to official ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed method, BiaSwap. The figure (a) shows the overall pipeline of the swapping augmentation framework and the figure (b) describes the patch samplers and bias-tailored patch discriminator in detail. We generate the bias-swapped images from this framework to augment the training dataset for learning debiased representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example images of each dataset we utilize in the paper. Rows (a) and (b) represent the bias-guiding samples which have a strong correlation between bias attribute and the target class. For rows (c) and (d), we additionally visualize the bias-contrary images with red boxes, which do not contain such correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparisons between the augmented images from (a) SIN and (b) our bias-tailored swapping augmentation. BiaSwap generates a more realistic and biasaware image compared to SIN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>-(b). Visualization of CAM In order from left to right in Figure 4, bias-guiding sample, bias-contrary sample, CAM, CAM heatmap visualized on the image (b), and the biasswapped image generated from BiaSwap are shown. Red regions in (d) correspond to the more discriminative region compared to the blue regions. As intended, the highlighted regions of CAM mainly appear in the regions where the bias attributes are exploited, e.g., colors in Colored MNIST and face in bFFHQ. Note that by exploiting the attributes of those attended regions in the biased classifier, our bias-Visualization of CAM utilized in our CAM-based patch sampler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative examples of divided images of BAR. Images on the left side represent the images classified as a biasguiding sample based on our threshold. Images on the right side correspond to the images classified as a bias-contrary sample. between the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative examples of generated images via Bi-aSwap on Colored MNIST (top), Corrupted CIFAR10 (middle) and bFFHQ (bottom). Each sample is listed in order of bias-guiding, bias-contrary, CAM on the bias-contrary, heatmap of CAM on the bias-contrary, and bias-swapped image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparisons of unbiased test accuracy between BiaSwap and SIN. We utilize 99.0% of biasguiding images for training.</figDesc><table><row><cell>contrary images while maintaining the bias-irrelevant at-</cell></row><row><cell>tributes from the bias-guiding images. For example, our</cell></row><row><cell>method translates a young female (first column) into a</cell></row><row><cell>young male (third column) by reflecting the gender attribute</cell></row><row><cell>from another young male (second column), while retaining</cell></row><row><cell>the bias-irrelevant aspects, such as wearing sunglasses and</cell></row><row><cell>smiling.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Quantitative comparisons of our proposed method</cell></row><row><cell>and its ablated versions on Colored MNIST and bFFHQ</cell></row><row><cell>datasets. The separation of bias-contrary and bias-guiding</cell></row><row><cell>pairs (c1), and CAM-based patch sampling (c2) are ablated.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unbiased look at dataset bias. CVPR &apos;11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning robust representations by projecting superficial statistics out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">L</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rubi: Reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from failure: Training debiased classifier from biased classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuntak</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapedriza</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Learning Deep Features for Discriminative Localization. CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning not to learn: Training deep neural networks with biased data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Repair: Removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9572" to="9581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9687" to="9695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worstcase generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model patching: Closing the subgroup performance gap with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Not using the car to see the sidewalk -quantifying and controlling the effects of context in classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8210" to="8218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6642" to="6651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sunny and dark outside?! improving answer consistency in vqa through entailed question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giedrius</forename><surname>Burachas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP, 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11486</idno>
		<title level="m">Latent adversarial debiasing: Mitigating collider bias in deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07836</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swapping autoencoder for deep image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devesh</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
