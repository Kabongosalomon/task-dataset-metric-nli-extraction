<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Zheng</surname></persName>
							<email>chaodazheng@link.</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
							<email>xuyan1@link.</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Research Institute of USV Engineering</orgName>
								<orgName type="institution" key="instit2">Shanghai University</orgName>
								<address>
									<addrLine>3 Baidu Inc</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibing</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong (Shenzhen)</orgName>
								<orgName type="institution" key="instit2">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current 3D single object tracking approaches track the target based on a feature comparison between the target template and the search area. However, due to the common occlusion in LiDAR scans, it is non-trivial to conduct accurate feature comparisons on severe sparse and incomplete shapes. In this work, we exploit the ground truth bounding box given in the first frame as a strong cue to enhance the feature description of the target object, enabling a more accurate feature comparison in a simple yet effective way. In particular, we first propose the BoxCloud, an informative and robust representation, to depict an object using the point-to-box relation. We further design an efficient box-aware feature fusion module, which leverages the aforementioned BoxCloud for reliable feature matching and embedding. Integrating the proposed general components into an existing model P2B <ref type="bibr" target="#b26">[27]</ref>, we construct a superior box-aware tracker (BAT) 1 . Experiments confirm that our proposed BAT outperforms the previous state-of-the-art by a large margin on both KITTI and NuScenes benchmarks, achieving a 15.2% improvement in terms of precision while running ?20% faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single object tracking (SOT) in 3D scene has a broad spectrum of practical applications, such as autonomous driving <ref type="bibr" target="#b19">[20]</ref>, semantic understanding <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref> and assistive robotics <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>. Given a 3D bounding box (BBox) of an object as the template in the first frame, the SOT task is to keep track of this object across all frames. In real scenes, LiDAR becomes a popular 3D sensor due to its precise measurement, reasonable cost and insensitivity to ambient light variations. In this paper, we focus on SOT on LiDAR data, which can be viewed as 3D point clouds in general.</p><p>Due to the moving environment and self-occlusion, point clouds generated by a LiDAR system are inevitably irregular and incomplete, making the SOT task very challenging. In 3D SOT, feature comparison plays an important role. The general idea to locate the target object is based on measuring the feature similarity between some candidate regions and the object template (initialized as the point cloud in the first given BBox). For example, SC3D <ref type="bibr" target="#b12">[13]</ref> uses the exhaustive search or Kalman Filter to generate a set of candidate shapes at the current frame, and comparing them to the template using a siamese network. The candidate with the maximum similarity is chosen to be the target object for the frame. Inspired by the success of the siamese region proposal network (RPN) <ref type="bibr" target="#b16">[17]</ref> in 2D SOT, P2B <ref type="bibr" target="#b26">[27]</ref> proposes a point-based correlation based on the pair-wise feature comparison. P2B executes such a correlation between the template and the search area to output the target-specific search features, on which a 3D RPN is applied to obtain the final target proposals. However, the features used for comparison are extracted from pure LiDAR point clouds, which face the following defects: 1) They do not encode the size information of objects. Since objects in LiDAR scans are mostly incomplete, it is hard to infer an object's size only from the partial point cloud. 2) They cannot capture the explicit part-aware structure information within each object BBox, e.g. some part belongs to the car front while others belong to the sunroof as <ref type="figure" target="#fig_0">Figure 1</ref> shows. Therefore, the feature comparison among such features may bring considerable ambiguities which weaken the tracking performance. What is a good representation for feature matching under 3D SOT? We revisit this problem by pointing out that the size and the part priors of the target object can be directly inferred from the template BBox given at the first frame. Based on this observation, we propose to address the above issues by explicitly utilizing the BBox to enhance the object features. Thus, we propose the BoxCloud, a robust and informative object representation depicting the point-to-box relation. Instead of using the xyz coordinate, it represents an object point via a canonical box coordinate, where the i-th dimension corresponds to the Euclidean distance between the object point and its i-th box point (i.e. the corner or center of a BBox). Unlike the original LiDAR point cloud, a BoxCloud is defined based on both the object and its BBox. Therefore, it naturally encodes the size and part information of an object. Note that we can directly compute the BoxCloud for a target template using the BBox given at the first frame. After the supervised training using groundtruth object BBoxes, the BoxClouds of objects in the search area can be easily predicted for the inference usage.</p><p>Based on the BoxCloud representation, the box-aware feature fusion (BAFF) module is further proposed to perform a correlation between the template and the search area to generate the target-specific search area features. It first measures the similarity between the search area and the template according to their BoxClouds. After such effective feature comparison, the BAFF module aggregates the topk similar template points into each corresponding searching point, yielding a high-quality target-specific search area. Finally, we construct a Box-Aware Tracker (BAT) by integrating the two proposed components into P2B <ref type="bibr" target="#b26">[27]</ref>. By taking the auxiliary 3D BBox as input, BAT captures more shape constraints and part-aware information, no matter whether the input shape is partial or not, enabling effective and robust tracking on LiDAR point clouds.</p><p>Our main contributions can be summarized as follows:</p><p>? To the best of our knowledge, we are the first to use free box information to boost the performance on the 3D SOT task. Specifically, we improve the feature comparison by designing a size-aware and part-aware BoxCloud feature, which is not only interpretable but also robust to sparseness and incompleteness. ? We propose a dedicated box-aware feature fusion module to generate better target-specific search areas in a box-aware manner. ? Experiments verify that our BAT achieves significant improvement over the state-of-the-arts on two benchmarks (i.e. KITTI and NuScenes), especially on extremely sparse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2D Siamese Tracking. Recently, Siamese based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b1">2]</ref> have demonstrated their advantages over those based on the traditional Discriminative Correlation Filter (DCF) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref> in 2D visual object tracking. Siamese based trackers formulate the visual object tracking as a feature matching problem. By utilizing the pre-trained feature extraction networks, siamese based trackers first project the target and search image onto a hidden embedding space and then compute their mutual similarity. Although 2D siamese based trackers have made remarkable progress in recent years, they focus on image patches and thus cannot be applied on point clouds directly. 3D Single Object Tracking. Early 3D SOT methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15]</ref> focus on the RGB-D information and tend to employ the 2D siamese architecture. SC3D <ref type="bibr" target="#b12">[13]</ref> proposes the first 3D siamese tracker on point clouds and introduces an auxiliary branch for shape completion to process the incomplete shapes. Although the object semantics can be learned via the shape completion task, SC3D lacks the ability of detecting the object relations in the search space. Moreover, it has low efficiency in target proposal generation and cannot be trained in an end-to-end manner. The follow-up <ref type="bibr" target="#b39">[40]</ref> accelerates SC3D by leveraging the SiamRPN <ref type="bibr" target="#b16">[17]</ref> to generate target proposals from the bird eye views. However, its RPN only operates in 2D, and thus it cannot sense the 3D object relations. To address these, P2B <ref type="bibr" target="#b26">[27]</ref> adapts the SiamRPN <ref type="bibr" target="#b16">[17]</ref> to the 3D case where pure point clouds are processed. Firstly, the target object information is fused into the search space. Following that, an object detection network is applied to the search space to detect the target. Equipped with a state-of-the-art detection network, i.e. VoteNet <ref type="bibr" target="#b23">[24]</ref>, P2B achieves significant improvement in terms of both accuracy and efficiency. Very recently, Feng et al. <ref type="bibr" target="#b10">[11]</ref> presents a two-stage object re-track framework for 3D point clouds, which can re-track the lost object at the coarse stage. However, none of these methods exploit the BBox provided in the first frame as an additional cue. The information loss incurred by the incompleteness of shapes cannot be compensated, and their performance drops drastically when they deal with incomplete scans. 3D Multi-Object Tracking. Most of the 3D Multi-Object Tracking (MOT) methods follow the tracking-by-detection paradigm <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. Unlike SOT where the 3D BBox of the target is provided in the first frame, the MOT tracker determines the number of objects in all frames by running an independent detector <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref>. After that, the tracking is done by performing data association on the detection results (i.e. linking the detected object BBoxes across all frames to obtain full trajectories), according to the motion estimation. The motion of the objects can be estimated using the handcrafted Kalman filter <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref>, or some learned features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39]</ref>. The RPN component denotes the 3D RPN used in <ref type="bibr" target="#b26">[27]</ref>, which is exploited to generate the final 3D target proposals. Part (b) illustrates the workflow of BAFF module. It first uses the distance map of BoxClouds as the metric to find out the k-nearest neighbors in the template point cloud with respect to each search area point. Then, a Mini-PointNet is used to aggregate the retrieved neighbors' features.</p><p>Bounding Box Utilization in 3D. In the 3D SOT task, though the BBox of the object is provided in the first frame, the previous methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> only expand the BBox with a fixed magnification and crop out the point cloud inside as a template. In 3D detection, <ref type="bibr" target="#b29">[30]</ref> predicts intra-object part locations of all 3D points within a proposal, using the freeof-charge part supervisions derived from 3D ground-truth BBoxes. However, it only focuses on part-aware information while ignores the sizes of BBoxes. Moreover, it can only use BBoxes as supervision instead of the network input. In contrast, our method explicitly takes the given target BBox as input to enhance the target features with both partaware and size-aware cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>A 3D BBox is represented as (x, y, z, w, l, h, ?) ? R 7 , where (x, y, z) stands for the coordinate of the BBox center, (w, l, h) is the BBox size and ? is the heading angle (the rotation around the up-axis). A point cloud is represented as {p i } N i=1 , where each point p i is a vector of its (x, y, z) coordinate and N is the number of points.</p><p>In 3D single object tracking (SOT), the BBox of the target in the first frame is given to the tracker. The goal of the tracker is to locate the same target in the search area frame by frame. Notice that in the 3D case, any BBox is amodal (covering the entire object even if only part of it is visible). A template point cloud P t = {p t i } Nt i=1 is generated by cropping and centering the target in the first frame with the given BBox B t . For any search area</p><formula xml:id="formula_0">P s = {p s i } Ns i=1</formula><p>in other frames, previous trackers take in the pair (P t , P s ) and output the amodal BBox of the target in the search area.</p><p>Formally, previous trackers can be formulated as:</p><formula xml:id="formula_1">track : R Nt?3 ? R Ns?3 ? R 4 ,<label>(1)</label></formula><formula xml:id="formula_2">track(P t , P s ) ? (x, y, z, ?).<label>(2)</label></formula><p>Note that the output has only 4 elements since we do not need to re-predict the size of the 3D BBox, which is unchanged across all frames.</p><p>Since previous works compare the search area only with the template that is usually incomplete, they lack the ability to capture the size and part information of the target object. Therefore, they are prone to produce false target proposals when similar objects are present. In this work, we aim at designing a box-aware tracker track box-aware that exploits the amodal BBox of the target:</p><formula xml:id="formula_3">track box-aware : R 7 ? R Nt?3 ? R Ns?3 ? R 4 ,<label>(3)</label></formula><formula xml:id="formula_4">track box-aware (B t , P t , P s ) ? (x, y, z, ?).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Box-Aware Tracker (BAT)</head><p>Our box-aware tracker is built upon P2B <ref type="bibr" target="#b26">[27]</ref>. Following the siamese paradigm, P2B first generates a target-specific search area by executing a point-wise correlation between the template and the search features, which are extracted by a shared backbone. The correlation is based on the pointwise similarity and utilizes PointNet <ref type="bibr" target="#b24">[25]</ref> to achieve permutation invariance. Then it applies a VoteNet <ref type="bibr" target="#b23">[24]</ref> based RPN to generate target proposals. Specifically, a voting module votes the seed points to potential target centers. Besides, a seed-wise targetness score is also predicted to regularize the feature learning. These potential target centers are then grouped into clusters using ball query <ref type="bibr" target="#b25">[26]</ref>, which are finally turned into target proposals through another PointNet. Each proposal is a 5D vector containing the coordinates of the target center (3D), the rotation in the horizontal plane (1D) and the targetness score (1D). The proposal with the highest targetness score is chosen as the final prediction.</p><p>Our work focus on improving the first part of P2B <ref type="bibr" target="#b26">[27]</ref> because generating a better target-specific search area is critical for robust tracking. The overall architecture of BAT is shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a). Given a template point clouds P t ? R N1?3 and a search area P s ? R N2?3 , a shared PointNet++ <ref type="bibr" target="#b25">[26]</ref> is utilized to extract features from both of them, resulting in F t ? R M1?D and F s ? R M2?D . Besides, the BoxCloud C t ? R M1?9 (Section 3.3) is obtained from the template and its BBox. The BoxCloud of search area C s is predicted by a multi-layer perceptron (MLP). After that, through BoxCloud comparison and feature aggregation sub-modules (Section 3.4), we obtain the targetspecific search areaF s ? R M2?D . After gettingF s , we yield the final target proposals using the same way as <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">BoxCloud Representation</head><p>Previous works only use the BBox given at the first frame to crop and center the target during the pre-processing, ignoring that it contains rich shape information about the target. For a BBox (x, y, z, w, l, h, ?) of an object, its (w, l, h) indicates the size of the full object, even if part of the object is occluded or truncated. Its (x, y, z, ?) indicates the object coordinate system. Knowing the size and the object coordinate system, we can infer the intra-object location of each point, i.e. the part information. Based on these observations, we design the BoxCloud representation to fully utilize such shape priors.</p><p>A BoxCloud is defined by the point-to-box relation between an object point cloud P and its BBox B. For each point p i in P , we first calculate its Euclidean distance to each of the eight corners and the center of B, resulting in nine distances. Then we arrange them in a predefined order with respect to the object coordinate system, generating a 9-D vector c i (as shown in <ref type="figure" target="#fig_2">Figure 3</ref>). We denote c i as the box coordinate of p i with respect to B. A BoxCloud is a set of box coordinates and can be formulated as follows:</p><formula xml:id="formula_5">C = {c i ? R 9 | c ij = ||p i ? q j || 2 , ?j ? [1, 9]} N i=1 ,<label>(5)</label></formula><p>where q j(j =9) is the j-th corner and q 9 is the center of B. Though simple, BoxCloud features own the following properties over the geometric point clouds (i.e. spatial xyz coordinates of point clouds): Size-Awareness. The full size of an object can be directly inferred from its BoxCloud regardless of shape incompleteness. This helps to distinguish similar object parts of incomplete scans from objects of different sizes. Part-Awareness. Each c i indicates the intra-object location of p i , because c i encodes the relative position of p i with respect to the corners and center. For example, if argmin j c ij = 1, which mean p i is closest to q 1 , we know p i belongs to the upper left part of the object head. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Box-Aware Feature Fusion</head><p>The goal of the Box-Aware Feature Fusion (BAFF) module is to generate an enhanced target-specific search area by augmenting the search area with the template, which enables us to locate the target in the search area using a 3D RPN. BAFF can also be regarded as performing a correlation on search area features using the template features as the kernel. BAFF mainly consists of the BoxCloud Comparison and the Feature Aggregation sub-modules, which are described in details as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">BoxCloud Comparison</head><p>BAFF depends on BoxCloud comparison to conduct a reliable comparison between the template and the search area. Considering the BoxCloud features of the search area are unknown during testing, we first utilize a point-wise MLP to predict the BoxClouds of objects in the search areas under the supervision of ground truth BBoxes.</p><p>Given the features of a search area</p><formula xml:id="formula_6">F s = {f s i ? R d } M i=1</formula><p>extracted by the backbone network, the MLP takes each feature point f s i as input, and outputs its 9-D box coordinate c s i . The predicted c s i is explicitly supervised by a Huber loss (i.e. smooth-L1 regression loss):</p><formula xml:id="formula_7">L bc = 1 i m i i ||c s i ?? s i || ? m i ,<label>(6)</label></formula><p>where {? s i } M i=1 ?? s are ground truth box coordinates from the BoxCloud of the search area pre-calculated before train-</p><formula xml:id="formula_8">ing. {m i } M i=1</formula><p>is a binary mask indicating whether the i-th point is inside an object BBox (m i = 1) or not (m i = 0).</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b), after obtaining the predicted BoxCloud C s of the search area, we can conduct BoxCloud comparison between the predicted C s of the search area and the BoxCloud C t of the template. The BoxCloud comparison is simply based on the pairwise l 2 distance:</p><formula xml:id="formula_9">Dist ? R M1?M2 = Pairwise(C t , C s ),<label>(7)</label></formula><p>where Pairwise(?, ?) denotes the pairwise l 2 distance between the two input BoxClouds. M 1 and M 2 are the numbers of points in C t and C s respectively. Dist denotes a M 1 by M 2 distance map whose entity Dist i,j represents the distance between c t i and c s j . Compared with the feature comparison using the extracted features <ref type="bibr" target="#b26">[27]</ref>, measuring the shape similarity using BoxCloud is not only more reliable and interpretable but also more efficient. This is because the dimension of a learned feature must be much higher than nine (the dimension of the BoxCloud point) to encode sufficient information. However, a higher dimension inevitably brings additional computational costs.</p><p>After that, BAT takes the resulting distance map as a guide to sift out the k most similar template points for each search point. The k-NN grouping is illustrated in <ref type="figure" target="#fig_1">Figure 2 (b)</ref>. BAT selects the top-k most similar template points for each search area point, resulting in an indices matrix where the i-th column contains the indices of the k nearest neighbors of the i-th search point in terms of BoxCloud distance. The k-NN grouping helps to discard false matchings and avoid noise interference from all template points, promoting the subsequent feature aggregation. Besides, using a smaller k helps to gain noticeable speed up, which provides an alternative option for performance and latency trade-off (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Feature Aggregation</head><p>Feature aggregation aims to fuse the top-k related template features into the search area. For the i-th point p s i in the search area, we use the i-th column in the indices matrix to select its k nearest template points. A mini-PointNet <ref type="bibr" target="#b24">[25]</ref> is then applied to aggregate these k pairs</p><formula xml:id="formula_10">{[p t j ; f t j ; c t j ; f s i ] ? R 2D+3+9 , ?j = 1, . . . , k}. Here p t j ? R 3 , f t j ? R D , c t j ? R 9</formula><p>denote the spatial coordinate, extracted feature and box coordinate of the j-th template point, respectively. f s i ? R D is the extracted feature of the corresponding search point. Formally, the mini-PointNet can be formulated as follows:</p><formula xml:id="formula_11">f s i = MaxPool({MLP([p t j ; f t j ; c t j ; f s i ])} k j=1 ),<label>(8)</label></formula><p>where MLP(?) is a multi-layer perceptron and MaxPool(?) is the max-pooling across points. Finally, the template features are effectively and reliably encoded in the newly acquired target-specific search areaF s = {f s i } M2 i=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation</head><p>Search Area Generation. In practice, the object movement between two consecutive frames is relatively small and hence there is no need to search the target in the whole frame. All we need is to search the target in the neighborhood of the previous object location. We follow this idea to generate the search areas for training and testing. During both training and testing, templates and their BBoxes are transformed to the object coordinate system before being sent to the model. Training. We generate training samples from any two consecutive frames at the time t ? 1 and t and use them to train BAT in an end-to-end manner. The template is generated by merging the point clouds inside the first given BBox and (t ? 1)-th target BBox. We randomly shift the (t)-th target BBox, enlarger it by 2 meters in each direction and collect the points inside to generate the search area. Our loss used for training the RPN is the same as that in <ref type="bibr" target="#b26">[27]</ref>, denoting it as L rpn , and the final loss for our model is L = L bc + ?L rpn . We used grid search to tune the ? and finally set it to 1. In fact, changing ? does not affect the performance too much. Our network is trained for 60 epochs using Adam optimizer with a batch size of 96. The learning rate of the network is initialized as 0.001 and is divided by 5 every 12 epochs.</p><p>Testing. During testing, we track a given target across all frames in a sequence. This is achieved by applying the trained BAT frame by frame. For the current frame, we update the template by merging the point clouds in the first given BBox and the previous predicted BBox. W e enlarge the previous predicted BBox by 2 meters and collect the points inside to form the new search area.</p><p>Architecture. For the model architecture, k is set as 4 in the BAFF module and the Mini-PointNet is constructed with a three-layer MLP, where the channel numbers of hidden layers are identically 256. We randomly sample N 1 = 512 points for each template point cloud P t and N 2 = 1024 points for each search area P s . They are then fed into a PointNet++ (as that in P2B <ref type="bibr" target="#b26">[27]</ref>) to obtain corresponding features F t ? R 128?256 and F s ? R 64?256 , which will be used for the subsequent processing. All experiments are conducted using NVIDIA RTX-2080 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we further present our experiment setting and superior results from different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>We conduct extensive experiments on two widelyadopted datasets (i.e. KITTI <ref type="bibr" target="#b11">[12]</ref> and NuScenes <ref type="bibr" target="#b4">[5]</ref>) to validate the effectiveness of the proposed BAT method. Both datasets contain point clouds scanned by LiDAR sensors. Metrics. For evaluation metrics, we follow <ref type="bibr" target="#b12">[13]</ref> to use One Pass Evaluation (OPE) <ref type="bibr" target="#b33">[34]</ref> to measure Success and Precision. For a predicted BBox and ground-truth (GT) BBox, "Success" is defined using the AUC for intersection over union between them. "Precision" is defined as AUC for distance between two boxes' centers from 0 to 2m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-arts</head><p>We compare our method against SC3D <ref type="bibr" target="#b12">[13]</ref>, its followup <ref type="bibr" target="#b39">[40]</ref> and P2B <ref type="bibr" target="#b26">[27]</ref>. We do not include <ref type="bibr" target="#b10">[11]</ref>   <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> using their official open-source codes.</p><p>The left part of the table records the results on the KITTI dataset, where BAT shows a significant performance gain over existing methods, outperforming P2B by over 10% on average. For the pedestrian category, BAT even achieves about 25% improvement over P2B, which strongly demon- strates the effectiveness of our box-aware tracking pipeline. Moreover, for the van category with only 1248 instances, BAT still achieves satisfactory results, outperforming the other two methods by a large margin. This implies that the additional box information helps to reduce the demand for training data. For the category of cyclist, which has an extremely small amount of training samples, our results still out perform P2B notably. Furthermore, as shown in the right part of <ref type="table" target="#tab_0">Table 1</ref>, BAT is consistently superior to the other two competitors on four main classes in NuScenes, which is a more challenging dataset with more instances and object-interference. Robustness to Sparsity. In realistic applications, objects collected by LiDAR sensors are mostly sparse and incomplete. According to <ref type="bibr" target="#b26">[27]</ref>, about 34% of cars in KITTI hold fewer than 50 points. Therefore, the robustness against sparsity is an indispensable property for practical trackers. <ref type="figure" target="#fig_3">Figure 4</ref> showcases the impressive robustness of BAT. Our  <ref type="figure">Figure 6</ref>. Pairwise distances measured on extracted features and BoxClouds. Distances are calculated between each point in the search area and the blue point in the template. method achieves over 55% success rate even when dealing with targets holding less than 10 points. This result is 10% higher than P2B and 30% higher than SC3D. For targets with fewer than 30 points, BAT still holds a huge performance advance over the previous SOTA. Even the performance gap narrows as the number of points approaches 50, our BAT still outperforms P2B by over about 2% success rate. This figure verifies that our BAT works not only for sparse point clouds, but also for dense ones. We observe that the performance of all methods drops with &gt; 50 points, which is counter-intuitive. To clarify this, we show that the average length for sequences with &gt;50 points is much longer than that of sequences with ?50 points (86.3 vs 47.7), making the task much more difficult regardless of more points. The performance drop is mainly due to the accumulated errors when tracking on longer sequences. <ref type="figure" target="#fig_4">Figure 5</ref> visualizes BAT's advantage over P2B in extremely sparse scenarios, where BAT's predictions hold tight to the ground truth boxes when P2B tracks off course or even fails. BoxCloud Comparison v.s. Feature Comparison. Our proposed BoxCloud is a more effective representation com-pared with features extracted by neural networks. <ref type="figure">Figure 6</ref> shows a case using PointNet++ to extract the features of the template and search point clouds. It shows that the target points in the search area have relatively low similarities in this case, while the similarities calculated using BoxClouds are much more satisfying. This result strongly proves that BoxCloud is much more reliable for feature comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of points on the first frame's car</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, comprehensive experiments are conducted to validate the design of the BoxCloud and the BAFF module. Behavior analysis of the proposed method is presented as well. All ablated experiments are conducted on the car class of the KITTI dataset as P2B <ref type="bibr" target="#b26">[27]</ref>.</p><p>Our box-aware feature module is specially designed for BoxCloud processing. However, since the BoxCloud provides additional clue for incomplete shapes, directly feeding it to the network must help to improve the performance. To validate this hypothesis, we add an extra branch to the pipeline of P2B, constructing a simple extension called BAT-Vanilla. It predicts the BoxCloud of the search area points. The BoxCloud of the template is also concatenated to the spatial coordinates for the feature fusion, while the other parts of P2B remain unchanged. Results in the bottom part of <ref type="table" target="#tab_2">Table 2</ref> confirm that the BoxCloud does help to improve the tracking performance. Specifically, BAT-Vanilla defeats P2B by a noticeable margin. Besides, the last two rows of <ref type="table" target="#tab_2">Table 2</ref> prove the effectiveness of our BAFF module, implying that the BoxCloud comparison and kNNgrouping techniques help to further exploit the useful clues encoded in the BoxClouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Analysis on Box-Aware Feature Fusion (BAFF)</head><p>The Choice of k. The value of k is a hyper-parameter of BAFF. <ref type="figure" target="#fig_5">Figure 7</ref> shows that a smaller k (i.e. k = 2) helps to speed up our model with little negative impact on the performance. Furthermore, setting a larger k does not improve the performance, which implies that the k-NN grouping in BAFF helps to reject false matching points in the template. With the default setting k = 4, BAT achieves the best performance with a satisfactory high speedup, about 20% faster than P2B. Also, it should be noted that BAT is stably superior to P2B under different k values in terms of both performance and speed. Even with k = 64 (i.e. using all the template points same with P2B), it is still about 10% faster than P2B. Note that here we only consider the time of model forwarding with batch size 96 when calculating the speedup. We do not consider the pre-and post-processing time because these operations are not optimized on GPUs.</p><p>During the inference where sequences are processed frame by frame, our BAT processes one frame with only 17.5ms on average, achieving 57 FPS. The Choice of Feature Aggregation. In Eqn. (8) of BAT, the feature we send for aggregation is [p t j ; f t j ; c t j ; f s i ]. In the upper part of <ref type="table" target="#tab_2">Table 2</ref>, other settings for the feature aggregation are listed. Firstly, without concatenating the template BoxCloud, our model suffers from a great performance decrease since it does not introduce useful clues to enhance the feature fusion. Furthermore, we observe that adding additional information does not bring any improvement. Specifically, the results of adding the predicted BoxCloud of the search area, pairwise distances in Eqn. <ref type="bibr" target="#b6">(7)</ref> and coordinates of the search point cloud all show drastic drops. A possible explanation for this is that such features bring vast redundancies, which may confuse the feature fusion. Effectiveness of BoxCloud Comparison. To further illustrate the effectiveness of the BoxCloud comparison, two settings are used to validate as follows: 1) BAT without Box-Cloud Comparison: We replace BoxCloud with feature (i.e. the feature extracted by PointNet++) to compute the distance map. 2) P2B with BoxCloud Comparison: Keeping the BoxCloud comparison, we follow the way in P2B <ref type="bibr" target="#b26">[27]</ref> for subsequent feature aggregation. As shown in the middle </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we enhance the SOT on LiDAR point clouds by fully exploiting the free bounding-box information. Two main components of our work, the BoxCloud representation and the box-aware feature fusion module, are designed specially for effective object representation and box-aware target-specific search area generation. Each of them independently improves the tracking performance while benefits from each other when they are applied jointly. We extend P2B with the two proposed components, constructing BAT. With such small modifications over P2B, our BAT outperforms the current state-of-the-art methods by a large margin, especially on sparser object tracking. We believe that BoxCloud provides a flexible and powerful tool for future work to enhance their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Analysis Experiments</head><p>Other BoxCloud Design. Our BoxCloud depicts the pointto-box relation using Euclidean distance. An alternative design is replacing the Euclidean distance with the pointto-point offset. We denote BoxCloud in this form as</p><formula xml:id="formula_12">? B ? R N ?27 ,</formula><p>where each row ofB is a concatenation of 9 threedimensional offsets. Compared to our original design, ? B is three times larger but contains almost the same amount of information as ours. As shown in <ref type="table" target="#tab_0">Table 1</ref>, using ? C incurs a performance decline, which implies that the extra dimension of ? C puts a burden on the training of the network. In contrast, the proposed BoxCloud is more compact and effective. Performance on Long/Short Term Tracking. We further follow different schemas to generate search areas. 1) To test the long-term tracking performance, we generate all the search areas based on previous results predicted by the models. In this setup, the trackers' ability to handle error accumulation and recover from failure is assessed. 2) For short-term performance evaluation, we use the ground-truth location of the target in the previous frame to generate the next search area. In this case, the tracker does not have to handle the error introduced by its last prediction and only need to focus on the "on time tracking" task. The results of the car category for all the competitors are shown in <ref type="table" target="#tab_2">Table 2</ref>. Overall, our BAT outperforms P2B and SC3D in both setups. In particular, BAT shows more notable superiority in long term setup. This implies that the performance of our method is more stable and robust across time, while the other two methods are more likely to suffer from tracking failure. In the realistic tracking scenario, it is impossible to obtain the "previous ground-truth". Hence, it is more proper to use long-term performance to evaluate a practical tracking system. Template Generation Strategy. During the testing, our default setting for template generation is to merge the target in the first frame (the ground truth) with the previous result predicted by the network. For consistent comparison, we further test our method under another three template generation strategies, which uses "the first ground-truth", "the previous result", and "all previous results" respectively to generate the template. The results are listed in <ref type="table" target="#tab_5">Table 3</ref>. BAT maintains notable advantages regardless of any strategies. It is worth mentioning that our BAT defeats P2B by the largest margin (?7%) under the "previous result" strategy. This also shows that our long-term performance is much better and robust than that of P2B. 0~0.05 &gt;0.08 0.05~0.08 <ref type="figure" target="#fig_0">Figure 1</ref>. Visualization of BoxCloud predictions. 4 car cases with different sparsity are presented. Points are colored according to its corresponding BoxCloud predictions MSE errors. The greens are points with MSE errors less than 0.05; The blues are points with MSE errors in the range between 0.05 and 0.08; while the reds denotes points with MSE errors higher than 0.08. It is obvious that most predictions are with small MSE errors. tracked identity is changed in a specific frame (the tracker infer the error relation), we will stop the further tracking.</p><p>We compare the two most representative methods in 3D MOT, AB3DMOT <ref type="bibr" target="#b31">[32]</ref> and PC3T <ref type="bibr" target="#b32">[33]</ref>, where they rank 25 and 4 in the KITTI MOT leaderboard in the car category, respectively. As shown in <ref type="table" target="#tab_6">Table 4</ref>, BAT significantly performs better than state-of-the-art 3D MOT methods. Since multiple objects need to be tracked, they cannot use targetspecific feature augmentation to enhance the template representation. Moreover, since they use detection to obtain all objects in the scene, their speed is much slower than ours, where both two MOT methods cannot achieve realtime speed. Visualization of BoxCloud Learning. BAT is trained to predict the BoxClouds of the points in search areas. In this part, we compare the BoxClouds predicted by our trained BAT with the ground-truths. We use the Mean Square Error  (MSE) as the metric to evaluate the performance of Box-Cloud learning. For each predicted BoxCloud point, we calculate the MSE between the corresponding ground-truth and the predicted BoxCloud point (only the target points are considered). <ref type="figure" target="#fig_1">Figure 2</ref> shows the distribution of MSEs of all the predictions in our KIITI test split. Most of the MSEs are less than 0.1, which implies a high accuracy of the Box-Cloud prediction.</p><p>We further visualize several cases of BoxCloud predictions in <ref type="figure" target="#fig_0">Figure 1</ref>. As shown in figures, our BAT can generally obtain accurate BoxCloud predictions. Some biases may occur in the edges between objects and backgrounds. This is because our training strategy only supervises the BoxCloud of the object rather than the whole search area. Nevertheless, such slight prediction biases have little impact on our BoxCloud comparison and the following tracking process, since they are filtered out by the k-NN grouping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture of BAT-Vanilla</head><p>The <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the architecture of BAT-Vanilla, which directly adds BoxCloud to P2B. After the feature extraction using a shared backbone, a shared MLP (256,256,9) (with layer output sizes 256, 256, 9) is applied to the extracted search area features F s for BoxCloud prediction. By adding this branch, the F s is supervised to be box-aware. During the feature augmentation stage, the BoxCloud of the template is simply concatenated together with its coordinates and the extracted features (as shown in <ref type="figure" target="#fig_3">Figure 4</ref>). The feature augmentation in BAT-Vanilla is almost the same with that in P2B, but introduces additional BoxCloud features as priors.</p><p>The right part of <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the workflow of the RPN, which is used in both BAT and BAT-Vanilla. A pointwise MLP (256, 256, 3+256) is applied to the target-specific search area features (only the sampled seed points) for object center voting. The MLP takes the per-point feature as input and outputs its offset to the corresponding object cen- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motivation behind BAT. Partial scans of two different cars (i.e. c1, c2) collected by a LiDAR sensor both have highly similar shapes with the target template (1st row), though their 3D BBoxes are quite different in size (2nd row). With explicit consideration of object bounding-boxes, our method can elegantly overcome such ambiguities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Box-Aware Tracker (BAT) pipeline and Box-Aware Feature Fusion (BAFF) module. Part (a) illustrates the pipeline of BAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>BoxCloud representation. BoxCloud depicts the distances between object points and box points (i.e. corners and the center of a 3D BBox). Box points are arranged in a predefined order. The figure shows the box coordinate of an object point pi. The object and its BBox are transformed to the object coordinate system with x-axis pointing to the left, y-axis pointing to the front and z-axis pointing upward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Robustness test. Methods are evaluated on sequences split by the number of points in the first frame's car. We use the average Success for each interval (horizontal axis) as the evaluation metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Advantageous cases of our BAT compared with P2B. We can observe BAT's advantage over P2B in extremely sparse scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Performance for different k values. The left y-axis shows success and precision, while the right illustrates the speedup with respect to P2B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .</head><label>2</label><figDesc>Distribution of MSEs for BoxCloud prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison among our BAT and the state-of-the-art methods on the KITTI (left) and NuScenes (right) dataset, where the instance number of each category is shown under category names. Bold denotes the best performance.</figDesc><table><row><cell></cell><cell>Approach</cell><cell></cell><cell></cell><cell>KITTI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NuScenes</cell></row><row><cell></cell><cell>Category</cell><cell>Car</cell><cell>Pedestrian</cell><cell>Van</cell><cell cols="2">Cyclist Mean</cell><cell>Car</cell><cell cols="2">Truck Trailer</cell><cell>Bus</cell><cell>Mean</cell></row><row><cell></cell><cell>Frame Number</cell><cell>6424</cell><cell>6088</cell><cell>1248</cell><cell>308</cell><cell cols="3">14068 64159 13587</cell><cell>3352</cell><cell>2953 84051</cell></row><row><cell></cell><cell>SC3D [13]</cell><cell>41.3</cell><cell>18.2</cell><cell>40.4</cell><cell>41.5</cell><cell>31.2</cell><cell>22.31</cell><cell>30.67</cell><cell>35.28</cell><cell>29.35 24.43</cell></row><row><cell>Success</cell><cell cols="2">SC3D-RPN [40] 36.3 P2B [27] 56.2</cell><cell>17.9 28.7</cell><cell>-40.8</cell><cell>43.2 32.1</cell><cell>-42.4</cell><cell>-38.81</cell><cell>-42.95</cell><cell>-48.96</cell><cell>-32.95 39.68 -</cell></row><row><cell></cell><cell>BAT(Ours)</cell><cell>65.4</cell><cell>45.7</cell><cell>52.4</cell><cell>33.7</cell><cell>55.0</cell><cell>40.73</cell><cell>45.34</cell><cell>52.59</cell><cell>35.44 41.76</cell></row><row><cell></cell><cell>SC3D [13]</cell><cell>57.9</cell><cell>37.8</cell><cell>47.0</cell><cell>70.4</cell><cell>48.5</cell><cell>21.93</cell><cell>27.73</cell><cell>28.12</cell><cell>24.08 23.19</cell></row><row><cell>Precision</cell><cell cols="2">SC3D-RPN [40] 51.0 P2B [27] 72.8</cell><cell>47.8 49.6</cell><cell>-48.4</cell><cell>81.2 44.7</cell><cell>-60.0</cell><cell>-43.18</cell><cell>-41.59</cell><cell>-40.05</cell><cell>-27.41 42.24 -</cell></row><row><cell></cell><cell>BAT (Ours)</cell><cell>78.9</cell><cell>74.5</cell><cell>67.0</cell><cell>45.4</cell><cell>75.2</cell><cell>43.29</cell><cell>42.58</cell><cell>44.89</cell><cell>28.01 42.70</cell></row></table><note>Datasets. For KITTI dataset, it contains 21 outdoor scenes and 8 types of targets. The NuScenes dataset is more chal- lenging, containing 1000 driving scenes across 23 object classes with annotated 3D BBoxes. Furthermore, there are much more objects in a scene, and thus tracking on NuScenes dataset is more challenging since the target is eas- ily submerged by other objects. For KITTI dataset, we fol- low [13, 27] to setup the traninig/valid/test splits for a fair comparison. For NuScenes, we use the train track split of its training set for training and test on its validation set. We only consider the key frames during both training and test- ing. Tracklets where the first BBoxes contain no point are not considered during the evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in our main table because it does not provide open source codes and only has published results on car category of KITTI, which are 58.4/73.4 in terms of Success/Precision. Performance across Categories. Table 1 summarizes the results in the KITTI and NuScenes dataset. For the KITTI dataset, we report the published results from corresponding papers. For the NuScenes dataset, since there are no published results, we evaluate</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of different ablations. The upper part shows different concatenation strategies for the feature aggregation submodule (FA) and the middle part illustrates the effectiveness of the BoxCloud Comparison. As for the lower part, it shows our baseline (P2B), BAT-Vanilla (in Section 4.3) and the best results.</figDesc><table><row><cell>method</cell><cell cols="2">Success Precision</cell></row><row><cell>FA without C t</cell><cell>58.9</cell><cell>75.3</cell></row><row><cell>FA with additional C s</cell><cell>56.7</cell><cell>74.7</cell></row><row><cell>FA with additional Dist</cell><cell>58.4</cell><cell>75.4</cell></row><row><cell>FA with additional P s</cell><cell>56.7</cell><cell>73.4</cell></row><row><cell cols="2">BAT without BoxCloud Comparison 57.1</cell><cell>72.9</cell></row><row><cell>P2B with BoxCloud Comparison</cell><cell>58.3</cell><cell>74.1</cell></row><row><cell>P2B (Baseline)</cell><cell>56.2</cell><cell>72.8</cell></row><row><cell>BAT-Vanilla (P2B + BoxCloud)</cell><cell>59.0</cell><cell>74.6</cell></row><row><cell>BAT (Ours)</cell><cell>60.5</cell><cell>77.7</cell></row><row><cell cols="3">part of Table 2, when using features of PointNet++ in com-</cell></row><row><cell cols="3">parison, the performance of BAT drops significantly, which</cell></row><row><cell cols="3">demonstrates that the BoxCloud Comparison is the key of</cell></row><row><cell cols="3">BAFF. While the result of P2B with BoxCloud Comparison</cell></row><row><cell cols="3">implies that a different feature aggregation styles can also</cell></row><row><cell cols="3">benefit from the BoxCloud comparison. The overall results</cell></row><row><cell cols="3">show that the BoxCloud comparison and the BAFF module</cell></row><row><cell>can benefit from each others.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Ablation on BoxCloud Designs.</figDesc><table><row><cell cols="3">BoxCloud Designs Success Precision</cell></row><row><cell>?</cell><cell></cell><cell></cell></row><row><cell>C (offset)</cell><cell>58.8</cell><cell>75.3</cell></row><row><cell>C (original)</cell><cell>60.5</cell><cell>77.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison on long/short term tracking performance for car. The right two columns differ in their ways to generate search area. Bold denotes the best performance.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Long Term Short Term</cell></row><row><cell></cell><cell>SC3D [13]</cell><cell>41.3</cell><cell>64.6</cell></row><row><cell>Success</cell><cell>P2B [27]</cell><cell>56.2</cell><cell>82.4</cell></row><row><cell></cell><cell>BAT(Ours)</cell><cell>60.5</cell><cell>83.5</cell></row><row><cell></cell><cell>SC3D [13]</cell><cell>57.9</cell><cell>74.5</cell></row><row><cell>Precision</cell><cell>P2B [27]</cell><cell>72.8</cell><cell>90.1</cell></row><row><cell></cell><cell>BAT(Ours)</cell><cell>77.7</cell><cell>90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Different ways for template generation. Methods are compared on the Car category. "First &amp; Previous" denotes "The first GT and Previous result". Bold denotes the best performance, and underline shows our default setting. All previous results 55.8 51.4 41.3 71.4 66.8 57.9</figDesc><table><row><cell>Source of template</cell><cell>Success BAT P2B SC3D BAT P2B SC3D Precision</cell></row><row><cell>The First GT</cell><cell>51.8 46.7 31.6 65.5 59.7 44.4</cell></row><row><cell>Previous result</cell><cell>59.2 53.1 25.7 75.6 68.9 35.1</cell></row><row><cell>First &amp; Previous</cell><cell>60.5 56.2 34.9 77.7 72.8 49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Comparison with MOT methods on KITTI dataset. MOT) methods, we evaluate 3D MOT methods with 3D SOT metrics. Since multiple objects need to be tracked in 3D MOT, we first find out the corresponding relation between the multiple objects with the single one. Specifically, for each object in SOT, we search its nearest neighbor in all objects of MOT. Hence, each MOT task can be transferred into several SOT tasks, and the metrics of SOT can play a normal role. For each object in MOT, if its</figDesc><table><row><cell>Class</cell><cell>Method</cell><cell>AB3D [32] PC3T [33]</cell><cell>BAT</cell></row><row><cell>Car</cell><cell cols="3">Succ/Prec 37.5 / 42.3 51.9 / 59.2 60.5 / 77.7</cell></row><row><cell>Ped.</cell><cell cols="3">Succ/Prec 17.6 / 27.3 23.6 / 34.1 42.1 / 70.1</cell></row><row><cell cols="4">Comparison with MOT Approaches. To illustrate the su-</cell></row><row><cell cols="4">periority of single object tracking (SOT) upon 3D multi-</cell></row><row><cell cols="2">object tracking (</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this supplementary material, we provide more analysis experiments in Section B. Then we describe the architecture of BAT-Vanilla in Section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster of potential target centers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D target proposal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal-wise targetness score</head><p>Verification with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Target Proposal and Verification</head><p>Newly proposed</p><p>Origin in P2B BoxCloud Point features <ref type="figure">Figure 3</ref>. The overall pipeline of vanilla Box-Aware Tracker (BAT-Vanilla). The left part is the box-aware feature fusion, which augments the search area with template information. The right part is a VoteNet-based RPN which generates final target proposals from the target-specific search area. For the i-th proposal, s p i is its targetness score and p t i is its (x, y, z, ?).  ter. Besides the coordinate offset (3D), the MLP also predicts a feature offset for each point. But only the coordinate offsets are supervised with the ground-truths. In addition to the voting MLP, another MLP (256, 256, 1) is used to predict a targetness score for each search area seed. After that, the predicted vote centers and targetness scores are concatenated together and then clustered into k groups through the furthest point sampling and the ball query. Finally, a mini-PointNet is used to produce the final target proposal of each group.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d part-based sparse tracker with automatic synchronization and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1439" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probabilistic 3d multi-object tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu-Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Prioletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bohg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust model-based tracking for robot vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Andrew I Comport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaumette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="692" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive spatiallyregularized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4670" to="4679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A novel object re-track framework for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging shape completion for 3d siamese tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Zarzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How to make an rgbd tracker?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="148" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object tracking by reconstruction with view-specific discriminative correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1339" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-aware threedimensional mean-shift with occlusion handling for robust object tracking in rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Ping</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="664" to="677" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">D3s-a discriminative single shot segmentation tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7133" to="7142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human motion tracking of mobile robot with kinect 3d sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Machida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meifen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Murao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Proceedings of SICE Annual Conference (SICE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Robotics Automation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9552" to="9557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust 3d tracking of unknown objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Bergstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Robotics Automation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2410" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">P2b: Point-to-box network for 3d object tracking in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pvrcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d multi-object tracking in point clouds based on prediction confidence-guided data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint group feature selection and discriminative filter learning for robust visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7950" to="7960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12549" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno>2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficient bird eye view proposals for 3d siamese tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Zarzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10168</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust multimodality multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2365" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4591" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
