<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-to-many Approach for Improving Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sieun</forename><forename type="middle">Park</forename><surname>Goldsmiths</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of London London</orgName>
								<address>
									<postCode>SE14 6NW</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Paul Math School Chung-cheong bukdo</orgName>
								<address>
									<postCode>28054</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One-to-many Approach for Improving Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been discussions on the ill-posed nature of super-resolution that multiple possible reconstructions exist for a given low-resolution image. Using normalizing flows, SRflow[23] achieves state-of-the-art perceptual quality by learning the distribution of the output instead of a deterministic output to one estimate. In this paper, we adapt the concepts of SRFlow to improve GAN-based superresolution by properly implementing the one-to-many property. We modify the generator to estimate a distribution as a mapping from random noise. We improve the content loss that hampers the perceptual training objectives. We also propose additional training techniques to further enhance the perceptual quality of generated images. Using our proposed methods, we were able to improve the performance of ESRGAN[1] in ?4 perceptual SR and achieve the state-of-the-art LPIPS score in ?16 perceptual extreme SR by applying our methods to RFB-ESRGAN[21].</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Super-resolution is the task of recovering a high-resolution (HR) image from a low-resolution (LR) image. Recent works have achieved significant performance in SR using deep convolutional neural network (CNN) based approaches. Some of them exploit strict content loss as the training objective for super-resolution and propose various network architectures to improve the PSNR score. However, these methods often result in overly smooth images and have poor perceptual quality <ref type="bibr" target="#b5">[6]</ref>. Another branch of works focuses on improving perceptual quality with perceptual training methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. These methods employ generative adversarial networks (GAN) and perceptual loss functions to drive the network's output towards the natural image manifold of possible HR images. We assess and further improve the perceptual quality of these works.</p><p>Because super-resolution is a one-to-many problem with multiple possible reconstructions for one image, methods based on strict content loss often lead to predicting the average of possible reconstructions <ref type="bibr" target="#b5">[6]</ref>. Perceptual-driven solutions utilize perceptual and adversarial loss, which both do not penalize the generator for generating equally realistic images with stochastic variance. Recently, there has been discussions on the ill-posed nature of SR that multiple possible reconstructions exist for a given low-resolution image. Specifically, <ref type="bibr" target="#b23">[23]</ref> learns the distribution of the output instead of a deterministic output using normalizing flows. We believe the current GAN-based solutions for super-resolution is missing key components of a one-to-many pipeline.</p><p>Specifically, we discover two incomplete aspects in the current perceptual SR pipeline. First, although perceptual objectives do not penalize stochastic variation, the final loss is mixed with the strict content loss which strictly penalizes these variations. Second, the generator doesn't have the ability to generate multiple estimates of the image despite a one-to-many problem. To improve the ESRGAN <ref type="bibr" target="#b0">[1]</ref> pipeline into a one-to-many pipeline, we provide the generator with pixel-wise noise. The generator estimates the output distribution as a mapping from the random distribution. We also improve the content loss so it doesn't restrict the variation in the image while ensuring the consistency of the content. Our loss is designed so that learning the reconstruction of photo-realistic details are learned on GAN-oriented training objectives while a weak content loss ensures the reconstruction to be consistent with the input image.</p><p>The key contributions of our work can be described as follows:</p><p>? We propose a weaker content loss that does not penalize generating high-frequency detail and stochastic variation in the image.</p><p>? We enable the generator to generate diverse outputs by adding scaled pixel-wise noise after each RRDB block.</p><p>? We filter blurry regions in the training data using Laplacian activation <ref type="bibr" target="#b9">[10]</ref>.</p><p>? We additionally provide the LR image to the discriminator to give better gradient feedback to the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since the pioneering work of SRCNN <ref type="bibr" target="#b8">[9]</ref>, many works have exploited the pixel-wise loss and PSNRoriented training objectives to learn the end-to-end mapping from LR to HR images. We denote such pixel-wise losses as the strict content loss. Many network architectures and techniques were experimented with to improve the complexity of such networks. Deeper network architectures <ref type="bibr" target="#b17">[17]</ref>, residual networks <ref type="bibr" target="#b5">[6]</ref>, channel attention <ref type="bibr" target="#b18">[18]</ref>, and techniques to remove batch normalization <ref type="bibr" target="#b19">[19]</ref> were introduced. Although these works achieved state-of-the-art SR performance in the peak signal-tonoise ratio (PSNR) metric, they often produce overly smooth images.</p><p>To improve the perceptual image quality of SR, SRGAN <ref type="bibr" target="#b5">[6]</ref> proposes perceptual loss and GAN-based training. The perceptual loss is measured using intermediate activations of the VGG-19 network and a discriminator is used for the adversarial training process. Enhanced SRGAN (ESRGAN) further improves SRGAN by modifying the generator architecture with Residual in Residual Dense Block (RRDB), the Relativistic GAN <ref type="bibr" target="#b16">[16]</ref> loss, and improving the perceptual loss. Such methods were superior to PSNR-oriented methods at generating photo-realistic SR images with sharp details, achieving high perceptual scores. However, we could still often find unpleasant artifacts and problematic artifacts in the reconstructions of ESRGAN. Such cases are exemplified in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>The ill-posed nature of the super-resolution problem has also been addressed in other works e.g. SRFlow <ref type="bibr" target="#b23">[23]</ref>. SRFlow approaches super-resolution based on normalizing flows to explicitly learn the conditional distribution of the SR output given the LR input point. Normalizing flows are a method for constructing complex distributions that also shows significance in image generation. Normalizing flows explicitly learn the data distribution by transforming a simple distribution with a sequence of invertible transformation functions. Normalizing flows explicitly learn the distribution using the change of variables theorem, allowing it to optimize using only the negative log likelihood loss, unlike GAN and VAE.</p><p>Traditional metrics for assessing image quality such as PSNR and SSIM (Structural Similarity Index Measure) fail to coincide with human perception <ref type="bibr" target="#b3">[4]</ref>. The PSNR score is calculated based on the pixel-wise MSE, so methods that minimize pixel-wise differences tend to achieve high PSNR scores <ref type="bibr" target="#b8">[9]</ref>. However, the PSNR-oriented solutions fail at generating high-frequency details and often drive the reconstruction towards the average of possible solutions, producing overly smooth images <ref type="bibr" target="#b5">[6]</ref>. The learned perceptual image patch similarity (LPIPS) score <ref type="bibr" target="#b3">[4]</ref> was proposed to measure the perceptual quality on various computer vision tasks. According to <ref type="bibr" target="#b1">[2]</ref>, the LPIPS score reliably coincides with human perception for assessing super-resolved images. We use the LPIPS score as an indicator of perceptual image quality in our experiments.</p><p>CycleGAN <ref type="bibr" target="#b7">[8]</ref> is a pipeline for image-to-image translation with unpaired images using generative adversarial nets and cycle loss. CycleGAN consists of 2 generators G 1 , G 2 , and 2 discriminators D 1 , D 2 , where G 1 and G 2 each translate the input image in a cycling manner. The generators are trained to minimize the adversarial loss and cycle loss ||G 2 (G 1 (x)) ? x|| 1 between the input image and cycled image. We were able to design a loss based on the cycle loss to reliably measure the content consistency without such a complicated design. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We design a one-to-many approach for perceptual super-resolution by modifying the generator and the training objective. We also describe additional modifications to the training process and discriminator to improve the perceptual quality of SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cycle consistency loss</head><p>Most works on perceptual super-resolution <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> combine the content loss, adversarial loss (GAN loss), and perceptual loss for the training objective as in Equation 1. Although the strict content loss and adversarial loss are fundamentally disagreeing objectives, relying exclusively on either loss each has significant issues. The strict content loss guides the network output to be exactly consistent with the HR image, guiding the network to learn the mean of possible reconstructions and thus tends to give overly-smoothed results. Although the GAN framework is a powerful method for photo-realistic image generation, adversarial learning is highly unstable, and while the adversarial loss and perceptual loss guide the network to be perceptually convincing, they do not enforce the content of the super-resolved image to be consistent with the low-resolution image.</p><formula xml:id="formula_0">L T otal = L percep + ?L GAN + ?L 1<label>(1)</label></formula><p>We regard simply trading off these disagreeing losses as an incomplete objective for super-resolution since the mixing of such losses will obstruct the optimization of either loss. An improved training objective must be GAN-oriented while ensuring consistent content of the image. That is, there needs a content loss that doesn't hamper the generation of images with high-frequency details.</p><p>We propose a soft content loss inspired by the cycle loss of CycleGAN <ref type="bibr" target="#b7">[8]</ref> to ensure the output of the generator to be consistent with the low-resolution image while not disturbing the generation of high-frequency information.</p><p>We view the super-resolution problem as an image-to-image translation task between the LR and HR image space and apply the CycleGAN framework. To simplify the problem, we exploit our prior knowledge on G 2 : HR? &gt; LR. We can denote the downsampling operation as f and set G 2 to be f instead of learning it. Consequently, our pipeline doesn't require learning D 2 which is a tool for learning G 2 . This leaves only G 1 and D 1 to be learned. We can write the cycle consistency loss as Equation 2. This loss won't penalize generating high-frequency details in any way while the SR image remains consistent with the LR image. Finally, we can conclude our generator loss as Equation <ref type="bibr" target="#b2">3</ref>. Intuitively, we train the generator to generate high-frequency details with the GAN and perceptual loss while the new content loss simply ensures the reconstructed image to be consistent with the original image.</p><formula xml:id="formula_1">L cyc (G 1 ) = ||f (G 1 (LR)) ? LR|| 1 (2) L T otal (G 1 ) = L cyc (G 1 ) + ?L GAN (G 1 , D 1 ) + ?L percep<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Gaussian noise to the generator</head><p>For the generator to be capable of generating more than one solution given a single image, it must receive and apply random information. The variation between super-resolved images will mostly be stochastic variation in high-frequency textures. StyleGAN <ref type="bibr" target="#b2">[3]</ref> achieves stochastic variation in images by adding pixel-wise Gaussian noise to the output of each layer in the generator. We adopt this method and add the noise after every RRDB layer in the generator.</p><p>However, this introduces new hyperparameters with regard to the magnitude of the noise. We also observe that the sensitivity and the desired magnitude of noise differs among layers and channels. Adding the same noise directly after every layer could rather harm the ability of the generator. For example, a channel that detects edges would be seriously harmed by the noise. To mitigate such possible issues, we allow each channel to adaptively learn the desired magnitude of the noise. Specifically, we multiply the noise with a channel-wise scaling factor before adding the noise to the output of each layer. The scaling factor is learned concurrently with the network parameters. The noise is not applied at evaluation.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, We observe that the desired magnitude actually differs along the network depth and between channels. The early and final layers seem to have a lower noise tolerance. We suspect this is because early layers focus on extracting the feature of the image and the final layers preferred the noise to be reduced before being applied to the final reconstruction. There was also differences of the magnitude between channels as shown in the boxplot. This suggests that our method of scaling the noise adaptive effectively provides random information for generating multiple realistic reconstructions for super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reference image for the discriminator</head><p>Traditionally, the discriminator network receives a single image and is trained to classify whether the given image is real or a generated image. This setting will provide the generator with gradients to "any natural image" instead of towards the corresponding HR image. In an extreme example, the traditional discriminator won't penalize the generator for generating completely different but equally realistic images from an LR image. Although this is unlikely due to the existence of other content and perceptual losses, the gradient feedback given by the discriminator is sub-optimal for the task of super-resolution.</p><p>As a solution, we provide the low-resolution image as a reference along with the target image to the discriminator. This enables the discriminator to learn more important features for discriminating the generated image and provide better gradient feedback according to the LR image. For details, refer to <ref type="figure" target="#fig_0">Figure 1</ref>. We upsample the LR image to the same size as the HR image and concatenate them, feeding a tensor of shape (H, W, 6) to the discriminator. Despite its simplicity, conditioning the discriminator on the input is a crucial modification for training such a supervised problem with GAN-oriented losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Blur detection</head><p>We recognized that there are often severely blurry regions in the images from the DIV2K <ref type="bibr" target="#b14">[14]</ref> and DIV8K <ref type="bibr" target="#b15">[15]</ref> datasets. Although the authors of <ref type="bibr" target="#b15">[15]</ref> argue that the data was collected by "paying special attention to image quality", there were many scenes with out-of-focus backgrounds. These blurry regions might plague the generator to learn to generate such blurry patches. Blurry backgrounds are often indistinguishable from finer objects based only on the LR image. Though some might argue that the blurry backgrounds must also be learned, we were able to achieve finer detail and higher LPIPS score by detecting and removing blurry patches from both datasets.</p><p>We propose to detect and remove blurry patches before the network is trained on those patches. There are various methods for blur detection e.g. algorithmic methods and deep-learning-based approaches <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. However, most deep-learning-based works focus on predicting pixel-wise blur maps of the image, which wouldn't be suited for our needs. Mostly, the algorithmic method of <ref type="bibr" target="#b9">[10]</ref> was successful at reliably detecting blurry patches as can be observed in <ref type="figure" target="#fig_2">Figure 3</ref>. We measure the variance of the Laplacian activation of the patch and consider patches with variance of under 100 as blurry patches. The algorithm detects 28.8% blurry patches in a sample of 16,000 randomly cropped patches of size 96?96 from the DIV2K dataset and 48.9% of patches in a sample of 140,000 patches from the DIV8K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments to evaluate the effectiveness of our proposed techniques in ?4 and ?16 resolution and compare them with the baseline ESRGAN. We first experiment the effects of blur detection, then we perform an ablation study of our proposed training methods to evaluate their effectiveness. Implementation detail and training logs can be found on GitHub 1 . All our experiments were performed on a single Tesla T4 or Tesla K80 GPU.</p><p>We observed that a large portion of the training was used for loading high-resolution images, despite most of the images not being used. As an implementation detail to improve training speed significantly, we extract multiple patches and save them in a buffer while training instead of extracting only a single patch after loading the image. We randomly pick images from the buffer for training and discard the selected patches from the buffer. In all of our experiments, we extract 128 patches from each image and create a buffer of 1024 patches. We employ the ESRGAN network architecture with 23 RRDB blocks and most of its training configurations for the baseline of our experiments on ?4 super-resolution. The training process is divided into two stages. We first pretrain the PSNR-oriented models then train the ESRGAN-based models.</p><p>The PSNR-oriented models are trained with the L1 loss with a batch size of 16 for 500K iterations. We apply learning rate decay with an initial learning rate of 2 ? 10 ?4 , decayed by a factor of 2 every 200k iterations. We initialize the GAN-based model with the PSNR-oriented model. We initialize the learning rate with 1 ? 10 ?4 for both G 1 and D 1 , decaying the learning rate by a factor of 2 at [50k, 100k, 200k, 300k] iterations. For optimization, we use the Adam optimizer for both pretrained networks and GAN-based models, with ? 1 = 0.9 and ? 2 = 0.99. The learning rate decay schedule corresponds to the one proposed by ESRGAN. We implement our models and methods with the Tensorflow framework. The loss function is scaled with ? = 10 and ? = 5 ? 10 ?3 , which is equivalent to the training configuration of ESRGAN used in the PIRM-SR challenge. This is slightly different from the configuration used in the released model trained with ? = 10 ?2 .</p><p>All of our networks are trained exclusively on the DIV2K dataset <ref type="bibr" target="#b14">[14]</ref>, while the original ESRGAN was trained with DIV2K, Flickr2K, and OST datasets combined. We obtained the LR images by downsampling the HR images with MATLAB bicubic interpolation. We compare the effects of our methods on LPIPS, PSNR, and SSIM scores on the Set5, Set14, BSD100, and Urban100 datasets. Scores evaluated on the Set5 and Set14 datasets are obtained by averaging the final 5 checkpoints, each recorded at [480k, 485k, 490k, 495k, 500k] iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation study</head><p>To study the effects of our proposed methods, we perform an ablation study of our proposed method. We enable our proposed methods one by one and list the resulting scores in <ref type="table" target="#tab_0">Table 1</ref>. Each training configuration was fully trained with the original training configurations. We provide the saved model and configuration files to reproduce our results in our project repository. We also list the results of the official ESRGAN for fair comparison. The improvements from the official results and the result from configuration(c) is because the ? value is different from the official model. First, blur detection is experimented with in configuration(b) and improves the LPIPS score for all benchmarks. We train our baseline ESRGAN in configuration(c) and get reasonable results. By applying the technique of Section 3.3 in configuration(d), we slightly harm the network in terms of the LPIPS score. However, providing conditional information to the discriminator is crucial for learning such a supervised problem with adversarial learning. Our method of directly concatenating the reference image in the input is not optimal. The low-resolution image could be applied through SPADE <ref type="bibr" target="#b20">[20]</ref> or alternative spatial transformation methods for improvements. Applying scaled noise shows large improvements as experimented in configuration(e).</p><p>The cycle consistency loss applied in configuration(f) shows neutral and slightly negative effects on the LPIPS score. The reason for this is mostly because of the incompetent GAN framework lacking the training techniques of modern GAN literature. Our statement is stated by the failure of configuration(g) where the GAN framework alone is responsible for learning the super-resolution The model is first trained with the L1 loss for 100K iterations with an initial learning rate 2 ? 10 ?4 , decayed by a factor of two every 2.5 ? 10 5 iteration. The GAN-based model is initialized with the pretrained model and is trained for 200K iterations, which is shorter than the original 400K iterations. Additionally, the batch size is decreased from 16 to 4 and we therefore approximately scale the initial learning rate of 10 ?4 to 2 ? 10 ?5 by a factor of 5. The learning rate is decayed at [50k, 100k] iterations. We do not use model ensemble to further stabilize the network. All other models and hyperparameter configurations are equal. We train the network on the DIV8K dataset <ref type="bibr" target="#b15">[15]</ref>, while the original network was trained with additional datasets including DIV2K, Flicker2K, OST dataset. The first 1,400 images of DIV8K are used as training data and the rest 100 validation images are used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation study</head><p>The PSNR-oriented method is improved using blur detection in configuration(b). Our GAN-baed model of configuration(c) achieves worse performance compared to the results reported in <ref type="bibr" target="#b21">[21]</ref> because of the lighter training configurations. We were able to make significant improvements in the LPIPS score from the baseline RFB-ESRGAN using our proposed methods in configuration(d).</p><p>We apply all of our proposed methods except the cycle consistency loss in configuration(d). We also train the model with cycle consistency loss and get similar results in configuration(e). We were able to make such improvements using much lighter training configurations with only half iteration steps, ?4 smaller batch size, and without model ensemble. The results are described in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a one-to-many approach for super-resolution and achieve improved perceptual quality and better LPIPS score from the baseline ESRGAN configuration and achieve the state-of-art LPIPS score in x16 perceptual super-resolution. We provide scaled pixel-wise to the generator to allow stochastic variation in the reconstructed image and implement a generator capable of a one-to-many pipeline. We also address the limitations of mixing the strict content loss with perceptual losses and propose an alternative based on the cycle loss. Our newly modified loss will ensure the consistency of the content while not penalizing high-frequency detail. Additionally, we further propose more techniques such as blur detection using Laplacian activation and redesign the discriminator input by providing a reference image to further improve the perceptual quality of ?4 and ?16 super-resolution. However, the GAN framework from ESRGAN was incompetent to guide the training on its own. Modern GAN training techniques could be applied to further improve the GAN framework used in super-resolution. Our proposed loss function will become more effective as a content loss when coupled with a robust GAN framework since it will reduce constraints in generating high-frequency detail. Such improvements are left for future work. We compare the poorly reconstructed outputs of ESRGAN from BSD100 and Urban100 datasets with our proposed model trained with configuration(f). Our method produces sharp textures and more realistic structures compared to the baseline ESRGAN, although it also fails to accurately reconstruct human faces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of our method. The cycle consistency loss is measured by comparing the LR image with the downsampled SR image. The discriminator is provided with the target image and a reference image generated by bicubic-upsampling the LR image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Boxplot of the scaling factors against the depth of the network trained on configuration(c) ?4. The desired magnitude of noise increases in deeper layers, while the final layers have smaller scaling factors. The sensitivity to random noise varies for each layer and channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Randomly selected samples of the blur detection algorithm tested on image 0031 from the DIV8K dataset. The top two rows are the patches classified as clear and the bottom rows are blurry patches. Regions that are clear in the image (person, pole) are correctly considered as clear patches by the detection algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison of our methods with the official ESRGAN on ?4 super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>LPIPS, PSNR, SSIM scores of various configurations for ?4. / 30.3603 / 0.8679 0.2223 / 26.7608 / 0.7525 0.2705 / 27.2264 / 0.7461 0.1761 / 24.8770 / 0.7764 +Blur detection (b) 0.1327 / 30.4582 / 0.7525 0.2229 / 26.8448 / 0.7547 0.2684 / 27.2545 / 0.7473 0.1744 / 25.0816 / 0.7821 ESRGAN (Official) 0.0597 / 28.4362 / 0.8145 0.1129 / 23.4729 / 0.6276 0.1285 / 23.3657 / 0.6108 0.1025 / 22.</figDesc><table><row><cell>Methods</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell></row><row><cell></cell><cell>(LPIPS / PSNR / SSIM)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pretrained (a)</cell><cell cols="4">0.1341 7912 / 0.7058</cell></row><row><cell cols="5">+blur detection (c) 0.0538 / 27.9285 / 0.7968 0.1117 / 24.5264 / 0.6602 0.1256 / 24.6554 / 0.6447 0.1026 / 23.2829 / 0.7137</cell></row><row><cell>+refGAN (d)</cell><cell cols="4">0.0536 / 27.9871 / 0.8014 0.1157 / 24.4505 / 0.6611 0.1275 / 24.5896 / 0.6470 0.1027 / 23.0496 / 0.7103</cell></row><row><cell>+Add noise (e)</cell><cell>0.04998 / 28.23 / 0.8081</cell><cell>0.1104 / 24.48 / 0.6626</cell><cell cols="2">0.1209 / 24.8439 / 0.6577 0.1007 / 23.2204 / 0.7203</cell></row><row><cell>+Cycle loss (f)</cell><cell cols="4">0.0524 / 28.1322 / 0.8033 0.1082 / 24.5802 / 0.6634 0.1264 / 24.6180 / 0.6468 0.1015 / 23.1363 / 0.7103</cell></row><row><cell cols="5">-Perceptual loss (g) 0.2690 / 23.4608 / 0.6312 0.2727 / 22.2703 / 0.5685 0.2985 / 24.1648 / 0.5859 0.2411 / 20.8169 / 0.6244</cell></row><row><cell cols="2">4.1 ?4 super-resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1.1 Training details</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LPIPS, PSNR scores for various configurations for ?16 super-resolution.We employ the RFB-ESRGAN of<ref type="bibr" target="#b21">[21]</ref> as the baseline for our experiments on ?16 super-resolution. The RFB-ESRGAN proposes an architecture using Receptive Field Blocks(RFB) and Residual of Receptive Field Dense Block(RRFDB), each as an alternative for convolution and RRDB blocks. The RFB-ESRGAN uses less memory compared to methods that manipulate the image in the intermediate ?4 resolution<ref type="bibr" target="#b22">[22]</ref> and this allowed larger batch size in our environment. We employ the RFB-ESRGAN network architecture with 16 RRDB blocks and 8 RRFDB blocks for the baseline of our experiments on ?16 super-resolution.</figDesc><table><row><cell>Methods</cell><cell>DIV8K validation</cell></row><row><cell>Pretrained (a)</cell><cell>0.4664 / 30.3603</cell></row><row><cell>+Blur detection (b)</cell><cell>0.4603 / 25.53</cell></row><row><cell>RFB-ESRGAN(official)</cell><cell>0.345 / 24.03</cell></row><row><cell cols="2">Baseline RFB-ESRGAN (c) 0.356 / 24.78</cell></row><row><cell>Ours w/o cycle-loss (d)</cell><cell>0.321 / 23.95</cell></row><row><cell>Ours w/ cycle-loss (e)</cell><cell>0.323 / 23.49</cell></row><row><cell cols="2">process. The GAN framework of ESRGAN is incapable of lead the training process and thus</cell></row><row><cell cols="2">the image quality wasn't improved when we gave more responsibility to the adversarial loss in</cell></row><row><cell cols="2">configuration(f). However, coupled with improved GAN techniques in further research, the cycle</cell></row><row><cell cols="2">consistency content loss will further enhance the image quality.</cell></row><row><cell>4.2 ?16 super-resolution</cell><cell></cell></row><row><cell>4.2.1 Training details</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/krenerd/ultimate-sr</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on perceptual extreme super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Investigating loss functions for extreme super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sejong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blur image detection using Laplacian operator and Open-CV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanupriya</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference System Modeling &amp; Advancement in Research Trends (SMART)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Defusionnet: Defocus blur detection via recurrently fusing and refining multi-scale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate and fast blur detection using a pyramid M-Shaped deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="86611" to="86624" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Div8k: Diverse 8k resolution image dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual extreme super-resolution network with receptive field block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taizhang</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Investigating loss functions for extreme super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sejong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
