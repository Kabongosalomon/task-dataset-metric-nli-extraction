<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic and Geometric Depth: Detecting Objects in Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<email>pangjiangmiao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Centre of Perceptual and Interactive Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic and Geometric Depth: Detecting Objects in Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Probabilistic and Geometric Depth, Monocular 3D Detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection is an important capability needed in various practical applications such as driver assistance systems. Monocular 3D detection, as a representative general setting among image-based approaches, provides a more economical solution than conventional settings relying on LiDARs but still yields unsatisfactory results. This paper first presents a systematic study on this problem. We observe that the current monocular 3D detection can be simplified as an instance depth estimation problem: The inaccurate instance depth blocks all the other 3D attribute predictions from improving the overall detection performance. Moreover, recent methods directly estimate the depth based on isolated instances or pixels while ignoring the geometric relations across different objects. To this end, we construct geometric relation graphs across predicted objects and use the graph to facilitate depth estimation. As the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty. It provides an important indicator to identify confident predictions and further guide the depth propagation. Despite the simplicity of the basic idea, our method, PGD, obtains significant improvements on KITTI and nuScenes benchmarks, achieving 1st place out of all monocular vision-only methods while still maintaining real-time efficiency. Code and models will be released at https://github.com/open-mmlab/mmdetection3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D object detection is an essential task for many robotic systems such as autonomous vehicles. Recent advanced methods in this field typically resort to various sensors, such as LiDAR <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, Radar <ref type="bibr" target="#b6">[7]</ref>, binocular vision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, or their combinations for accurate depth information. Nevertheless, these perceptual systems are complicated, expensive, and difficult to maintain in complex environments. In contrast, monocular 3D detection, a setting that aims at perceiveing 3D objects from 2D monocular images, has drawn increasing attention due to its low costs. However, as the depth information is not directly manifest in the input, this task is inherently ill-posed, making the problem particularly challenging. This paper starts from a systematic study about this problem on two authoritative benchmarks in a quantitative way. Although we already knew the depth information is critical to this task, the study surprisingly shows that inaccurate depth estimation blocks all the other localization predictions from improving the final results. As instance depth has shown to be the bottleneck, we can simplify monocular 3D detection as an instance depth estimation problem to tackle it essentially. Previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> first use an extra cumbersome depth estimation model to complement 2D detectors on depth information. The following methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> simplify the frameworks by directly regarding depth as one dimension of the 3D localization task. However, they still use simple methods that estimate depth from isolated instances or pixels in a regression manner. We observe that aside from each object itself, other objects are co-existing in an image and the geometric relations across them can be valuable constraints to guarantee accurate estimation. Motivated by these observations, we propose Probabilistic and Geometric Depth (PGD) that jointly leverages probabilistic depth uncertainty and geometric relationships across co-existed objects for accurate depth estimation. Specifically, as the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty of the estimated depth. We first bucket the depth values into a set of intervals and calculate the depth by the expectation of the distribution <ref type="figure">(Fig. 1(a)</ref>). The average of top-k confidence scores from the distribution is taken as the uncertainty of the depth. To model the geometric relations, <ref type="figure">Figure 1</ref>: In this paper, to tackle the dominating depth estimation problem in the monocular 3D detection, we first (a) predict the depth of each instance with a probabilistic representation to capture the uncertainty, and (b) further construct a geometric relation graph to enhance the estimation from contextual connections. (c) The proposed method outperforms the other work in terms of both performance and speed significantly on the KITTI 3D car detection benchmark.</p><p>we further construct a depth propagation graph to enhance the estimations with their contextual relationship. The uncertainty of each instance depth provides useful guidance for the propagation therein. Benefiting from this overall scheme, we can easily identify the predictions with higher confidence, and more importantly, estimate their depths more accurately with the graph-based synergistic mechanism. We implement the methods on a simple monocular 3D object detector FCOS3D <ref type="bibr" target="#b14">[15]</ref>. Despite the simplicity of the basic idea, our PGD results in significant improvements on KITTI <ref type="bibr" target="#b15">[16]</ref> and nuScenes <ref type="bibr" target="#b16">[17]</ref> with different benchmark settings and evaluation metrics. It achieves 1st place out of all monocular vision-only methods while still maintaining real-time efficiency. The simple yet effective method proves that with only designs tailored to depth, a 2D detector can be capable of detecting objects in perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>2D Object Detection According to the base of initial guesses, modern 2D detection methods can be divided into two branches, anchor-based and anchor-free. Anchor-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> benefit from the predefined anchors in terms of much easier regression, while anchor-free methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> do not need complicated prior settings and thus have better universality. For simplicity, we take FCOS3D <ref type="bibr" target="#b14">[15]</ref>, the 3D adapted version of FCOS <ref type="bibr" target="#b23">[24]</ref>, as the baseline considering its capability of handling overlapped ground truths and scale variance problem. Monocular Depth Estimation Monocular depth estimation is also a challenging ill-posed problem like monocular 3D detection. It aims at predicting dense and global depth field at pixel level given an RGB image. Early works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> predict depth from hand-crafted features with non-parametric optimization methods. With the rapid progress of CNNs, fully supervised methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, selfsupervised methods based on stereo pairs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> and monocular videos <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> gradually emerged. Although this problem has been explored for a long time, there are very few works <ref type="bibr" target="#b35">[36]</ref> studying it in a specific task, like detection, where the dense depth supervision is always not guaranteed and we only care about the accuracy of instance depth instead of the global depth field. As for the reformulation of depth learning problems, there are a few attempts in this field. For example, DORN <ref type="bibr" target="#b36">[37]</ref> recasts the depth learning problem as ordinal regression and proposes a spacingincreasing discretization (SID) strategy to improve network training and reduce computations. It is similar to the underlying idea of our probabilistic representation for uncertainty modeling while different in terms of motivation and design details. Monocular 3D Object Detection Monocular 3D detection is more complicated than the 2D case. The underlying problem is the inconsistency of input 2D data modal and the output 3D predictions. Methods involving sub-networks Earlier work uses sub-networks to assist 3D detection. 3DOP <ref type="bibr" target="#b37">[38]</ref> and MLFusion <ref type="bibr" target="#b9">[10]</ref> use a depth estimation network while Deep3DBox <ref type="bibr" target="#b38">[39]</ref> uses a 2D object detector. They rely on the design and performance of these sub-networks, even external data and pre-trained models, which makes the training inconvenient and introduces additional system complexity. Transform to 3D representation Another category is to convert the RGB input to 3D representations like OFTNet <ref type="bibr" target="#b39">[40]</ref> and Pseudo-Lidar <ref type="bibr" target="#b10">[11]</ref>. Although these methods have shown promising Figure 2: Oracle analyses with different datasets and metrics. From left to right: 3D IoU based mAP on KITTI, NuScenes Detection Score (NDS) and distance-based mAP on nuScenes. We replace our predictions with ground truth values step by step and observe the performance improvements. It can be seen that an accurate depth can bring significant performance improvement (green lines), and only with accurate depth can the improvements brought by other oracles be realized. performance, they actually rely on dense depth labels and hence are not regarded as pure monocular approaches. There are also domain gaps between different depth sensors, making them hard to generalize smoothly to a new practical setting. Furthermore, the efficiency of processing a large number of point clouds is also a significant issue to deal with in practical applications. End-to-end designs like 2D detection Recent work notices these drawbacks, and end-to-end frameworks are thus proposed. M3D-RPN <ref type="bibr" target="#b12">[13]</ref> implements a single-stage multi-class detector with an end-to-end region proposal network and depth-aware convolution. SS3D <ref type="bibr" target="#b40">[41]</ref> proposes to detect 2D key points and further predicts object characteristics with uncertainties. MonoDIS <ref type="bibr" target="#b11">[12]</ref> introduces a disentangling loss to reduce the instability of the training procedure. Some of them still have multiple training stages or post-optimization phases. In addition, they all follow anchor-based manners, and thus the consistency of 2D and 3D anchors is needed to be determined. In contrast, anchor-free methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b14">15]</ref> do not need to make statistics on the given data and have better generalized ability to more various classes or different intrinsic settings, so we choose to follow this paradigm.</p><p>Nevertheless, all of these works rarely have customized designs for instance depth estimation in particular, and only take it as one common regression target for isolated points or instances. It actually hinders the breakthrough of this problem, which will be discussed in our quantitative study and specifically addressed in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary and Motivating Study</head><p>In this section, we aim at making an in-depth quantitative error analysis on top of a basic adapted monocular 3D detector to investigate the key challenge in the specific 3D detection setting.</p><p>Typically, conventional 2D detection expects the model to predict 2D bounding boxes and category labels for each object of interest, while a monocular 3D detector needs to predict 7-DoF 3D boxes given the same input. From this perspective of problem formulation, the main difference lies on the regression targets. An intuitive reason for the much worse performance of monocular 3D detection compared to 2D is that there exist much more difficult targets to regress in the localization. Hence, we choose a simple detector FCOS3D <ref type="bibr" target="#b14">[15]</ref> to study the specific problem, which keeps the welldeveloped designs for 2D feature extraction and is adapted for this 3D task with only basic designs for specific 3D detection targets. As shown in the left part of <ref type="figure">Fig. 3</ref>, there are overall two branches for classification and localization respectively. Formally, for the regression branch, the detector predicts 3D attributes, including offsets ?x, ?y to the projected 3D center, depths d, 3D size w 3D , l 3D , h 3D , sin value of rotation ?, direction class C ? , center-ness c, and distances to four sides of 2D boxes l, r, t, b, for each location on the output dense map. We further equip it with a basic consistency loss between 3D and 2D localization, which will be detailed in the appendix.</p><p>On this basis, we apply this baseline on two representative benchmarks, KITTI and nuScenes, and replace the predictions with ground truths step by step to identify the performance bottleneck ( <ref type="figure">Fig. 2)</ref>. Unexpectedly, the inaccurate depth blocks all the other sub-task predictions from improving the overall detection performance, on both datasets under different metrics. Hence, current monocular 3D detection, especially 3D localization, can be reduced to the dominating instance depth estimation problem to a great extent, which will be the focus of our method to be presented next. See more details about the explanation of oracle analyses in the appendix. <ref type="figure">Figure 3</ref>: An overview of our framework. We start from a basic monocular 3D detector, FCOS3D, while focus on tackling the difficulty of instance depth estimation with our proposed customized module in the head. With the feature map from the regression branch as the input, we first introduce a branch for probabilistic depth estimation to model the uncertainty, then derive the geometric depth with the depth propagation graph and finally integrate them to get the final depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>Given images collected from similar cameras, previous work typically resorts to direct regression for instance depth estimation and expects the model to directly learn that objects with certain appearances and sizes always exist at locations with certain depths. Our baseline also follows this way. However, it is hard to learn due to the large variance and also obviously not enough for the accuracy needed in 3D detection. Given the inherent downside of hard regression for isolated points, in our approach, we aim at constructing an uncertainty-aware depth propagation graph to enhance the estimation from contextual connections among instances. Next, we will first elaborate on the adopted probabilistic representation and technical details of the constructed geometric graph, and finally present how we integrate these obtained depth estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Uncertainty Modeling with Probabilistic Representation</head><p>For a one-stage detector, a general design for direct depth estimation is a small head along the regression branch expected to output a dense depth map. Formally, as shown in <ref type="figure">Fig. 3</ref>, suppose the input feature map has shape H ? W , then the direct depth regression output 2 can be denoted as D R ? R H?W . On this basis, to establish an effective depth propagation mechanism, modeling the uncertainty of depth estimation for each instance is an important preliminary, which can provide useful guidance for weighing the propagation among instances. We adopt a simple yet effective probabilistic representation to achieve this: Considering the depth value is continuous in a certain range, we uniformly quantize the depth interval into a set of discrete values and represent the prediction with the expectation of the distribution. Suppose the detection range is 0 ? D max , the discretized unit is U , then we have C = D max /U + 1 split points. Denote the set of points as a weight vector ? ? R C , and then we introduce a new head parallel with direct regression to produce a probabilistic output map D P M , which will be decoded with:</p><formula xml:id="formula_0">DP = ? T sof tmax(DP M )<label>(1)</label></formula><p>where D P is the so-called probabilistic depth. It is equivalent to compute the expectation of the probabilistic distribution formed by sof tmax(D P M ). Apart from the D P , we can further obtain the depth confidence score, denoted as s d ? S D , from the depth distribution of each instance. In practice, we take the average of top-2 confidence as the depth score for U = 10m. It will be multiplied by the center-ness and classification score as the final ranking criterion for predictions during inference.</p><p>Subsequently, we fuse D R and D P with the sigmoid response of a data-agnostic single parameter ?:</p><formula xml:id="formula_1">DL = ?(?)DR + (1 ? ?(?))DP<label>(2)</label></formula><p>Here D L is regarded as a local depth estimation for each isolated instance, which together with the depth score derived from D P M serve as the foundation of constructing the depth propagation graph.</p><p>It is worth noting that our implementation is different from the typical way used in monocular depth estimation <ref type="bibr" target="#b36">[37]</ref>, which usually adopts a fine-grained quantization for the depth interval and further estimates the value with classification and residual regression. In comparison, our method is more memory-efficient, more straightforward for regressing continuous value, and provides a natural indicator for uncertainty estimation. Please refer to the appendix for empirical results about comparison with other depth interval division methods. <ref type="figure">Figure 4</ref>: For objects hard to approximate depth accurately (like car 2), propagating (green arrows) reliable depth predictions from other objects (car 1, 3, 4) with perspective geometry can enhance the reasoning from the global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Depth Propagation from Perspective Geometry</head><p>With the depth prediction D L for isolated instances and their depth confidence scores S D for uncertainty estimation, we can further construct the propagation graph based on the contextual geometric relationship. Consider the typical driving scenarios: a general constraint can be leveraged, i.e., almost all the objects are on the ground. Early in <ref type="bibr" target="#b42">[43]</ref>, Hoiem et al. utilized the scene projection with this constraint to put objects in the context of the overall 3D scene by modeling the relationship of different elements. Here, targeting the depth estimation problem, we instead propose a geometric depth propagation mechanism with consideration of interdependence between instances. Next, we will first derive the perspective relationship between two instances, and then present the details of the graph-based depth propagation scheme with edge pruning and gating.</p><p>Perspective Relationship Consider in the general perspective projection, suppose the camera projection matrix P is:</p><formula xml:id="formula_2">P = f 0 c u ?f b x 0 f c v ?f b y 0 0 1 ?f b z<label>(3)</label></formula><p>where f is the focal length, c u and c v are the vertical and horizon position of camera in the image, b x , b y and b z denote the baseline with respect to the reference camera (non-zero in KITTI while zero in nuScenes). Note that we represent the focal length with a single f considering most cameras share the same one for the u and v axis. Then a 3D point x 3D = (x, y, z, 1) T in the camera coordinates can be projected to a point x 2D = (u , v , 1) T in the image with:</p><formula xml:id="formula_3">dx 2D = P x 3D<label>(4)</label></formula><p>To simplify the result, we replace v with v + c v , then v represents the distance to the horizon line (Down is the positive direction in <ref type="figure">Fig. 4</ref>). Then we get:</p><formula xml:id="formula_4">vd = f (y ? b y + c v b z )<label>(5)</label></formula><p>The relation for u is similar. Considering the constraint that all the objects are on the ground, the bottom centers of objects always share the same y (height in the camera coordinates), so we mainly consider this relation for v next. Given two objects 1 and 2, the relationship between their center depths can be derived from Eqn. 5:</p><formula xml:id="formula_5">d 2 = v 1 v 2 d 1 + f v 2 (y 2 ? y 1 ) ? v 1 v 2 d 1 + f 2v 2 (h 3D 1 ? h 3D 2 ) d P 1?2<label>(6)</label></formula><p>with which we can predict d 2 given d 1 precisely with the height difference between 3D centers. Besides, we can also leverage an approximation of this relationship given the assumption that objects share the same bottom height, then y 2 ? y 1 can be substituted by the difference of half heights of 3D boxes</p><formula xml:id="formula_6">1 2 (h 3D 1 ? h 3D 2 ), defined as d P 1?2 . In this relation, when h 3D 1 = h 3D 2 , v 1 d 1 = v 2 d 2 ,</formula><p>which is easy to understand, i.e., an object closer to vanishing line is farther away. It is a clear relationship connecting different instances but also yields errors. Suppose |(y 2 ? y 1 ) ? 1 2 (h 1 ? h 2 )| = ?, the error of depth will be ?d = f v2 ?. When ? = 0.1m, v 2 = 50 (pixels), ?d can be about 1.5m. Although it is acceptable for objects 30m away (corresponding with v 2 = 50), we also need a mechanism to avoid possible large errors. It consists of the edge pruning and gating scheme to be described next and the location-aware weight map to be mentioned in the Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-Based Depth Propagation</head><p>With the pairwise perspective relationship, we can estimate the depth of any object from the cues of other objects. Then we can construct a dense directed graph with two bidirectional edges between any two objects representing the depth propagation ( <ref type="figure">Fig. 4</ref>). Formally, suppose we have N predicted objects with indices from P = {1, 2, ..., n}, we can estimate the depth of object i given d P j?i for all the j ? P, defined as the geometric depth d G i ? D G . Considering the computational efficiency and possible large errors mentioned previously, we propose an edge pruning and gating scheme to improve the propagation graph. From our observation, the same category of nearby objects can well satisfy the "same ground" condition, so we select the following 3 most important factors to decide which edges are influential and reliable, including the depth confidence s d j , 2D distance score s 2D ij , and classification similarity s cls ij . The latter two and the overall edge score s e j?i are computed as follows:</p><formula xml:id="formula_7">s 2D ij = 1 ? t 2D ij t 2D max , s cls ij = f i ? f j ||f i || 2 ||f j || 2 , s e j?i = s d j ? s 2D ij ? s cls ij k j=1 s d j ? s 2D ij ? s cls ij<label>(7)</label></formula><p>where t 2D ij is the 2D distance between projected centers of object i and j, t 2D max is set to the length of image diagonal, f i and f j are the output confidence vectors of two objects from classification branch and k is the maximum number of edges to be kept after pruning (edges with top-k scores are kept). The edge score is then used for gating so that each node attends its edges with their importance:</p><formula xml:id="formula_8">d G i = k j=1 s e j?i d P j?i<label>(8)</label></formula><p>Note that obtaining the geometric depth map D G from this graph is free of learnable parameters. To avoid influencing the learning of other components, we cut off the gradients backpropagated from this computation and only focus on how to integrate D L and D G , which will be discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Probabilistic and Geometric Depth Estimation</head><p>So far, we have obtained two depth predictions D L and D G from isolated and graph-based contextual estimations, respectively. Then we integrate these two complementary components in a learning manner. Unlike the data-agnostic single parameter used in the local estimation, integrating these two results should be more complex considering their flexible roles in various complicated cases. So we further introduce a branch to produce a location-aware weight map ? ? R H?W to fuse them ( <ref type="figure">Fig. 3)</ref>:</p><formula xml:id="formula_9">D = ?(?) ? DL + (1 ? ?(?)) ? DG<label>(9)</label></formula><p>The fused depth D will replace the direct regressed D R in the baseline and trained with the common smooth L1 loss in the same end-to-end way. Note that adding intermediate supervisions empirically makes the training more stable but does not bring any performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present our experimental setup and implementation details, and then make the quantitative analysis on the KITTI and nuScenes dataset with details of both performance and efficiency. Finally, detailed ablation studies are conducted to show the efficacy of each component in our method. Refer to the appendix for more qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets &amp; Evaluation Metrics</head><p>We evaluate our method on two datasets, KITTI <ref type="bibr" target="#b15">[16]</ref> and nuScenes <ref type="bibr" target="#b16">[17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Network Architectures As shown in <ref type="figure">Fig. 3</ref>, our baseline framework basically follows the design of FCOS3D <ref type="bibr" target="#b14">[15]</ref>. Given the input image, we utilize ResNet101 <ref type="bibr" target="#b43">[44]</ref> as the feature extraction backbone followed by FPN <ref type="bibr" target="#b44">[45]</ref> for generating multi-level predictions. Detection heads are shared among multi-level feature maps except that three scale factors are used to differentiate some of their final  regressed results, including offsets, depths, and sizes, respectively. For the hyperparameters in the depth estimation module, U is set to 10m and k is set to 5. The overall framework is built on top of MMDetection3D <ref type="bibr" target="#b45">[46]</ref>. Please refer to FCOS3D <ref type="bibr" target="#b14">[15]</ref> and appendix for the design of loss and other implementation details.</p><p>Training Parameters For all the experiments, we trained randomly initialized networks from scratch following end-to-end manners. Models are trained with SGD optimizer, in which gradient clip and warm-up policy are exploited with learning rate 0.001, number of warm-up iterations 500, warm-up ratio 0.33 and batch size 32/12 on 16/4 GTX 1080Ti GPUs for nuScenes/KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>We only implement image flip for augmentation, where offset and 2D targets are flipped for the 2D image while 3D boxes are transformed correspondingly in 3D space. No other augmentation (right image augmentation, cropping, resizing, etc.) methods are utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative Analysis</head><p>We make quantitative analyses both on KITTI (Tab. 1 and 4, <ref type="figure">Fig. 1(c)</ref>) and much harder, less commonly validated nuScenes dataset (Tab. 2). It can be seen that our method achieves the state-ofthe-art on both benchmarks with different settings and metrics while maintains outstanding speed.</p><p>We list part of early monocular methods with extra data or pre-trained models and recent image-only methods that have related results for comparison on the KITTI dataset. Only the results for car detection are compared here because the performance of small objects is always unstable due to their limited samples. Our framework based on the simple adapted FCOS3D achieves much better performance than others, especially considering M3D-RPN <ref type="bibr" target="#b12">[13]</ref> and RTM3D <ref type="bibr" target="#b13">[14]</ref> adopt stronger backbone and data augmentation. Furthermore, our method can run at the speed of 36Hz to achieve this, thanks to most of our modules not introducing extra computational costs to inference. It is an excellent trade-off between performance and efficiency.</p><p>Then for the nuScenes dataset, we also compare the results on the test set and validation set, respectively. On the test set, we first compared all the methods using RGB images as the input data. Our single model achieved the best performance among them with mAP 37.0% and NDS 43.2%, in which we particularly exceeded the previous best method more than 3% in terms of mAP. We also list benchmarks based on other data modality, including lightweight, real-time PointPillars <ref type="bibr" target="#b1">[2]</ref> with LiDAR, CenterFusion <ref type="bibr" target="#b6">[7]</ref> with RGB image and Radar, and CenterPoint <ref type="bibr" target="#b51">[52]</ref> ensemble results with all the sensors. It can be seen that although our method has a certain gap with the high-performance CenterPoint, it even surpasses PointPillars and CenterFusion on mAP, which shows that this ill-posed    problem can be solved decently with enough data. At the same time, the methods using other modal data usually yield better NDS, mainly because the mAVE is smaller. The reason is that they can predict the speed of objects from continuous multi-frame point clouds or velocity measurement of Radar. In contrast, we only use the single-frame image in our experiments. So how to mine the speed information from consecutive frame images will be a direction worthy of exploring in the future. On the validation set, we compare our method with the best open-source center-based detector, CenterNet. Our method is not only much more efficient to train and inference (3 days to train the CenterNet vs. only one day to train our model with comparable performance), but also achieves better performance, especially in terms of the mAP and mAOE. On this basis, we finally achieved an improvement of about 9% on NDS. See more detailed results about depth estimation accuracy and per-class detection performance in the appendix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>Finally, we conduct ablation studies to validate the efficacy of our proposed key components on KITTI (Tab. 3) and nuScenes (Tab. 5). We can observe that local constraints can basically enhance the baseline, and our probabilistic and geometric depth further boost the performance significantly, especially in terms of mAP and translation error (mATE). Tab. 7 and Tab. 8 show more details of two core components for improving depth estimation with the metrics average precision under IOU?0.7.</p><p>It can be seen that combining the probabilistic representation (prop. branch in Tab. 7) with direct regression (w/ direct) and leveraging the depth score in the inference (depth score) can finally make the most of this design. For geometric depth, the basic fusion with local estimation can not bring the desirable gain. Improving the propagation graph via edge pruning and gating (edge gating) and cutting off the unexpected gradients propagation (cut off grad.) can help remove possible noises and prompt the learning more focused on the final integration, thus making the overall scheme much more effective. As for alternative implementations, we compare feasible methods of computing the depth score from the probabilistic distribution (Tab. 6). Compared to other more complicated ways, normalized entropy and standard deviation, our exploited top-2 score can achieve decent results with better efficiency. See more results about different depth division methods and detailed analyses for these two datasets from other perspectives like the Precision-Recall curve in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper targets the key challenge lying in monocular 3D object detection, instance depth estimation. Started from a basic adapted 3D detector, we firstly make in-depth oracle analyses. We surprisingly find that depth estimation is the dominating bottleneck for current 3D detection, especially in terms of localization. To tackle the discovered challenge, we propose a novel approach, Probabilistic and Geometric Depth (PGD), which leverages the geometric relationship in perspective to construct a graph connecting instance estimations with uncertainty and thus predicts depths more accurately.</p><p>The efficacy of this solution is demonstrated on both KITTI and large-scale nuScenes datasets. In the future, we will further extend the geometric depth scheme to more general cases by relaxing the "ground" assumption via 2D height regression or ground normal estimation, and validate this pipeline on other 2D detectors. How to better leverage temporal geometry information to address the difficulty of instance depth estimation is also a promising direction worthy of further exploration. Appendix <ref type="figure">Figure 5</ref>: An overview of our framework ( <ref type="figure">Figure 3</ref> in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Implementation Details</head><p>This section first presents the adopted local geometric constraints between 2D and projected 3D bounding boxes in the enhanced baseline. Subsequently, we will elaborate on the details of training loss and inference procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Local Geometric Constraints</head><p>Our baseline FCOS3D <ref type="bibr" target="#b14">[15]</ref> only stiffly adjusts the output of networks to fit the requirements of 3D detection. There is no relationship or constraints between these predicted attributes, making this network hard to train, especially when the data is limited. Considering our detector can achieve 90% accuracy on 2D vehicle detection, we add 2D localization into our targets and use it to regularize 3D outputs. Actually, this closed-loop and self-supervised approach is also consistent with what humans do in the annotation procedure <ref type="bibr" target="#b53">[54]</ref>. In practice, as shown in <ref type="figure">Fig. 5</ref>, we add a consistency loss (GIoU loss) between our estimated 2D boxes and the exterior 2D boxes of 3D predictions to enhance our baseline, which is particularly important on the small KITTI dataset. Note that due to the difficulty of regressing accurate depth, we use the ground truth depth for deriving the 3D bounding boxes when computing the consistency loss.</p><p>Here we provide an example to show the intuition behind this design. Typically when the data is limited, it is hard for the network to direct regress different 3D targets (offset, depth, orientation, etc.) independently. For example, in <ref type="figure" target="#fig_0">Fig. 6</ref>, the orientation of nearby large objects predicted by our baseline can be very inaccurate (the top line in the figure) even though it can be easily rectified with simple verification. So we add the more reliable 2D localization into our targets to regularize our 3D predictions. It turns out that the simple local constraint could alleviate this problem in the learning procedure while does not introduce extra computational costs to inference. The improved results after adding this constraint can be seen in <ref type="figure" target="#fig_0">Fig. 6</ref> (the bottom line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Loss</head><p>Overall Loss Design We basically follow the loss design of FCOS3D except our proposed consistency loss and the adjustments for different datasets.</p><p>To have a brief review, firstly, we use the focal loss <ref type="bibr" target="#b54">[55]</ref> as the object classification loss:</p><formula xml:id="formula_10">L cls = ??(1 ? p) ? logp<label>(10)</label></formula><p>where p is the class probability of a predicted box, and we follow the common settings, ? = 0.25 and ? = 2. For attribute classification on nuScenes, we use a simple softmax classification loss, denoted as L attr .</p><p>For regression branch, we use the smooth L1 loss for each regression target except centerness:</p><p>L loc = b?(?x,?y,d,w,l,h,?,vx,vy)</p><formula xml:id="formula_11">SmoothL1(?b)<label>(11)</label></formula><p>The weights of ?x, ?y, d, w, l, h, ? error are 1 and the weights of v x , v y on nuScenes are 0.05. We use the softmax classification loss and binary cross entropy (BCE) loss for direction classification and centerness regression, denoted as L dir and L ct respectively. For local geometric constraints, denote our predicted 2D boxes as B 2D , the minimum exterior 2D boxes of projected 3D boxes as B proj , then the consistency loss is:</p><formula xml:id="formula_12">L geo = GIoU (B 2D , B proj )<label>(12)</label></formula><p>Finally, the total loss is:</p><formula xml:id="formula_13">L = 1 N pos (? cls L cls + ? attr L attr + ? loc L loc + ? dir L dir + ? ct L ct + ? geo L geo )<label>(13)</label></formula><p>N pos is the number of positive predictions and ? cls = ? attr = ? loc = ? dir = ? ct = ? geo = 1.</p><p>Note that the attribute loss L attr and velocity loss in the L loc are only required in the nuScenes experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific Loss Designs for KITTI experiments</head><p>Because the KITTI dataset has relatively limited samples and much more strict metrics, we adopt two specific loss designs for training the networks. First, we add an auxiliary key-points loss to enhance the local geometric consistency further. Denote the 2D offsets of eight key-points (eight corners of a 3D bounding box) relative to a foreground point as k ? R 1?16 , and then we take these offsets as 16 additional dimensions of b in Eqn. 11 and set their weights to 0.2. To make the FPN-based learning stable, we normalize these offsets just as we normalize those offsets to four sides of a 2D box.</p><p>In addition, we use a much stronger uncertainty formulation for this multi-task learning problem as presented in <ref type="bibr" target="#b55">[56]</ref>. Specifically, referring to its formulation of maximum likelihood and homoscedastic uncertainty, we formulate the depth loss as:</p><formula xml:id="formula_14">L depth = L 1 (D, D) 2? 2 + log?<label>(14)</label></formula><p>HereD and D are the targets and predictions of depth, L 1 represents the original smooth L1 loss with ? = 3.0 and ? is the variable for uncertainty. In practice, to make the learning easier, we train the network to predict the log variance s = log? 2 only for depth estimation, which is more numerically stable than directly predicting the variance. Correspondingly, exp(?s) serves as the weight of depth loss. In this way, the depth loss will be adaptively weighted relative to other regression losses. Additionally, the uncertainty exp(?s) can also be used as another confidence score to be multiplied when inference, such that predictions with more accurate depths will have particularly higher scores. Note that this strong uncertainty indicator can only bring a significant gain on KITTI experiments while seriously hurting the general performance as evaluated on the nuScenes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative Depth Loss Designs Considering we have several intermediate depth predictions, such</head><p>as D R , D P and D L in <ref type="figure">Fig. 5</ref>, a natural idea is to add intermediate supervisions for these predictions to guarantee that each branch can learn meaningful information. So we further defined several depth L1 losses for these predictions and tried to replace the original depth loss in the L loc with their weighted summation. It turns out that although this approach can make the training procedure more stable, it does not bring any performance gain. We also find that the framework never overfits to only relying on one kind of estimation even with only supervision for the final prediction, as to be shown in Sec. 3.2. It indicates that these predictions and components indeed work together from complementary aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Inference</head><p>The inference procedure is to forward the input image through the framework and obtain bounding boxes with their class scores, attribute scores (if necessary) and centerness predictions. We multiply the class score, the predicted centerness and the depth confidence score as the overall confidence for each prediction and conduct rotated Non-Maximum Suppression (NMS) in the bird view as most 3D detectors to get the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Explanation of Oracle Analyses</head><p>In this section, we will explain more about our empirical analysis, from the specific settings to more details in the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reason for Replacing Dense Predictions</head><p>First, we would like to emphasize one detail in our analysis, i.e., we replace the dense predictions from the direct output of detection head with oracles to purely observe the problems of our networks. In comparison, other alternatives exist, such as replacing the decoded dense output or predictions after post-processing, which can not reveal some entangling problem lying in the formulation. One example to show the difference between these two implementations is that we replace the offset with corresponding ground truth while the latter approach replaces the decoded X, Y in the 3D space with targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comparison of Different Metrics</head><p>As mentioned in the main paper, KITTI and nuScenes adopt different evaluation metrics. The former is relatively strict and the latter is more comprehensive. Specifically, for mAP of these two datasets, we regard predictions with 3D IoU larger than a threshold (0.7 or 0.5) as positive samples on KITTI while define the match by 2D center distance d 2D in the bird eye view on nuScenes. The latter is a simpler criterion as it decouples the detection from object size and orientation. Therefore, we only plot points with category/location related oracles (classification, depth and offset) in the mAP analysis on nuScenes <ref type="figure" target="#fig_1">(Fig. 7)</ref>. In addition, to be more specific, mAP is computed over several different matching thresholds, D = {0.5, 1, 2, 4} meters, and all categories C on nuScenes:</p><formula xml:id="formula_15">mAP = 1 |C||D| c?C d 2D ?D AP c,d 2D<label>(15)</label></formula><p>Then we can see that it will also consider predictions with relatively inaccurate locations (like objects with the distance error larger than 2 meters but smaller than 4 meters). This difference is especially notable when discussing the improvements from depth score, which will be detailed in Sec. 3.2.</p><p>Finally we basically describe how the NuScenes Detection Score (NDS) is calculated. To begin with, we first define that predictions with center distance from the matching ground truth d 2D ? 2m will be considered as true positives (TP) and thus introduce 5 True Positive metrics, Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE) and Average Attribute Error (AAE). Given these metrics, we compute the mean TP metric (mTP) over all categories:</p><formula xml:id="formula_16">mT P = 1 |C| c?C T P c<label>(16)</label></formula><p>Then the NDS is calculated as follows:</p><formula xml:id="formula_17">N DS = 1 10 [5mAP + mT P ?TP (1 ? min(1, mT P ))]<label>(17)</label></formula><p>Therefore, NDS is a combination of several decoupled metrics and could reflect the performance of 3D detectors from another perspective. See more details about the intermediate computation in its original paper <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Detailed Explanations and Conclusions</head><p>Due to the space limitation in the main paper, we do not discuss much about the results shown in <ref type="figure" target="#fig_1">Fig. 7</ref>. Next, we will analyze it in detail and summarize a series of important conclusions.  <ref type="figure">(Figure 2</ref> in the main paper). From left to right: 3D IoU based mAP on KITTI, NuScenes Detection Score (NDS) and distance-based mAP on nuScenes. We replace our predictions with ground truth values step by step and observe the performance improvements. It can be seen that an accurate depth can bring significant performance improvement (green lines), and only with accurate depth can the improvements brought by other oracles be realized.</p><p>Basic Observations As shown in <ref type="figure" target="#fig_1">Fig. 7</ref>, we replace the predicted attributes with their ground truth values step by step and observe the performance improvements. We can see that:</p><p>1. With only one oracle (circle dots), only depth can bring a considerable improvement (green lines). It shows that with current depth estimation, other predicted attributes do not drag down the performance, while with other predictions, the current accuracy of depth estimation is far not enough.</p><p>2. With accurate depth, other oracles (triangle dots in the figures) could bring the expected performance gains. While with current depth estimation, even all the other predictions are accurate (green rhombus dots), the results are always disappointing, even almost like the baseline.</p><p>3. Although KITTI and nuScenes are different in terms of category variety and metrics, the trend of these curves is the same. The difference is reflected in the importance of localization and classification oracles. Localization is more important on the KITTI, which has less category variety and more strict metrics. Classification is another important factor apart from depth on nuScenes, e.g., our monocular predictions with location oracle is still not better than the best LiDAR-based methods. In contrast, with an accurate depth and classification map, the performance is almost ideal.</p><p>From these observations, we can conclude that the inaccurate depth blocks all the other sub-task predictions from improving the overall detection performance. Hence, as mentioned in the main paper, the current monocular 3D detection, especially 3D localization, can be actually reduced to the dominating instance depth estimation problem.</p><p>Comparison with Best LiDAR-Based Methods There is an interesting phenomenon not much related to depth estimation in the above analysis, i.e., the comparison with best LiDAR-based methods on nuScenes. We can see that classification is particularly important on nuScenes, and our monocular predictions with location oracles are still not better than the state-of-the-art LiDAR-based methods. This result is a little dataset-specific. We conjecture it is because the classification for ten categories on nuScenes is relatively hard, or the annotation is mainly conducted in the point clouds, leading to missing objects in the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supplementary Experimental Results</head><p>In this section, we will show more experimental results to help further understand our approach. First, we will provide toy examples to explain and validate our derived pairwise perspective relationship in the depth propagation. Subsequently, we make more detailed analyses in quantitative and qualitative ways to reveal the working mechanism and effect of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Validation of Depth Propagation</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 8</ref>, we provide two samples with many objects in one image. We first have a brief review of the perspective relationship derived in the main paper. Given two objects 1 and 2, the  relationship between their centers strictly satisfies:</p><formula xml:id="formula_18">d 2 = v 1 v 2 d 1 + f v 2 (y 2 ? y 1 )<label>(18)</label></formula><p>Considering two objects share the same ground (bottom height), we can get the approximate relationship as follows:</p><formula xml:id="formula_19">d 2 = v 1 v 2 d 1 + f 2v 2 (h 3D 1 ? h 3D 2 )<label>(19)</label></formula><p>where d denotes the depth, v denotes the distance between the projected 2D object center and the horizon line in the image, y is the 3D height of object center and h 3D is the height of the 3D bounding box. Taking the left sample in <ref type="figure" target="#fig_2">Fig. 8</ref>  We can see that similar to the general case of depth estimation, our propagation mechanism also yields more notable errors for distant objects, which has been analyzed in the main paper (The effect of ? over ?d will be enlarged as the v 2 decreases.)</p><p>Next, we can further observe the inconsistent bottoms problem shown in <ref type="figure" target="#fig_2">Fig. 8</ref>. We mark some representative instances in the figure. It can be seen that it is sometimes caused by the actual topography, like pedestrians and cars in the second sample. Nevertheless, the noise only exists between objects far away from each other most of the time. We conjecture this is related to the   annotation pipeline, e.g., we tend to make use of nearby annotations when the information for labeling the current instance is inadequate. Alternatively, sometimes it is just because the LiDAR only sweeps the top part of the distant objects such that the annotator can not determine its bottom accurately.</p><p>In conclusion, although the ground constraint holds most of the time, it is still important to design mechanisms to avoid these possible noises and incorporate the geometric depth adaptively, such as the edge pruning/gating scheme and location-aware integration in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative Analysis</head><p>Difference Between Datasets Here we mainly show the observation in the ablation study to explain the different effects from the same component on these two datasets. We take the depth score as an example. First, Tab. 7 in the main paper has shown the especially important role of depth score on the KITTI. However, it does not contribute much to the improvements on nuScenes. Specifically, it only brings about 0.3% increase on NDS by reducing the mATE instead of boosting the mAP. To figure out the reason, we take a closer look at the performance from the Precision-Recall (PR) curve. As shown in <ref type="figure" target="#fig_3">Fig. 9</ref>, we can see that the depth score (solid line) significantly improves the precision under low recall and strict matching thresholds (like 0.5 and 1.0 meters, blue and yellow lines) while influences the performance under high recall and less strict cases (like 2.0 and 4.0 meters, green and red lines). This problem is especially notable for large objects. It reveals the effect of depth score from another perspective, i.e., it can overly suppress those predictions with inaccurate depth, of which we should be tolerant under some circumstances, like distant and small objects. Therefore, designing a more suitable depth score with better interval division methods or other approaches can be a direction worthy of further exploration.</p><p>Mean AP for Multi-Class Detection on nuScenes To present the multi-class detection results more comprehensively, we provide the mean AP results (over all the matching thresholds) for each category on nuScenes in Tab. 9. We can see that our method shows the superiority especially on small (from pedestrian to barrier) and quite large objects (bus). Firstly, the better capability of handling objects with different scales should partly come from the leveraged well-developed backbone and FPN. Furthermore, our probabilistic and geometric depth also improves the accuracy of depth estimation, which is especially important for small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of Each Depth Estimation</head><p>To understand the role of each component for depth estimation more clearly, we make statistics about the fusion weights. Firstly, for local depth estimation, we find that the direct regression accounts for about 25.6% in the results, i.e., ?(?) is about 0.256. It implies that the direct regression may be responsible for regressing the residual of the probabilistic estimation, which plays an auxiliary but important role according to the ablation study in the main paper (Tab. 7). On the other hand, for final integration, we make statistics for the location-aware weights ?(?) of predictions with matching ground truths before NMS on the validation set, and plot its distribution in <ref type="figure" target="#fig_4">Fig. 10</ref> (higher value means more contribution from local estimation). We can see that although the preliminary local estimation plays a more important role in many cases, the propagated geometric depth does contribute a lot to the overall estimation. In addition, we also plot the scatter diagram of these weights with respect to the estimated depth and different categories ( <ref type="figure" target="#fig_5">Fig. 11 and 12</ref>). We can see that the geometric depth contributes more to the estimation of very nearby (can be truncated in the image) and small objects like pedestrians, which is consistent with our common sense that these two cases are relatively hard such that we need to incorporate some contextual information in the reasoning procedure.  Ablation Studies for Alternative Depth Division Methods We also made ablation studies for alternative probabilistic depth settings, including the different settings for the depth unit U and different division methods to bucket the depth value into intervals. First, Tab. 10 shows that more fine-grained division can not bring performance gains. As for the division methods, we test several alternatives as shown in Tab. 11, among which Log and Linear refer to the spacing-increasing discretization (SID) <ref type="bibr" target="#b36">[37]</ref> and linear-increasing discretization (LID) <ref type="bibr" target="#b56">[57]</ref>, respectively. We directly take their split points and compute the depth estimation with Eqn. 1 in the main paper. In contrast, Uniform Log means that we take the split points that are uniformly distributed in the log space as the base to compute the depth estimation in the log space with Eqn. 1, and then apply the exponential transformation to get the final result. We can see that although the simplicity, our adopted uniform division method achieves the best performance. Note that this ablation study is conducted with U = 10m. There may be different conclusions if we exploit more fine-grained divisions or use classification and residual regression to implement the probabilistic depth estimation.  Ablation Studies for Geometric Depth Recall that we select three important factors for edge pruning and gating in the depth propagation graph. We also tried other alternatives for the distance score, including the height difference between 3D bottoms, the distance of 3D centers and our adopted 2D centers (Tab. 12). It can be observed that using the 2D centers yields the best performance. We conjecture that it is because the 3D criteria are based on the inaccurate depths such that they are less reliable than the disentangled 2D distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Error Analysis</head><p>We have validated the efficacy of our method in the main paper by comparing the detection performance of our method and the baseline, especially in terms of the improved mean average precision (mAP) and the mean translation error (mATE). Here we further prove its effectiveness with the depth error analysis. We make depth error statistics for the predictions (before NMS) which have corresponding ground truths on the KITTI validation set (Tab. 13). We can observe <ref type="figure">Figure 13</ref>: Qualitative analysis of detection results. 3D bounding box predictions are projected onto images from six different views and bird-view, respectively. Boxes from different categories are marked with different colors. We can see that the detection results of FCOS3D and PGD are both reasonable. However, from the bird-eye-view, the depth accuracy is remarkably improved by our method, especially for those objects marked with red circles.</p><p>that our method significantly reduces the mean error of depth estimation, both on the absolute error and relative error ((Abs. and Rel. in Tab. 13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Qualitative Analysis</head><p>Then we show some qualitative results on nuScenes in <ref type="figure">Fig. 13</ref> by drawing the predicted 3D bounding boxes in the six-view images and the top-view point clouds. We compare the results predicted by our model and the baseline FCOS3D to demonstrate the improvements in terms of depth estimation intuitively. We can see that from the perspective of images, both detection results are appealing, especially for some small objects that are not labeled. For example, the barriers in the rear right camera are not labeled but detected by these two models. However, from the bird-eye-view, the depth accuracy of the two methods is notably different, especially for those objects marked with red circles:</p><p>The accuracy is significantly improved by our proposed method. It is also in line with the quantitative results (the mATE is reduced remarkably) and further validates the efficacy of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 6 :</head><label>6</label><figDesc>The top line shows that it is easy to validate the accuracy of 3D predictions according to its exterior 2D bounding box. So we add the 2D localization into our targets and use the relatively reliable 2D boxes to regularize 3D predictions. This results in significant improvement as shown by the bottom line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Oracle analyses with different datasets and metrics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Inconsistent bottoms of different instances. Although all the objects in an image share similar heights for bottoms most of the time, corner cases still exist. Here we mark the heights of bottoms in the camera coordinates (down is the positive direction). This problem can be caused by the actual topography, e.g., pedestrians are on the step. It can also be caused by annotation noises, especially for different categories and distant objects. This observation is the foundation of our proposed edge pruning/gating scheme in the depth propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of PR curves for models with (solid line) and without (dotted line) depth score. The depth score encourages predictions with accurate depth while suppresses those with inaccurate depth, which results in higher precision under low recall and strict matching thresholds while lower precision under high recall. This problem is more notable for large objects like cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Distribution of weights for the final integration in our PGD module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Scatter plot of location-aware weights with respect to depths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>Location-aware weights of predictions from different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>There are 7481/7518 samples for training/testing respectively on KITTI, and the training samples are generally divided into 3712/3769 samples as training/validation splits. We first validate our method on this popular benchmark. Nevertheless, the variety of scenes and categories is limited on KITTI, so we further test our approach on the large-scale nuScenes dataset. NuScenes consists of multi-modal data collected from 1000 scenes, including RGB images from 6 cameras, points from 5 Radars, and 1 LiDAR. It is split into 700/150/150 scenes for training/validation/testing. There are overall 1.4M annotated 3D bounding boxes from 10 categories. In addition, nuScenes uses different metrics, distance-based mAP and NDS, which can help evaluate our method from another perspective. See more explanations about metrics in the appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the KITTI validation dataset</figDesc><table><row><cell>Methods</cell><cell>Venue</cell><cell>Extra Labels</cell><cell>Time</cell><cell cols="3">AP BEV IOU? 0.7 Easy Mod. Hard</cell><cell cols="3">AP 3D IOU? 0.7 Easy Mod. Hard</cell></row><row><cell>Mono3D [47]</cell><cell>CVPR 2016</cell><cell>Mask</cell><cell>4.2s</cell><cell>5.22</cell><cell>5.19</cell><cell>4.13</cell><cell>2.53</cell><cell>2.31</cell><cell>2.31</cell></row><row><cell>3DOP [38]</cell><cell>TPAMI 2017</cell><cell>Stereo</cell><cell>3s</cell><cell>12.63</cell><cell>9.49</cell><cell>7.59</cell><cell>6.55</cell><cell>5.07</cell><cell>4.10</cell></row><row><cell>MF3D [10]</cell><cell>CVPR 2018</cell><cell>Dense Depth</cell><cell>-</cell><cell>22.03</cell><cell>13.63</cell><cell>11.60</cell><cell>10.53</cell><cell>5.69</cell><cell>5.39</cell></row><row><cell>Mono3D++ [48]</cell><cell>AAAI 2018</cell><cell>Dense Depth+Shape</cell><cell>&gt;0.6s</cell><cell>16.70</cell><cell>11.50</cell><cell>10.10</cell><cell>10.60</cell><cell>7.90</cell><cell>5.70</cell></row><row><cell>PL [11, 36] (AVOD)</cell><cell>CVPR 2019</cell><cell>Dense Depth</cell><cell>-</cell><cell>19.0</cell><cell>15.3</cell><cell>13.0</cell><cell>7.5</cell><cell>6.1</cell><cell>5.4</cell></row><row><cell>ForeSeE [36] (AVOD)</cell><cell>AAAI 2020</cell><cell>Dense Depth</cell><cell>-</cell><cell>23.4</cell><cell>17.4</cell><cell>15.9</cell><cell>15.0</cell><cell>12.5</cell><cell>12.0</cell></row><row><cell>Deep3DBox [39]</cell><cell>CVPR 2018</cell><cell>None</cell><cell>-</cell><cell>9.99</cell><cell>7.71</cell><cell>5.30</cell><cell>5.85</cell><cell>4.10</cell><cell>3.84</cell></row><row><cell>MonoGRNet [49]</cell><cell>AAAI 2019</cell><cell>None</cell><cell>0.06s</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.88</cell><cell>10.19</cell><cell>7.62</cell></row><row><cell>FQNet [50]</cell><cell>CVPR 2019</cell><cell>None</cell><cell>3.33s</cell><cell>9.50</cell><cell>8.02</cell><cell>7.71</cell><cell>5.98</cell><cell>5.50</cell><cell>4.75</cell></row><row><cell>GS3D [51]</cell><cell>CVPR 2019</cell><cell>None</cell><cell>2.3s</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.46</cell><cell>10.97</cell><cell>10.38</cell></row><row><cell>M3D-RPN [13]</cell><cell>ICCV 2019</cell><cell>None</cell><cell>0.16s</cell><cell>25.94</cell><cell>21.18</cell><cell>17.90</cell><cell>20.27</cell><cell>17.06</cell><cell>15.21</cell></row><row><cell>MonoDIS [12]</cell><cell>ICCV 2019</cell><cell>None</cell><cell>-</cell><cell>24.26</cell><cell>18.43</cell><cell>16.95</cell><cell>18.05</cell><cell>14.98</cell><cell>13.42</cell></row><row><cell>RTM3D [14]</cell><cell>ECCV 2020</cell><cell>None</cell><cell>0.055s</cell><cell>25.56</cell><cell>22.12</cell><cell>20.91</cell><cell>20.77</cell><cell>16.86</cell><cell>16.63</cell></row><row><cell>FCOS3D [15]</cell><cell>ICCVW 2021</cell><cell>None</cell><cell>-</cell><cell>18.16</cell><cell>14.02</cell><cell>13.85</cell><cell>13.90</cell><cell>11.61</cell><cell>10.98</cell></row><row><cell>PGD (Ours)</cell><cell>-</cell><cell>None</cell><cell>0.028s</cell><cell>30.56</cell><cell>23.67</cell><cell>20.84</cell><cell>24.35</cell><cell>18.34</cell><cell>16.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the nuScenes dataset.</figDesc><table><row><cell>Methods</cell><cell>Split</cell><cell>Modality</cell><cell>mAP</cell><cell>mATE</cell><cell>mASE</cell><cell>mAOE</cell><cell>mAVE</cell><cell>mAAE</cell><cell>NDS</cell></row><row><cell>PointPillars (Light) [2]</cell><cell>test</cell><cell>LiDAR</cell><cell>0.305</cell><cell>0.517</cell><cell>0.290</cell><cell>0.500</cell><cell>0.316</cell><cell>0.368</cell><cell>0.453</cell></row><row><cell>CenterFusion [7]</cell><cell>test</cell><cell>Cam. &amp; Radar</cell><cell>0.326</cell><cell>0.631</cell><cell>0.261</cell><cell>0.516</cell><cell>0.614</cell><cell>0.115</cell><cell>0.449</cell></row><row><cell>CenterPoint v2 [52]</cell><cell>test</cell><cell>Cam. &amp; LiDAR &amp; Radar</cell><cell>0.671</cell><cell>0.249</cell><cell>0.236</cell><cell>0.350</cell><cell>0.250</cell><cell>0.136</cell><cell>0.714</cell></row><row><cell>LRM0</cell><cell>test</cell><cell>Camera</cell><cell>0.294</cell><cell>0.752</cell><cell>0.265</cell><cell>0.603</cell><cell>1.582</cell><cell>0.14</cell><cell>0.371</cell></row><row><cell>MonoDIS [12]</cell><cell>test</cell><cell>Camera</cell><cell>0.304</cell><cell>0.738</cell><cell>0.263</cell><cell>0.546</cell><cell>1.553</cell><cell>0.134</cell><cell>0.384</cell></row><row><cell>CenterNet [26]</cell><cell>test</cell><cell>Camera</cell><cell>0.338</cell><cell>0.658</cell><cell>0.255</cell><cell>0.629</cell><cell>1.629</cell><cell>0.142</cell><cell>0.4</cell></row><row><cell>Noah CV Lab</cell><cell>test</cell><cell>Camera</cell><cell>0.331</cell><cell>0.660</cell><cell>0.262</cell><cell>0.354</cell><cell>1.663</cell><cell>0.198</cell><cell>0.418</cell></row><row><cell>FCOS3D [15]</cell><cell>test</cell><cell>Camera</cell><cell>0.358</cell><cell>0.690</cell><cell>0.249</cell><cell>0.452</cell><cell>1.434</cell><cell>0.124</cell><cell>0.428</cell></row><row><cell>PGD (Ours)</cell><cell>test</cell><cell>Camera</cell><cell>0.386</cell><cell>0.626</cell><cell>0.245</cell><cell>0.451</cell><cell>1.509</cell><cell>0.127</cell><cell>0.448</cell></row><row><cell>CenterNet [26]</cell><cell>val</cell><cell>Camera</cell><cell>0.306</cell><cell>0.716</cell><cell>0.264</cell><cell>0.609</cell><cell>1.426</cell><cell>0.658</cell><cell>0.328</cell></row><row><cell>FCOS3D [15]</cell><cell>val</cell><cell>Camera</cell><cell>0.343</cell><cell>0.725</cell><cell>0.263</cell><cell>0.422</cell><cell>1.292</cell><cell>0.153</cell><cell>0.415</cell></row><row><cell>PGD (Ours)</cell><cell>val</cell><cell>Camera</cell><cell>0.369</cell><cell>0.683</cell><cell>0.260</cell><cell>0.439</cell><cell>1.268</cell><cell>0.185</cell><cell>0.428</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on KITTI.Method   AP 3D IOU? 0.7 AP 3D IOU? 0.5</figDesc><table><row><cell></cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>FCOS3D [15]</cell><cell>9.55</cell><cell>5.51</cell><cell>4.78</cell><cell>34.29</cell><cell>25.78</cell><cell>23.66</cell></row><row><cell>+Local cons.</cell><cell>14.62</cell><cell>12.42</cell><cell>11.02</cell><cell>39.11</cell><cell>26.86</cell><cell>25.62</cell></row><row><cell>+Prob. depth</cell><cell>19.10</cell><cell>16.04</cell><cell>14.83</cell><cell>47.64</cell><cell>37.45</cell><cell>33.29</cell></row><row><cell>+Depth prop.</cell><cell>21.36</cell><cell>16.60</cell><cell>15.60</cell><cell>50.57</cell><cell>39.78</cell><cell>34.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on the KITTI test set.</figDesc><table><row><cell>Method</cell><cell cols="3">AP 3D IOU? 0.7 Easy Mod. Hard</cell></row><row><cell>MonoDIS [12]</cell><cell>10.37</cell><cell>7.94</cell><cell>6.40</cell></row><row><cell>M3D-RPN [13]</cell><cell>14.76</cell><cell>9.71</cell><cell>7.42</cell></row><row><cell>MonoPair [42]</cell><cell>13.04</cell><cell>9.99</cell><cell>8.65</cell></row><row><cell>MoVi-3D [53]</cell><cell>15.19</cell><cell>10.90</cell><cell>9.26</cell></row><row><cell>RTM3D [14]</cell><cell>14.41</cell><cell>10.34</cell><cell>8.77</cell></row><row><cell>PGD (Ours)</cell><cell>19.05</cell><cell>11.76</cell><cell>9.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on nuScenes.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>mATE</cell><cell>mASE</cell><cell>mAOE</cell><cell>mAAE</cell><cell>NDS</cell></row><row><cell>FCOS3D [15]</cell><cell>0.319</cell><cell>0.743</cell><cell>0.265</cell><cell>0.543</cell><cell>0.155</cell><cell>0.389</cell></row><row><cell>+Local cons.</cell><cell>0.325</cell><cell>0.721</cell><cell>0.266</cell><cell>0.546</cell><cell>0.164</cell><cell>0.393</cell></row><row><cell>+Prob. depth</cell><cell>0.339</cell><cell>0.716</cell><cell>0.265</cell><cell>0.511</cell><cell>0.163</cell><cell>0.404</cell></row><row><cell>+Depth prop.</cell><cell>0.348</cell><cell>0.701</cell><cell>0.268</cell><cell>0.452</cell><cell>0.166</cell><cell>0.415</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study for the depth score.</figDesc><table><row><cell>Method</cell><cell cols="3">AP 3D IOU? 0.7 Easy Mod. Hard</cell></row><row><cell>Top-2 score</cell><cell>20.58</cell><cell>16.30</cell><cell>14.99</cell></row><row><cell>Norm. Entropy</cell><cell>20.15</cell><cell>15.89</cell><cell>14.67</cell></row><row><cell>1 -Std.</cell><cell>20.68</cell><cell>16.26</cell><cell>14.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study for probabilistic depth.</figDesc><table><row><cell>prop. branch</cell><cell>w/ direct</cell><cell>depth score</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell></cell><cell></cell><cell></cell><cell>14.53</cell><cell>11.93</cell><cell>10.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell>16.58</cell><cell>13.82</cell><cell>13.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell>19.10</cell><cell>16.04</cell><cell>14.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation study for geometric depth.</figDesc><table><row><cell>fusion</cell><cell>edge gating</cell><cell>cut off grad.</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell></cell><cell></cell><cell></cell><cell>18.54</cell><cell>15.44</cell><cell>13.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell>19.50</cell><cell>15.87</cell><cell>14.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>21.36</cell><cell>16.60</cell><cell>15.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>as an example, the depths of the 8 cars are {5.23, 11.80, 16.50, 22.05, 23.64, 28.53, 29.07, 42.85}. With our derived relationship, we can estimate them with only the first 2 accurate depths: {5.23, 11.74, 16.78, 22.92, 21.13, 26.59, 25.78, 36.51}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Average precision for each class on the nuScenes test benchmark. CV and TC are abbreviation of construction vehicle and traffic cone in the table.</figDesc><table><row><cell>Methods</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>trailer</cell><cell>CV</cell><cell>ped</cell><cell>motor</cell><cell>bicycle</cell><cell>TC</cell><cell>barrier</cell><cell>mAP</cell></row><row><cell>LRM0</cell><cell>0.467</cell><cell>0.21</cell><cell>0.17</cell><cell>0.149</cell><cell>0.061</cell><cell>0.359</cell><cell>0.287</cell><cell>0.246</cell><cell>0.476</cell><cell>0.512</cell><cell>0.294</cell></row><row><cell>MonoDIS [12]</cell><cell>0.478</cell><cell>0.22</cell><cell>0.188</cell><cell>0.176</cell><cell>0.074</cell><cell>0.37</cell><cell>0.29</cell><cell>0.245</cell><cell>0.487</cell><cell>0.511</cell><cell>0.304</cell></row><row><cell>CenterNet [26] (HGLS)</cell><cell>0.536</cell><cell>0.27</cell><cell>0.248</cell><cell>0.251</cell><cell>0.086</cell><cell>0.375</cell><cell>0.291</cell><cell>0.207</cell><cell>0.583</cell><cell>0.533</cell><cell>0.338</cell></row><row><cell>Noah CV Lab</cell><cell>0.515</cell><cell>0.278</cell><cell>0.249</cell><cell>0.213</cell><cell>0.066</cell><cell>0.404</cell><cell>0.338</cell><cell>0.237</cell><cell>0.522</cell><cell>0.49</cell><cell>0.331</cell></row><row><cell>PGD (Ours)</cell><cell>0.561</cell><cell>0.299</cell><cell>0.285</cell><cell>0.266</cell><cell>0.134</cell><cell>0.441</cell><cell>0.397</cell><cell>0.314</cell><cell>0.605</cell><cell>0.561</cell><cell>0.386</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Ablation study for the depth unit setting with our lightweight model on nuScenes.</figDesc><table><row><cell>U (meters)</cell><cell>mAP</cell><cell>mATE</cell><cell>mASE</cell><cell>mAOE</cell><cell>mAAE</cell><cell>NDS</cell></row><row><cell>5</cell><cell>0.298</cell><cell>0.79</cell><cell>0.266</cell><cell>0.563</cell><cell>0.164</cell><cell>0.371</cell></row><row><cell>10</cell><cell>0.303</cell><cell>0.775</cell><cell>0.265</cell><cell>0.548</cell><cell>0.164</cell><cell>0.376</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Ablation study for alternative depth division methods.</figDesc><table><row><cell>Methods</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>Log</cell><cell>9.91</cell><cell>8.68</cell><cell>7.95</cell></row><row><cell>Linear</cell><cell>18.63</cell><cell>14.49</cell><cell>13.25</cell></row><row><cell>Uniform Log</cell><cell>8.62</cell><cell>13.48</cell><cell>13.28</cell></row><row><cell>Uniform</cell><cell>19.10</cell><cell>16.04</cell><cell>14.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Ablation study for alternative distance scores in the edge gating scheme on KITTI.</figDesc><table><row><cell>Method</cell><cell cols="3">AP 3D IOU? 0.7 Easy Mod. Hard</cell><cell cols="3">AP 3D IOU? 0.5 Easy Mod. Hard</cell></row><row><cell>3D bottoms</cell><cell>15.18</cell><cell>11.96</cell><cell>10.72</cell><cell>46.27</cell><cell>37.99</cell><cell>33.09</cell></row><row><cell>3D centers</cell><cell>21.04</cell><cell>16.07</cell><cell>14.89</cell><cell>47.03</cell><cell>37.58</cell><cell>32.97</cell></row><row><cell>2D centers</cell><cell>21.36</cell><cell>16.60</cell><cell>15.60</cell><cell>50.57</cell><cell>39.78</cell><cell>34.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Depth error statistics for predictions having corresponding matching ground truths.</figDesc><table><row><cell>Methods</cell><cell>Mean Abs. Error (m) ?</cell><cell>Mean Rel. Error ?</cell></row><row><cell>FCOS3D</cell><cell>0.0528</cell><cell>4.27%</cell></row><row><cell>PGD (Ours)</cell><cell>0.0483</cell><cell>3.63%</cell></row><row><cell>Rel. Delta</cell><cell>-8.5%</cell><cell>-15.0%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To make learning easier, the output of direct regression branch is applied an exponential transformation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by Centre for Perceptual and Interactive Intelligence Limited, in part by the GRF through the Research Grants Council of Hong Kong under Grants (Nos. 14208417, 14207319 and 14203518) and ITS/431/18FX, in part by CUHK Strategic Fund and CUHK Agreement TS1712093, in part by the Shanghai Committee of Science and Technology, China (Grant No. 20DZ1100800).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconfigurable voxels: A new representation for lidar-based point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ssn: Shape signature networks for multi-class object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Centerfusion: Center-based radar and camera fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ida-3d: Instance-depth-aware 3d object detection from stereo vision for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13015" to="13024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FCOS3D: Fully convolutional one-stage monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.11027" />
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Objects as points. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.07850" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.132</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Task-aware monocular depth estimation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection. CoRR, abs/1811.08188</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.08188" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.08070" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">MMDetection3D: OpenMMLab next-generation platform for general 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<title level="m">Center-based 3d object detection and tracking. CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards generalization across depth for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Flava: Find, localize, adjust and verify to annotate lidar-based point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology, UIST &apos;20 Adjunct</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Center3d: Center-based monocular 3d object detection with joint depth understanding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Savani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.13423" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
