<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Whole-Body Human Pose Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
							<email>jinsheng@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
							<email>xulumin@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
							<email>wangcan@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<email>liuwentao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Whole-Body Human Pose Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whole-body human pose estimation</term>
					<term>facial landmark de- tection</term>
					<term>hand keypoint estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates the task of 2D human whole-body pose estimation, which aims to localize dense landmarks on the entire human body including face, hands, body, and feet. As existing datasets do not have whole-body annotations, previous methods have to assemble different deep models trained independently on different datasets of the human face, hand, and body, struggling with dataset biases and large model complexity. To fill in this blank, we introduce COCO-WholeBody which extends COCO dataset with whole-body annotations. To our best knowledge, it is the first benchmark that has manual annotations on the entire human body, including 133 dense landmarks with 68 on the face, 42 on hands and 23 on the body and feet. A single-network model, named ZoomNet, is devised to take into account the hierarchical structure of the full human body to solve the scale variation of different body parts of the same person. ZoomNet is able to significantly outperform existing methods on the proposed COCO-WholeBody dataset. Extensive experiments show that COCO-WholeBody not only can be used to train deep models from scratch for whole-body pose estimation but also can serve as a powerful pre-training dataset for many different tasks such as facial landmark detection and hand keypoint estimation. The dataset is publicly available at https://github.com/jin-s13/COCO-WholeBody.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is important for the development of downstream applications, such as virtual reality, augmented reality, human mesh recovery, and action recognition.</p><p>In recent years, deep neural networks (DNNs) become popular for keypoint estimation. However, to our knowledge, existing datasets of human pose estimation do not have manual annotations of the entire human body. Therefore, previous works trained their models separately on different datasets of face, hand and human body. For example, OpenPose <ref type="bibr" target="#b7">[8]</ref> combines multiple DNNs trained independently on different datasets, including one DNN for body pose estimation on COCO [31], one DNN for face keypoint detection by combining many datasets (i.e. Multi-PIE <ref type="bibr" target="#b13">[14]</ref>, FRGC [44] and i-bug [49]), and another DNN for hand keypoint detection on Panoptic <ref type="bibr">[51]</ref>. These methods may have several drawbacks. First, the data size of the current in-the-wild datasets of 2D hand keypoints is limited. Most approaches of hand pose estimation have to use labrecorded <ref type="bibr" target="#b21">[56,</ref><ref type="bibr" target="#b12">13]</ref> or synthetic datasets <ref type="bibr">[34,</ref><ref type="bibr">35,</ref><ref type="bibr">50]</ref>, hampering the performance of the existing methods in real-world scenarios. Second, the variations such as illumination, pose and scales in the existing human face <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">26,</ref><ref type="bibr">29,</ref><ref type="bibr">33,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b34">69]</ref>, hand <ref type="bibr" target="#b21">[56,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">34]</ref>, and body datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">62,</ref><ref type="bibr">31]</ref> are different, inevitably introducing dataset biases to the learned deep networks, thus hindering the development of algorithms to comprehensively consider the task as a whole.</p><p>To address the above issues, we propose a novel large-scale dataset for wholebody pose estimation, named COCO-WholeBody, which fully annotates the bounding boxes of face and hand, as well as the keypoints of face, hand, and foot for the images from COCO <ref type="bibr">[31]</ref>. To our knowledge, this is the first dataset that has whole-body annotations. COCO-WholeBody enables us to take into account the hierarchical structure of the human body and the correlations between different body parts to estimate the entire body pose. Therefore, it enables the development of a more reliable human body pose estimator. In addition, it will also stimulate productive research on related areas such as face and hand detection, face alignment and 2D hand pose estimation. The effectiveness of COCO-WholeBody is validated by using cross-dataset evaluation, which demonstrates that COCO-WholeBody can be used as a powerful pre-training dataset for various tasks, such as facial landmark localization and hand keypoint estimation. We overview the cross-dataset evaluations as shown in <ref type="figure" target="#fig_0">Fig.1c</ref>., d.</p><p>The task of whole-body pose estimation has not been fully exploited in the literature because of missing a representative benchmark. Previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref> are mainly the bottom-up approaches, which simultaneously detect the keypoints for all persons in an image at once. They are generally efficient, however, they might suffer from scale variance of persons, causing inferior performance for small persons. Recent works <ref type="bibr" target="#b18">[53,</ref><ref type="bibr" target="#b30">65]</ref> found that the top-down alternatives would have higher accuracy, because top-down methods normalize the human instances to roughly the same scale and are less sensitive to the scale variance of different human instances. However, to our knowledge, there is no existing top-down approach for whole-body pose estimation. With COCO-WholeBody, we are able to fill in this blank by designing a top-down whole-body pose estimator. However, predicting all the keypoints for whole-body pose estimation will lead to inferior performance, because the scales of human body, face and hand are different. For example, human body pose estimation requires a large receptive field to handle occlusion and complex poses, while face and hand keypoint estimation requires higher image resolution for accurate localization. If all the keypoints are treated equally and directly predicted at once, the performance is suboptimal.</p><p>To solve this technical problem, we propose ZoomNet to effectively handle the scale variance in whole-body pose estimation. ZoomNet follows the top-down paradigm. Given a human bounding box of each person, ZoomNet first localizes the easy-to-detect body keypoints and estimates the rough position of hands and face. Then it zooms in to focus on the hand/face areas and predicts keypoints using features with higher resolution for accurate localization. Unlike previous approaches <ref type="bibr" target="#b6">[7]</ref> which usually assemble multiple networks, ZoomNet has a single network that is end-to-end trainable. It unifies five network heads including the human body pose estimator, hand and face detectors, and hand and face pose estimators into a single network with shared low-level features. Extensive experiments show that ZoomNet outperforms the state-of-the-arts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref> by a large margin, i.e. 0.541 vs 0.338 <ref type="bibr" target="#b6">[7]</ref> for whole-body mAP on COCO-WholeBody.</p><p>Our major contributions can be summarized as follows. (1) We propose the first benchmark dataset for whole-body human pose estimation, termed COCO- WholeBody, which encourages more exploration of this task. To evaluate the effectiveness of COCO-WholeBody, we extensively examine the performance of several representative approaches on this dataset. Also, the generalization ability of COCO-WholeBody is validated by cross-dataset evaluations, showing that COCO-WholeBody can serve as a powerful pre-training dataset for many tasks, such as facial landmark localization and hand keypoint estimation.</p><p>(2) We propose a top-down single-network model, ZoomNet to solve the scale variance of different body parts in a single person. Extensive experiments show that the proposed method significantly outperforms previous state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">2D Keypoint Localization Dataset</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, there are many datasets separately annotated for localizing the keypoints of body <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b27">62]</ref>, hand <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">34,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b21">56,</ref><ref type="bibr" target="#b32">67]</ref> or face <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">26,</ref><ref type="bibr">29,</ref><ref type="bibr">33,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b34">69]</ref>. These datasets are briefly discussed in this section. Body Pose Dataset. There have been several body pose datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">30,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b27">62]</ref>. COCO [31] is one of the most popular, which offers 17-keypoint annotations in uncontrolled conditions. Our COCO-WholeBody is an extension of COCO, with densely annotated 133 face/hand/foot keypoints. The task of whole-body pose estimation is more challenging, due to 1) higher localization accuracy required for face/hands and 2) scale variance between body and face/hands.</p><p>Hand Keypoint Dataset. Most existing 2D RGB-based hand keypoint datasets are either synthetic <ref type="bibr">[34,</ref><ref type="bibr" target="#b35">70]</ref> or captured in the lab environment <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b36">71]</ref>.</p><p>For example, Panoptic [51] is a well-known hand pose estimation dataset, recorded in the CMU's Panoptic studio with multiview dome settings. However, it is limited to a controlled laboratory environment with a simple background. One-Hand10K <ref type="bibr" target="#b25">[60]</ref> is a recent in-the-wild 2d hand pose dataset. However, the size is still limited. Our COCO-WholeBody is complementary to these RGB-based hand keypoint datasets. It contains about 100K 21-keypoint labeled hands and hand boxes that are captured in unconstrained environment. To the best of our knowledge, it is the largest in-the-wild dataset for 2D RGB-based hand keypoint estimation. It is very challenging, due to occlusion, hand-hand interaction, hand-object interaction, motion blur, and small scales.</p><p>Face Keypoint Dataset. Face keypoint datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">26,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b28">63]</ref> play a crucial role for the development of facial landmark detection a.k.a. face alignment. Among them, 300W [49] is the most popular. It is a combination of LFPW <ref type="bibr" target="#b3">[4]</ref>, AFW <ref type="bibr" target="#b34">[69]</ref>, HELEN [29], XM2VTS [33] with 68 landmarks annotated for each face image. Our proposed COCO-WholeBody follows the same annotation settings as 300W and 68 keypoints for each face are annotated. Compared to 300W, COCO-WholeBody is much larger and is more challenging as it contains more blurry and small-scale facial images (see <ref type="figure" target="#fig_5">Fig 5a.</ref>).</p><p>DensePose Dataset. Our work is also related to DensePose <ref type="bibr" target="#b0">[1]</ref> which provides a dense 3D surface-based representation for human shape. However, since the keypoints in DensePose are uniformly sampled, they lack specific joint articulation information and details of face/hands are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Keypoints Localization Method</head><p>Body Pose Estimation. Recent multi-person body pose estimation approaches can be divided into bottom-up and top-down approaches. Bottom-up approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">19,</ref><ref type="bibr">20,</ref><ref type="bibr">21,</ref><ref type="bibr">22,</ref><ref type="bibr">23,</ref><ref type="bibr">37,</ref><ref type="bibr">39,</ref><ref type="bibr">41,</ref><ref type="bibr">45]</ref> first detect all the keypoints of every person in images and then group them into individuals. Top-down methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">32,</ref><ref type="bibr">38,</ref><ref type="bibr">42,</ref><ref type="bibr" target="#b18">53,</ref><ref type="bibr" target="#b30">65]</ref> first detect the bounding boxes and then predict the human body keypoints in each box. By resizing and cropping, top-down approaches normalize the poses to approximately the same scale. Therefore, they are more robust to human-level scale variance and recent state-of-the-arts are obtained by top-down approaches. However, direct usage of the top-down methods for whole-body pose estimation will encounter the problem of scale variance of different body parts (body vs face/hand). To tackle this problem, we propose ZoomNet, a single-network top-down approach that zooms in to the hand/face regions and predicts the hand/face keypoints using higher image resolution for accurate localization.</p><p>Face/Hand/Foot Keypoint Localization. Previous works mostly treat the tasks of face/hand/foot keypoint localization independently and solve by different models. For facial keypoint localization, cascaded networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">55,</ref><ref type="bibr" target="#b23">58,</ref><ref type="bibr" target="#b31">66]</ref> and multi-task learning <ref type="bibr" target="#b22">[57,</ref><ref type="bibr" target="#b33">68]</ref> are widely adopted. For hand keypoint estimation, most work rely on auxiliary information such as depth information <ref type="bibr">[40,</ref><ref type="bibr">50,</ref><ref type="bibr" target="#b17">52]</ref> or multi-view <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">36]</ref> information. For foot keypoint estimation, Cao et al. <ref type="bibr" target="#b6">[7]</ref> proposed a generic bottom-up method. In this paper, we propose ZoomNet to solve the tasks of face/hand/foot keypoint localization as a whole. It takes into account the inherent hierarchical structure of the full human body to solve the scale variation of different parts in the same person.</p><p>Whole-Body Pose Estimation. Whole-body pose estimation has not been well studied in the literature due to the lack of a representative benchmark. OpenPose <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">51]</ref> applies multiple models (body keypoint estimator) to handle different kinds of keypoints. It first detects body and foot keypoints, and estimates the hand and face position. Then it applies extra models for face and hand pose estimation. Since OpenPose relies on multiple networks, it is hard to train and suffers from increased runtime and computational complexity. Unlike OpenPose, our proposed ZoomNet is a single-network method as it integrates five previously separated models (human body pose estimator, hand/face detectors, and hand/face pose estimators) into a single network with shared low- which encompasses the expressive power for body, hands, and facial expression. Their methods still rely on OpenPose <ref type="bibr" target="#b6">[7]</ref> to localize 2d body keypoints in images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COCO-WholeBody Dataset</head><p>COCO-WholeBody is the first large-scale dataset with the whole-body pose annotation available, to the best of our knowledge. In this section, we will describe the annotation protocols and some informative statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Annotation</head><p>We annotate the face, hand and foot keypoints on the whole train/val set of COCO [31] dataset and form the whole-body annotations with the original body keypoint labels together (see <ref type="figure" target="#fig_1">Fig. 2</ref>). For each person, we annotate 4 types of bounding boxes (person box, face box, left-hand box, and right-hand box) and 133 keypoints (17 for body, 6 for feet, 68 for face and 42 for hands). The face/hand box is defined as the minimal bounding rectangle of the keypoints. The keypoint annotations are illustrated in <ref type="figure" target="#fig_3">Fig. 3a</ref>. The face/hand boxes are labeled as valid, only if the face/hand images are clear enough for keypoint labeling. Invalid boxes may be blurry or severely occluded. We only label keypoints for valid boxes. Manual annotation for whole-body poses in an uncontrolled environment, especially for massive and dense hand and face keypoints, requires trained experts and enormous workload. As a rough estimate, the manual labeling cost of a professional annotator is up to: 10 min/face, 1.5 min/hand, and 10 sec/box. To speed up the annotation process, we follow the semi-automatic methodology to use a set of pre-trained models (for face and hand separately) to pre-annotate and then conduct manual correction. Foot keypoints are directly manually labeled, since its labeling cost is relatively low. Specifically, the annotation process contains the following steps:</p><p>1. For each individual person, we manually label the face box, the left-hand box, and the right-hand box. The validity of the boxes is also labeled. 2. Quality control. The annotation quality of the boxes is guaranteed through the strict quality inspection performed by another group of the annotators. 3. For each valid face/hand box, we use pre-trained face/hand keypoint detectors to produce pseudo keypoint labels. We use a combination of the publicly available datasets to train a robust face keypoint detector and a hand keypoint detector based on HRNetV2 <ref type="bibr" target="#b18">[53]</ref>. 4. Manual correction of pseudo labels and further quality control. About 28% of the hand keypoints and 6% of the face keypoints are labeled as invalid and manually corrected by human annotators. By using the semi-automatic annotation, we saw about 89% reduction in the time required for annotation. To measure the annotation quality, we also had 3 annotators to label the same batch of 500 images for face/hand/foot keypoints. The standard deviation of the human annotation is calculated for each keypoint (see <ref type="figure" target="#fig_4">Fig. 4a</ref>.), which is used to calculate the normalized factor of whole-body keypoint for evaluation. For "body keypoints", we directly use the standard deviation reported in COCO [31].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Protocol and Evaluation Metrics</head><p>The evaluation protocol of whole-body pose estimation follows the current practices in the literature <ref type="bibr">[31,</ref><ref type="bibr" target="#b27">62]</ref>. All algorithms are trained on COCO-WholeBody training set and evaluated on COCO-WholeBody validation set. We use mean Average Precision (mAP) and Average Recall (mAR) for evaluation, where Object Keypoint Similarity (OKS) is used to measure the similarity between the prediction and the ground truth poses. Invalid boxes and keypoints are masked out during both training and evaluation, thus not affecting the results. The ignored regions are masked out, and only visible keypoints are considered during evaluation. As shown in <ref type="figure" target="#fig_4">Fig. 4b</ref>., we also develop a tool for deeper performance analysis based on [48] which will be provided to facilitate offline evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Statistics</head><p>Dataset Size. COCO-WholeBody is a large-scale dataset with keypoint and bounding box annotations. The number of annotated keypoints as well as boxes of left hand (lhand), right hand (rhand), face and body are shown in <ref type="figure" target="#fig_3">Fig. 3b</ref>. About 130K face and left/right hand boxes are labeled, resulting in more than 800K hand keyponits and 4M face keypoints in total. Scale Difference. Distribution of the average keypoint distance of different parts in WholeBody Dataset is summarized in <ref type="figure" target="#fig_5">Fig. 5a</ref>. We calculate the distance between keypoint pairs in the tree-structured skeleton. Hand/face have obviously much smaller scales than body. The various scale distribution makes it challenging to localize keypoints of different human parts simultaneously.</p><p>Facial Image "Blurriness". Face image "blurriness" is a key factor for facial landmark localization. We choose a variation of the Laplacian method [43] to measure it. Specifically, an image is first converted into a grayscale image and resized into 112 ? 112. The log10 of the Laplacian of the converted image is used as the "blurriness" measurement (the higher the better). The distribution of the blurriness is shown in <ref type="figure" target="#fig_5">Fig. 5b</ref>. We find that most facial images fall in the interval between 1 and 3 and are clear enough for accurate keypoint localization. Compared with 300W [49], WholeBody has a larger variance of blurriness and contains more challenging images (blurriness &lt; 1).</p><p>Gesture Variances for Hands. We first normalize the 2D hand poses by rotating and scaling and then cluster them into three main categories: "fist", "palm" and "others". Unlike most previous hand datasets that are collected in constrained environments, our WholeBody-Hand is collected in-the-wild. Compared with Panoptic <ref type="bibr" target="#b12">[13]</ref>, WholeBody-Hand is more challenging as it contains a larger proportion of hand images grasping or holding objects.</p><p>Overall, COCO-WholeBody is a large-scale dataset with great diversity, which will not only promote researches on the whole-body pose estimation but also contribute to other related areas, such as face and hand keypoint estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ZoomNet: Whole-Body Pose Estimation</head><p>In this section, we will introduce our whole-body pose estimation pipeline. Given an RGB image, we follow <ref type="bibr" target="#b30">[65,</ref><ref type="bibr" target="#b18">53]</ref> to use an off-the-shelf FasterRCNN [46] human detector to generate human body candidates. For each human body candidate, ZoomNet localizes the whole-body keypoints. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, ZoomNet predicts body/foot keypoints and face/hand keypoints successively in a single network, consisting of the following submodules:</p><p>FeatureNet: the input image is processed by FeatureNet to extract shared features (F 1 and F 2). It consists of two convolutional layers, each of which downsamples the corresponding input to 1/2 resolution, and a bottleneck block for effective feature learning. The input image size is 384 ? 288 and the output feature map sizes for F 1 and F 2 are 192 ? 144 and 96 ? 72, respectively.</p><p>BodyNet: using the features extracted from FeatureNet, BodyNet predicts body/foot keypoints and face/hand bounding boxes at the same time. Each bounding box is represented by four corner points and one center point. In total, 38 keypoints are generated for each person simultaneously. BodyNet is a multiresolution network with 38 output channels.</p><p>HandHead and FaceHead: Using face and hand bounding boxes predicted by BodyNet, we crop the features in the corresponding areas from F1 and F2. The features from F1 are resized to 64 ? 64 and those from F2 are resized to 32 ? 32. Then HandHead and FaceHead are applied to predict the heatmaps of face/hand keypoints with the output resolution of 64 ? 64 in parallel.</p><p>ZoomNet can be based on any state-of-the-art network architecture. In our implementation, we choose HRNet-W32 <ref type="bibr" target="#b18">[53]</ref> as the backbone of BodyNet and HRNetV2p-W18 <ref type="bibr" target="#b19">[54]</ref> as the backbone of FaceHead/HandHead. Please refer to Supplementary for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Localizing body keypoints and face/hand boxes with BodyNet</head><p>Our face/hand box localization is inspired by CornerNet [28], which represents the object with keypoint pairs and designs a one-stage keypoint-based detector. In our case, each person has three types of bounding boxes to predict: the face box, the left-hand box, and the right-hand box. Four corner points and one center point are used to represent a box. We use 2D confidence heatmaps to encode both the human body keypoints and the corner keypoints. During inference, the bounding box is obtained by the closest bounding box of the 4 corner points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Face/hand keypoint estimation with HandHead and FaceHead</head><p>Given the face/hand bounding boxes predicted by BodyNet, RoIAlign <ref type="bibr" target="#b15">[16]</ref> is applied to extract the features of the face/hand areas from the feature maps In this way, we are able to preserve the high-resolution for the hand/face regions, and larger receptive fields for body keypoint estimation at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on COCO-WholeBody Dataset</head><p>To the best of our knowledge, there are only two existing approaches that target at the 2D whole-body pose estimation task, i.e. OpenPose <ref type="bibr" target="#b6">[7]</ref> and SN <ref type="bibr" target="#b16">[17]</ref>. To extensively evaluate the performance of the existing methods on the proposed COCO-WholeBody Dataset, we also build upon the existing multi-person human body pose estimation approaches, including both bottom-up (i.e. Partial Affinity Field (PAF) <ref type="bibr" target="#b7">[8]</ref> and Associate Embedding (AE) [37]) and top-down methods (i.e. HRNet <ref type="bibr" target="#b18">[53]</ref>), and adapt them to the more challenging whole-body pose estimation task using official codes (see Supplementary for more details). For fair comparisons, we retrain all methods on COCO-WholeBody and evaluate their performance with single-scale testing as shown in <ref type="table" target="#tab_1">Table 2</ref>. We show that our proposed ZoomNet outperforms them by a large margin.</p><p>Among these methods, SN <ref type="bibr" target="#b16">[17]</ref>, PAF <ref type="bibr" target="#b7">[8]</ref>, AE [37] and HRNet <ref type="bibr" target="#b18">[53]</ref> follow a onestage paradigm and predict all the keypoints simultaneously. Interestingly, we find that in the task of whole-body pose estimation, directly learning to predict all 133 keypoints simultaneously, including body, face, hand keypoints, may harm the original body keypoint estimation accuracy. In <ref type="table" target="#tab_1">Table 2</ref>, "-body" means that we only train the model on the original COCO-body keypoint (17 keypoints). We compare the body keypoint estimation results of the model learning the whole-body keypoints versus the model learning the body keypoints only. We observe considerable accuracy decrease by comparing PAF vs PAF-body (-14.3% mAP and -14.2% mAR), AE vs AE-body(-17.7% mAP and -17.0% mAR) and HRNet vs HRNet-body(-9.9% mAP and -10.0% mAR). In comparison, our proposed ZoomNet uses a two-stage framework, which decouples the body keypoint estimation and face/hand keypoint estimation. The accuracy of body keypoint estimation is less affected (-1.5% mAP and -0.7% mAR).</p><p>HRNet <ref type="bibr" target="#b18">[53]</ref> can be viewed as the one-stage alternative of ZoomNet, since they share the same network backbone (HRNet-W32). ZoomNet significantly outperforms HRNet by 10.9% mAP and 13.8% mAR, demonstrating the effectiveness of the "zoom-in" design for solving the scale variation.</p><p>OpenPose <ref type="bibr" target="#b6">[7]</ref> is a multi-model approach, where the hand/face model and the body model are not jointly trained, leading to sub-optimal results. In addition, the hand/face boxes of OpenPose are roughly estimated by hand-crafted rules from the estimated body keypoints. Therefore, the accuracy of the hand/face boxes is limited, which will hinder hand/face pose estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-dataset Evaluation</head><p>In this section, we show that the proposed COCO-WholeBody is complementary to other separately labeled benchmarks by evaluating its generalization ability.</p><p>WholeBody-Face (WBF) Dataset. We build WholeBody-Face (WBF) by extracting cropped face images/annotations from COCO-WholeBody. We conduct experiments on 300W [49] benchmark. We follow the common settings <ref type="bibr" target="#b19">[54]</ref> to train models on 3,148 training images, validate on the "common" set and evaluate on the "challenging", "full" and "test" sets. We use the normalized mean error (NME) for evaluation and inter-ocular distance as normalization. The results are shown in <ref type="table" target="#tab_2">Table 3a</ref>. HR-Ours is our implementation of HRNetV2-W18 <ref type="bibr" target="#b19">[54]</ref> (HR). * HR-Ours is obtained by training HR on WBF only and directly testing on 300W, which already outperforms RCN <ref type="bibr">[18]</ref>. After finetuning on 300W, it gets significantly better performance on "challenging" (4.73 vs 5.15), "full" (3.21 vs 3.33) and "test" (3.68 vs 3.91) than the prior arts.</p><p>WholeBody-Hand (WBH) Dataset. For hand pose estimation, we experiment with HRNetV2-W18 (HR) on CMU Panoptic [51] (Pano.), which is a standard benchmark for hand keypoint localization. We randomly split Pano [51] by a rule of 70%-30% for training and validation. We report both the end-pointerror (EPE) and the normalized mean error (NME) for evaluation. In NME, the hand bounding box is used as normalization. As shown in <ref type="table" target="#tab_2">Table 3b</ref>, we analyze the generalization ability of WholeBody-Hand (WBH) by comparing the (1) HR trained on Pano., (2) HR pretrained on WBH and then finetuned on Pano., (3) HR trained on WBH, and (4) HR pretrained on Pano. and then finetuned on WBH. Comparing #1 and #2, we observe that pretraining on WBH brings about 6.5% improvement (from 7.49 to 7.00) in EPE on Pano. Comparing #1 and #3, we find that WBH vs Pano. is (6.66 vs 0.68) NME and (2.76 vs 7.49) EPE, when training/testing on its own dataset. This implies that the proposed WBH is much more challenging and that hand scales in WBH are smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>Effect of the bounding box accuracy on the keypoint estimation. We experiment by replacing our predicted face/hand bounding boxes with the groundtruth bounding boxes and re-run our FaceHead/HandHead of ZoomNet to obtain the final face/hand keypoint detection result. As shown in table 4a, using ground truth bounding boxes (Oracle) significantly improves the mAP of face/hand/whole-body by 19.6%, 8.4% and 23.6% respectively. Effect of the person scale on whole-body pose estimation. As shown in <ref type="table" target="#tab_3">Table 4b</ref>, we investigate the effect of person scales. Interestingly, for bottomup whole-body methods (PAF, SN and AE), the mAP for medium scale is worse than that of large scale, since they are more sensitive to the scale variance and are difficult in detecting smaller-scale people. For top-down approaches such as HRNet and ZoomNet, mAP for medium scale is better, since larger-scale person requires relatively more accurate keypoint localization. For ZoomNet, the gap between the medium and large person scale is about 7.5% mAP and 4.2% mAR.</p><p>Effect of blurriness and poses on facial landmark detection. In Table. 5, we evaluate the performance on different levels of image blurriness and facial poses (yaw angles) on WBF. The model is significantly affected by image blur (2.51 vs 19.13), while more robust to different face poses (9.02 vs 13.77).</p><p>Effect of hand poses on hand keypoint estimation. As shown in Table. 5, we evaluate the performance on different hand poses (fist, palm or others)  on WBH (NME). We show that estimating the poses of "palm" or "others" (with various gestures) is more challenging than that of "fist" (with similar patterns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed the first large-scale benchmark for whole-body human pose estimation. We extensively evaluate the performance of the existing approaches on our proposed COCO-WholeBody Dataset. Cross-dataset evaluation also demonstrates the generalization ability of the proposed dataset. Moreover, to solve the problem of extreme scale difference among body parts, ZoomNet is proposed to pay more attention to the hard-to-detect face/hand keypoints. Experiments show that ZoomNet significantly outperforms the prior arts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Annotation Details</head><p>The annotation of face/hand keypoints in our COCO-WholeBody dataset follows semi-automatic methodology. Firstly, face/hand bounding boxes are annotated manually. Secondly, we utilize a face model and a hand model, which are trained on large-scale face datasets and hand datasets respectively, to pre-annotate the face and hand keypoints. Next, manual correction of the face/hand keypoints is conducted. Foot keypoints are directly manually labeled. Note that, quality inspections are conducted in every step.</p><p>Face and hand bounding box: To ensure the quality of face/hand bounding boxes, well-defined standards are followed. Face bounding box is labeled only if the box is bigger than 8 pixels and the rotation angle of the face is less than 100 ? from the frontal view. As for some special cases, faces of real persons in photos, posters, and clothes are labeled but faces of sculptures, models, cartoons, paintings, and animals are not. The face bounding box is defined as the minimal bounding rectangle of the face keypoints. Quality inspections are conducted by another group of annotators and bounding boxes whose positions are inaccurate are re-annotated. Hand bounding box is labeled when the hand image is vivid and the position of the hand keypoints can be well-determined. The box is regarded as invalid if the corresponding hand is severely occluded or part of the hand is out of the image. Special case settings follow those of face bounding box and independent quality inspections are conducted. Examples of face/hand bounding boxes are shown in <ref type="figure" target="#fig_9">Fig. 7</ref>, where only the green boxes meet our annotation requirements. More visualization results for bounding boxes are demonstrated in <ref type="figure" target="#fig_10">Fig. 8</ref> Line#1. We have three types of bounding boxes, i.e. body (green), face (purple), left hand (blue) and right hand (red).</p><p>Face Keypoints: We apply the 68-joint face model <ref type="bibr">[49]</ref> as shown in <ref type="figure" target="#fig_9">Fig. 7(b)</ref>. A few occluded keypoints may be estimated by annotators if most keypoints are visible in the image. In <ref type="figure" target="#fig_10">Fig. 8</ref>, Line#2 and Line#3 visualize more examples of the face keypoint annotations.</p><p>Hand Keypoints: Self-occlusion is very common for hand keypoints. As a result, the annotation for hand keypoints requires trained experts and enormous workload although pseudo labels are given. We use 21-joint hand model [51] and annotate quite a lot of challenging cases. Annotation is shown in <ref type="figure" target="#fig_9">Fig. 7(c)</ref> and more examples are visualized in <ref type="figure" target="#fig_10">Fig. 8</ref>, where Line#4 and Line#5 visualize some examples of the hand keypoint annotations for various hand poses.</p><p>Foot Keypoints: Six foot keypoints are defined following <ref type="bibr" target="#b6">[7]</ref>. The order in the annotation file is as follows: left big toe, left small toe, left heel, right big toe, right small toe, and right heel. The keypoints are defined in the inner center rather than on the surface to fit in images in different views. Qualitative examples are shown in <ref type="figure" target="#fig_9">Fig. 7(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Baseline Implementation Details</head><p>We used the official codes to reproduce existing methods. We keep all training parameters (e.g. input size, #iterations, learning rate, and so on) the same, except #keypoints (# means the number of). We also trained all the existing methods on the original 17-keypoint COCO dataset and verified that our reimplementation is the same as the original papers. For fair comparisons, all experimental results are obtained with single-scale testing. The implementation details of the baseline methods we used in the experiments are listed as following:</p><p>OpenPose Whole-body System <ref type="bibr" target="#b6">[7]</ref> is a Multi-Network whole-body pose estimation system, which consists of a body keypoint model, a facial landmark detector and a hand pose estimator. We reimplement the approach by training these models on COCO-WholeBody dataset separately based on the official training codes 1 .</p><p>Single-Network Whole-body Pose Estimation <ref type="bibr" target="#b16">[17]</ref> is a recently proposed method for whole-body pose estimation. We follow <ref type="bibr" target="#b16">[17]</ref> and retrain the whole-body keypoint estimator 2 in our COCO-WholeBody dataset. The number of keypoints is 133, and the number of PAFs is 134 as we designed a tree structure except for the two loops around the lips. Face, hand and foot keypoints are connected to the corresponding nearest body keypoints. Following <ref type="bibr" target="#b16">[17]</ref>, we applied 3 stages for PAF and 1 stage for confidence maps. We use a batch size of 10 images in each GPU and Adam optimization with an initial learning rate of 1e-3 to train the model.</p><p>Part-affinity Fields (PAF) <ref type="bibr" target="#b7">[8]</ref> is also re-implemented for the whole-body pose estimation task based on the open-source codes 3 . The settings of PAFs and confidence maps are the same as Single-Network <ref type="bibr" target="#b16">[17]</ref> and CPM <ref type="bibr" target="#b26">[61]</ref> network  is used as its backbone. We use SGD with an initial learning rate of 1 to train the model. Note that, the direction of limb (or value of the affinity fields) is calculated in the image scale before down-sampling, see <ref type="figure" target="#fig_11">Fig. 9</ref>. Therefore, for most tiny hands and faces, the PAF prediction and keypoint grouping will not be affected.</p><p>Associative Embedding (AE) [37] learns to group keypoints by associative embedding, which is flexible in terms of various numbers of keypoints to predict. The official open-source codes 4 are used in our implementation. We use the 4-stacked hourglass backbone and follow the same training settings as in <ref type="bibr">[37]</ref> in our experiments. <ref type="table">Table 6</ref>: Body-foot AP on COCO-foot benchmark <ref type="bibr" target="#b6">[7]</ref>. Some results are copied from <ref type="bibr" target="#b16">[17]</ref>. Our proposed ZoomNet outperforms SN significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Body AP Foot AP HRNet <ref type="bibr" target="#b18">[53]</ref> is the recent state-of-the-art model for the task of multi-person human pose estimation. We retrain the model 5 to fit for the whole-body pose estimation task by directly adding the number of keypoints to 133. For fair comparisons, we choose HRNet-w32 as the backbone in the experiments. Note that this model can be viewed as the single-stage alternative of our multi-stage ZoomNet. The comparison between HRNet and ZoomNet demonstrates the effectiveness of the multi-stage keypoint localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ZoomNet Implementation Details</head><p>We use 2D gaussian confidence heatmaps with ? = 3 to encode the keypoint locations. The sum of squared error (SSE) loss function between the predicted heatmaps and the ground truth heatmaps is used for training both corner keypoints and body keypoints. The losses of different body parts (body, face, hand, and feet) are summed up with the same loss weight.</p><p>We follow the same setting as HRNet <ref type="bibr" target="#b18">[53]</ref> to use data augmentation with random scaling ([-35%, 35%]), random rotation ([?45 ? , 45 ? ]) and flipping. BodyNet and FaceHead/HandHead are first pre-trained separately and then end-to-end finetuned as a whole for 120 epochs in total. ZoomNet is trained on 8 GPUs with a batch size of 32 in each GPU. We use Adam [25] with the base learning rate of 1e-3, and decay it to 1e-4 and 1e-5 at the 80th and 100th epochs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis</head><p>Experiments on Foot Keypoint Dataset Cao et al. released the first human foot dataset <ref type="bibr" target="#b6">[7]</ref> (COCO-foot), which extends COCO [31] dataset with 15k foot annotations. We also evaluate our proposed ZoomNet on COCO-foot dataset and directly compare with OpenPose <ref type="bibr" target="#b6">[7]</ref> and SN <ref type="bibr" target="#b16">[17]</ref> in <ref type="table">Table 6</ref>. We find that our proposed ZoomNet outperforms SN significantly. usually treat body/face/hands as normal objects and detect all of them at once. However, note that the human body is inherently a multi-level structure, where the face/hands are low-level objects of the high-level human body. Intuitively, the location of the human body will guide the detection of face/hands. Common detection methods usually ignore the inherent correlation between the human body and the face/hands, which will lead to inferior performance. To deal with the scale variance problem, ZoomNet first locates all the person bounding boxes from the image and then detects the face and hands in each bounding box. This multi-level design enables the model to focus on the potential location of the sub-objects and ignore the disturbing background. Therefore, it is beneficial for detecting small sub-objects such as face and hands. As shown in <ref type="table" target="#tab_7">Table 8</ref>, ZoomNet outperforms the Faster RCNN model by a large margin, demonstrating the effectiveness of our multi-level object detection.</p><p>Error Analysis In this section, we provide a more detailed error analysis for ZoomNet and Single-Network <ref type="bibr" target="#b16">[17]</ref>. The breakdown of errors over different body parts is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. We follow [48] to define four types of localization errors, i.e. Jitter, Miss, Inversion, and Swap. Jitter means small error around the correct keypoint location, while Miss means the detection is not within the proximity of any ground truth body part. Inversion means the joint type of  detected keypoint is wrong. Swap means the detected keypoint is grouped to a wrong person instance. On the other hand, Good indicates correct prediction. We use the pie chart to show the distribution of the localization errors for the body, face, hand, and whole-body. Miss is the major error for all parts, and the accuracy of the hand keypoints is lower than that of the body and face keypoints. Also, ZoomNet has a higher proportion of Good keypoints than Single-Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size Sensitivity</head><p>In this section, we analyze the sensitivity of our proposed ZoomNet to different person sizes. To this end, we separate the COCO-WholeBody dataset into four size groups: i.e. medium (M), large (L), extra-large (XL) and extra-extra large (XX). We follow [48] to use the area of the person to measure the person size, M for area ? [32 2 , 64 2 ], L for area ? [64 2 , 96 2 ], XL for area ? [96 2 , 128 2 ], and XX for area &gt; 128 2 . In <ref type="figure" target="#fig_0">Fig. 11</ref>, we show the AP improvement obtained after correcting each type of localization error. We find that for body and face keypoint localization, the performance can be significantly improved by correcting small-scale human poses, especially the Missing error. <ref type="figure" target="#fig_0">Fig. 11</ref>: The AP improvement obtained by correcting each type of error (including Miss, Swap, Inversion, and Jitter) for body, face, and hand separately. We use the dashed red lines to indicate performance improvement over all the instance sizes.</p><p>For hand pose estimation, errors impact performance more on larger instances. For larger-scale instance, instead of only estimating the rough position, more accurate keypoint localization is required. However, due to the frequent motion blur and severe occlusion (interaction with objects), it is still very challenging to estimating the hand poses of large instances.</p><p>Qualitative Analysis <ref type="figure" target="#fig_0">Fig. 12</ref> shows the qualitative evaluation results of our approach, and <ref type="figure" target="#fig_0">Fig. 13</ref> qualitatively compares the results of ZoomNet, Open-Pose <ref type="bibr" target="#b6">[7]</ref> and Single-Network <ref type="bibr" target="#b16">[17]</ref>. Both of them show the capacity of our proposed ZoomNet in handling challenges including occlusion, close proximity, and small scale persons. We find that our ZoomNet significantly outperforms the previous state-of-the-art method <ref type="bibr" target="#b16">[17]</ref>, especially for face/hand keypoints. First, we observe that compared to these bottom-up approaches, ZoomNet better handles the small scale problem of human instances (see Line#1,2,3). Second, we find that the grouping of OpenPose <ref type="bibr" target="#b6">[7]</ref> and Single-Network <ref type="bibr" target="#b16">[17]</ref> is sometimes erroneous due to lack of human body constraints (see Line#4). Third, Zoom-Net is generally better at localizing the hand/face keypoints with occlusion, pose variations, and small scales (see Line#6,7). ZoomNet improves upon the state-of-the-art methods by zooming in to the hand area for higher resolution. However, we also find some failure cases of our proposed ZoomNet. We observe that it still has difficulty in dealing with small face/hands with low-resolution and motion blur.  and Single-Network <ref type="bibr" target="#b16">[17]</ref>. Our approach outperforms the state-of-the-art approaches especially on face/hand keypoints and are more robust to scale variance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The proposed COCO-WholeBody dataset provides manual annotations of dense landmarks on the entire human body including body, face, hands, and feet. (a) visualizes an image as an example. The whole-body human pose estimation is challenging because different body parts have different variations such as scale. (b) shows that ZoomNet significantly outperforms the prior arts on this challenging task. (c) and (d) show that existing facial/hand landmark estimation algorithms can be improved by pretraining on COCO-WholeBody.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Annotation examples for face/hand keypoints in COCO-WholeBody.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>level features. Recently, Hidalgo et al. proposes an elegant method SN [17] for bottom-up whole-body keypoint estimation. SN is based on PAF [8] which predicts the keypoint heatmaps for detection and part affinity maps for grouping. Since there exists no such dataset with whole-body annotations, they used a set of different datasets and carefully designed the sampling rules to train the model. However, bottom-up approaches cannot handle scale variation problem well and would have difficulty in detecting face and hand keypoints accurately. In comparison, our ZoomNet is a top-down approach that well handles the extreme scale variance problem. Recent works [24,47,64] also explore the task of monocular 3D whole-body capture. Romero et al. proposes a generative 3D model [47] to express body and hands. Xiang et al. introduces a 3D deformable human model [64] to reconstruct whole-body pose and Joo et al. presents Adam [24]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a) COCO-WholeBody annotation for 133 keypoints. (b)Statistics of COCO-WholeBody. The number of annotated keypoints and boxes of left hand (lhand), right hand (rhand), face and body are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>(a) The normalized standard deviation of manual annotation for each keypoint. Body keypoints have larger manual annotation variance than face and hand keypoints. (b) An example of error diagnosis results of ZoomNet for wholebody pose estimation: jitter, inversion, swap and missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>COCO-WholeBody is challenging as it contains (a) large "scale variance" of body/face/hand, measured by the average keypoint distance, (b) more blurry face images than 300W and (c) more complex hand poses than Panoptic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>ZoomNet is a single-network model, which consists of FeatureNet, Bo-dyNet and Face/HandHead. FeatureNet extracts low-level shared features for BodyNet and Face/HandHead. BodyNet predicts body/foot keypoints and the approximate regions of face/hands, while Face/HandHead zooms in to these regions and predict face/hand keypoints with features of higher resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F</head><label></label><figDesc>1 and F 2 of FeatureNet. The corresponding visual features are cropped and up-scaled to a higher resolution. With the extracted features, HandHead and FaceHead are used for face and hand keypoint estimation. HandHead and Face-Head use the same network architecture (HRNet-W18). The features extracted by RoIAlign are processed by the HandHead and FaceHead separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Acknowledgement.</head><label></label><figDesc>This work is partially supported by the SenseTime Donation for Research, HKU Seed Fund for Basic Research, Startup Fund, General Research Fund No.27208720, the Australian Research Council Grant DP200103223 and Australian Medical Research Future Fund MRFAI000085. 18. Honari, S., Yosinski, J., Vincent, P., Pal, C.: Recombinator networks: Learning coarse-to-fine feature aggregation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 19. Insafutdinov, E., Andriluka, M., Pishchulin, L., Tang, S., Levinkov, E., Andres, B., Schiele, B., Campus, S.I.: Arttrack: Articulated multi-person tracking in the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 20. Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka, M., Schiele, B.: Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In: Proceedings of the European Conference on Computer Vision (ECCV) (2016) 21. Iqbal, U., Milan, A., Gall, J.: Pose-track: Joint multi-person pose estimation and tracking. arXiv preprint arXiv:1611.07727 (2016) 22. Jin, S., Liu, W., Ouyang, W., Qian, C.: Multi-person articulated tracking with spatial and temporal embeddings. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 23. Jin, S., Ma, X., Han, Z., Wu, Y., Yang, W., Liu, W., Qian, C., Ouyang, W.: Towards multi-person pose tracking: Bottom-up and top-down methods. In: IEEE International Conference on Computer Vision Workshop (2017) 24. Joo, H., Simon, T., Sheikh, Y.: Total capture: A 3d deformation model for tracking faces, hands, and bodies. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 25. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 26. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. In: IEEE International Conference on Computer Vision Workshop (2011) 27. Kowalski, M., Naruniec, J., Trzcinski, T.: Deep alignment network: A convolutional neural network for robust face alignment. In: IEEE Conference on Computer Vision and Pattern Recognition Workshop (2017) 28. Law, H., Deng, J.: Cornernet: Detecting objects as paired keypoints. In: Proceedings of the European Conference on Computer Vision (ECCV) (2018) 29. Le, V., Brandt, J., Lin, Z., Bourdev, L., Huang, T.S.: Interactive facial feature localization. In: Proceedings of the European Conference on Computer Vision (ECCV) (2012) 30. Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.S., Lu, C.: Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 10863-10872 (2019) 31. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Proceedings of the European Conference on Computer Vision (ECCV) (2014) 32. Liu, W., Chen, J., Li, C., Qian, C., Chu, X., Hu, X.: A cascaded inception of inception network with attention modulated feature fusion for human pose estimation. In: The Thirty-Second AAAI Conference on Artificial Intelligence (2018) 33. Messer, K., Matas, J., Kittler, J., Luettin, J., Maitre, G.: Xm2vtsdb: The extended m2vts database. In: Second international conference on audio and video-based biometric person authentication (1999) 34. Mueller, F., Bernard, F., Sotnychenko, O., Mehta, D., Sridhar, S., Casas, D., Theobalt, C.: Ganerated hands for real-time 3d hand tracking from monocular rgb. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 35. Mueller, F., Mehta, D., Sotnychenko, O., Sridhar, S., Casas, D., Theobalt, C.: Real-time hand tracking under occlusion from an egocentric rgb-d sensor. In: Proceedings of International Conference on Computer Vision (ICCV) (2017) 36. Neverova, N., Wolf, C., Taylor, G.W., Nebout, F.: Multi-scale deep learning for gesture detection and localization. In: Proceedings of the European Conference on Computer Vision (ECCV) (2014) 37. Newell, A., Huang, Z., Deng, J.: Associative embedding: End-to-end learning for joint detection and grouping. In: Advances in Neural Information Processing Systems (2017) 38. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose estimation. In: Proceedings of the European Conference on Computer Vision (ECCV) (2016) 39. Nie, X., Feng, J., Xing, J., Yan, S.: Generative partition networks for multi-person pose estimation. arXiv preprint arXiv:1705.07422 (2017) 40. Oikonomidis, I., Kyriazis, N., Argyros, A.A.: Tracking the articulated motion of two strongly interacting hands. In: IEEE Conference on Computer Vision and Pattern Recognition (2012) 41. Papandreou, G., Zhu, T., Chen, L.C., Gidaris, S., Tompson, J., Murphy, K.: Personlab: Person pose estimation and instance segmentation with a bottom-up, partbased, geometric embedding model. arXiv preprint arXiv:1803.08225 (2018) 42. Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C., Murphy, K.: Towards accurate multi-person pose estimation in the wild. arXiv preprint arXiv:1701.01779 (2017) 43. Pech-Pacheco, J. L., C., G., Chamorro-Martinez, J., Fernndez-Valdivia, J.: Diatom autofocusing in brightfield microscopy: a comparative study. In: ICPR (2000) 44. Phillips, P.J., Flynn, P.J., Scruggs, T., Bowyer, K.W., Chang, J., Hoffman, K., Marques, J., Min, J., Worek, W.: Overview of the face recognition grand challenge. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2005) 45. Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, P.V., Schiele, B.: Deepcut: Joint subset partition and labeling for multi person pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 46. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing Systems (NIPS) (2015) 47. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics (ToG) (2017) 48. Ronchi, M.R., Perona, P.: Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation. Proceedings of International Conference on Computer Vision (ICCV) (2017) 49. Sagonas, C., Tzimiropoulos, G., Zafeiriou, S., Pantic, M.: 300 faces in-the-wild challenge: The first facial landmark localization challenge. In: IEEE International Conference on Computer Vision Workshop (2013) 50. Sharp, T., Keskin, C., Robertson, D., Taylor, J., Shotton, J., Kim, D., Rhemann, C., Leichter, I., Vinnikov, A., Wei, Y., et al.: Accurate, robust, and flexible real-time hand tracking. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (2015) 51. Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single images using multiview bootstrapping. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Face/hand bounding box annotation. Bounding boxes should tightly enclose all the keypoints. Positive (green) and negative (orange) cases are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 :</head><label>8</label><figDesc>Annotation examples. Line #1: We use different colors to distinguish different types of bounding boxes, i.e. body (green), face (purple), left hand (blue) and right hand (red). Line #2 and Line#3: Face keypoints. Line #4 and Line#5: Hand keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>Visualizations of Part-affinity Fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 :</head><label>10</label><figDesc>Localization error comparison between our proposed ZoomNet (top) and Single-Network [17] (bottom). ZoomNet significantly outperforms Single-Network in the distribution of the localization error for body, face, hand and whole-body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>Qualitative evaluation results of our approach in handling challenges including occlusion, close proximity, and small scale persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 :</head><label>13</label><figDesc>Qualitative comparison between our proposed ZoomNet, OpenPose<ref type="bibr" target="#b6">[7]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of some popular public datasets for 2D keypoint estimation in RGB images. Kpt stands for keypoints, and #Kpt means the annotated number. "Wild" denotes whether the dataset is collected in-the-wild. * means head box.</figDesc><table><row><cell>DataSet</cell><cell cols="4">Images #Kpt Wild Body Hand Face Body Hand Face Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Box Box Box Kpt Kpt Kpt Instances</cell></row><row><cell>MPII [3]</cell><cell>25K</cell><cell>16</cell><cell>*</cell><cell>40K</cell></row><row><cell>MPII-TRB [10]</cell><cell>25K</cell><cell>40</cell><cell>*</cell><cell>40K</cell></row><row><cell>CrowdPose [30]</cell><cell>20K</cell><cell>14</cell><cell></cell><cell>80K</cell></row><row><cell>PoseTrack [2]</cell><cell>23K</cell><cell>15</cell><cell></cell><cell>150K</cell></row><row><cell cols="2">AI Challenger [62] 300K</cell><cell>14</cell><cell></cell><cell>700K</cell></row><row><cell>COCO [31]</cell><cell>200K</cell><cell>17</cell><cell>*</cell><cell>250K</cell></row><row><cell cols="2">OneHand10K [60] 10K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>SynthHand [35]</cell><cell>63K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>RHD [70]</cell><cell>41K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>FreiHand [71]</cell><cell>130K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>MHP [13]</cell><cell>80K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>GANerated [34]</cell><cell>330K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>Panoptic [51]</cell><cell>15K</cell><cell>21</cell><cell></cell><cell>-</cell></row><row><cell>WFLW [63]</cell><cell>10K</cell><cell>98</cell><cell></cell><cell>-</cell></row><row><cell>AFLW [26]</cell><cell>25K</cell><cell>19</cell><cell></cell><cell>-</cell></row><row><cell>COFW [5]</cell><cell>1852</cell><cell>29</cell><cell></cell><cell>-</cell></row><row><cell>300W [49]</cell><cell>3837</cell><cell>68</cell><cell></cell><cell>-</cell></row><row><cell cols="3">COCO-WholeBody 200K 133</cell><cell></cell><cell>250K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Whole-body pose estimation results on COCO-WholeBody dataset. For fair comparisons, results are obtained using single-scale testing. Model complexity analysis. The model complexity of ZoomNet is 27.36G Flops. By contrast, the model complexity of OpenPose [7] is 451.09G Flops in total (137.52G for BodyNet, 106.77G for FaceNet and 103.40 ? 2 = 206.80G for HandNet), and that of SN [17] is 272.30G Flops. We also report the average runtime cost on COCO-WholeBody on one GTX-1080 GPU. SN is about 215.5ms/image, while ZoomNet is about 174.7ms/image on average (including a Faster RCNN human detector which takes about 106ms/image).</figDesc><table><row><cell>Method</cell><cell cols="2">body</cell><cell cols="2">foot</cell><cell cols="2">face</cell><cell cols="2">hand</cell><cell cols="2">whole-body</cell></row><row><cell></cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell></row><row><cell>OpenPose [7]</cell><cell cols="10">0.563 0.612 0.532 0.645 0.482 0.626 0.198 0.342 0.338 0.449</cell></row><row><cell>SN [17]</cell><cell cols="10">0.280 0.336 0.121 0.277 0.382 0.440 0.138 0.336 0.161 0.209</cell></row><row><cell>PAF [8]</cell><cell cols="10">0.266 0.328 0.100 0.257 0.309 0.362 0.133 0.321 0.141 0.185</cell></row><row><cell>PAF-body [8]</cell><cell cols="2">0.409 0.470</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AE [37]</cell><cell cols="10">0.405 0.464 0.077 0.160 0.477 0.580 0.341 0.435 0.274 0.350</cell></row><row><cell>AE-body [37]</cell><cell cols="2">0.582 0.634</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNet [53]</cell><cell cols="10">0.659 0.709 0.314 0.424 0.523 0.582 0.300 0.363 0.432 0.520</cell></row><row><cell cols="3">HRNet-body [53] 0.758 0.809</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ZoomNet</cell><cell cols="10">0.743 0.802 0.798 0.869 0.623 0.701 0.401 0.498 0.541 0.658</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>(a) Facial landmark localization (NME) on 300W: "common" (for val), "challenging", "full" and "test". * means only training on WBF. ? means lower is better. (b) Cross-dataset evaluation results of HR. Different training and testing settings are evaluated on two datasets: WBH and Panoptic (Pano.) [51]. extra. comm. ? chall. ? full ? test ?</figDesc><table><row><cell>RCN [18]</cell><cell>-</cell><cell>4.67</cell><cell>8.44 5.41</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DAN [27]</cell><cell>-</cell><cell>3.19</cell><cell cols="2">5.24 3.59 4.30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DCFE [59] w/3D 2.76</cell><cell cols="2">5.22 3.24 3.88</cell><cell>#</cell><cell>Train-set</cell><cell cols="3">Test-set EPE ? NME ?</cell></row><row><cell>LAB [63]</cell><cell>w/B</cell><cell>2.98</cell><cell>5.19 3.49</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HR [54]</cell><cell>-</cell><cell>2.87</cell><cell cols="2">5.15 3.32 3.85</cell><cell>1</cell><cell>Pano.</cell><cell>Pano.</cell><cell>7.49</cell><cell>0.68</cell></row><row><cell>*  HR-Ours</cell><cell>-</cell><cell>4.61</cell><cell cols="2">7.50 5.17 5.66</cell><cell cols="4">2 WBH ? Pano. Pano. 7.00</cell><cell>0.63</cell></row><row><cell>HR-Ours</cell><cell>-</cell><cell>2.89</cell><cell cols="2">5.15 3.33 3.91</cell><cell>3</cell><cell>WBH</cell><cell>WBH</cell><cell>2.76</cell><cell>6.66</cell></row><row><cell cols="2">HR-Ours WBF</cell><cell>2.84</cell><cell cols="2">4.73 3.21 3.68</cell><cell cols="4">4 Pano. ? WBH WBH 2.70</cell><cell>6.49</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of bounding box accuracy on keypoint estimation, where Oracle means using gt boxes. (b) Effect of person scales on whole-body pose estimation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>mAP</cell><cell>mAR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">medium large medium large</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PAF [8]</cell><cell cols="2">0.100 0.220 0.113 0.284</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SN [17]</cell><cell cols="2">0.117 0.252 0.132 0.315</cell></row><row><cell>Method</cell><cell>face</cell><cell>hand</cell><cell>whole-body</cell><cell>AE [37]</cell><cell cols="2">0.190 0.401 0.241 0.499</cell></row><row><cell></cell><cell cols="3">AP AR AP AR AP AR</cell><cell cols="3">OpenPose [7] 0.398 0.302 0.425 0.484</cell></row><row><cell cols="4">Oracle 0.819 0.854 0.485 0.578 0.777 0.856</cell><cell>HRNet [53]</cell><cell cols="2">0.471 0.410 0.538 0.497</cell></row><row><cell>Ours</cell><cell cols="3">0.623 0.701 0.401 0.498 0.541 0.658</cell><cell>Ours</cell><cell cols="2">0.594 0.519 0.677 0.635</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>? 2 2 ? 3 &gt; 3 ALL &lt; 15 ? 15 ? ? 30 ? 30 ? ? 45 ? &gt; 45 ? ALLfist palm others ALL 19.13 10.85 4.91 2.51 10.17 9.02 10.56 12.10 13.77 10.17 6.09 7.10 6.33 6.66</figDesc><table><row><cell>WBF (NME ?)</cell><cell></cell><cell>WBH (NME ?)</cell></row><row><cell>Blurriness</cell><cell>Yaw Angles</cell><cell>Pose</cell></row><row><cell>&lt; 1 1</cell><cell></cell><cell></cell></row></table><note>left: Effect of blurriness/poses on facial landmark detection (NME) on WholeBody-Face (WBF). right: Effect of hand poses on hand keypoint estima- tion (NME) on WholeBody-Hand (WBH).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effectiveness of joint learning. Face/Hand Bounding Box Detection In this section, we compare the results of face and hand bounding box detection. Compared to human body detection, detecting small objects such as face and hands are more challenging, since they only occupy a relatively small area in the whole image. General detection approaches such as Faster RCNN [46]</figDesc><table><row><cell>Method</cell><cell cols="5">Body AP Foot AP Face AP Hand AP WholeBody AP</cell></row><row><cell>joint training</cell><cell>0.743</cell><cell>0.798</cell><cell>0.623</cell><cell>0.401</cell><cell>0.541</cell></row><row><cell>reusing features</cell><cell>0.745</cell><cell>0.796</cell><cell>0.609</cell><cell>0.393</cell><cell>0.539</cell></row><row><cell cols="2">fully independent 0.745</cell><cell>0.796</cell><cell>0.623</cell><cell>0.419</cell><cell>0.543</cell></row><row><cell cols="4">D.1 Experiments about joint learning.</cell><cell></cell><cell></cell></row><row><cell cols="6">In Table 7, we explore the effectiveness of joint training of BodyNet, FaceHead</cell></row><row><cell cols="6">and HandHead in ZoomNet. We compare (1) joint training, (2) reusing fea-</cell></row><row><cell cols="6">tures, and (3) fully independent face/hand detectors. Joint learning improves</cell></row><row><cell cols="6">over "reusing features" on the performance of face (0.623 vs 0.609) and hand</cell></row><row><cell cols="6">(0.401 vs 0.393) for more efficient feature learning. Fully independent method</cell></row><row><cell cols="6">requires two additional models with increased complexity, but achieves limited</cell></row><row><cell>gain (0.543 vs 0.541).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Face/hand bounding box detection results on our COCO-WholeBody benchmark. Our proposed ZoomNet outperforms Faster RCNN [46] because of its multi-level design which better handles the scale variance.</figDesc><table><row><cell>Method</cell><cell cols="2">face</cell><cell cols="2">lefthand</cell><cell cols="2">righthand</cell></row><row><cell></cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell></row><row><cell cols="7">Faster RCNN [46] 0.439 0.712 0.266 0.440 0.262 0.430</cell></row><row><cell>ZoomNet</cell><cell cols="6">0.582 0.728 0.349 0.463 0.356 0.458</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/CMU-Perceptual-Computing-Lab/openpose 2 https://github.com/CMU-Perceptual-Computing-Lab/openpose train 3 https://github.com/tensorboy/pytorch Realtime Multi-Person Pose Estimation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/princeton-vl/pose-ae-train</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trb: A novel triplet representation for understanding 2d human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9479" to="9488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez-Donoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03742</idno>
		<title level="m">Large-scale multiview 3d hand pose dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-view appearancebased 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Singlenetwork whole-body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and robust hand tracking using detection-guided optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarseto-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valdes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mask-pose cascaded cnn for 2d hand pose estimation from single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<title level="m">Ai challenger: a large-scale dataset for going deeper in image understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bighand2. 2m benchmark: Hand pose dataset and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01389</idno>
		<title level="m">Learning to estimate 3d hand pose from single rgb images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
