<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual embedding and model weighting by fusing domain knowledge on Biomedical Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lu</surname></persName>
							<email>luyuxuanleo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Technology Chaoyang Qu</orgName>
								<address>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<addrLine>Chaoyang Qu</addrLine>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Qi</surname></persName>
							<email>zhixuanqi@outlook.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<addrLine>Chaoyang Qu</addrLine>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ge</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<addrLine>Chaoyang Qu</addrLine>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongping</forename><surname>Du</surname></persName>
							<email>ypdu@bjut.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<addrLine>Chaoyang Qu</addrLine>
									<settlement>Beijing Shi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual embedding and model weighting by fusing domain knowledge on Biomedical Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>biomedical question answering</term>
					<term>contextual embedding</term>
					<term>model weighting</term>
					<term>domain knowledge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biomedical Question Answering aims to obtain an answer to the given question from the biomedical domain. Due to its high requirement of biomedical domain knowledge, it is difficult for the model to learn domain knowledge from limited training data. We propose a contextual embedding method that combines open-domain QA model AoA Reader and BioBERT model pre-trained on biomedical domain data. We adopt unsupervised pre-training on large biomedical corpus and supervised fine-tuning on biomedical question answering dataset. Additionally, we adopt an MLP-based model weighting layer to automatically exploit the advantages of two models to provide the correct answer. The public dataset biomrc constructed from PubMed corpus is used to evaluate our method. Experimental results show that our model outperforms state-ofthe-art system by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Question answering is a classic task in Natural Language Processing, requiring a model to understand natural lang-uages. Cloze-style question answering problem has been a popular task because it is relatively easier to build cloze-style datasets. The cloze style question aims to select the best candidate answer regarding the specified context and fill in the blank of the question. Multiple clozestyle datasets have been published, such as CNN/Daily Mail <ref type="bibr" target="#b12">[12]</ref>, Children's Book Test <ref type="bibr" target="#b13">[13]</ref>, etc. Models based on neural networks are proposed, such as AS READER <ref type="bibr" target="#b14">[14]</ref>, CAS READER <ref type="bibr" target="#b4">[5]</ref>, AoA Reader <ref type="bibr" target="#b3">[4]</ref>and BERT <ref type="bibr" target="#b5">[6]</ref>.</p><p>These models have achieved good performance on several datasets. However, they do not perform well when facing domainoriented problems. The main reason is that domain-oriented questions require more background knowledge to give an answer, and a large dataset is needed to allow the models to learn the required domain knowledge.</p><p>We make improvements to the existing model, AoA Reader, and validate our results on the biomrc dataset <ref type="bibr" target="#b20">[20]</ref>, the public biomedical dataset constructed from corpus from PubMed. We put forward the Contextual Word Embedding method and the MLP-based model weighting strategy for the biomedical question answering task.</p><p>By combining the open-domain QA model and domain-oriented contextual word embedding, the proposed method outperforms state-of-the-art system on biomedical domain question answering significantly, setting up a new state-of-the-art system.</p><p>The main contributions of this paper are listed as follows:</p><p>? Combining BioBERT and AoA Reader, which can take full advantage of contextual word embedding model pre-trained on large domain corpus and mining semantic and contextual information to choose the best answer. In particular, multiple aggregation methods are adopted and evaluated. ? An MLP-based model weighting strategy is proposed, which can automatically learn the preferences and biases of different models and exploit the advantages of both models to provide the correct answer. ? Our method is evaluated on the biomrc dataset, and the results show that it outperforms state-of-the-art system significantly. Our code is available at https://github.com/leoleoasd/ MLP-based-weighting.  <ref type="bibr" target="#b12">[12]</ref>, which includes over 1 million cloze-style data. More high-quality datasets have been released since then, such as SQuAD <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">22]</ref>, Facebook Children's Book Test <ref type="bibr" target="#b13">[13]</ref>, etc. Models based on deep learning technologies significantly outperform traditional models in extracting context information. Hermann et al. <ref type="bibr" target="#b12">[12]</ref> propose an attention-based neural network and proves that the incorporation of attention mechanism is more effective than traditional, statistical-based baselines. Seo et al. propose the BiDAF <ref type="bibr" target="#b25">[24]</ref> model, which uses different levels of encoding for linguistic representation and uses a bidirectional attention flow mechanism to obtain the query-aware context representation. Kadlec et al. <ref type="bibr" target="#b14">[14]</ref> propose a simple model, the Attention Sum Reader (AS Reader), which uses attention to directly pick answer. Fu et al. propose Ea-Reader <ref type="bibr" target="#b9">[9]</ref>, whose memory updating rule is able to maintain the understanding of document through read, update and write operations. Chen et al. propose McR 2 <ref type="bibr" target="#b2">[3]</ref>, which enables relational reasoning on candidates based on fusion representations of document, query and candidates. Fu et al. propose ATNet <ref type="bibr" target="#b7">[8]</ref>, which utilities both intra-attention and inter-attention to answer close-style questions over documents. <ref type="bibr">Cui et al.</ref> propose Attention-over-Attention Reader ( AoA READER) <ref type="bibr" target="#b3">[4]</ref>, which puts another level of document-to-query attention on top of query-to-document attention, achieving stateof-the-art performance on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In recent years, researchers have focused on combining the unsupervised pre-training on large corpus and supervised fine-tuning on the specific task. <ref type="bibr">Vaswani</ref>   <ref type="bibr" target="#b5">[6]</ref> on top of Transformer to pre-train the language representation model using the masked language model task and the next sentence prediction task, and innovatively propose a training strategy that separates pre-training and finetuning. While sharing the same pre-trained weights, BERT achieves state-of-the-art performance on many different downstream tasks and datasets. Other BERT-based model is proposed, for instance, Liu et al. propose RoBERTa <ref type="bibr" target="#b17">[17]</ref>, providing servel techniques to robustly pre-train language models. Lan et al. propose ALBERT <ref type="bibr" target="#b15">[15]</ref>, providing two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.</p><p>In the biomedical domain, Tsatsaronis et al. launch the BioASQ <ref type="bibr" target="#b27">[26]</ref> challenges. It contains multiple subtasks, including article / snippet retrieval, document classification and question answering. Pappas et al. construct two cloze-style datasets, bioread <ref type="bibr" target="#b19">[19]</ref> and biomrc <ref type="bibr" target="#b20">[20]</ref>, and compare the accuracy of experts, non-expert human and different baseline models and neural network models. The results show that the baseline methods fail to correctly answer questions of the biomrc dataset while neural MRC models perform well, indicating that the biomrc dataset is less noisy and has enough features for the model to learn. Tang et al. initiate the CORD-19 <ref type="bibr" target="#b26">[25]</ref> dataset at the beginning of the global COVID-19 pandemic to help researchers in the biomedical field retrieve articles quickly.</p><p>Traditional neural network models require large-scale, highquality supervised training data to obtain better results. However, it is challenging to build a large-scale and high-quality dataset for domain-oriented tasks because it requires domain experts' annotations. Recent researchers prove that combining the pre-training on large corpus and fine-tuning on supervised training data can achieve better performance on domain-oriented tasks. Gururangan et al. <ref type="bibr" target="#b11">[11]</ref> proves that a second phase of pre-training in domain (domain-adaptive pre-training) leads to performance gains. <ref type="bibr">Gu</ref>   <ref type="bibr" target="#b0">[1]</ref>, both of which learn the corpus representation of papers on a large-scale corpus of biomedical papers or scientific papers and have obtained better results on natural language processing tasks in biomedical domains. We aim to further improve the performance by using BioBERT to obtain biomedical contextual and semantic information and by using model weighting layers to combine different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We propose a pre-training strategy based on the scientific pretraining model (SciBERT) and open-domain QA model (AoA Reader) to obtain the final answer to the question. In particular, different embedding and weighting strategies are used in the training process. <ref type="figure">fig. 1</ref> shows the full structure of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Task Description</head><p>This model is aiming at tasks that comprise cloze-style questions, to which answers are closely related to the comprehension of the context documents included in the problems. A set of candidate answers are also provided alongside, and the model is supposed to choose an answer from the candidates. This task can be formalized as a triplet ?C, Q, A? that is inclusive of the given</p><formula xml:id="formula_0">context C = { 1 , 2 , . . . , } made of words , a query Q = { 1 , 2 , . . . , [ ],</formula><p>. . . , } where the special token [MASK] marks the position where the answers are supposed to be placed, and answer candidates A = { 1 , 2 , . . . , }. A function is expected to be learned by the model to predict the answer A of question Q based on its comprehension of the proffered context C:</p><formula xml:id="formula_1">? ? A, ( |C, Q) = 1</formula><p>is the correct answer 0</p><p>is not the correct answer</p><formula xml:id="formula_2">(1) (C, Q, A) = max ?A ( |C, Q)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training of SciBERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Training data.</head><p>We use the SciBERT <ref type="bibr" target="#b0">[1]</ref>, which has been pretrained on the Semantic Scholar corpus. The corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. The unsupervised pre-training process using the large-scale corpus allows the model to obtain the semantic information of biomedical texts. Further, in order to let the model adapt to the cloze-style question answering task, biomrc dataset is used to fine-tune the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Answer extraction.</head><p>We adopt the answer extraction strategy Pappas et al. used in their biomrc dataset <ref type="bibr" target="#b20">[20]</ref>. For each contextquestion pair, we first divide the context into sentences using NLTK <ref type="bibr" target="#b1">[2]</ref>. Each sentence is concatenated to the question by [SEP] token, and they are fed to SciBERT respectively. In this way, we obtain the top-level embedding of the candidate entities and the placeholder in the question. The embeddings of each entity in the sentence are connected to the placeholder's embedding and are sent to a multilayer perceptron to obtain the score for the particular entity. If an entity appears multiple times in the paragraph, we choose the maximum value of its score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training of AoA Reader</head><p>In order to make AoA Reader <ref type="bibr" target="#b3">[4]</ref> achieve better performance on domain-oriented tasks, we adopt different contextualized word embedding and attention aggregation strategies.  <ref type="figure">Figure 1</ref>: Model structure based on Pre-training and Weighting Strategy C and the question Q into a one-hot vector. It then converts them into a continuous representation using the shared embedding matrix . Using this method in biomedical question answering may result in severe Out-Of-Vocabulary issues, given that most terms in the biomedical domain are made through several word-formation methods. Meanwhile, the model cannot learn necessary knowledge and the meanings of terms due to the lack of domain-oriented training data. Instead, we adopt BioBERT to generate contextualized word embedding, which has been pre-trained on a large biomedical corpus from PubMed containing biomedical literature proposed by Devlin <ref type="bibr" target="#b5">[6]</ref>. Thus, we can obtain domain-oriented knowledge and terms.</p><p>For tokenization, WordPiece <ref type="bibr" target="#b29">[28]</ref> through which new words can be represented by known tokens is used. Further, it can solve the Out-Of-Vocabulary issue and allow the model to better understand domain terms made by word-formation methods.</p><p>After connecting the context and the question with [SEP] token, it is fed into BioBERT. If it is longer than length limit (512 tokens), we trim the back of the context and keep the original question unchanged:</p><formula xml:id="formula_3">(C, Q) = ([[CLS]; C; [SEP]; Q; [SEP]])<label>(3)</label></formula><p>E(C, Q) in eq. (3) is the contextual embedding of the context C and the query Q.</p><p>To obtain the embeddings of context C and the query Q, we've applied a masking operation on (C, Q) for segmentation. This would conceal the representation of the other segment by zero vectors, leaving the desired half acquired:</p><formula xml:id="formula_4">(C) = (C, Q)</formula><p>is a context token 0 is a question token (4)</p><formula xml:id="formula_5">(Q) = (C, Q)</formula><p>is a question token 0 is a context token.</p><p>We adopt bi-directional RNN to further obtain the contextual representations ? (C) ? R ( | C |+| Q |+3) * 2 of the context C:</p><formula xml:id="formula_7">? ????????? ? ? (C) = ? ?? ? ( (C)) (6) ? ????????? ? ? (C) = ? ?? ? ( (C))<label>(7)</label></formula><p>?</p><formula xml:id="formula_8">(C) = [ ? ????????? ? ? (C); ? ????????? ? ? (C)]<label>(8)</label></formula><p>and similarly we obtain ? (Q) ? R ( | C |+| Q |+3) * 2 for the question Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Pair-wise Matching Score. After obtaining the contextual embedding of the context ? and the question ? , we calculate a pairwise matching matrix, which indicates the relevance between a token in the context and question, by calculating their dot product:</p><formula xml:id="formula_9">( , ) = ? ( ) ? ? ( ) ? C, ? Q<label>(9)</label></formula><p>3.3.3 Attentions over Attention Mechanism. After getting the pairwise matching matrix M, column-wise softmax is used to get the Context Because of reports of anaplastic transformation following irradiation, this study examines the incidence of anaplastic transformation and local control of these lesions. This review of seven @entity1 who had @entity189 of the @entity135 that was treated with irradiation shows local control in 71% of cases. There were no cases of anaplastic transformation. This report adds to the literature two cases of "de-differentiation" to less differentiated @entity957 ; one such case occurred after surgery alone. The literature is reviewed. Overall, anaplastic transformation is reported in 7% of @entity1 who had irradiation. De-differentiation occurs after surgery as well. The rate of local control with irradiation is less than 50%; with surgery it is 85%.  context-level attention regarding each token in the question:</p><formula xml:id="formula_10">( ) = softmax( (1, ), . . . , (|C|, )) (10) = [ (1), (2), . . . , (|Q|)]<label>(11)</label></formula><p>We calculate a reversed attention using row-wise softmax to obtain the "importance" of each token in the question regarding each token in the context:</p><formula xml:id="formula_11">( ) = softmax( ( , 1), ( , 2), . . . , ( , |Q|)).<label>(12)</label></formula><p>We average all ( ) to get a question-level attention :</p><formula xml:id="formula_12">= 1 | C | ?? =1 ( )<label>(13)</label></formula><p>Then we adopt the attention-over-attention mechanism, by merging these two attentions to get the "attended context-level attention":</p><formula xml:id="formula_13">= ?<label>(14)</label></formula><p>where denotes the importance of each token in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Final Predictions.</head><p>AoA Reader regards an entity as a word no matter how many words it has. It uses sum attention mechanism proposed by Kadlec et al. <ref type="bibr" target="#b14">[14]</ref> to get the confidence score of each candidate entity. In our model which uses WordPiece to obtain contextualized word embeddings, an entity may be either segmented into multiple tokens or composed of multiple words, and each token of the entity may occur multiple times in the context. So the confidence score of each candidate answer is calculated by aggregating all the occurrences of all its tokens in the context:</p><formula xml:id="formula_14">( |C, Q) = F 1 ? T ( ) ( F 2 ? ( ,C) ( ))<label>(15)</label></formula><p>where T ( ) is the result of segmenting the candidate answer using WordPiece; 1 and 2 are aggregating functions, which can be either maximum or sum, and ( , C) indicates the position that the token appears in the context C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Weighting strategy</head><p>After completing the training of AoA Reader and SciBERT, a model weighting strategy is used to obtain the final answer by combining the advantages of both models. Our previous study <ref type="bibr" target="#b6">[7]</ref> demonstrates that better performance can be achieved using a dual-model weighting strategy. The weighting process is performed by calculating a weighted average of the answer's confidence score and the similarity of the answer derived from two models. Further, considering that different models perform differently against data with different features, we use a simple MLP with one hidden layer to allow the model weighting layer to automatically learn this difference and to take advantage of both models.</p><formula xml:id="formula_15">= ([ , ])<label>(16)</label></formula><p>Where , ? R | A | is the confidence score of each model. In this way, the weighting layer would be able to learn the predilection and biases of each candidate model and achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Datasets</head><p>We conduct the experiments on the biomrc lite dataset <ref type="bibr" target="#b20">[20]</ref> to verify the effect of our method. biomrc is a biomedical cloze-style dataset for machine reading comprehension. The contexts in each sample are extracted from PUBTATOR, a repository containing 25 million abstracts and their corresponding titles on PubMed. Biomedical entities in the abstract are extracted to form the candidate entities. The contexts are the abstracts themselves, and the questions are construct by randomly replacing a biomedical entity in the title with a placeholder. <ref type="figure" target="#fig_0">fig. 2</ref> gives a sample in the biomrc dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>Our experiments are carried out on the machine with Intel i9-10920X (24) @ 4.700GHz, GPU of GeForce GTX 3090 24G, using pytorch 1.9.1 as the deep learning framework. To avoid overfitting, all models are trained for a maximum of 40 epochs, using early stopping on the dev, with a patience of 3 epochs. The test set of biomrc tiny dataset only contains 30 samples, and so the results on it may be unstable. On the other hand, the demonstrated accuracy of human experts comes from averaging the results of multiple experts, so it is a bit more stable than other results.</p><p>During the process of fine-tuning SciBERT, the batch size is set to 1 and the top layer of SciBERT is frozen; other layers are trained with the learning rate of 0.001.</p><p>During the process of fine-tuning BioBERT and training AoA Reader, the batch size is set to 30, the learning rate is set to 0.001, and the learning rate for BioBERT is set to 10 ?5 . To reduce GPU memory usage, we use the mixed-precision training technique <ref type="bibr" target="#b18">[18]</ref>, setting precision to 16 bits. We train our model on the biomrc lite dataset and evaluate it both on the biomrc lite and tiny dataset, which have 100,000 and 30 samples, respectively. We use Setting A for biomrc, in which all pseudo-identifier like @entity1 have a global scope, i.e., all biomedical entities have a unique pseudo-identifier in the whole dataset.</p><p>The model weighting layer is implemented after completing the training process of the two models, SciBERT and AoA Reader. The best weights evaluated by the Dev Acc are chosen to obtain the individual scores of each sample, which will later be used to train the model weighting layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Performance of the Contextualized Word Embedding Strategy.</head><p>Contextualized word embedding strategy based on BioBERT is used to obtain the final prediction answer. The selection of aggregating functions is crucial to the model performance. Therefore, multiple combinations of different aggregating functions are evaluated, and the results are shown in table 1.</p><p>It can be seen that choosing sum as both aggregation functions obtains better performance, and our model outperforms the stateof-the-art model significantly, which is about 6.77% absolute improvements on the biomrc lite test sets.</p><p>Our model also shows an improvement on the biomrc tiny dataset, though the dataset contains only 30 samples, and this result may be unstable. Our performance on the larger biomrc lite test set still exceeds the average human expert performance on the biomrc tiny test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance of the Weighting Model.</head><p>The MLP-based weighting model is used to further achieve better performance. We implement the SCIBERT-MAX-READER proposed by Pappas et al. <ref type="bibr" target="#b20">[20]</ref>, and use the model weighting strategy on top of AoA Reader with BioBERT embedding and SCIBERT-MAX-READER. The results of our experiments are shown in table 2. The result of SCIBERT-MAX-READER comes from our implementation of this model, which is used to train our MLP-based weighting layer. The results slightly differs to those in table 1.</p><p>It can be seen that our MLP-based weighting model improves the accuracy effectively. Especially, the accuracy on the test set is improved by 1.26% over the AoA Reader on the biomrc lite test dataset.</p><p>In order to evaluate the effectiveness of the model weighting layer, we compare the result to the union accuracy of two single models, i.e., the percentage of the union of the questions answered correctly by the two models in the total number of questions.</p><p>The results when excluding data that both models failed to answer are shown in table 3.The number of questions answered correctly by different models on the biomrc lite dataset is shown in   <ref type="figure">3</ref>. Here, Both in the figure refers to the the questions correctly answered by AoA-READER and SciBERT, and Neither in the figure refers to questions that cannot be answered correctly by any model. As expected, both of the two models correctly answer some questions that the other model failed to answer. The proposed MLPbased weighting model not only gives the correct answer to the question that at least one model answers correctly, but also a small number of questions that both models fail to answer.</p><p>To further corroborate the model's improvements in performance, we've applied the McNemar test to the results. Letting null hypothesis ? 0 be our model has the same performance as SciBERT-MAX-READER, the alternate hypothesis ? 1 would be there is a notable difference between the performance of our model and SciBERT-MAX-READER. The ? 1 is accepted by the test where n=2, significance = 0.025. Since our model has a lower error rate in all tests, it sufficiently supports the hypothesis that our model has significantly outperformed the SciBERT-MAX-READER.</p><p>In general, our MLP-based weighting model improves the performance by 2.17% significantly compared to the original single model. Step Test accuracy max-max max-sum sum-max sum-sum These results demonstrate that the proposed method can automatically learn the biases and preferences and exploit the strengths of both models to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Model Training Analysis.</head><p>In our structure, SCIBERT-MAX-READER and AoA Reader are trained separately, the outputs of which are then gathered for the final weighting layer to learn. The BioBERT embedded in the AoA Reader model is pre-trained on PubMed, and fine-tuned with the AoA Reader model on the BIOMRC dataset. It takes about 26 minutes for the AoA Reader to finish one epoch of learning based on the biomrc lite dataset.</p><p>All models generated in epochs are saved, among which the one that has the best performance on the dev set will be used to train the weighting layer.</p><p>The training process for the AoA reader is illustrated in <ref type="figure" target="#fig_2">fig. 4</ref>. It can be seen that model using sum as both token and occurence aggregation converges faster compated to most models while giving best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We propose a contextual embedding and model weighting method, which can combine model pre-trained on a large corpus and opendomain QA model to mine semantic and contextual information in biomedical question answering. Especially, we adopt an MLPbased model weighting strategy which can automatically learn and utilize the preferences and biases of two models to combine their advantages. The results show that our method outperforms stateof-the-art system and has higher accuracy than experts. In future work, how to use the semantic similarity between entity tokens and context tokens in getting final predictions should be studied, i.e., a context token should contribute to the score of an entity if its semantic information is similar to that of entity token.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>A example of the biomrc dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>26 Figure 3 :</head><label>263</label><figDesc>The number of question answered correctly by different models on the biomrc lite dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The validation accuracy of AoA Reader after each epoch.Here, 'max-max' represent token aggregation and occurence aggregation respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. propose the Transformer [27] model, which uses attention mechanism to replace the CNN and RNN parts of the traditional model to improve the model while speeding up the training process. Devlin et al. build a large-scale unsupervised pre-training model BERT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>et al. propose BLURB (Biomedical Language Understanding &amp; Reasoning Benckmark) [10] to test the biomedical language understanding ability of language models. Lee et al. propose BioBERT [16] and Cohan et al. propose SciBERT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>It is concluded that surgery should be used if the procedure has acceptable morbidity. Otherwise, irradiation can be used. Failures can be salvaged surgically. "Anaplastic transformation" should not affect treatment approach.</figDesc><table><row><cell>Candidate</cell><cell>@entity1: ['patients'] @entity135: ['head and neck'] @entity957: ['squamous carcinomas'] @entity189: ['verrucous</cell></row><row><cell>Entities</cell><cell>carcinoma']</cell></row><row><cell>Question</cell><cell>Radiotherapy in the treatment of XXXX of the @entity135 .</cell></row><row><cell>Answer</cell><cell>@entity189: ['verrucous carcinoma']</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>The result of different aggregation functions, compared to the state-of-the-art model and human experts</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>biomrc</cell></row><row><cell>Method</cell><cell>Occurrence</cell><cell>Token</cell><cell>Train Time 1</cell><cell cols="2">biomrc lite</cell><cell>tiny 2</cell></row><row><cell></cell><cell>Aggregation</cell><cell>Aggregation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Dev AccTest AccTest Acc</cell></row><row><cell>AS-READER</cell><cell>-</cell><cell>-</cell><cell>16.56hr</cell><cell cols="3">62.29 62.38 66.67</cell></row><row><cell>AoA-READER</cell><cell>-</cell><cell>-</cell><cell>60.90hr</cell><cell cols="3">70.00 69.87 70.00</cell></row><row><cell>SCIBERT-MAX-READER</cell><cell>-</cell><cell>-</cell><cell>83.22hr</cell><cell cols="3">80.06 79.97 90.00</cell></row><row><cell>Human Experts</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.00</cell></row><row><cell></cell><cell>max</cell><cell>max</cell><cell>1.50hr</cell><cell cols="3">78.54 78.11 90.00</cell></row><row><cell>AoA-READER with BioBERT</cell><cell>max</cell><cell>sum</cell><cell>0.88hr</cell><cell cols="3">83.40 83.36 93.33</cell></row><row><cell>Embedding</cell><cell>sum</cell><cell>max</cell><cell>3.60hr</cell><cell cols="3">80.98 81.20 90.00</cell></row><row><cell></cell><cell>sum</cell><cell>sum</cell><cell>1.76hr</cell><cell cols="3">87.22 86.74 93.33</cell></row><row><cell cols="7">1: We conduct some code optimizations on the AoA-Reader model, so the training time of our implementation with BioBERT can not be</cell></row><row><cell></cell><cell cols="3">compared to their original implementation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The results of two single models and our MLP-based model weighting model, compared to the union accuracy of two single models.</figDesc><table><row><cell></cell><cell cols="2">biomrc lite</cell><cell>biomrc tiny</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dev Acc</cell><cell>Test Acc</cell><cell>Test Acc</cell></row><row><cell>AoA-READER with BioBERT Embedding</cell><cell>87.22</cell><cell>86.74</cell><cell>93.33</cell></row><row><cell>SCIBERT-MAX-READER</cell><cell>79.74</cell><cell>80.21</cell><cell>86.67</cell></row><row><cell>MLP-based Weighting Model (ours)</cell><cell>88.76</cell><cell>88.00</cell><cell>96.66</cell></row><row><cell>The union of two single models (ideal result)</cell><cell>93.07</cell><cell>92.26</cell><cell>96.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The results of our MLP-based Weighting Model, excluding data that both models failed to answer The test set of biomrc tiny dataset only contains 30 samples, and so the results on it may be unstable.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>biomrc</cell></row><row><cell></cell><cell cols="2">biomrc lite</cell><cell>tiny 1</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Test</cell></row><row><cell></cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell></row><row><cell>AoA-READER with BioBERT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>93.71</cell><cell>93.21</cell><cell>96.55</cell></row><row><cell>Embedding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCIBERT-MAX-READER</cell><cell>86.67</cell><cell>86.94</cell><cell>89.66</cell></row><row><cell>MLP-based Weighting</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>95.36</cell><cell>95.38</cell><cell>100.00</cell></row><row><cell>Model (ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1:</cell><cell></cell><cell></cell><cell></cell></row></table><note>fig.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Conference'17, July 2017, Washington, DC, USA Yuxuan Lu, et al.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1371" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NLTK: The Natural Language Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219044.1219075</idno>
		<ptr target="https://doi.org/10.3115/1219044.1219075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Interactive Poster and Demonstration Sessions</title>
		<meeting>the ACL Interactive Poster and Demonstration Sessions<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="214" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Choice Relational Reasoning for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengcheng</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.567</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.coling-main.567" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-over-Attention Neural Networks for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1055</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Consensus Attention-based Neural Networks for Chinese Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1777" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual Model Weighting Strategy and Data Augmentation in Biomedical Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongping</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingnan</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<meeting>the 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ATNet: Answering Cloze-Style Questions via Intra-attention and Inter-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>Qiang Yang, Zhi-Hua Zhou, Zhiguo Gong, Min-Ling Zhang, and Sheng-Jun Huang</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-030-16145-3_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-16145-3_19" />
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="242" to="252" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EA Reader: Enhance Attentive Reader for Cloze-Style Question Answering via Multi-Space Context Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016375</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016375" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6375" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458754</idno>
		<idno>1-2:23</idno>
		<ptr target="https://doi.org/10.1145/3458754" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Downey</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.740" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Goldilocks Principle: Reading Children&apos;s Books with Explicit Memory Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text Understanding with the Attention Sum Reader Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1086</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BioBERT: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ggh5qq" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">682</biblScope>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<idno>arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixed Precision Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BioRead: A New Dataset for Biomedical Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BioMRC: A Dataset for Biomedical Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Stavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.bionlp-1.15</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.bionlp-1.15" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</title>
		<meeting>the 19th SIGBioMed Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-2124" />
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="784" to="789" />
			<pubPlace>Melbourne, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bidirectional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuong</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11339</idno>
		<idno>arXiv:2004.11339</idno>
		<title level="m">Rapidly Bootstrapping a Question Answering Dataset for COVID-19</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Overview of the BIOASQ Large-Scale Biomedical Semantic Indexing and Question Answering Competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0564-6</idno>
		<ptr target="https://doi.org/10.1186/s12859-015-0564-6" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
