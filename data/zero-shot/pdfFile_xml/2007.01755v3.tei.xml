<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACCEPTED BY IEEE TRANS. IMAGE PROCESSING 1 Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin-Bin</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">ACCEPTED BY IEEE TRANS. IMAGE PROCESSING 1 Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multi-label</term>
					<term>multi-class</term>
					<term>two-stream</term>
					<term>image recognition</term>
					<term>attentional region</term>
					<term>global to local</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-label image recognition is a practical and challenging task compared to single-label image classification. However, previous works may be suboptimal because of a great number of object proposals or complex attentional region generation modules. In this paper, we propose a simple but efficient two-stream framework to recognize multi-category objects from global image to local regions, similar to how human beings perceive objects. To bridge the gap between global and local streams, we propose a multi-class attentional region module which aims to make the number of attentional regions as small as possible and keep the diversity of these regions as high as possible. Our method can efficiently and effectively recognize multi-class objects with an affordable computation cost and a parameter-free region localization module. Over three benchmarks on multi-label image classification, our method achieves new state-of-the-art results with a single model only using image semantics without label dependency. In addition, the effectiveness of the proposed method is extensively demonstrated under different factors such as global pooling strategy, input size and network architecture. Code has been made available at https://github.com/gaobb/MCAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Multi-label image recognition is a practical and challenging task compared to single-label image classification. However, previous works may be suboptimal because of a great number of object proposals or complex attentional region generation modules. In this paper, we propose a simple but efficient two-stream framework to recognize multi-category objects from global image to local regions, similar to how human beings perceive objects. To bridge the gap between global and local streams, we propose a multi-class attentional region module which aims to make the number of attentional regions as small as possible and keep the diversity of these regions as high as possible. Our method can efficiently and effectively recognize multi-class objects with an affordable computation cost and a parameter-free region localization module. Over three benchmarks on multi-label image classification, our method achieves new state-of-the-art results with a single model only using image semantics without label dependency. In addition, the effectiveness of the proposed method is extensively demonstrated under different factors such as global pooling strategy, input size and network architecture. Code has been made available at https://github.com/gaobb/MCAR. Index Terms-multi-label, multi-class, two-stream, image recognition, attentional region, global to local.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ONVOLUTIONAL Neural Networks (CNNs) have made revolutionary breakthroughs on various computer vision tasks. For example, single-label image recognition (SLR), as a fundamental vision task, has surpassed human-level performance <ref type="bibr" target="#b0">[1]</ref> on large-scale ImageNet. Unlike SLR, multilabel image recognition (MLR) needs to predict a set of objects or attributes of interest present in a given image. Meanwhile, these objects or attributes usually have complex variations like spatial location, object scale and occlusion etc.. Nonetheless, MLR still has wide applications such as scene understanding <ref type="bibr" target="#b1">[2]</ref>, face or human attribute recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and multi-object perception <ref type="bibr" target="#b4">[5]</ref> etc.. These make MLR become a practical and challenging task. In recent years, a significant amount of learning approaches have been proposed to dealing with multi-label data <ref type="bibr" target="#b5">[6]</ref>.</p><p>MLR can be simply addressed by using SLR framework to predict whether each category object presents or not. Recently, there are many works using deep CNNs to improve the performance of MLR. These works can be roughly divided into three types: spatial information <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, visual attention <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and label dependency <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>Since the goal of MLR is to predict a set of object categories instead of producing accurate spatial locations of all possible objects, we argue that it is not necessary to waste computation resource for hundreds of object proposals in HCP <ref type="bibr" target="#b4">[5]</ref> or consume labor cost for the bounding box annotation of objects in Fev+Lv <ref type="bibr" target="#b6">[7]</ref>. RARL <ref type="bibr" target="#b7">[8]</ref> and RDAL <ref type="bibr" target="#b8">[9]</ref> introduce a reinforcement learning module and a spatial transformer layer to localize attentional regions, respectively, and sequentially predict label distribution based on generated regions. The main problem of these two methods is that the generated attentional regions are always category-agnostic and it is also difficult to guarantee the diversity of these local regions. In fact, we should ask the number of attentional regions to be as small as possible while maintaining the high diversity.</p><p>Recently, MLGCN <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref> and SSGRL <ref type="bibr" target="#b14">[15]</ref> try to model the label dependency with graph CNN to boost the performance of MLR. However, in this paper, we aim to improve the performance of MLR with only image semantics.</p><p>In order to exploit the semantic information of image, let us recall how we humans recognize multiple objects appeared in an image. Firstly, people may have a glimpse of a given image to discover some possible object regions from a global view. Then, these possible object regions guide the eye movements and help to make decisions on specific object categories following a region-by-region manner. In other words, most of time we humans difficultly recognize multi-objects using a single glance but at least two steps from a global view to local regions. In fact, there actually exists evidence in cognitive science that global visual processing precedes local reaction in visual perception <ref type="bibr" target="#b16">[17]</ref>. Also, such global-to-local mechanism is supported by studies in neurobiology <ref type="bibr" target="#b17">[18]</ref> and psychology <ref type="bibr" target="#b18">[19]</ref>. In this paper, we wonder if machines can acquire the learning ability like humans to recognize multiobjects.</p><p>Inspired by this observation, we propose a novel multi-label image recognition framework with Multi-Class Attentional Regions (MCAR) as illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>. This framework contains a global image stream, a local region stream, and a multi-class attentional region module. Firstly, the global image stream takes an image as the input for a deep CNN and learns global representations supervised by the corresponding labels. Then, the multi-class attentional region module is used to discover possible object regions with the information from the global stream, which is similar to the way we recognize multiple objects. Finally, these localized regions are fed to the shared CNN to obtain their predicted class distributions using the local region stream. The local region stream can recognize objects better since it flexibly focuses on details of each object which helps to alleviate the difficulty of recognition for these objects at different spatial locations and arXiv:2007.01755v3 [cs.CV] 9 Jun 2021 object scales.</p><p>The contributions of this paper can be summarized as follows.</p><p>? Firstly, we present a multi-label image recognition framework that can efficiently and effectively recognize multiobjects following a global to local manner. To the best of our knowledge, the learning mechanism of global to local in a unified model is the first time being proposed to find possible regions for multi-label images. ? Secondly, we propose a simple but effective multi-class attentional region module which includes three steps: generation, selection, and localization. In practice, it can dynamically generate a small number of attentional regions while keeping their diversity as high as possible. ? Thirdly, we achieve new state-of-the-art results on three widely used benchmarks with only a single model. Our method provides an affordable computation cost and needs no extra parameters. ? In addition, we also extensively demonstrate the effectiveness of the proposed method under different conditions like global pooling strategy, input size and network architecture. The rest of this paper is organized as follows. We first review the related work in Section II. Then, Section III proposes our approach, including two-stream framework, MCAR module (from global to local) and two-stream learning. After that, the experiments are reported in Section IV. Finally, Section V presents discussions and the conclusion is given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Recently, many efforts have been devoted into multi-label image recognition, using spatial information <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, visual attention <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and label dependency <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In this section, we briefly review these related approaches. Spatial Information. How to utilize the spatial information of image is very crucial for almost all visual recognition tasks such as image recognition <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, object detection <ref type="bibr" target="#b21">[22]</ref> and semantic segmentation <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. It is closely related to how to design (or learn) effective features. The reason is that objects usually present with different scales at different spatial locations. HCP <ref type="bibr" target="#b4">[5]</ref> uses EdgeBox <ref type="bibr" target="#b24">[25]</ref> or BING <ref type="bibr" target="#b25">[26]</ref> to generate hundreds of object proposals for each image using a like RCNN <ref type="bibr" target="#b21">[22]</ref> method, and aggregates prediction scores of these proposals to obtain the final prediction. However, a large number of proposals usually bring a huge computation cost. Fev+Lv <ref type="bibr" target="#b6">[7]</ref> generates proposals using bounding box annotations. Their approach combined the local proposal features and global CNN features to produce the final feature representations. It reduces the number of proposals but introduces the labor cost of annotation. Visual Attention. Attention mechanism has been widely used in many vision tasks, such as visual tracking <ref type="bibr" target="#b26">[27]</ref>, fine-grained image recognition <ref type="bibr" target="#b27">[28]</ref>, image captioning <ref type="bibr" target="#b28">[29]</ref>, image question answering <ref type="bibr" target="#b29">[30]</ref>, and semantic segmentation <ref type="bibr" target="#b30">[31]</ref>. RARL <ref type="bibr" target="#b7">[8]</ref> uses a recurrent attention reinforcement learning module <ref type="bibr" target="#b31">[32]</ref> to localize a sequence of attention regions and further predict label scores conditioned on these regions. Instead of reinforcement learning in RARL, RDAL <ref type="bibr" target="#b8">[9]</ref> introduces a spatial transformer layer <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> for localizing attentional regions from an image and an LSTM unit to sequentially predict the category distribution based on features of these localized regions. Unlike RARL and RDAL, SRN <ref type="bibr" target="#b9">[10]</ref> and ACfs <ref type="bibr" target="#b10">[11]</ref> combine attention regularization loss and multilabel loss to improve performance. Specifically, SRN <ref type="bibr" target="#b9">[10]</ref> captures both spatial semantic and label correlations based on the weighted attention map, while ACfs <ref type="bibr" target="#b10">[11]</ref> enforces the network to learn attention consistency that the classification attention map should follow the same transformation when input image is spatially transformed. Label Dependency. In order to exploit label dependency, CNN-RNN <ref type="bibr" target="#b11">[12]</ref> jointly learns image feature and label correlation in a unified framework composed of a CNN module and an LSTM layer. The limitation is that it requires a pre-defined label order for model training. Similar to <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref> also jointly learn multi-label classifiers with both spatial object relationships and semantic label correlations. Order-Free RNN <ref type="bibr" target="#b12">[13]</ref> relaxes the label order constraint via learning visual attention model and a confidence-ranked LSTM. But it requires an explicit module for removing duplicate prediction labels and needs a threshold for stopping the sequence outputs. In order to alleviate the issues, PLA <ref type="bibr" target="#b35">[36]</ref> proposes two alternative losses which dynamically order the labels based on the prediction label sequence of an LSTM model. Recently, SSGRL <ref type="bibr" target="#b14">[15]</ref> directly uses a graph convolutional network to model the label dependency among all labels.</p><p>There have been some other attempts on multi-label researches, such as multi-label image retrieval <ref type="bibr" target="#b36">[37]</ref>, multi-label dictionary learning <ref type="bibr" target="#b37">[38]</ref>, zero-shot <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b38">[39]</ref> and few-shot <ref type="bibr" target="#b39">[40]</ref> multi-label classification. While in this paper, we deliberately avoid using any information from label dependency and aim to improve the performance of multi-label recognition with only image semantic information. We leave them as future works to further boost recognition performance or extend application fields by integrating the label correlation and other paradigms to our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MCAR FRAMEWORK</head><p>In this section, we firstly present a two-stream framework which contains a global image stream and a local region stream. Then, we elaborate the multi-class attentional region module, which tries to bridge the gap between global and local views. Finally, we present the optimization details of our framework.  We assume that A = F(I; ?) is the activation map of the last convolutional layer of a CNN, where ? denotes the parameters of the CNN and A ? R h ?w ?d . Then, a global pooling function P(?) encodes the activation map A to a single vector f ? R 1?1?d , i.e., f = P(A). Here f can be considered as a global feature representation of the image I. In order to get it's prediction score, a 1?1 fully convolutional layer transfers f to x ? R C by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Two-Stream Framework</head><formula xml:id="formula_0">x = W T f + b.</formula><p>(1)</p><p>We then use a sigmoid function ?(?) to turn x into a range [0, 1], that is?</p><formula xml:id="formula_1">g = 1 1 + exp(?x) ,<label>(2)</label></formula><p>where? g stands for the global prediction distribution. Local Regions Stream. Local stream is, in fact, to perform a multi-instance multi-label learning <ref type="bibr" target="#b40">[41]</ref>. By decomposing an image into object regions, each image becomes a bag containing several positive instances, i.e., regions with the target objects, and negative instances, i.e., regions with background or other objects. We assume that {L 1 , L 2 , ? ? ? , L N } is a set of N local regions cropped from input image I. These local regions are firstly resized to the input size by bilinear upsampling. Then, they are fed to the shared CNN (with the global stream) to get prediction distributions {? L1 ,? L2 , ? ? ? ,? LN } with Eq. 1 and 2. Finally, these local region distributions are aggregated by a category-wise max-pooling operation:</p><formula xml:id="formula_2">y i l = max ? i L1 ,? i L2 , ? ? ? ,? i L N ,<label>(3)</label></formula><p>where? i l is the i-th category score of the local prediction? l . The subscript l means the distribution is from N local regions.</p><p>Note that the global and local streams share the same network without introducing additional parameters. It is obviously different from the classical two-stream architecture which usually contains two parallel subnetworks. The inputs of our two-stream are the whole image and local regions from it, respectively. These local regions are dynamically generated by using the information of the global stream. Therefore, it is also different from the existing methods whose inputs are always two parallel views like video frame and optical flow in video classification <ref type="bibr" target="#b41">[42]</ref>.</p><p>During the training stage, we jointly train these two streams. At the early stage of learning, there may be little difference between the number of positive and negative instances (local regions). With the gradual convergence of the global stream, positive instances will dominate the local stream and thus also tend to converge. At the inference stage, we fuse the predictions from global stream (? g ) and local stream (? l ) with a category-wise max-pooling operation to generate the final predicted distribution of image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From Global to Local</head><p>Potential object regions are not available in image-level labels, which must be generated in an efficient manner. The desirable generation module and candidate regions should satisfy some basic principles. First, the diversity of candidate regions should be as high as possible such that they can cover all possible objects of a given multi-label image. Second, the number of these candidate regions should be as small as possible in order to ensure efficiency. In contrast, more candidate regions require more computation resources since these regions need to be fed to the shared CNN simultaneously. Last but not least, the candidate regions generation module should have a simple network architecture and few parameters to alleviate the computation cost and storage overhead. Attentional Maps Generation. The class activation mapping method <ref type="bibr" target="#b42">[43]</ref> intuitively shows the discriminative image regions and helps us understand how to identify a particular category with a CNN. To obtain class-specific activation maps, we  directly apply the 1?1 convolutional layer to the class-agnostic activation maps A from the global stream, that is</p><formula xml:id="formula_3">F = W T A + b,<label>(4)</label></formula><p>where F ? R h ?w ?c . The class-specific activation map of the i-th category is denoted as F i ? R h ?w and it directly indicates the importance of the activation map at spatial leading to the classification of an image to class i. The discriminative class regions of a specific F i are significantly different among all possible class maps</p><formula xml:id="formula_4">{F i } C i=1 . If we employ class maps {F i } C i=1</formula><p>to localize the potential object regions then it is easy to satisfy the first principle: to increase the diversity of different proposals. Attentional Maps Selection. The number of class activation maps is equal to that of all categories associated with a dataset. For example, there are 20 and 80 categories on PASCAL VOC and MS-COCO datasets, respectively. If we use all class maps, it leads to two problems. First, the generated regions are too many to ensure efficiency. Second, a majority of regions will be redundant or meaningless because an image usually consists of a few instances.</p><p>A fact is that the predicted distribution will be close to the ground-truth distribution with the learning of the network which is supervised by ground-truth labels. It is a reasonable assumption that the high category confidence means that the corresponding object presents on the image with a high probability. Therefore, we sort the predicted scores? g (whose dimension is equal to the number of classes) following a descending order and select the topN class attentional maps. In experiments, we can see that a satisfied performance can be achieved when the topN is a small number (such as 2 or 4) which is far less than the number of all categories. Another benefit is that the proposed method may force network to implicitly learn label correlation if selective attentional maps don't fully cover all object categories. This is because the local stream is also supervised by the ground-truth label distribution. Local Regions Localization. We still denote topN class attentional maps as {F i } topN i=1 for notation simplification. Each F i is normalized to the range [0, 1] by a sigmoid function (Eq. 2). Furthermore, we simply upsample F i to the input size to align the spatial semantics between F i and the input image I. The value of F i (x, y) represents a probability that it belongs to the i-th category at spatial location (x, y). In order to efficiently localize regions of interest, we decompose each selective attentional map F i into a row and a column marginal distribution, which represents a probability distribution of objects present at the corresponding location (as shown in <ref type="figure" target="#fig_4">Fig. 2</ref>). We compute the marginal distribution based on the class attentional map F i over x and y axis, respectively, which is</p><formula xml:id="formula_5">p x = max 1?y?h F i (x, y), p y = max 1?x?w F i (x, y).<label>(5)</label></formula><p>Then, p x and p y are normalized by min-max normalization such that the distribution is scaled to the range in [0, 1] , that is</p><formula xml:id="formula_6">p x = p x ? min i (p x i ) / max i (p x i ) ? min i (p x i ) , p y = p y ? min j (p y j ) / max j (p y j ) ? min j (p y j ) ,<label>(6)</label></formula><p>where p x i represents the i-th element of p x . In order to localize one discriminative region, we need to solve the following integer inequalities:</p><formula xml:id="formula_7">p x i ? ?, s.t. i = {1, 2, ? ? ? , w}, p y j ? ?, s.t. j = {1, 2, ? ? ? , h},<label>(7)</label></formula><p>where ? ? (0, 1) is a constant threshold. The solution of Eq. 7 may be a single interval or a union of multiple ones, and each interval corresponds to the spatial location of a specific object region. The fact is that p x or p y may have one peak when input image only contains an object in <ref type="figure" target="#fig_5">Fig. 3a</ref> and also may have multiple peaks when input image consists of multiple objects of the same category at different spatial locations in <ref type="figure" target="#fig_5">Fig. 3b</ref> and 3c. However, our objective is to recognize multiclass objects in a given image, and only one discriminative region needs to be selected for each category. Therefore, some constraints have to be added such that a unique interval among multiple feasible intervals can be chosen. To achieve this goal, we pick the interval contained in the global maximum peak for the case of multiple local maximum peaks as shown in <ref type="figure" target="#fig_5">Fig. 3b</ref> and choose the widest interval for multiple global maximum peaks as shown in <ref type="figure" target="#fig_5">Fig. 3c</ref>. For all selected topN class attentional maps, topN discriminative regions would be generated by solving the Eq. 7 conditioned on the above constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two-Stream Learning</head><p>Given a training dataset</p><formula xml:id="formula_8">{I i , y i } M i=1 , which I i is the i-th image and y i = [y 1</formula><p>i , ? ? ? , y C i ] T represents the corresponding labels. The learning goal of our framework is to find ?, W and b via jointly learning global and local streams in an endto-end manner. Thus, our overall loss function is formulated as the weighted sum of two streams,</p><formula xml:id="formula_9">L = L g + L l ,<label>(8)</label></formula><p>where L g and L l represent the global and the local loss, respectively. Specifically, we adopt the binary cross entropy loss for global and local stream,</p><formula xml:id="formula_10">L g = M i=1 C j=1 y j i log(? g j i ) + (1 ? y j i ) log(1 ?? g j i ) L l = M i=1 C j=1 y j i log(? l j i ) + (1 ? y j i ) log(1 ?? l j i ),<label>(9)</label></formula><p>where? g j i and? l j i are the prediction scores of the j-th category of the i-th image from global and local streams, respectively. Optimization is performed using SGD and standard back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we firstly report extensive experimental results and comparisons that demonstrate the effectiveness of the proposed method. Then, we present ablation studies to carefully evaluate and discuss the contribution of the crucial components in our MCAR.</p><p>A. Experiment Setting Implementation Details. We perform experiments to validate the effectiveness of the proposed MCAR on three benchmarks in multi-label classification: MS-COCO <ref type="bibr" target="#b45">[46]</ref>, PASCAL VOC 2007 and 2012 <ref type="bibr" target="#b46">[47]</ref>, using the open-source framework Py-Torch.</p><p>Following recent MLR works, we compare the proposed method with state-of-the-arts using the powerful ResNet-50 and ResNet-101 <ref type="bibr" target="#b47">[48]</ref> models. Some popular and lightweight models, such as MobileNet-v2 <ref type="bibr" target="#b48">[49]</ref>, are also used to further evaluate our method. In general, for each of these networks we remove the fully-connected layers before the final output and replace them with global pooling followed by a 1?1 convolutional layer and a sigmoid layer. These models are all pre-trained on ImageNet and we train them using image-level labels only. The stochastic gradient descent (SGD) optimizer is used with the momentum of 0.9 and the weight decay of 0.0001. The initial learning rate is set to 0.001 for all layers but 0.01 for the 1?1 convolution, and they are decreased by a factor of 10 in the 30 th and 50 th epoch and the network is trained for 60 epochs in total.</p><p>During training, all input images are resized into a fixed size (i.e., 256?256 or 448?448) with random horizontal flips and color jittering for data augmentation. In order to speed up the convergence of the network, we don't use the random crop although it can bring performance improvement but need more training time. Unless otherwise stated, we set topN as 4 and ? as 0.5 in our experiments. The effects of hyper-parameters (topN and ? ) is discussed in Section IV-C. Evaluation Metrics. The performance of MLR mainly employ two metrics which are the average precision (AP) for each category and the mean average precision (mAP) overall categories. We first employ AP and mAP to evaluate all the methods. Following conventional setting <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, we also compute the precision, recall and F1-measure for comparison performance on MS-COCO dataset. For each image, we assign a positive label if its prediction probability is greater than a threshold (0.6) and compare them with the ground-truth labels. The overall precision (OP), recall (OR), F1-measure (OF1) and per-category precision (CP), recall (CR), F1-measure (CF1) are computed as follows:</p><formula xml:id="formula_11">OP = i M i c i M i p , OR = i M i c i M i g , CP = 1 C i M i c M i p , CR = 1 C i M i c M i g , OF1 = 2 * OP * OR OP + OR , CF1 = 2 * CP * CR CP + CR ,<label>(10)</label></formula><p>where M i c is the number of images correctly predicted for the i-th category, M i p is the number of predicted images for the i-th category, M i g is the number of ground truth images for the i-th category. We also compute these above metrics via another way that each image is assigned labels with top3 highest score. It is worthy to notice that these metrics may be affected by the threshold. Among these metrics, OF1 and CF1 are more stable than OP, CP, OR and CR. AP and mAP are the most important metrics which can provide a more comprehensive comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons with State-of-the-Arts</head><p>To verify the effectiveness of our method, we compare the proposed method with state-of-the-arts on MS-COCO <ref type="bibr" target="#b45">[46]</ref> and PASCAL VOC 2007 &amp; 2012 <ref type="bibr" target="#b46">[47]</ref>. MS-COCO. MS-COCO <ref type="bibr" target="#b45">[46]</ref> is a widely used dataset to evaluate multiple tasks such as object detection, semantic segmentation and image caption, and it has been adopted to evaluate multi-label image recognition recently. It contains 82,081 images as the training set and 40,137 images as validation set and covers 80 object categories. Compared to VOC 2007 &amp; 2012 <ref type="bibr" target="#b46">[47]</ref>, both the size of training set and the number of object categories are increased. Meanwhile, the number of labels of different images, the scale of different objects and the number of images in each category vary considerably, which makes it more challenging.</p><p>Results on MS-COCO. The results on MS-COCO are reported in <ref type="table">Table I</ref>. When the input size is 448?448 (the most common setting in MLR), our method is already comparable to the state-of-the-art SSGRL <ref type="bibr" target="#b14">[15]</ref> which uses additional label dependency and larger input to boost performance. Moreover, if we simply resize the input image to 576?576 during the testing stage while still using the model weights trained with <ref type="table">Table I  COMPARISONS OF MAP, CP, CR, CF1 AND OP, OR, OF1 IN % OF OUR MODEL AND STATE-OF-THE-ART METHODS ON THE MS-COCO DATASET. *  INDICATES THAT THE RESULTS ARE REPRODUCED BY USING THE OPEN-SOURCE CODE</ref>  <ref type="bibr" target="#b14">[15]</ref>, AND -DENOTES THE CORRESPONDING RESULT IS NOT PROVIDED. <ref type="table">Backbone  mAP  All  Top3  CP  CR  CF1  OP  OR  OF1  CP  CR  CF1  OP  OR  OF1  CNN-RNN [12]</ref> -VGG16 61.  448?448 inputs, our method achieves 84.5% mAP which outperforms the SSGRL by 0.7%. In order to fairly compare with the SSGRL, we re-implement the experiment with 448?448 input following the same setting as described in the SSGRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Size</head><p>In <ref type="table">Table I</ref>, we can see that our method significantly beats the SSGRL and improves it by 1.9 points (83.8% vs. 81.9%). Note that PLA <ref type="bibr" target="#b35">[36]</ref> models label correlation through exploiting LSTM model. Using the same input size (288?288), our method gets higher F1 scores than PLA, which further indicates that it is very important to exploit image semantics for multi-label image recognition.</p><p>The performance of our method is also significantly better than that of Multi-Evidence <ref type="bibr" target="#b44">[45]</ref>, and it improves CF1 by 3.1%, OF1 by 1.9%, CF1-top3 by 4.5%, OF1-top3 by 2.0%. Note that our baseline ResNet-101 model achieves 77.1% mAP, which should be close to that of the baseline of Multi-Evidence <ref type="bibr" target="#b44">[45]</ref> because of nearly the same F1-measures. In comparison to the baseline, our method is 6.7% higher in mAP (83.8% vs. 77.1%).</p><p>Meanwhile, we show the AP performance of each class for further comparison with the baseline model in <ref type="figure" target="#fig_6">Fig 4.</ref> It is obvious that our method has significant improvements on almost all categories, especially for some difficult categories such as "toaster" and "hair drier". In short, MCAR outperforms all state-of-the-art methods and significantly surpasses the baseline by a large margin even though it does not need a large number of proposals or label dependency information. This further demonstrates the effectiveness of the proposed method for large-scale multi-label image recognition. PASCAL VOC 2007 and 2012. PASCAL VOC 2007 and 2012 <ref type="bibr" target="#b46">[47]</ref> are the most widely used datasets for MLR. There are 9,963 and 22,531 images in VOC 2007 and 2012, respectively. Each image contains one or several labels, corresponding to 20 object categories. These images are divided into three parts including train, val and test sets. In order to fairly compare with other competitors, we follow the common  <ref type="table" target="#tab_1">Table III</ref>. We compare state-of-the-arts with our method on several backbone networks. First, we still win the best mAP performance with a smaller input size compared to SSGRL <ref type="bibr" target="#b14">[15]</ref> when ResNet-101 is considered as a backbone. Second, our method achieves better performance using lightweight networks, i.e.MobileNet-v2 and ResNet-50, than that of VGG. This implies that it may be easy to extend our method to resource-restricted devices such as mobile phones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In order to comprehend how MCAR works, we perform exhaustive experiments to analyze the components in MCAR. We firstly analyze the contribution of each component in our two-stream architecture and demonstrate its effectiveness. The training details are exactly the same as those described in Section IV-B. Then, the effect of the attentional maps selection criteria and learning strategy is analyzed. Next, we also present the effects of MCAR in different hyper-parameters (topN and ? ) appearing in the local region localization module. The experiment is conducted on VOC 2007 and MS-COCO using different backbone networks, e.g.MobileNet-v2, ResNet-50 and ResNet-101, and we set the input size to 256?256. Finally, we extensively analyze the effects of our method under different conditions such as different global pooling strategies, various input sizes, and different network architectures.</p><p>Contributions of proposed global-to-local framework. To explore the effectiveness of two streams, we jointly train the global and local streams in MCAR, and during the inference stage, we report the influence of using each stream in <ref type="table" target="#tab_2">Table IV</ref>. Firstly, thanks to the joint training strategy, our MCAR significantly outperforms the baseline method even when the same global image is taken as input (line 1 vs. line 0). Such improvement is very intuitive because MCAR is more robust and generalized by learning on not only global image but also various scales of local regions. Secondly, we can see that using local stream alone performs better than only using global stream (line 2 vs. line 1), which is because the local stream is able to flexibly focus on the details of each object. Nonetheless, we want to emphasize that the global stream plays an important role in guiding the learning of local stream. Last but not least, it is obvious that employing both global and local streams achieves the best results (line 3). This is similar to humans perception because we usually make a final decision after our brain gathers information from different spatial locations and object scales. Importances of attentional maps selection. In our method, all attentional maps are firstly sorted by global stream score following a descending order and then the topN attentional maps are chosen. In order to further verify the selection strategy, we conduct other criterions to see if the performance is sensitive to the score ranking. Specifically, we design two criterions to compare with our topN strategy. The first one is that we still sort global stream scores but pick bottom N feature maps. The second one is that we randomly sample N maps among all attentional maps.</p><p>For simplicity, we test MCAR with top4, random4 and bottom4 local regions while using the weights trained on MCAR with top4 setting in <ref type="table" target="#tab_3">Table V</ref>. We can see that the performance of MCAR using high-confidence local regions (top4) is significantly better than that of using lowconfidence ones (bottom4) or random manner (random4). This indicates the effectiveness of the local region selection strategy based on the ranking of the global scores. Single or Pair loss? Instead of two-stream learning using a pair of parallel losses in Eq. 8, we design a simple way to train the network utilizing a single loss. Specifically, we firstly fuse global and local prediction scores with a max-wised aggregation? i = max ? i g ,? i l ,</p><p>and then train the network with a single BCE loss as the same in Eq. 8,</p><formula xml:id="formula_13">L = M i=1 C j=1 y j i log(? j i ) + (1 ? y j i ) log(1 ?? j i )<label>(12)</label></formula><p>Using ResNet-101 backbone and keeping the rest settings the same, the experimental results are reported in <ref type="table" target="#tab_3">Table VI</ref>. We can see that MCAR with single loss obtains 94.4 mAP on VOC2007 and 82.6 mAP on MS-COCO which improves the baseline by 1.5 and 5.5 mAP, but is 0.4 and 1.2 mAP worse than our main method (with pair loss). Why is MCAR equipped with a pair of losses better than that of a single loss? The main insight is that global visual processing usually precedes the local one. The pair loss may ensure that after the global stream has been converged fast, then it guides the local stream to find possible local regions. Indeed, we find that the single loss setting usually needs more epochs when arriving at the similar performance. This indicates that the convergence of single loss is slower than that of our pair loss. Number of local regions. We fix ? to 0.5 and choose the value topN from a given set {0, 1, 2, 4, 6, 8}. Note that, topN = 0 implies we train the model using global stream only, which is equal to our baseline. In the first row of <ref type="figure" target="#fig_7">Fig. 5</ref>, we show the mAP performance curves when topN is set to different numbers. First, the mAP performance shows an upward trend with the number of topN gradually being increased. This means that it is useful to improve the multi-label classification performance using more local regions. Second, the performance tends to be stable when topN is set to 4 or 6, which implies that the improvements will be not significant when applying a large topN . Third, the performance of a small topN , (e.g., 1, 2, or 4) is significantly better than that of a pure global stream (i.e., topN =0). This further verifies the effectiveness of the proposed selection strategy of generated high-confidence local regions. Another benefit of the region selection strategy is to help reduce the cost of computation resources. Threshold of localization. To explore the sensitivity of the ? in Eq. 7, we fix topN to 4 and test different ? values from {0, 0.1, 0.3, 0.5, 0.7, 0.9}. The whole image will be considered as a local region when ? equals to 0, and it is also equivalent to the baseline method. We show the mAP performances as the function of ? in the second row of <ref type="figure" target="#fig_7">Fig. 5</ref>. First, we observe that the performance is better when ? is greater than 0. Second, the performance drops when ? is either too small or too large. We argue that if ? is too small, local regions may contain more context information and lack discriminative features because all local regions are close to the original input image. When ? is too large, it makes local regions only contain the most discriminative parts of an object and easily leads to overfitting. It is a good choice when the value ? is in the interval between 0.3 and 0.7. Global pooling strategy. Encoding spatial feature descriptors to a single vector is a necessary step in state-of-the-art CNNs. The early works, e.g., AlexNet and VGGNet, use a fully connected layer, and the recent ResNet usually employs global average pooling (GAP) which outputs the spatial average of each feature map. Specifically, considering class-agnostic feature map A from the top block of a backbone network. The GAP operation outputs the spatial average of the A, returning a vector f a ? R d with the k-th element being</p><formula xml:id="formula_14">f a k = 1 h w h i=1 w j=1 A i,j,k .<label>(13)</label></formula><p>We denote the output of global maximum pooling (GMP) as  f m ? R d , whose the k-th element is</p><formula xml:id="formula_15">f m k = max{A i,j,k } h w i=1 j=1 .<label>(14)</label></formula><p>The GMP easily falls into over-fitting because it enforces the network to learn the most discriminative feature. Generally, GAP usually has a better generalization ability than GMP. However, GAP may lead to under-fitting and slow convergence because it equally gives the same importance for all spatial feature descriptors. Our local region localization needs to discover the discriminative region which seems to be opposite to the objective of GAP. In order to alleviate this conflict, we propose a simple solution termed as Global Weighted Pooling (GWP) which is an average of f a and f m , as</p><formula xml:id="formula_16">f = ?f a + (1 ? ?)f m ,<label>(15)</label></formula><p>where ? ? [0, 1] is a weight which balances the importance between GAP and GMP. In our paper, the weight ? is empirically set to 0.5.</p><p>In <ref type="table" target="#tab_1">Table VII</ref>, we can see that MCAR with GWP further boosts performance on MS-COCO dataset. It improves the mAP by 4.1 points and 3.3 points compared to the common GAP on ResNet-50 and ResNet-101 when input size is 448?448. Nevertheless, the overall performance of GWP is comparable to that of GAP on the PASCAL-VOC dataset as reported in <ref type="table" target="#tab_1">Table VIII</ref>. This may be associated with a specific dataset that the task of PASCAL-VOC is relatively simpler than that of MS-COCO because of small-scale samples, fewer classes and fewer instances per image in PASCAL-VOC. Generally, MCAR equipped with GWP is better than GAP, especially on more challenging tasks.</p><p>Network architecture. The recent state-of-the-art methods usually take ResNet-101 as a backbone to report their performance. However, in real applications, lightweight networks have been widely adopted. To meet such requirements, we extensively evaluate the proposed method with MobileNet-v2 and ResNet-50 besides ResNet-101 on PASCAL-VOC and MS-COCO and report their results in Tables VII and VIII. The deeper network tends to obtain better performance. This is not surprised because the big network has more parameters and a deeper structure to ensure strong capacity and transferability. Note that our method still has good performance using the lightweight MobileNet-v2. In addition, the proposed method has significant improvements for all backbones. On the MS-COCO dataset, our MCAR with GWP improves the baseline by about 7% using the input size of 448?448.</p><p>Input size. The performance of multi-label recognition is sensitive to the choice of input size. Generally, the larger size tends to get the better performance as reported in <ref type="table" target="#tab_1">Tables VII  and VIII</ref> . However, it is more practicable to employ smallsized input on resource-restrict devices. Somewhat surprisingly, MCAR performs better using small inputs. In <ref type="table" target="#tab_1">Table VII</ref> and VIII, we can see that our method always tends to produce  <ref type="figure">Figure 6</ref>. Selected examples of region localization and classification results on PASCAL VOC 2012 testing images (first two rows) and MS-COCO validation images (last two rows). Our MCAR achieves 94.3% mAP on the VOC 2012 testing set and 83.8% mAP on the MS-COCO validation set by using ResNet-101 backbone and the input size of 448?448. Note that these attentional regions are generated by using the model trained on image-level labels only (without bounding box annotations). Each region box is associated with a category label (c), a global stream score (? c g ) and a two-stream score (max{? c g ,? c l }), organized as "category name:global score/two-stream score", e.g., "train: 0.14/0.99" in the image at the fourth row and second column. These region boxes are displayed with conditions on? c l &gt; 0.1, max{? c l ,? c g } &gt; 0.6, topN = 4 and ? = 0.5. For each image, one color represents one object category in that image. The proposed two-stream MCAR framework recognizes objects of a wide range of scales, especially for those small-sized or occluded objects, such as the car in <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref>, the bird in <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5)</ref>, the cat in <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4)</ref> and the sofa in (2,5) on VOC 2012 testing images and the snowboard in (3,1), the car in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>, the dog in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6)</ref>, the cell phone in (4,1), the train in (4,2) and the mouse in (4,5) on MS-COCO validation images, where (i, j) represents the image at i-th row and j-th column. It is noteworthy that MCAR may produce incorrect or incomplete predictions when the local region is too small or too blurry such as as the bench in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, the book in <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b5">6)</ref> and the couch in (4,5) on the MS-COCO testing images. (Best view in color and zoom in.) more improvements when a smaller input size is employed. This advantage comes from the two-stream architecture which can look at an image in a comprehensive manner (global to local). This indicates that our method is more friendly for lowresolution inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this section, we try to understand how the network recognizes multi-objects for a multi-label image via visualizing the produced local regions and discuss why MCAR is a simple and efficient multi-label framework.</p><p>Visualization. To analyze where our model focuses on an image, we show the class-specific attentional regions generated by a multi-class attentional region module in <ref type="figure">Fig. 6</ref>. It can be seen that these attentional regions cover almost all possible objects in each image which is consistent with our initial intention. Furthermore, we can find that global prediction scores of some small-scale objects are low, e.g. the train in (1,1), the car in <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3)</ref>, the bird in <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5)</ref>, the chair in (1,1), the cat in <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4)</ref> and the sofa in (2,5) on the PASCAL VOC 2012 testing set and the snowboard in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref>, the car in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>, the dog in (3,6), the cell phone in (4,1), the train in (4,2) and the mouse in (4,5) on the MS-COCO validation images, where (i, j) is the image at i-th row and j-th column in <ref type="figure">Fig. 6</ref>. This indicates that it is suboptimal to use global image stream solely, especially for small-scaled and partly occluded objects. This limitation would be improved by our two-stream network because it recognizes this type of object from a closer view (high score of two-stream). Compared to the baseline method, our method significantly improves the multi-label image recognition performance. Note that MCAR may produce incorrect or incomplete predictions when local regions are too small or too blurry such as the bench in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, the book in <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b5">6)</ref> and the couch in (4,5) on MS-COCO testing images. Furthermore, the local region stream is hardly ensured to cover all target objects even if we use a larger number (topN ). However, the local stream is able to contain a majority of target objects because of the high diversity of local regions. Moreover, our two streams can complement each other by finding missing discriminative regions. Considering this is a weakly supervised problem and the computation efficiency, we think such this situation can be acceptable. Simplicity. Our framework aims at proposing a simple and efficient method that puts forward to learn global and local image semantics in a single unified model. On one hand, we generate object proposals only using the network itself while HCP utilizes external tools such as EdgeBox <ref type="bibr" target="#b24">[25]</ref> or BING <ref type="bibr" target="#b25">[26]</ref>. On the one hand, our method can efficiently obtain multi-class regions with a parameter-free region localization module because of the parameter share mechanism in Eq. 1 and 4. Unlike some existing attention-based methods, they always need a slightly complex module such as LSTM unit in <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> or reinforcement learning module in RARL <ref type="bibr" target="#b7">[8]</ref>.</p><p>Complexity. The computation complexity linearly grows with the region number (i.e., topN ). However, it is worth noting that the number of local regions has been significantly reduced by using our framework compared to region-based methods such as HCP (e.g., 500). Our method works well when a small topN (e.g., 4) is used and thus the complexity is controllable and the computation cost is affordable. For example, the number of object proposals in HCP is 500 while we reduce this number to 4. So about 100-time speedup is obtained.</p><p>We test the forward running time of each model using the input sizes of 256?256 and 448?448. This evaluation is conducted on one P40 GPU accelerated by cuDNN v7.4.1. The actual inference time is reported in Table IX. We can see that the total time of our MCAR (topN =4) is about 4 to 5 times compared to baselines. This is not surprising because there is at least topN +1 times computation cost with our method. We also report the inference time of each component (global stream, global-to-local and local stream) of our MCAR. It can be seen that those local regions' generation and their forward inference dominate the computation cost of our MCAR, reducing the number of local regions would accelerate inference speed greatly. In addition, our MCAR significantly outperforms the baseline method (81.9% vs. 77.1% mAP on MS- <ref type="table" target="#tab_2">COCO Table IV</ref>) even if only global image (without local regions) is taken as input, which implies our method still better than the baseline under the same inference time. Meanwhile, our method needs no additional parameters for generating local regions because of the parameter sharing mechanism between global and local streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We observe that humans recognize multiple objects following two steps. In practice, they usually obey a rule of global to local. Through looking at the whole image at first, people can discover places that need to be focused with more attentions. These attentional regions are then checked closer for a better decision. Inspired by this observation, we develop a two-stream framework to recognize multi-label images from global to local as human's perception system works. In order to localize object regions, we propose an efficient multiclass attentional region module which significantly reduces the number of regions and keeps their diversity. Our method can efficiently and effectively recognize multi-class objects with an affordable computation cost and a parameter-free region localization module. On three prevalent multi-label benchmarks, the proposed method achieves state-of-the-art results. In the future, we will try to integrate the label dependency into our method to further boost the performance. It is also an interesting direction to explore how to extend the proposed method for weakly supervised image detection and semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Bin Bin Gao is with the Tencent YouTu Lab, Shenzhen 518057, China. (E-mail: gaobb@lamda.nju.edu.cn. Corresponding author: Bin-Bin Gao) Hong-Yu Zhou is with the Department of Computer Science, The University of Hong Kong, Hong Kong. (E-mail: whuzhouhongyu@gmail.com)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Global Image Stream. Given an input image I ? R h?w?3 , where h, w are the image's height and width. Let's denote its corresponding label as y = [y 1 , y 2 , ? ? ? , y C ] T , where y i is a binary indicator. y i = 1 if image I is tagged with label i, otherwise y i = 0. C is the number of all possible categories in this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>The pipeline of our MCAR framework for multi-label image recognition. MCAR firstly feeds an input image into a deep CNN model to extract its global feature representation through the global image stream. Then, the multi-class attentional region module roughly localizes possible object regions by integrating that information from the global stream. Finally, these localized regions are fed to the shared CNN to obtain their predicted class distributions through the local region stream. At the inference stage, MCAR aggregates predictions from global and local streams with category-wise max-pooling and produces the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>The visualization of local region localization with class attentional map. We firstly decompose the class attentional map into two marginal distributions along row and column. Then, the class attentional region is localized by these two marginal distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Some examples of margin distribution. Black curves represent the margin distribution, and blue dash is the threshold ? , and the best interval between two red dashes is the desirable localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>AP (in %) of each category of our proposed framework and the ResNet-101 baseline on MS-COCO dataset. Our MCAR has significant improvements on almost all categories, especially for some difficult categories such as "toaster" and "hair drier".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>mAP comparisons of our MCAR with different values of topN and ? . The left three columns are based on PASCAL-VOC 2007 and the right three columns are based on MS-COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II COMPARISONS</head><label>II</label><figDesc>OF AP AND MAP IN % OF OUR MODEL AND STATE-OF-THE-ART METHODS ON THE PASCAL VOC 2007. * INDICATES METHODS USING LARGER INPUT SIZE (576?576). Methods Backbone aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP CNN-RNN [12] VGG16 96.7 83.1 94.2 92.8 61.2 82.1 89.1 94.2 64.2 83.6 70.0 92.4 91.7 84.2 93.7 59.8 93.2 75.3 99.7 78.6 84.0 VGG+SVM [50] VGG16&amp;19 98.9 95.0 96.8 95.4 69.7 90.4 93.5 96.0 74.2 86.6 87.8 96.0 96.3 93.1 97.2 70.0 92.1 80.3 98.1 87.0 89.7 Fev+Lv [7] VGG16 97.9 97.0 96.6 94.6 73.6 93.9 96.5 95.5 73.7 90.3 82.8 95.4 97.7 95.9 98.6 77.6 88.7 78.0 98.3 89.98.5 98.2 85.4 96.9 97.4 98.9 83.7 95.5 88.8 99.1 98.2 95.1 99.1 84.8 97.1 87.8 98.3 94.8 94.8 Table III COMPARISONS OF AP AND MAP IN % OF OUR MODEL AND STATE-OF-THE-ART METHODS ON THE PASCAL VOC 2012. * INDICATES METHODS USING LARGER INPUT SIZE (576?576). 98.8 87.0 96.9 85.0 98.7 98.3 97.3 99.0 83.8 96.8 83.7 98.3 93.5 94.3 setting to train our model on the train-val sets, and then evaluate produced models on the test set. VOC 2007 contains a train-val set of 5,011 images and a test set of 4,952 images. VOC 2012 consists of 11,540 images as train-val set and 10,991 images as the test set. Results on VOC 2007. We first report the AP for each category and the mAP for all categories on VOC 2007 test set inTable II. The current state-of-the-art is SSGRL<ref type="bibr" target="#b14">[15]</ref> which uses GCN to model label dependency to boost the performance. We can see that our method achieves the best mAP performance among all methods. It largely outperforms the SSGRL [14] by 1.4 points (94.8% vs. 93.4%) when SSGRL uses a larger input size 576?576. Moreover, the proposed method improves the baseline ResNet-101 model by 1.9% under the same setting such as data augmentation and hyperparameters of optimization. Last but not least, our framework shows good performance for some difficult categories such as "bottle", "table" and "sofa". This shows that exploiting global and local vision information is very crucial for multi-label recognition.Results on VOC 2012. We report the results on VOC 2012 test set with PASCAL VOC evaluation server in</figDesc><table><row><cell></cell><cell></cell><cell>0 90.6</cell></row><row><cell>HCP [5]</cell><cell>VGG16</cell><cell>98.6 97.1 98.0 95.6 75.3 94.7 95.8 97.3 73.1 90.2 80.0 97.3 96.1 94.9 96.3 78.3 94.7 76.2 97.9 91.5 90.9</cell></row><row><cell>RDAL [9]</cell><cell>VGG16</cell><cell>98.6 97.4 96.3 96.2 75.2 92.4 96.5 97.1 76.5 92.0 87.7 96.8 97.5 93.8 98.5 81.6 93.7 82.8 98.6 89.3 91.9</cell></row><row><cell>RARL [8]</cell><cell>VGG16</cell><cell>98.6 97.1 97.1 95.5 75.6 92.8 96.8 97.3 78.3 92.2 87.6 96.9 96.5 93.6 98.5 81.6 93.1 83.2 98.5 89.3 92.0</cell></row><row><cell>SSGRL* [15]</cell><cell cols="2">ResNet-101 99.5 97.1 97.6 97.8 82.6 94.8 96.7 98.1 78.0 97.0 85.6 97.8 98.3 96.4 98.1 84.9 96.5 79.8 98.4 92.8 93.4</cell></row><row><cell>Baseline</cell><cell cols="2">ResNet-101 99.0 97.9 97.2 97.6 80.2 93.6 96.0 98.0 81.8 92.0 84.6 97.5 97.2 95.3 97.9 81.8 94.6 84.1 98.2 93.6 92.9</cell></row><row><cell cols="3">MCAR ResNet-101 99.7 99.0 Methods Backbone aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</cell></row><row><cell cols="3">VGG+SVM [50] VGG16&amp;19 99.0 89.1 96.0 94.1 74.1 92.2 85.3 97.9 79.9 92.0 83.7 97.5 96.5 94.7 97.1 63.7 93.6 75.2 97.4 87.8 89.3</cell></row><row><cell>Fev+Lv [7]</cell><cell>VGG16</cell><cell>98.4 92.8 93.4 90.7 74.9 93.2 90.2 96.1 78.2 89.8 80.6 95.7 96.1 95.3 97.5 73.1 91.2 75.4 97.0 88.2 89.4</cell></row><row><cell>HCP [5]</cell><cell>VGG16</cell><cell>99.1 92.8 97.4 94.4 79.9 93.6 89.8 98.2 78.2 94.9 79.8 97.8 97.0 93.8 96.4 74.3 94.7 71.9 96.7 88.6 90.5</cell></row><row><cell>SSGRL* [15]</cell><cell cols="2">ResNet-101 99.5 95.1 97.4 96.4 85.8 94.5 93.7 98.9 86.7 96.3 84.6 98.9 98.6 96.2 98.7 82.2 98.2 84.2 98.1 93.5 93.9</cell></row><row><cell>MCAR</cell><cell cols="2">MobileNet-v2 98.6 92.3 95.4 93.3 77.7 93.8 92.6 97.6 80.8 90.9 82.3 96.5 96.6 95.5 98.3 78.4 92.6 78.7 96.8 90.9 91.0</cell></row><row><cell>MCAR</cell><cell cols="2">ResNet-50 99.6 95.6 97.5 95.2 85.1 95.5 94.3 98.6 85.2 95.8 83.9 98.4 98.0 97.2 98.8 81.6 95.5 81.8 98.3 93.6 93.5</cell></row><row><cell>MCAR</cell><cell cols="2">ResNet-101 99.6 97.1 98.3 96.6 87.0 95.5 94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table IV ABLATIVE</head><label>IV</label><figDesc>STUDY OF TWO STREAMS IN MCAR WITH RESNET-101 BACKBONE AND THE INPUT SIZE OF 448?448.</figDesc><table><row><cell>Line No. 0 1 2 3</cell><cell>Methods Global Local Baseline ? MCAR ? ? ? ?</cell><cell>VOC 2007 92.9 93.4 ?0.5 94.2 ?1.3 94.8 ?1.9</cell><cell>MS-COCO 77.1 81.9 ?4.8 82.9 ?5.8 83.8 ?6.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table V ABLATIVE</head><label>V</label><figDesc>STUDY OF ATTENTIONAL MAPS SELECTION STRATEGY IN MCAR WITH RESNET-101 BACKBONE AND THE INPUT SIZE OF 448?448.</figDesc><table><row><cell cols="2">Methods Selection criteria</cell><cell>VOC 2007</cell><cell>MS-COCO</cell></row><row><cell></cell><cell>bottom4</cell><cell>93.8</cell><cell>81.2</cell></row><row><cell>MCAR</cell><cell>random4</cell><cell>94.2</cell><cell>82.1</cell></row><row><cell></cell><cell>top4</cell><cell>94.8</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table VI ABLATIVE</head><label>VI</label><figDesc>STUDY OF LEARNING STRATEGY IN MCAR WITH RESNET-101 BACKBONE AND THE INPUT SIZE OF 448?448.</figDesc><table><row><cell>Methods</cell><cell>Learning Strategy</cell><cell>VOC 2007</cell><cell>MS-COCO</cell></row><row><cell>Baseline</cell><cell>single</cell><cell>92.9</cell><cell>77.1</cell></row><row><cell>MCAR</cell><cell>single pair</cell><cell>94.4 94.8</cell><cell>82.6 83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table VII COMPARISONS</head><label>VII</label><figDesc>OF MAP IN % OF OUR METHODS AND BASELINE ON THE MS-COCO DATASET. COMPARED TO THE BASELINE METHOD, THE IMPROVEMENTS OF OUR METHOD ARE HIGHLIGHTED IN RED.Table VIII COMPARISONS OF MAP IN % OF OUR METHODS AND BASELINE ON THE PASCAL VOC 2007 DATASET. COMPARED TO THE BASELINE METHOD, THE IMPROVEMENTS OF OUR METHOD ARE HIGHLIGHTED IN RED..</figDesc><table><row><cell>Methods</cell><cell cols="2">MobileNet-v2</cell><cell cols="2">ResNet-50</cell><cell cols="2">ResNet-101</cell></row><row><cell>Input Size</cell><cell>256</cell><cell>448</cell><cell>256</cell><cell>448</cell><cell>256</cell><cell>448</cell></row><row><cell>Baseline</cell><cell>61.5</cell><cell>67.8</cell><cell>70.1</cell><cell>75.4</cell><cell>71.2</cell><cell>77.1</cell></row><row><cell>MCAR (GAP)</cell><cell>66.6 ?5.1</cell><cell>74.3 ?6.5</cell><cell>75.9 ?5.8</cell><cell>78.0 ?2.6</cell><cell>77.4 ?6.2</cell><cell>80.5 ?3.4</cell></row><row><cell>MCAR (GWP)</cell><cell>69.8 ?8.3</cell><cell>75.0 ?7.2</cell><cell>78.0 ?7.9</cell><cell>82.1 ?6.7</cell><cell>79.4 ?8.2</cell><cell>83.8 ?6.7</cell></row><row><cell>Backbone</cell><cell cols="2">MobileNet-v2</cell><cell cols="2">ResNet-50</cell><cell cols="2">ResNet-101</cell></row><row><cell>Input Size</cell><cell>256</cell><cell>448</cell><cell>256</cell><cell>448</cell><cell>256</cell><cell>448</cell></row><row><cell>Baseline</cell><cell>85.5</cell><cell>89.5</cell><cell>89.1</cell><cell>91.8</cell><cell>89.2</cell><cell>92.9</cell></row><row><cell>MCAR (GAP)</cell><cell>88.1 ?2.6</cell><cell>91.3 ?1.8</cell><cell>92.3 ?3.2</cell><cell>94.1 ?2.3</cell><cell>93.0 ?3.8</cell><cell>94.8 ?1.9</cell></row><row><cell>MCAR (GWP)</cell><cell>88.5 ?3.0</cell><cell>91.7 ?2.2</cell><cell>92.0 ?2.9</cell><cell>93.7 ?1.9</cell><cell>92.6 ?3.4</cell><cell>94.3 ?1.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table IX COMPARISONS</head><label>IX</label><figDesc>OF AVERAGE INFERENCE TIME OF PER-IMAGE BETWEEN OUR MCAR (INCLUDING EACH COMPONENT) AND BASELINES WITH DIFFERENT BACKBONES AND INPUT SIZES. THE TIME IS MEASURED IN MILLISECONDS (MS) ON ONE P40 GPU.</figDesc><table><row><cell>Methods</cell><cell>ImgSize</cell><cell>Baseline</cell><cell>Total</cell><cell cols="3">MCAR (topN =4) Global G-to-L Local</cell></row><row><cell>MobileNet-v2</cell><cell>256?256 448?448</cell><cell>6.7 6.9</cell><cell>22.7 34.3</cell><cell>6.6 6.9</cell><cell>9.2 13.0</cell><cell>6.9 14.4</cell></row><row><cell>ResNet-50</cell><cell>256?256 448?448</cell><cell>8.5 11.2</cell><cell>31.7 55.4</cell><cell>8.2 11.1</cell><cell>9.1 13.2</cell><cell>14.3 31.2</cell></row><row><cell>ResNet-101</cell><cell>256?256 448?448</cell><cell>15.7 18.4</cell><cell>46.8 82.8</cell><cell>16.1 18.8</cell><cell>8.6 13.5</cell><cell>22.1 50.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human attribute recognition by deep hierarchical contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HCP: a flexible cnn framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent attentional reinforcement learning for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CNN-RNN: a unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Order-free RNN with visual attention for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6714" to="6721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning graph convolutional networks for multi-label recognition and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forest before trees: The precedence of global features in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Navon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="197" to="200" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Time course of visual perception: coarse-to-fine processing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hegd?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="439" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attending to global versus local stimulus features modulates neural processing of low versus high spatial frequencies: an analysis with event-related brain potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Flevaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hillyard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning attentional policies for tracking and recognition in video with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="937" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3204" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DELTA: a deep dual-stream network for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint input and output space learning for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orderless recurrent models for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Yazici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V D</forename><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="440" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instance-aware hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2469" to="2479" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-label dictionary learning for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2712" to="2725" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep ranking for image zero-shot multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="6549" to="6560" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knowledge-guided multi-label few-shot learning for general image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2291" to="2320" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
