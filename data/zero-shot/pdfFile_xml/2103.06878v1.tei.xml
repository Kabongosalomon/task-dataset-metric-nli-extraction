<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diverse Semantic Image Synthesis via Probability Distribution Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhentao</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Cloud AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
							<email>jingliao@cityu.edu.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
							<email>qchu@</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<email>ganghua@gmail.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Wormpex AI Research LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diverse Semantic Image Synthesis via Probability Distribution Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic image synthesis, translating semantic layouts to photo-realistic images, is a one-to-many mapping problem. Though impressive progress has been recently made, diverse semantic synthesis that can efficiently produce semantic-level multimodal results, still remains a challenge. In this paper, we propose a novel diverse semantic image synthesis framework from the perspective of semantic class distributions, which naturally supports diverse generation at semantic or even instance level. We achieve this by modeling class-level conditional modulation parameters as continuous probability distributions instead of discrete values, and sampling per-instance modulation parameters through instance-adaptive stochastic sampling that is consistent across the network. Moreover, we propose prior noise remapping, through linear perturbation parameters encoded from paired references, to facilitate supervised training and exemplar-based instance style control at test time. Extensive experiments on multiple datasets show that our method can achieve superior diversity and comparable quality compared to state-of-the-art methods. Code will be available at https://github.com/tzt101/ INADE.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image synthesis has recently seen impressive progress, particularly with the help of generative adversarial networks (GANs) <ref type="bibr" target="#b6">[8]</ref>. Besides stochastic approaches that generate high-quality images from random latent variables <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b17">19]</ref>, conditional image synthesis is attracting equal or even more attention due to the practical advantages of its controllability. The conditional input, to guide the synthesis, can be of various forms, including RGB images, edge/gradient maps, semantic labels, etc. In this work, semantic image synthesis is one particular task that aims to generate a photo-realistic image from a semantic label mask. In particular, we further explore its diversity and controllability without loss of generation quality. Some samples are shown in <ref type="figure">Figure 1</ref>.</p><p>Previous works <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b38">42]</ref> propose solutions within the general image-to-image translation framework, which directly feeds the semantic mask into the encoder-decoder network. For higher quality, some recent methods <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b47">51,</ref><ref type="bibr" target="#b32">36]</ref> adopt spatially-varying conditional normalization to avoid the loss of semantic information due to conventional normalization layers <ref type="bibr" target="#b36">[40]</ref>. Although proven successful in synthesizing certain types of content, these methods lack controllability over the generation diversity, which is particularly important for such a one-to-many problem. Some methods <ref type="bibr" target="#b46">[50,</ref><ref type="bibr" target="#b40">44]</ref> attempt to yield multimodal results by incorporating variational auto-encoder (VAE) or introducing noises. However, these methods only support global imagelevel diversity. To obtain finer-grained controllability, a recent work <ref type="bibr" target="#b48">[52]</ref> proposes to use group convolution for different semantics to achieve semantic-level diversity. However, it is computationally expensive and difficult to be extended to support diversity at the instance level.</p><p>In this paper, we attempt to achieve controllable diversity in semantic image synthesis from the perspective of semantic probability distributions. The intuition is to treat each semantic class as one distribution, so that each instance of this class could be drawn from this distribution as a discrete sample. Following this idea, we propose a novel semantic image synthesis framework, which is naturally capable of producing diverse results at semantic or even instance level.</p><p>Specifically, our method contains three key ingredients. Firstly, we propose variational modulation models ( ? 3.2) that extend discrete modulation parameters to class-wise continuous probability distributions, which embed diverse styles of each semantic category in a class-adaptive manner. Secondly, based on the variational models built per normalization layer, we further develop an instance-adaptive sampling method ( ? 3.3) that achieves instance-level diversity by stochastically sampling modulation parameters from the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pants Hairs</head><p>Specified clothes Specified car Painting <ref type="figure">Figure 1</ref>. Semantic-level (left three columns) and Instance-level (right two columns) multimodal images generated by the proposed method. Text of each column indicates which semantic class or instance will be changed in the following results. variational models. We harmonize the sampling across the network via consistent randomness and a learnable transformation function for each normalization layer. Finally, to more efficiently embed the instance diversity to the modulation models, we propose prior noise remapping ( ? 3.4) that transforms the noise samples with perturbation parameters encoded from arbitrary references. We adopt this step to facilitate supervised training and enable test-time reference-based style guidance. Inspired by <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b34">38,</ref><ref type="bibr" target="#b33">37]</ref>, the proposed method is called INADE (INstance-Adaptive DEnormalization).</p><p>To evaluate the proposed method, we conduct extensive experiments on multiple datasets, including Cityscapes <ref type="bibr" target="#b2">[3]</ref>, ADE20K <ref type="bibr" target="#b44">[48]</ref>, CelebAMask-HQ <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b27">29]</ref>, and Deep-Fashion <ref type="bibr" target="#b26">[28]</ref>. Both quantitative and qualitative results show that our method significantly outperforms state-of-the-art methods by achieving much better instance-level diversity while keeping comparable generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conditional Image Synthesis</head><p>Conditional image synthesis aims at generating photorealistic images conditioned on different types of input. We are interested in a special form of it, called semantic image synthesis, which takes segmentation layouts as input. Many impressive works have been proposed for this task. The most representative work, Pix2Pix <ref type="bibr" target="#b13">[15]</ref> adopts an encoderdecoder generator for unified image-to-image translation. Pix2pixHD <ref type="bibr" target="#b38">[42]</ref> improves Pix2Pix by proposing coarseto-fine generator and discriminators. Subsequent meth-ods <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr" target="#b42">46,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b47">51]</ref> further explore how to synthesize high quality images from semantic masks and achieve significant improvements. Besides using class-level semantic masks, some works also consider instance-level information for image synthesis, since the semantic mask itself does not provide sufficient information to synthesize instances especially in complex environments with multiple of them interacting with each other. Some works <ref type="bibr" target="#b38">[42,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b33">37]</ref> extract boundary information from the instance map and concatenate it with the semantic mask. While recent work <ref type="bibr" target="#b5">[6]</ref> proposes to use the instance map to guide convolution and upsampling layers for better exploiting both semantic and instance information. Different from these methods, we are interested in taking full advantage of information from instance maps to achieve instance-level diversity control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Diversity in Image Synthesis</head><p>Diversity is a core target for image synthesis, which aims to generate multiple possible outputs from a single input image. Early conditional image synthesis networks either trained with paired data, like Pix2Pix <ref type="bibr" target="#b13">[15]</ref> and Pix2pixHD <ref type="bibr" target="#b38">[42]</ref>, or with unpaired data, like Cycle-GAN <ref type="bibr" target="#b45">[49]</ref>, DiscoGAN <ref type="bibr" target="#b18">[20]</ref> and UNIT <ref type="bibr" target="#b24">[26]</ref>, are singlemodal. They produce one single output conditioned solely on an input image. Later, some multimodal unpaired image synthesis networks <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b0">1]</ref> are proposed. However, constrained by the reconstruction loss, the semantic image synthesis task trained with paired data is more difficult to support diversity. Simply concatenating a random noise vector to the input segmentation mask is usually not effective, because the generator often ignores the additional noise vectors and mode collapse may occur easily. To tackle this problem, BicycleGAN <ref type="bibr" target="#b46">[50]</ref> enforces the bijection mapping between the noise vector and target domain. DSCGAN <ref type="bibr" target="#b40">[44]</ref> proposes a simple regularization which can be easily integrated into most conditional GAN objectives. More recently, a variational autoencoder architecture is used to handle multimodal synthesis by <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b25">27]</ref>. However, these multimodal image synthesis networks only support diversity at the global level. To further control the diversity at the semantic level, the method proposed by <ref type="bibr" target="#b7">[9]</ref> builds several auto-encoders for each face component to extract different component representations. GroupDNet <ref type="bibr" target="#b48">[52]</ref> unifies the generation process in only one model, but still requires high computing resources, and the use of group convolution layer makes it difficult to extend to the instance level. In contrast, we propose a novel instance-aware conditional normalization framework that allows diverse instance-level generation with less overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We are interested in the task of semantic image synthesis, which is defined as to map a semantic mask m ? L H?W In addition to this basic formulation <ref type="bibr" target="#b38">[42,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b33">37]</ref>, instance-aware semantic image synthesis <ref type="bibr" target="#b5">[6]</ref> adopts the instance map p ? L H?W p as an extra input, which differentiates different object instances sharing a same semantic label by denoting each individual instance with a unique index from the instance label set L p = {1, 2, . . . , L p } in the image. By enforcing an identical semantic label within each instance, pixels belonging to a same instance label l p in p should always have a same semantic label l m in m. We represent instance to semantic label mapping as a function l m = G(l p ).</p><p>Overall, image synthesis with instance information can be basically formulated as a function T (m, p) : (L m , L p ) ? R 3 . And feed-forward image translation neural networks, trained in a supervised manner, can be used to model this function. In the following, we introduce the proposed method with both the inputs of semantic and instance maps. When there is no instance label, p degenerates into m. And the diversity of the synthesized images changes from the instance level to the semantic level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Normalization</head><p>Our solution to semantic image synthesis is based on a novel instance-level conditional normalization method. Before introducing our method, here we give a brief overview of the general framework of conditional normalization first.</p><p>Let X i ? R C i ?H i ?W i be the activation tensor to the i-th normalization layer, where C i , H i , W i are the channel depth, height, and width, respectively. In the channel-wise normalization framework similar to <ref type="bibr" target="#b12">[14]</ref>, we can generally formulate the normalization operations as two steps: In the normalization step, X i is normalized toX i by channelwise mean and standard deviation {? i , ? i } ? R C i in the mini-batch containing X i . Then, the modulation step scales and translatesX i with learned modulation param-</p><formula xml:id="formula_0">eters {? i , ? i } ? R C i ?H i ?W i</formula><p>, which are not necessarily channel-wise constant. Let Y i be the output, for each element (k ? C i , x ? H i , y ? W i ) in the tensor, we have:</p><formula xml:id="formula_1">X i k,x,y = (X i k,x,y ? ? i k )/? i k , Y i k,x,y = ? i k,x,yX i k,x,y + ? i k,x,y .<label>(1)</label></formula><p>For conditional normalization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">12]</ref>, the modulation parameters ? i and ? i are learned with extra conditions. Specifically, for semantic image synthesis, the modulation is usually conditioned on the semantic mask m [30, 37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variational Modulation Model</head><p>Conditional normalization ( ? 3.1), either spatiallyadaptive <ref type="bibr" target="#b28">[30]</ref> or class-adaptive <ref type="bibr" target="#b33">[37]</ref>, has been proven helpful for semantic image synthesis. The semantic-conditioned modulation is able to largely prevent the "wash-away" effect of semantic information caused by repetitive normalizations. However, challenges still exist to achieve promising generation results with semantic-level or even instancelevel diversity, given that normalization is solely conditioned on the semantic map and only global randomness is used to diversify the image styles <ref type="bibr" target="#b28">[30]</ref>. Semantic-level diversity is realized by <ref type="bibr" target="#b48">[52]</ref> through group convolution, but using this convolution cuts off the possibility of its extension to instance-level diversity through instance map. Recent efforts on instance-aware synthesis <ref type="bibr" target="#b38">[42,</ref><ref type="bibr" target="#b5">6]</ref> are majorly focused on better object boundaries, but not the diversity and realism of each individual instance. Due to the lack of proper instance conditioning, existing methods tend to converge instances with the same semantic label into a similar style, which significantly harms the diversity of generation. The key to instance-level diversity is a proper combination of uniform semantic-level distributions that deterministically decide the general features of a particular semantic label, and instance-level randomness that introduces allowed diversity covered by the semantic distribution models. Therefore, we model the modulation parameters as parametric probability distributions for each semantic label l m ? L m , instead of discrete values. With such a, namely, variational modulation model, given an instance l p ? L p , instance-level diversity is achievable via sampling modulation parameters from the probability distributions of the corresponding semantic label G(l p ). For the sake of simplicity and efficiency, following <ref type="bibr" target="#b33">[37]</ref>, we make the modulation parameters spatially-invariant and only depend on the local instance labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Noise Remapping</head><formula xml:id="formula_2">INADE ResBIK INADE ResBIK ? INADE ResBIK INADE ReLU 3?3 Conv INADE ReLU 3?3 Conv ? ?? ? ?? [ ], ? [ ] IAP IAP Instance Average Pooling (IAP) ? [ = 3] [ ], ? [ ] sample sample ? [ ] ? [ ] INADE generator INADE ResBlk [ ] [ ] encoder [ = 2] [ = 1]</formula><p>Specifically, for each semantic category l m , its channelwise modulation parameters are modeled as learnable probability distributions, which are built for each normalization layer in the network respectively. Formally, for the i-th layer with channel depth</p><formula xml:id="formula_3">C i , we have {a i ? , b i ? , a i ? , b i ? } ? R L m ?C i</formula><p>as the distribution transformation parameters of ? and ?, respectively. All of them are treated as learnable parameters that are jointly trained with the network. Given stochastic noise matrices {N i ? , N i ? } ? R L p ?C i from the same distribution for sampling, the corresponding modulation parameters of one instance label l p in p are:</p><formula xml:id="formula_4">? i [l p ] = a i ? [G(l p )] ? N i ? [l p ] + b i ? [G(l p )], ? i [l p ] = a i ? [G(l p )] ? N i ? [l p ] + b i ? [G(l p )],<label>(2)</label></formula><p>where ? represents element-wise multiplication, and [?] accesses the vector from a matrix in the row-major order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance-Adaptive Modulation Sampling</head><p>Our multimodal synthesis method follows the basic form of conditional modulation ( ? 3.1), but further extends the conditional inputs to include not just the segmentation mask m, but also the instance map p and random noises to initiate sampling, as shown in <ref type="figure">Figure 2</ref>. Utilizing our variational modulation models ( ? 3.2), we are able to generate diverse modulation parameters obeying the same set of probability distributions. However, considering that the generation network contains multiple conditional normalization layers, a unified sampling solution is still essential to harmonize all these layers. A straight-forward approach, independent stochastic sampling for each normalization layer, could potentially introduce inconsistency and cause the diversity to be severely neutralized. Therefore, in this paper, we propose an instance-adaptive modulation sampling method that achieves consistent instance sampling across multiple normalization layers with unequal channel depths.</p><p>To initialize, for each layer i, we resize and convert each input pair of semantic mask m and instance map p into the one-hot format as</p><formula xml:id="formula_5">M i ? B L m ?H i ?W i and P i ? B L p ?H i ?W i</formula><p>, respectively, which will then be used as the conditional inputs to that layer, as shown in <ref type="figure">Figure 8</ref>. Here, B represents the Boolean domain, and L m , L p are the aforementioned total numbers of semantic/instance labels.</p><p>For the sake of simplicity, since scale ? i and shift ? i are generated similarly and independently, without loss of generality, here we take scale ? i as the example. The sampling contains the following steps.</p><p>First of all, random samples N ? ? R L p ?C 0 are independently sampled from the standard normal distribution: N ? ? N (0, 1). We use the same set of random noise samples N ? for all instances of normalization layers in the network, which helps enforce consistent instance styles throughout the network. Here C 0 is a hyper-parameter that defines the number of the initial sampling channels.</p><p>To sample the modulation parameters for each normalization layer i, we translate the initial samples N ? t? N i ? with a learnable linear transformation mapping F i ? :</p><formula xml:id="formula_6">R L p ?C 0 ? R L p ?C i :N i ? = F i ? (N ? ),<label>(3)</label></formula><p>where C i is exactly the channel depth of i-th activations X i , so that the outputN i ? ? R L p ?C i assigns a transformed sample for each instance per each channel, in a spatiallyinvariant manner. Thus, the same source of randomness helps achieve style consistency, while the learnable transformations enforce compatible target dimensions and preserve certain ability to adapt the samples for each layer.</p><p>Finally, given the distribution transformation parameters a i ? , b i ? and transformed noise samplesN i ? , the scale parameters ? i are calculated with Equation 2. And similarly for the shift parameters ? i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Prior Noise Remapping</head><p>While our variational modulation models ( ? 3.2) help achieve instance-level diversity, the noises, sampled regardless of instance styles ( ? 3.3), can potentially introduce ambiguities during the supervised training (especially for the popular perceptual and feature matching losses), since similar noise samples can possibly correspond to instances of distinct styles. This will affect the effective diversity of generated instances and prohibit the possibility to control the instance styles with certain references.</p><p>In light of this, we propose a prior noise remapping step, during which a set of linear perturbation parameters are encoded from given references, to remap the noise samples while preserving the original distribution, in order to provide guidance to embed more meaningful instance diversity in the modulation models. To achieve this, we adopt a noise remapping encoder E(r) that translates the reference image r ? R H?W into four perturbation maps {? ? ,b ? ,? ? ,b ? } ? R H?W , which are per-pixel linear transformation parameters, including scale? and shiftb, for both N ? and N ? . A instance aware partial convolution <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b23">25]</ref> is used to avoid information contamination between different instances. Based on these dense perturbation maps, we apply an instance average pooling layer to each of these maps to get the instance-wise perturbation</p><formula xml:id="formula_7">parameters {? ? ,b ? ,? ? ,b ? } ? R L p</formula><p>. Take N ? as an example, for an instance label l p ? L p and its occupying pixels</p><formula xml:id="formula_8">x(l p ) = {x|p[x] = l p }, we hav? a ? [l p ] = ( x?x(l p )? ? [x])/|x(l p )|, b ? [l p ] = ( x?x(l p )b ? [x])/|x(l p )|.<label>(4)</label></formula><p>This remapping encoder, together with the main generator, forms a variational autoencoder (VAE) <ref type="bibr" target="#b20">[22]</ref>. The remapped noise samples after perturbation are:</p><formula xml:id="formula_9">N ? [l p ] =? ? [l p ]N ? [l p ] +b ? [l p ],<label>(5)</label></formula><p>where KL-Divergence loss <ref type="bibr" target="#b20">[22]</ref> is used to enforce a same normal distribution? ? ? N (0, 1). These remapped noise samples are used instead of N ? during modulation sampling, as described in ? 3.3.</p><p>During training, the reference r is exactly the groundtruth paired image. At test time, while initially sampled noises can be used to achieve random style synthesis by default, as described in ? 3.3, it is also allowed to provide r as instance references to control the style of the result at the instance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our INADE generator <ref type="figure">(Figure 8</ref>) follows a similar architecture of the SPADE generator <ref type="bibr" target="#b28">[30]</ref>, but with all the SPADE layers replaced by the INADE layers. Following SPADE <ref type="bibr" target="#b28">[30]</ref>, the overall loss function consists of four loss terms: conditional adversarial loss, feature matching loss <ref type="bibr" target="#b38">[42]</ref>, perceptual loss <ref type="bibr" target="#b14">[16]</ref> and KL-Divergence loss <ref type="bibr" target="#b20">[22]</ref>. Details are provided in the supplementary material.</p><p>During training, by default, Adam optimizer <ref type="bibr" target="#b19">[21]</ref> (? 1 = 0, ? 2 = 0.9) is used with fixed epoch number of 200. The learning rates for the generator and the discriminator are set to 0.0001 and 0.0004 respectively, which are gradually decreased to zero after 100 epochs. Noise C 0 has 64 initialized channels, while the input noise has 256 channels, same as <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b25">27]</ref>. All experiments are implemented in Py-Torch <ref type="bibr" target="#b29">[31]</ref> and conducted on TITAN XP GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Metrics</head><p>Datasets. Experiments are conducted on five popular datasets: Cityscapes <ref type="bibr" target="#b2">[3]</ref>, ADE20K <ref type="bibr" target="#b44">[48]</ref>, CelebAMask-HQ <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b27">29]</ref>, DeepFashion <ref type="bibr" target="#b26">[28]</ref>, and DeepFashion2. DeepFashion2 is built from DeepFashion that each image contains two persons, which is only used for testing. More details can be found in the supplementary material. Quality Metrics. To evaluate the result quality, we adopt two types of metrics following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">42,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b33">37]</ref>. One is the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b9">[11]</ref>, which measures the distance of distributions between results and real images.</p><p>The other one is semantic-segmentation-based, which evaluates the semantic segmentation accuracy on the results by comparing the predicted masks with the groundtruth layouts on both mean Intersection-over-Union (mIoU) and pixel accuracy (accu). State-of-the-art pretrained segmentation models are used for different datasets: DRN <ref type="bibr" target="#b41">[45,</ref><ref type="bibr">7]</ref> for Cityscapes, UperNet101 <ref type="bibr" target="#b39">[43,</ref><ref type="bibr" target="#b3">4]</ref> for ADE20k, and UNet <ref type="bibr" target="#b31">[34,</ref><ref type="bibr">35]</ref> for CelebAMask-HQ. For DeepFashion, we use the same UNet-based network but train the model by ourselves. For fair evaluation, we run the model for 10 times and report the average scores when noise input is required. Diversity Metrics. To evaluate the diversity of the results, we adopt the LPIPS metric as proposed by <ref type="bibr" target="#b43">[47,</ref><ref type="bibr">33]</ref>. Similar to <ref type="bibr" target="#b48">[52]</ref>, we also adopt two metrics to measure the semanticlevel diversity (mCSD and mOCD) and expand them to instance level (mISD and mOID). More details can be found  in the supplementary material. Subjective Metrics. We conduct human evaluations to assess both the quality and the diversity of the methods. For quality, we ask the volunteers to select the most realistic one among the results generated by different methods on the same input. mHE (mean Human Evaluation) denotes the percentage of results being selected for each method.</p><p>For diversity, we expand the metric proposed by <ref type="bibr" target="#b48">[52]</ref> to the instance level. A pair of results, with one random semantic class or instance manipulated, are given to volunteers. The percentage of pairs that are judged to be different in only one area represents the human evaluation, namely SHE (Semantic Human Evaluation) and IHE (Instance Human Evaluation). We invite 20 volunteers for evaluation and the evaluated number of images is 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative and Qualitative Comparisons</head><p>We compare INADE with several SOTA works, including quality-oriented (pix2pixHD <ref type="bibr" target="#b38">[42]</ref>, SPADE <ref type="bibr" target="#b28">[30]</ref>, CLADE <ref type="bibr" target="#b34">[38,</ref><ref type="bibr" target="#b33">37]</ref> and SEAN <ref type="bibr" target="#b47">[51]</ref>) and diversity-oriented (BicycleGAN <ref type="bibr" target="#b46">[50]</ref>, DSCGAN <ref type="bibr" target="#b40">[44]</ref>, and GroupDNet <ref type="bibr" target="#b48">[52]</ref>) methods. For a fair comparison, we directly use the pretrained models provided by the authors when available, otherwise train the models by ourselves using the codes and settings provided by the authors. For SPADE, which has the strategy for multimodal synthesis, we train the model with that extra encoder but ignore it when testing its diversity performance (namely VSPADE). Reference images are not allowed for any of the methods during testing except for SEAN which requires the reference input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Multimodal Image Synthesis</head><p>Several methods that support multimodal image synthesis are compared to demonstrate the superior diversity performance of INADE. In additional, we also compare with two ablation baselines: w/o US (without Unified Sampling, sampled independently, while w/o PNR means that the PNR is not used during training. The quantitative results on the DeepFashion dataset are summarized in <ref type="table">Table 1</ref>.</p><p>In general, INADE achieves superior performance regarding both quality and diversity compared to previous methods. For single-subject images in the DeepFashion dataset, our method exhibits better performance than Bicy-cleGAN, DSCGAN, and VSPADE in terms of FID, and is comparable to GroupDNet. While for multiple-subject images (DeepFashion2), INADE shows the lowest FID.</p><p>In terms of diversity, our method achieves the best <ref type="table">Table 1</ref>. Comparison with other multimodal methods on diversity. mHE, SHE and IHE are aforementioned metrics. ? and ? represent the higher the better and the lower the better. Bold and underlined numbers are the best and the second best of each metric, respectively.  <ref type="table">Table 2</ref>. Comparison with SOTA methods on result quality. All the numbers are collected by running the evaluation on our machine. Here M, A, F, and L represent mIoU, accu, FID, and LPIPS, respectively. Note that the L score of SEAN is almost zero even with noise input.  scores on metrics including overall measurement (LPIPS) and semantic-level/instance-level metrics (mCSD/mISD). As for mOCD, methods only support image-level diversity, such as BicycleGAN, DSCAN, and VSPADE, produce much more unwanted changes outside the instance area.</p><formula xml:id="formula_10">Methods DeepFashion DeepFashion2 FID ? LPIPS ? mCSD ? mOCD ? mHE ? SHE ? FID ? LPIPS ? mISD ? mOID ? mHE ? IHE ?<label>BicycleGAN</label></formula><formula xml:id="formula_11">Methods Cityscapes ADE20K CelebAMask-HQ DeepFashion M ? A ? F ? L ? M ? A ? F ? L ? M ? A ? F ? L ? M ? A ? F ? L ? SPADE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Although both being relatively low, INADE is higher than</head><p>GroupDNet, which is because GroupDNet uses group convolution to prevent premature fusion between features of different classes, while INADE uses conventional convolution for more consistent combinations. For mOID, as the only method that supports instance-wise control, INADE easily gets the best score. More analysis of mOID and mOCD can be found in the supplementary material. As for subjective evaluations, our method also outperforms others on both semantic-and instance-level cases.</p><p>Compared to the two ablation baselines, we find that both prior noise remapping and unified sampling play indispensable roles in our method. Removing prior noise remapping (w/o PNR) leads to ambiguities during the supervised training, which seriously affects the quality and diversity of synthesized results. Independently sampling (w/o US) for each normalization destroys the consistency of information and significantly degenerates the generation result.</p><p>The qualitative comparisons are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Although all methods support multimodal synthesis, the quality by BicycleGAN and DSCGAN is not satisfactory. VS-PADE achieves good visual quality, but does not support semantic-or instance-level control. GroupDNet is capable of changing the appearance of a specific semantic class (results on the left), but tends to generate identical style for different cloth instances (results on the right). On the contrary, INADE supports both fine-grained multimodal controls with high visual fidelity. As for the ablation baselines, we notice that removing PNR significantly decreases the quality, while the whole task fails without US. <ref type="table">Table 3</ref>. Comparison with other semantic image synthesis methods on model complexity and efficiency. All the numbers are collected by running the evaluation on Titan XP. Here P and T denote the number of generator parameters and inference run time, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Cityscapes  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Semantic Image Synthesis</head><p>The quantitative comparisons against semantic image synthesis methods are summarized in <ref type="table">Table 2</ref>. Compared to methods that don't support multimodal (i.e. 0 LPIPS), especially SEAN which has additional reference image input, INADE has an advantage or near the best on almost all metrics. It seems that SPADE has slight advantage in segmentation metrics, but INADE still shows its overall superiority when considering FID score and visual results.</p><p>Compared to existing multimodal methods, INADE leads all metrics. In terms of quality (e.g. mIoU, acc, and FID), BicycleGAN and DSCGAN are much lower than ours on all datasets. GroupDNet achieves similar or slightly better performance on CelebAMask-HQ and Deep-Fashion datasets, but has a significant gap on more complicated scenes such as Cityscapes and ADE20K. This demonstrates the superiority of the proposed method on synthesizing complex scenes. The LPIPS score shows that all these methods are able to generate multimodal images to some extent. BicycleGAN gets higher scores than ours on some datasets, but is not able to do high-quality synthesis. GroupDNet shows good performance on person-related tasks, but falls into strong bias when dealing with complex scenes which greatly restricts its performance. Therefore, considering both quality and diversity, our method achieves the best overall performance.</p><p>Qualitative comparisons on these four datasets are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. In general, the images generated by INADE are more realistic than others on various datasets, which is consistent with the quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Computational and Model Complexity</head><p>In this section, we analyze the computational and model overhead of different methods. The quantitative results (generator networks only) are summarized in <ref type="table">Table 3</ref>.</p><p>BicycleGAN and DSCGAN share a similar small network architecture with the least parameters, FLOPs (floating-point operations per second), and run-time cost. However, the quality of the synthesized images is far from satisfactory. CLADE seems to get a good trade-off between performance and efficiency, but still falls short of INADE in terms of overall performance and functionality. Compared to all other methods, INADE achieves the smallest network (parameters and FLOPs), as well as one of the fastest run-time performance. Specifically, compared with GroupDNet, the only method that can achieve semanticlevel diversity, our method provides control over both semantic and instance levels with much less overhead, introducing 82% ? 89% fewer FLOPs and 59% ? 79% less inference time compared with GroupDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Applications</head><p>Thanks to its superior capability of controllable diverse synthesis, INADE can be used in many image editing applications. Here we show two examples as follows. Reference appearance editing. With the noise remapping mechanism described in ? 3.4, we can extract the instancewise style from an arbitrary reference image. This makes it possible for INADE to perform reference-based editing to different parts of an image at the instance level. As shown in <ref type="figure" target="#fig_4">Figure 6</ref> (a), we can change the appearance of hairstyles, tops, and pants to match the reference. Semantic manipulation. Similar to most existing semantic image synthesis methods, INADE also supports semantic manipulation. We show some examples in <ref type="figure" target="#fig_4">Figure 6</ref> (b), such as changing the semantic class to an object, or insert a new semantic object into the image. And more creative editing results can be achieved by modifying the semantic mask and the instance map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we focus on multimodal image synthesis and propose a novel diverse semantic image synthesis method based on instance-aware conditional normalization. Different from previous works, we learn the class-wise probability distributions and perform instance-wise stochastic sampling to generate the per-instance modulation parameters. Our method improves the network's ability to model semantic categories and make it easier to synthesize diverse images at semantic-or instance-level without scarifying the visual fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional Implementation details</head><p>Network architectures. Here we give detailed network designs for each part. <ref type="figure">Figure 7</ref> shows the architecture of our encoder. We use instance partial convolution and instance average pooling to get the parameters of each instance independently. The architecture of generator network is shown in <ref type="figure">Figure 8</ref>. The synthesis process starts with a random noise and goes through a series of the proposed INADE ResBIKs. Since the training is carried out on multiple GPUs, the batch normalization layer in INADE adopts the synchronous version. We use a multi-scale Path-GAN <ref type="bibr" target="#b13">[15]</ref> based discriminator whose architecture is shown in <ref type="figure">Figure 9</ref>. Loss function. The loss function we adopted consists of four components:</p><p>Conditional adversarial loss. Let E be the prior noise remapping, G be the INADE generator, D be the discriminator, m be a given semantic mask, o and p be the corresponding image and instance map. The conditional adversarial loss built with hinge loss is formulated as:</p><formula xml:id="formula_12">L GAN (E, G, D) = E[max(0, 1 ? D(o, m, p))] +E[max(0, 1 + D(G(E(o, p), m, p), m, p))].<label>(6)</label></formula><p>Feature matching loss. Let D i and N i be the output feature maps and the number of elements of the i-the layer of D respectively, S D and E D be the start number of layer for loss calculation and total number layers in D respectively. The feature matching loss is denoted as: <ref type="figure">E(o, p)</ref>, m, p), m, p)) 1 ].</p><formula xml:id="formula_13">L F = E E D i=S D 1 N i [ D i (o, m, p)? D i (G(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>To reduce the ambiguity, we only use high-level features and set S D to 3. Perceptual loss. Let V i and M i be the output feature maps and the number of elements of the i-the layer of VGG network respectively, S V and E V be the start number of layer for loss calculation and total number layers in VGG network respectively. The perceptual loss is denoted as:</p><formula xml:id="formula_14">L P = E E V i=S V 1 M i [ V i (o) ? V i (G(E(o, p), m, p)) 1 ].<label>(8)</label></formula><p>Similar to feature matching loss, we only use high-level features and set S D to 3. KL-Divergence loss. Let q ? (z|o, p) and q ? (z|o, p) be the variational distribution of N ? and N ? respectively. p(z) be a standard Gaussian distribution. The KL-Divergence loss is denoted as:</p><p>L KL = 0.5 ? (D(q ? (z|o, p) p(z)) + D(q ? (z|o, p) p(z))).</p><p>The overall loss is made up of the above-mentioned loss terms as: <ref type="bibr" target="#b8">(10)</ref> Following SPADE, We set ? 1 = 10, ? 2 = 10, ? 3 = 0.05.</p><formula xml:id="formula_16">min E,G (max D (L GAN ) + ? 1 L F + ? 2 L P + ? 3 L KL ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Details of Datasets</head><p>The details about each dataset are described as follows:</p><p>? Cityscapes dataset <ref type="bibr" target="#b2">[3]</ref> is a widely used dataset for semantic image synthesis <ref type="bibr" target="#b37">[41,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b38">42]</ref>. The highresolution images with fine semantic and instance annotations are taken from street scenes of German cities. There are 2,975 training images and 500 validation images. The number of annotated semantic classes is 35.</p><p>? ADE20K dataset <ref type="bibr" target="#b44">[48]</ref> consists of 25,210 images (20,210 for training, 2,000 for validation and 3,000 for testing). The images in ADE20K dataset cover a wide range of scenes and object categories, including a total of 150 object and stuff classes.</p><p>? CelebAMask-HQ dataset <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b27">29]</ref> is based on Cele-bAHQ face imgae dataset. It contains of 28,000 training images and 2,000 validation images with 19 different semantic classes.</p><p>? DeepFashion dataset <ref type="bibr" target="#b26">[28]</ref> contains 52,712 person images with fashion clothes. We use the processed dataset provided by GroupDNet <ref type="bibr" target="#b48">[52]</ref> which consists of 30,000 training images and 2,247 validation images. There are 8 different semantic classes.</p><p>? DeepFashion2 dataset is built from DeepFashion. We combine two adjacent images to generate the images containing two persons. The new semantic mask and the instance map are also derived from the corresponding two semantic masks. This dataset is only used to evaluate the performance of models trained on Deep-Fashion dataset in terms of instance level diversity.</p><p>In these datasets, Cityscapes and DeepFashion2 have semantic and instance annotations, while the rest have only semantic annotations. In our experiment, the resolution of images is 256?256 except that Cityscapes dataset is 256?512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Details of Diversity Metrics</head><p>We adopt the LPIPS <ref type="bibr" target="#b43">[47,</ref><ref type="bibr">33]</ref> to evaluate the overall diversity of the results. Specifically, we generate 10 groups of images or evaluation with randomly sampled noise, and calculate the diversity score between 2 random groups at a time. A total of 10 scores are calculated, and we measure the mean of these scores to reduce the potential fluctuation caused by random sampling.</p><p>To evaluate the instance-level diversity, we expand the metrics proposed by <ref type="bibr" target="#b48">[52]</ref>, called mean Instance-Specific Diversity (mISD) and mean Other-Instances Diversity (mOID), which represent the degree of change inside and outside the instance region when being manipulated. Specially, we generate several images by changing sampled noise for specified instance while keeping the noise for others unchanged. Then, the similarity inside and outside the instance region between these images makes up the mISD and mOID metrics. For datasets which have no instance annotations, these metrics degenerate to semantic level (mean Class-Specific Diversity (mCSD) and mean Other-Classes Diversity (mOCD)) which are the same with <ref type="bibr" target="#b48">[52]</ref>. A high diversity inside the instance area (high mISD), as well as a low outside diversity (low mOID), are desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Additional ablation study</head><p>Here we give the additional ablation study for C 0 which represents the length of the initial sampling. Intuitively, the longer the sampling length is, the higher the diversity of the synthesized image will be. We conduct experiments on the Cityscapes and CelebAMask-HQ datasets, which include complex street scenes and delicate facial images. As summarized in <ref type="table" target="#tab_4">Table 4</ref>, we compare the default setting (C o = 64) with two variant settings: a shorter sampling length (C o = 8, INADE-8) and a longer sampling length (C o = 128, INADE-128). We find that INADE-8 shows the lower LPIPS score than INADE, while IANDE-128 correspondingly gets the highest score in this metric. And the model with the default setting (INADE) gets the best scores in terms of quality metrics. In our understanding, a short sampling length (e.g. 8) may limit the information capacity, thus reducing the generation quality (low scores of mIoU, acc and FID) and diversity (low score of LPIPS). In contrast, a longer sampling length (e.g. 128) can increase the diversity of the synthesized image (high score of LPIPS), but also increases the difficulty of high-quality image generation (low scores of mIoU, acc and FID).</p><p>In terms of model parameters, FLOPs and run time, INADE-8 is best, but the advantage is not obvious compared with INADE and INADE-128. Based on the above results, we set C o = 64 on different datasets by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Additional results</head><p>In <ref type="figure">Figure 10</ref>, we show more multi-modal qualitative results on different datasets that only change one specified class or instance. The conclusions are basically the same as we mention in the main submission. BicycleGAN, DSC-GAN and VSPADE show the global style controllabitity, GroupDNet expands it to semantic level, while the synthesis results of our method can be controlled at both the semantic level and instance level. We notice that in some results,  <ref type="figure">Figure 7</ref>. Architecture of our encoder network. We use UNet <ref type="bibr" target="#b31">[34]</ref> based network to extract the features with the same resolution of input image, and then obtain the {??,b?,? ? ,b ? } through independent instance partial convolution (InstConv) and instance average pooling (InstPool).</p><p>when we change one part, other parts slightly change as well, which is also mentioned in GroupDNet <ref type="bibr" target="#b48">[52]</ref>. In fact, this is reasonable in some cases to increase the generation fidelity. For example, as shown in <ref type="figure">Figure 10</ref> (h), the lighting often changes with the sky, if the appearance of the grass is totally unchanged, the final generated image will look unnatural to some extent. Therefore, though the metric mOCD (or mOID) may be a good indication of semantic/instancelevel controllability, a slightly high mOCD or mOID do not represent worse quality. In other words, we do not expect them to be zero in real applications. In <ref type="figure">Figure 11</ref>, <ref type="figure">Figure 12</ref>, <ref type="figure" target="#fig_1">Figure 13</ref>, <ref type="figure" target="#fig_2">Figure 14</ref>, we further show additional qualitative comparison results between the proposed INADE and other methods on the DeepFashion, Cityscapes, ADE20K and CelebAMask-HQ datasets. These results show that the images quality of INADE is better than or at least comparable to existing methods.  <ref type="figure" target="#fig_1">Figure 13</ref>. Qualitative comparison of our INADE with previous state-of-the-art methods on the ADE20K dataset.  <ref type="figure" target="#fig_2">Figure 14</ref>. Qualitative comparison of our INADE with previous state-of-the-art methods on the CelebAMask-HQ dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 2 .</head><label>22</label><figDesc>The illustration diagram of the proposed INstance-Adaptive DEnormalization (INADE). It combines semantic-level distribution modeling and instance-level noise sampling. IGS denotes the Instance Guided Sampling which is similar to the guided sampling in [37]. to a photo-realistic image o ? R 3?H?W . Here, m is a class-level label map with each pixel representing an integer index to a pre-defined set of semantic categories L m = {1, 2, . . . , L m }. Each pair of input m and output o is spatially-aligned and of the same dimension H ? W , so that the synthesized content in o should comply with the corresponding semantic labels in m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The overall framework of the proposed INADE generator, which consists of a remapping encoder E and INADE generator. E is used to transform the noise sample based on arbitrary references ( ? 3.4), while the generator consists of several INADE ResBlks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison with other multimodal models and two baselines. The results on the left show the performance of class level diversity while the results on the right are for instance level diversity. The first two rows represent the results of different models when changing upper clothes while the last two rows represent the results of changing pants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>?3.3) and w/o PNR (without Prior Noise Remapping, ?3.4). w/o US means that the noise for each INADE layers are Qualitative comparison with the state-of-the-art semantic image synthesis methods on four datasets: DeepFashion, CelebAMask-HQ, Cityscapes and ADE20K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Exemplar applications of the proposed INADE. (a) Results of our method for reference appearance editing. From left to right, we change the appearance of part of the target image based on the reference image (the changed instance is indicated by the text in the blue dotted box). (b) Application of our method for semantic manipulation. The text in the blue dotted box indicates what is edited each time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ADE20K CelebAMask-HQ DeepFashion P (M) FLOPs (G) T (s) P (M) FLOPs (G) T (s) P (M) FLOPs (G) T (s) P (M) FLOPs (G)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>BicycleGAN 30.47 78.26 59.87 0.122 5.33 42.68 77.49 0.443 65.98 89.77 35.73 0.362 73.09 96.75 31.10 0.225 DSCGAN 43.70 87.80 50.84 0.216 8.07 58.10 82.30 0.324 75.98 95.08 52.83 0.198 75.92 96.97 29.79 0.146 GroupDNet 59.20 92.78 41.12 0.073 26.09 73.07 39.11 0.177 76.13 95.21 29.39 0.309 76.19 97.48 9.72 0.222 INADE 61.02 93.16 38.04 0.248 34.96 78.51 29.60 0.400 74.08 94.31 22.55 0.365 76.27 97.44 9.96 0.225</figDesc><table><row><cell></cell><cell cols="2">61.38 93.26 51.98</cell><cell>0</cell><cell cols="2">36.28 78.13 29.79</cell><cell>0</cell><cell>75.22 94.76 31.40</cell><cell>0</cell><cell>76.76 97.65 11.22</cell><cell>0</cell></row><row><cell cols="3">pix2pixHD 60.50 93.06 66.04</cell><cell>0</cell><cell cols="2">27.27 72.61 45.87</cell><cell>0</cell><cell>76.11 95.67 36.95</cell><cell>0</cell><cell>73.99 97.02 15.27</cell><cell>0</cell></row><row><cell>CLADE</cell><cell cols="2">60.44 93.42 50.62</cell><cell>0</cell><cell cols="2">35.43 77.37 30.48</cell><cell>0</cell><cell>75.37 95.05 33.54</cell><cell>0</cell><cell>75.63 97.33 12.76</cell><cell>0</cell></row><row><cell>SEAN</cell><cell cols="2">56.22 92.28 50.43</cell><cell>0</cell><cell cols="2">32.65 76.58 28.11</cell><cell>0</cell><cell>75.94 95.03 24.30</cell><cell>0</cell><cell>76.28 97.46 7.37</cell><cell>0</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reference</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Upper clothes</cell><cell></cell><cell></cell><cell>Pants</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Upper clothes (right) Upper clothes (left)</cell><cell>Pants (right)</cell><cell>Pants (left)</cell><cell>Hair (right)</cell><cell>Hair (left)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original semantic mask and image</cell><cell></cell><cell>Change sea to grass</cell><cell></cell><cell>Add one tree</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original semantic mask and image</cell><cell></cell><cell>Change floor to grass</cell><cell></cell><cell cols="2">Add one lamp</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of INADE with different C o on the Cityscapes and CelebAMask-HQ daasets. P, F and T represent the generator parameters, FLOPs and run time respectively.Figure 11. Qualitative comparison of our INADE with previous state-of-the-art methods on the DeepFashion and DeepFashion2 datasets.</figDesc><table><row><cell>Label</cell><cell>Ground Truth</cell><cell>BicycleGAN</cell><cell>DSCGAN</cell><cell>pix2pixHD</cell><cell>SPADE</cell><cell>CLADE</cell><cell>SEAN</cell><cell>GroupDNet</cell><cell>INADE</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Cityscapes CelebAMask-HQ mIoU acc FID LPIPS P (M) F (G) T (s) mIoU acc FID LPIPS P (M) F (G) T (s) INADE-64 (default) 61.02 93. <ref type="bibr" target="#b14">16</ref>     <ref type="figure">Figure 9</ref>. The discriminator of our method is based on the Patch-GAN <ref type="bibr" target="#b13">[15]</ref>. It takes the concatenation the segmentation map, instance map and the image as input.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Augmented cyclegan: Learning many-to-many mappings from unpaired data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10151</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pytorch implementation for semantic segmentation/scene parsing on mit ade20k dataset</title>
		<ptr target="https://github.com/CSAILVision/semantic-segmentation-pytorch.git" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CSAILVision</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<title level="m">Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10289</idno>
		<title level="m">Panoptic-based image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask-guided portrait editing with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3436" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation-aware convolutional networks using local attention masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic op</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on learning representations</title>
		<meeting>international conference on learning representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Michigan: multi-input-conditioned hair image generation for portrait editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhentao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="95" to="96" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semantic image synthesis via efficient class-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhentao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04644</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhentao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02867</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dual attention gans for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Video-tovideo synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Diversity-sensitive conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-domain correspondence learning for exemplar-based image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5143" to="5153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5104" to="5113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantically multi-modal image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
