<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Composition Assessment with Saliency-augmented Multi-pattern Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<email>bo-zhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Artificial Intelligence Shanghai</orgName>
								<orgName type="institution">Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Artificial Intelligence Shanghai</orgName>
								<orgName type="institution">Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
							<email>zhang-lq@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Artificial Intelligence Shanghai</orgName>
								<orgName type="institution">Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><surname>Key</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Artificial Intelligence Shanghai</orgName>
								<orgName type="institution">Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Composition Assessment with Saliency-augmented Multi-pattern Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHANG, NIU, ZHANG: IMAGE COMPOSITION ASSESSMENT WITH SAMP 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image composition assessment is crucial in aesthetic assessment, which aims to assess the overall composition quality of a given image. However, to the best of our knowledge, there is neither dataset nor method specifically proposed for this task. In this paper, we contribute the first composition assessment dataset CADB with composition scores for each image provided by multiple professional raters. Besides, we propose a composition assessment network SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP) module, which analyses visual layout from the perspectives of multiple composition patterns. We also leverage composition-relevant attributes to further boost the performance, and extend Earth Mover's Distance (EMD) loss to weighted EMD loss to eliminate the content bias. The experimental results show that our SAMP-Net can perform more favorably than previous aesthetic assessment approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image aesthetic assessment aims to judge aesthetic quality automatically in a qualitative or quantitative way, which can be widely used in many down-stream applications such as assisted photo editing, intelligent photo album management, image cropping, and smartphone photography <ref type="bibr">[5,</ref><ref type="bibr">7,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. Among the factors related to image aesthetics, image composition, which mainly concerns the arrangement of the visual elements inside the frame <ref type="bibr" target="#b37">[38]</ref>, is very critical in estimating image aesthetics <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref>, because composition directs the attention of viewer and has a significant impact on the aesthetic perception <ref type="bibr">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Despite the importance of image composition, there is no dataset readily available for image composition assessment. Some existing aesthetic datasets contain annotations related to image composition <ref type="bibr">[3,</ref><ref type="bibr">19,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b34">35]</ref>. However, they only have composition-relevant attributes without overall composition score except for PCCD dataset <ref type="bibr">[3]</ref>, but PCCD dataset only presents one reviewer's composition rating for each image and this reviewer, an anonymous website visitor, may be unprofessional. So the ratings might be biased and inaccurate, which are far below the requirement for scientific evaluation. To this end, we contribute a new image Composition Assessment DataBase (CADB) on the basis of Aesthetics and Attributes DataBase (AADB) dataset <ref type="bibr">[22]</ref>. Our CADB dataset contains 9,497 images with each image rated by 5 individual raters who specialize in fine art for the overall composition quality. The details of our CADB dataset will be introduced in Section 3. <ref type="figure">Figure 1</ref>: Evaluating composition quality from the perspectives of different composition patterns. The first (resp., second) row shows a good example and a bad example considering symmetrical (resp., radial) balance.</p><p>To the best of our knowledge, there is no method specifically designed for image composition assessment. However, some previous aesthetic assessment methods also take composition into consideration. We divide the existing compositionrelevant approaches into two groups. 1) The composition-preserving methods <ref type="bibr">[4,</ref><ref type="bibr" target="#b31">32]</ref> can maintain image composition during both training and testing. However, these approaches fail to extract compositionrelevant feature for composition assessment task.</p><p>2) The composition-aware approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52]</ref> extract composition-relevant feature by modeling the mutual dependencies between all pairs of objects or regions in the image. However, redundant and noisy information is likely to be introduced during this procedure, which may adversely affect the performance of composition assessment. Moreover, there are some previous methods <ref type="bibr">[1,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> designed to model the well-established photographic rules (e.g., rule of thirds and golden ratio <ref type="bibr">[20]</ref>), which humans use in evaluating image composition quality. However, these rule-based methods have two major limitations: 1) The hand-crafted feature extraction is tedious and laborious compared with deep learning features <ref type="bibr" target="#b26">[27]</ref>. 2) Each rule is valid only for specific scenes and they did not consider which rules are applicable for a given scene <ref type="bibr" target="#b46">[47]</ref>.</p><p>Interestingly, composition pattern, as an important aspect of composition assessment, is not explicitly considered by the above methods. As shown in <ref type="figure">Figure 1</ref>, each composition pattern divides the holistic image into multiple non-overlapping partitions, which can model human perception of composition quality. In particular, by analyzing the visual layout (e.g., positions and sizes of visual elements) according to composition pattern, i.e., comparing the visual elements in various partitions, we can quantify the aesthetics of visual layout in terms of visual balance (e.g., symmetrical balance and radial balance) <ref type="bibr">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>, composition rules (e.g., rule of thirds, diagonals and triangles) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref>, and so on. Different composition patterns offer different perspectives to evaluate composition quality. For example, the composition pattern in the top (resp., bottom) row in <ref type="figure">Figure 1</ref> can help judge the composition quality in terms of symmetrical (resp., radial) balance.</p><p>To dissect visual layout based on different composition patterns, we propose a novel multi-pattern pooling module at the end of backbone to integrate the information extracted from multiple patterns, in which each pattern provides a perspective to evaluate the composition quality. Considering that the sizes and locations of salient objects are representative of visual layout and fundamental to image composition <ref type="bibr" target="#b29">[30]</ref>, we further integrate visual saliency <ref type="bibr">[17]</ref> into our multi-pattern pooling module to encode the spatial and geometric information of salient objects, leading to our Saliency-Augmented Multi-pattern Pooling (SAMP) module. Additionally, since some composition patterns may play more important roles, we design weighted multi-pattern aggregation to fuse multi-pattern features, which can adaptively <ref type="figure" target="#fig_2">Figure 2</ref>: The overall pipeline of our SAMP-Net for composition assessment. We use ResNet18 <ref type="bibr">[14]</ref> as backbone. The detailed structure of our Saliency-Augmented Multi-pattern Pooling (SAMP) module and Attentional Attribute Feature Fusion (AAFF) module are illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> and <ref type="figure" target="#fig_1">Figure 4</ref> respectively. assign different weights to different patterns.</p><p>Moreover, because our dataset is built upon AADB dataset <ref type="bibr">[22]</ref> with composition-relevant attributes, we further leverage composition-relevant attributes to boost the performance of composition assessment. Specifically, we propose an Attentional Attribute Feature Fusion (AAFF) module to fuse composition feature and attribute feature. Finally, after noticing the content bias existing in our dataset, that is, composition score distribution is severely influenced by object category, we extend Earth Mover's Distance (EMD) loss in <ref type="bibr">[15]</ref> to weighted EMD loss to eliminate the content bias.</p><p>The main contributions of this paper can be summarized as follows: 1) We contribute the first image composition assessment dataset CADB, in which each image has the composition scores annotated by five professional raters. 2) We propose a novel composition assessment method with Saliency-Augmented Multi-pattern Pooling (SAMP) module. 3) We investigate the effectiveness of auxiliary attributes and weighted EMD loss for composition assessment. 4) Our model outperforms previous aesthetic assessment methods on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aesthetic Assessment Dataset</head><p>Many large-scale aesthetic assessment datasets have been collected in recent years, like Aesthetic Visual Analysis database (AVA) <ref type="bibr" target="#b34">[35]</ref>, AADB <ref type="bibr">[22]</ref>, Photo Critique Captioning Dataset (PCCD) <ref type="bibr">[3]</ref>, AVA-Comments <ref type="bibr" target="#b59">[60]</ref>, AVA-Reviews <ref type="bibr" target="#b52">[53]</ref>, FLICKER-AES <ref type="bibr" target="#b41">[42]</ref>, and DPC-Captions <ref type="bibr">[19]</ref>. However, these datasets only have composition-relevant attributes without overall composition score, or only have one inaccurate composition score for each image, which are far below the requirement for composition assessment research. Unlike the existing aesthetic datasets, our CADB dataset contains composition ratings assigned to each image by multiple professional raters. Besides, we guarantee the reliability of our dataset based on sanity check and consistency analysis (see Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Composition-relevant Aesthetic Assessment</head><p>We can divide existing composition-relevant aesthetic assessment methods into traditional methods and deep learning methods. As surveyed in <ref type="bibr">[2,</ref><ref type="bibr">9]</ref>, traditional methods <ref type="bibr">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61]</ref> usually employed hand-crafted features or generic image features (e.g., bag-of-visual-words <ref type="bibr" target="#b45">[46]</ref> and Fisher vectors <ref type="bibr" target="#b36">[37]</ref>) to learn image aesthetic evaluation, yet their generalization ability is limited by the complexity of image composition assessment task. The deep learning based methods can be divided into two groups. The composition-preserving approaches <ref type="bibr">[4,</ref><ref type="bibr" target="#b31">32]</ref>, without explicitly learning composition representations, produce inferior results on composition evaluation task. The composition-aware approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52]</ref> consider the relationship between all pairs of objects or regions in the image for modeling image composition, which is likely to introduce redundant and noisy information. Moreover, the above methods did not explicitly consider composition patterns. In contrast, we design a novel Saliency-Augmented Multi-pattern Pooling (SAMP) module, which provides an insightful and effective perspective for evaluating composition quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Composition Assessment DataBase (CADB)</head><p>To the best of our knowledge, there is no prior dataset specifically constructed for composition assessment. To support the research on this task, we build a dataset upon the existing AADB dataset <ref type="bibr">[22]</ref>, from which we collect a total of 9,958 real-world photos. We adopt a composition rating scale from 1 to 5, where a larger score indicates better composition. We make annotation guidelines for composition quality rating and train five individual raters who specialize in fine art. So for each image, we can obtain five composition scores ranging from 1 to 5. Given the subjective nature of human aesthetic activity <ref type="bibr">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref>, we perform sanity check and consistency analysis. Similar to <ref type="bibr" target="#b56">[57]</ref>, we use 240 additional "sanity check" images during annotating to roughly verify the validness of our annotations. We also examine the consistency of composition ratings provided by five individual raters (see <ref type="figure">Supplementary)</ref>. Similar to <ref type="bibr">[22,</ref><ref type="bibr" target="#b34">35]</ref>, we average the composition scores as the ground-truth composition mean score for each image, which is denoted as?. More details about our CADB dataset will be elaborated in Supplementary.</p><p>Besides, we observe the content bias in our CADB dataset, that is, there are some biased categories whose score distributions are concentrated in a very narrow interval. After removing 461 biased images, we split the remaining images into 8,547 training images and 950 test images, in which the test set is made less biased for better evaluation (see Supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>To accomplish the composition assessment task, we propose a novel network SAMP-Net, which is named after Saliency-Augmented Multi-pattern Pooling (SAMP) module. The overall pipeline of our method is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, where we first extract the global feature map from input image by backbone (e.g., ResNet18 <ref type="bibr">[14]</ref>) and then yield aggregated pattern feature through our SAMP module, which is followed by Attentional Attribute Feature Fusion (AAFF) module to fuse the composition feature and attribute feature. After that, we predict composition score distribution based on the fused feature and predict the attribute score based on the attribute feature, which are supervised by weighted EMD loss and Mean Squared Error (MSE) loss respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Saliency-augmented Multi-pattern Pooling</head><p>Multi-pattern Pooling: As demonstrated in <ref type="figure" target="#fig_0">Figure 3</ref>(a), we empirically design eight basic composition patterns inspired by classic composition guidelines. For instance, pattern 1,2,6,7 are inspired by symmetrical composition. Pattern 3,4 are inspired by diagonal composition. Pattern 5 is inspired by centre composition. Pattern 8 is inspired by rule of thirds <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref>. Although our pattern design is inspired by composition rules, there is no strict one-to-one correspondence between composition rules and patterns. Each pattern provides a perspective for evaluating composition quality, which may be beyond the scope of a single rule. For example, Pattern 8 is related to rule of thirds, but not limited to rule of thirds. Based on Pattern 8, more useful information can be excavated by comparing the visual elements in nine partitions.</p><p>Since humans typically employ multiple perspectives when analysing image composition, composition assessment should be accomplished based on all composition patterns in a comprehensive way. Therefore, we propose multi-pattern pooling to achieve this goal, which is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>(b). Given an H ? W global feature map F with C channels, which is extracted from input image by backbone, we represent the pixel-wise feature at each location as x i, j , where 0 &lt; i ? H, 0 &lt; j ? W . For the p-th pattern, we divide F into K p non-overlapping partitions {X p 1 , X p 2 , . . . , X p K p } and K p is the total number of partitions in this pattern. Then, the feature of the k-th partition can be obtained via average pooling:</p><formula xml:id="formula_0">? (X p k ) = 1 |X p k | ? (i, j)?X p k x i, j ? R C .</formula><p>Saliency-augmented Multi-pattern Pooling: Considering the significance of salient objects for composition assessment, we further incorporate the saliency information (i.e., locations and scales of salient objects) into multi-pattern pooling. To achieve this goal, we utilize an unsupervised saliency detection method <ref type="bibr">[17]</ref> to produce saliency maps for input images.</p><p>We have also tried several supervised methods <ref type="bibr">[6,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b58">59]</ref>, which prove to be less effective. After obtaining the saliency map, we downsample it to H sal ? W sal through max pooling.</p><p>Recall that the size of global feature map is H ? W , we set H sal = 8H and W sal = 8W for retaining more details of salient objects. Different from ? (X p k ) using average pooling, we directly reshape each partition of saliency map into a vector, because the pooling operation will result in significant information loss. Specifically, for the k-th partition in the p-th pattern, we reshape the saliency map in this partition into a saliency vector ?(X p k ) ? R D p k , in which D p k varies with partition and pattern. Then, we concatenate ?(X p k ) and ? (X p k ) to generate the partition feature [?(X p k ), ? (X p k )]. For the p-th pattern, we concatenate the partition features of K p partitions into a long vectorf p samp , which is followed by a fc layer and ReLU activation function to produce the pattern vector f p samp ? R C . Intuitively, [?(X p k ), ? (X p k )] extracts the visual information in each partition and f p samp encodes the relationship among visual elements in different partitions.</p><p>Weighted Multi-pattern Aggregation: Since some composition patterns may play more important roles when evaluating image composition, our model is trained to assign different weights for different patterns. Precisely, we apply global average pooling, a fc layer, and softmax normalization to the global feature map F, producing the multi-pattern weight w p for the p-th pattern. Then, we have the aggregated pattern feature via weighted summation f samp = ? P p=1 w p f p samp , in which P is the number of composition patterns (P = 8). Based on the learnt weights, we can know the dominant patterns in determining the overall composition quality and provide interpretable guidance for users (see Section 5.4). Comparison with Spatial Pyramid Pooling: Although the proposed SAMP and Spatial Pyramid Pooling (SPP) <ref type="bibr">[13]</ref> are similar in architecture, both of which pool features from multiple sets of partitions, SAMP is significantly different from SPP in three main aspects: 1) our pooling patterns are specifically designed and well-tailored for image composition evaluation, which can analyse the composition quality from the viewpoint of composition patterns; 2) we introduce visual saliency into multi-pattern pooling; 3) we learn pattern weights which provide interpretable guidance for improving composition quality. Since our dataset is built upon AADB <ref type="bibr">[22]</ref>, which is associated with composition-relevant attributes, it is natural to consider using them to help composition assessment. We use five composition-relevant attribute annotations: rule of thirds, balancing elements, object emphasis, symmetry, and repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attentional Attribute Feature Fusion</head><p>Specifically, as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, we decompose the aggregated pattern feature f samp ? R C into composition feature f comp and attribute feature f atts by using two separate fc layers, the dimensions of which are both set to C 2 . We dynamically weigh the contributions of f comp and f atts for the composition assessment task, as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>. First, we apply a fc layer and sigmoid activation to the concatenation of f comp and f atts , to learn the attention coefficients [e 1 , e 2 ] for two types of features. Then, we concatenate the weighted composition feature and attribute feature, yielding the fused feature f f used = [e 1 f comp , e 2 f atts ] ? R C . During training, an additional layer is added to perform attribute prediction based on the attribute feature f atts . We employ MSE loss denoted as L atts for attribute prediction.</p><p>As mentioned in Section 3, we observe the content bias in our dataset, in which case the network may find a shortcut to simply rate images based on their contents. To mitigate the content bias in training set, we extend EMD loss to weighted EMD loss denoted as L wEMD  <ref type="bibr">[13]</ref>. ? means Multi-scale Pyramid Pooling (MPP) <ref type="bibr" target="#b55">[56]</ref>. WE means weighted EMD loss. MP means multi-pattern pooling. PW means pattern weights. SA means saliencyaugmented. AF indicates attribute feature and AA indicates attentional attribute feature fusion.</p><p>(see <ref type="figure">Supplementary)</ref>, which assigns smaller weights to biased samples when calculating EMD Loss. Finally, our SAMP-Net can be trained in an end-to-end manner with attribute prediction loss L atts and weighted EMD loss L wEMD :</p><formula xml:id="formula_1">L = L wEMD + ? L atts ,<label>(1)</label></formula><p>where ? is a trade-off parameter set as 0.1 via cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details and Evaluation Metric</head><p>We use ResNet18 <ref type="bibr">[14]</ref> pretrained on ImageNet <ref type="bibr">[8]</ref> as the backbone of our SAMP-Net. Unless otherwise specified, all input images are resized to 224 ? 224 for both training and testing following <ref type="bibr">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref>, leading to a global feature map of H ? W = 7 ? 7, and the saliency map is downsampled to H sal ?W sal = 56 ? 56 before passing to the SAMP. More details can be found in Supplementary. All experiments are conducted on our CADB dataset. To evaluate the composition score distribution and composition mean score predicted by different models, it is natural to adopt EMD and MSE as the evaluation metrics. EMD measures the closeness between the predicted and ground-truth composition score distributions as in <ref type="bibr">[15]</ref>. MSE is computed between the predicted and ground-truth composition mean scores. Moreover, following existing aesthetic assessment approaches <ref type="bibr">[4,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b47">48]</ref>, we also report the ranking correlation measured by Spearman's Rank Correlation Coefficient (SRCC) and the linear association measured by Linear Correlation Coefficient (LCC) between the predicted and ground-truth composition mean scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To evaluate the effectiveness of each individual component in our SAMP-Net, we conduct a series of experiments and report all the evaluation metrics described in Section 5.1. In this section, we start from ResNet18 backbone and build up our holistic model step by step.  <ref type="table" target="#tab_4">Table 2</ref>: Comparison of different methods on the composition assessment task. All models are trained and evaluated on the proposed CADB dataset.</p><p>Weighted EMD Loss: We start from basic ResNet18 <ref type="bibr">[14]</ref>, and report the results using EMD loss and weighted EMD loss in <ref type="table">Table 1</ref>. The experimental results show that training with weighted EMD loss (row 2) performs better than standard EMD loss (row 1) with a clear gap of test EMD between these two models, which is attributed to the advantage of weighted EMD loss in eliminating content bias. Saliency-Augmented Multi-pattern Pooling (SAMP): Based on ResNet18 with weighted EMD loss (row 2), we add our SAMP module and also explore its ablated versions. We first investigate vanilla multi-pattern pooling without saliency or pattern weights (row 3), in which saliency vector is excluded from partition feature and the pattern features of multiple patterns are simply averaged. Then, we learn pattern weights to aggregate multiple pattern features (row 4). By comparing row 3 and row 4, it is beneficial to adaptively assign different weights to different pattern features. We further incorporate saliency map into SAMP module (row 5). The comparison between row 4 and row 5 proves that is useful to emphasize the layout information of salient objects. Considering the architecture similarity between Spatial Pyramid Pooling (SPP) <ref type="bibr">[13]</ref> and our multi-pattern pooling, we replace our multi-pattern pooling with SPP using scales {1 ? 1, 2 ? 2, 3 ? 3} following <ref type="bibr">[4]</ref> (row 6). In addition, we also show the results of using Multi-scale Pyramid Pooling (MPP) <ref type="bibr" target="#b55">[56]</ref> in row 7, in which we make an image pyramid containing three scaled images. The comparisons (row 5 v.s. row 6, row 5 v.s. row7) show that the model using multi-pattern pooling outperforms both SPP and MPP. The reason is that our proposed multi-pattern pooling is specifically designed and well-tailored for composition assessment task. Attentional Attribute Feature Fusion (AAFF): Built on row 2 (resp., row 5) in <ref type="table">Table 1</ref>, we additionally learn attribute feature and directly concatenate it with composition feature, leading to row 8 (resp., row 9). The experimental results demonstrate that compositionrelevant attributes can help boost the performance of composition evaluation. This sheds light on that composition-relevant attribute prediction and composition evaluation are two related and reciprocal tasks. Finally, we complete our attentional attribute feature fusion module by learning weights for weighted concatenation (row 10). From row 9 and row 10, we can observe that the model using weighted concatenation is better than that using plain concatenation, which validates the superiority of attentional fusion mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Existing Methods</head><p>To the best of our knowledge, there is no method specifically designed for image composition assessment. Nevertheless, some previous aesthetic assessment methods [4, 22, 28, 31, 32, <ref type="figure">Figure 5</ref>: Analysis of the correlation between an image and its dominant pattern with the largest weight. We show the estimated pattern weights and the largest weight is colored green. We also show the ground-truth/predicted composition mean score in blue/red. 52] explicitly take composition into consideration. Since most of these methods do not yield score distribution, we make a slight modification on the prediction layer of these methods to be compatible with EMD loss <ref type="bibr">[15]</ref>. For fair comparison, all methods are trained and tested on our CADB dataset with ResNet18 pretrained on ImageNet <ref type="bibr">[8]</ref> as backbone.</p><p>In <ref type="table" target="#tab_4">Table 2</ref>, we compare our method with different composition-relevant aesthetic assessment methods. The baseline model (ResNet18) only consists of the pretrained ResNet18 and a prediction head, which is the same as row 1 in <ref type="table">Table 1</ref>. Among these baselines, A-Lamp is the most competitive one, probably because A-Lamp introduces additional saliency information to learn the pairwise spatial relationship between objects. Our SAMP-Net clearly outperforms all the composition-relevant baselines, which demonstrates that our method is more adept at image composition assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of Composition Pattern</head><p>To take a close look at the learnt pattern weights which indicate the importance of different patterns on the overall composition quality, we show the input image, its saliency map, its ground-truth/predicted composition mean score, and its pattern weights in <ref type="figure">Figure 5</ref>.</p><p>For each image, the composition pattern with the largest weight is referred to as its dominant pattern. For each pattern, we show one example image with this pattern as dominant pattern and overlay this pattern on the image in <ref type="figure">Figure 5</ref>, which reveals from which perspective the input image is given a high or low score. For example, in the right figure of the last row, the surfer is placed at the intersection point between the gridlines of pattern 8, which implicates that the image conforms to the rule of thirds properly, yielding a relatively high score. On the contrary, in the right figure of the first row, the arch slightly deviates from its <ref type="figure" target="#fig_4">Figure 6</ref>: We show some failure cases in the test set, which have the highest absolute errors between the predicted composition mean scores (out of bracket) and the ground-truth composition mean scores (in bracket).</p><p>symmetrical axis under pattern 2. So the low score implies that maintaining horizontal symmetry may enhance the composition quality. In the left figure of the third row, per the low score under pattern 5, the dog is suggested to be moved to the center. In summary, our SAMP module can facilitate composition assessment by integrating the information from multiple patterns and provide constructive suggestions for improving the composition quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Additional Experiments in Supplementary</head><p>Due to the space limitation, we present some experiments in Supplementary, including the results of using different training set sizes, backbones, and hyper-parameters ? in (1), weighted EMD loss analysis, the effectiveness of each pattern, the impact of using more composition patterns, comparison with the performance of human raters, more results on the CADB and PCCD <ref type="bibr">[3]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Limitations</head><p>While our method can generally achieve accurate and reliable composition assessment, it still has some failure cases. We show several failure cases in <ref type="figure" target="#fig_4">Figure 6</ref>, which have the highest absolute errors between the predicted and ground-truth composition mean scores. We can observe that our model tends to predict relatively low scores for these images with high composition mean scores, which is probably due to the distracting backgrounds and complicated composition patterns. In addition, there is a clear gap between our method and human raters on ranking the composition quality of different images (see <ref type="figure">Supplementary)</ref>, which needs to be enhanced in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have contributed the first composition assessment dataset CADB with five composition scores for each image. We have also proposed a novel method SAMP-Net with saliency-augmented multi-pattern pooling. Equipped with SAMP module, AAFF module, and weighted EMD loss, our method is capable of achieving the best performance for composition assessment. In this document, we provide additional materials to supplement our main submission. We first present more details about constructing our Composition Assessment DataBase (CADB) in Section 1. Then, we describe the detailed consistency analysis of the collected composition scores in Section 2, which verifies that our composition quality annotations are reliable for scientific research. Next, we use some examples to illustrate the content bias in the CADB dataset in Section 3. Meanwhile, in Section 4, we describe the proposed weighted EMD loss and study the effect of using weighted EMD loss to mitigate the content bias. Besides, more implementation details of the proposed method are provided in Section 5. In Section 6, Section 7, Section 8, Section 9, and Section 10, experiments on the hyper-parameter, training set size, backbone, each composition pattern, and using more composition patters further prove the effectiveness of our method. The we compare the performance of our method and human raters in Section 11. Finally, in Section 12, we provide additional visualization results on images inside/outside our CADB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Composition Assessment with Saliency-augmented Multi-pattern Pooling Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Our CADB Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Data Collection</head><p>Recently, many large-scale aesthetic assessment datasets have been created to facilitate research on image aesthetic evaluation, like Aesthetic Visual Analysis database (AVA) <ref type="bibr">[13]</ref>, Aesthetics and Attributes DataBase (AADB) <ref type="bibr">[11]</ref>, Photo Critique Captioning Dataset (PCCD) <ref type="bibr">[2]</ref>, AVA-Comments <ref type="bibr">[22]</ref>, AVA-Reviewes <ref type="bibr">[21]</ref>, FLICKER-AES <ref type="bibr">[17]</ref>, and DPC-Captions <ref type="bibr">[8]</ref>. Therefore, we can build the CADB dataset upon those existing datasets. <ref type="table">Table 1</ref> provides a summary comparison of CADB to other related aesthetic datasets. Here we select real photos to construct our dataset, because we target at the real-world application of composition assessment. To the best of our knowledge, among them, only the images in AADB and PCCD datasets are all real photos, while the images in other datasets (e.g., AVA dataset) may be heavily edited or synthetic. Besides, PCCD dataset contains 4,235 images downloaded from a professional photo critique website, most of which are taken by professional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Images All Real Photo Composition Score Raters AVA <ref type="bibr">[13]</ref> 255,530 N N -PCCD <ref type="bibr">[2]</ref> 4,235 Y Y 1 AADB [11] 10,000 Y N -CADB(Ours) 9,497 Y Y 5 <ref type="table">Table 1</ref>: Comparison with existing aesthetic assessment datasets. Our CADB dataset is built upon the AADB dataset, taking into account its all real-world images, and more balanced distribution of professional and amateurish photos <ref type="bibr">[11]</ref>.</p><p>photographers and have relatively high composition quality. Differently, AADB dataset provides 10,000 images and contains a much more unbiased distribution of professional photos and amateurish photos. So we choose to construct our CADB dataset based on the AADB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Guidelines for Image Composition Evaluation</head><p>In this section, we show the annotation guidelines for evaluating the quality of image composition, aiming to make the annotation consistent across five individual raters who specialize in fine art. 1) We provide a composition rating scale from 1 to 5, where a larger score indicates better composition. 2) To help raters quickly learn the rules of image composition rating, we provide them with 100 images with high composition quality and 100 images with low composition quality selected from PCCD dataset <ref type="bibr">[2]</ref> to serve as examples. 3) When assessing image composition quality, the photographic rules that should be considered are including but not limited to: rule of thirds, centred composition, symmetry, repetition, shallow depth of field, diagonals, triangles, golden ratio, frame within the frame, leading lines, fill the frame, isolate the subject, vanishing point, juxtaposition, balancing elements, and object emphasis. In addition, we draw a 3?3 dotted grid on each image as auxiliary lines that divide the image into nine equal rectangles, which is displayed for the raters together with the original image. 4) The raters are requested to complete the composition rating independently, and the rating procedure for each single image should not be shorter than 20 seconds. Besides, the same five raters annotate all images to mitigate the inconsistency across different raters. Following <ref type="bibr">[11,</ref><ref type="bibr">13]</ref>, we average the composition scores of the five raters as the ground-truth composition mean score for each image. In <ref type="figure">Figure 1</ref>, we present some examples in our CADB dataset with five composition scores and composition mean score that is obtained by averaging those composition scores for each image. For better visualization, we divide these examples into three groups according to the composition mean score: images with high, low, medium scores. From <ref type="figure">Figure 1</ref>, we can roughly verify the validness of the composition annotations. <ref type="figure">Figure 1</ref>: Some example images in our CADB dataset with high/low/medium composition mean scores. We show five composition scores ranging from 1 to 5 provided by five raters in blue and the calculated composition mean score in red. We also show the aesthetic scores annotated by AADB dataset <ref type="bibr">[11]</ref> on a scale from 1 to 5 in green. In <ref type="figure">Figure 1</ref>, we also present the aesthetic scores annotated by AADB dataset <ref type="bibr">[11]</ref> , which is also rated by multiple individual human raters on a scale from 1 to 5 for the overall aesthetic quality, a larger score indicates higher aesthetic quality. We can see that, for some images, a high (resp., low) composition score does not mean a high (resp., low) aesthetic score. This is because that the composition assessment focuses on analyzing the placement of visual elements in the image, while aesthetic evaluation quantifies the aesthetic quality of the image in a comprehensive manner by taking not only image composition but also other visual factors (e.g., interesting content, good lighting, color harmony, vivid color, motion blur, and shallow depth of field) into consideration. The essential difference between the above tasks further sheds light on the significance of specially developing methods for evaluating overall composition quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Annotation Examples and Statistics</head><p>Furthermore, we calculate the distribution of composition mean score in <ref type="figure" target="#fig_2">Figure 2</ref>, where indicates that the scores are well fit by Gaussian distribution similar to the observation in AADB dataset <ref type="bibr">[11]</ref> and AVA dataset <ref type="bibr">[13]</ref>. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the average and variance of composition mean score is 2.70 and 0.35, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Consistency Analysis of Annotations</head><p>Considering the subjective nature of human aesthetic activity <ref type="bibr">[5,</ref><ref type="bibr">16,</ref><ref type="bibr">19]</ref>, we carry out consistency analysis on the composition scores provided by multiple raters to verify that our CADB dataset is qualified for scientific evaluation. Following <ref type="bibr">[11]</ref>, we employ both Kendall's concordance coefficient (also known as Kendall's W ) and Spearman's rank correlation coefficient (also known as Spearman's ?) in the experiments. Kendall's W indicates the agreement among multiple raters and accounts for tied ranks, the value of which varies from 0 (no agreement) to 1 (complete agreement). Spearman's ? is computed between the predicted and ground-truth composition score distribution to measure their closeness.</p><p>Since five raters annotated a collection of 10,200 images (including 240 sanity check images), we calculate an average Kendall's W of 0.5734 over all images, which demonstrates significant consistency among different raters. Then, following <ref type="bibr">[11]</ref>, we conduct a permutation test over global Kendall's W to obtain the distribution of W under the null hypothesis, the curves of which p(W ) vs. W and p(W &lt; t) vs. t are illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. We can observe that the empirical Kendall's W on our CADB dataset is statistically significant from both curves.</p><p>Then, similar to <ref type="bibr">[11]</ref>, we investigate the consistency of composition scores at batch level and randomly split all annotated samples into multiple batches with each batch containing 100 images. For each batch, we calculate Kendall's W to evaluate the consistency of annotations provided by different raters and confirm its statistical significance by using Benjamini-Hochberg procedure <ref type="bibr">[1]</ref> controlling the false discovery rate (FDR) for multiple comparisons. At FDR level Q = 0.05, 100.0% batches have significant agreement, which means that all <ref type="figure" target="#fig_1">Figure 4</ref>: Illustration of biased categories in the CADB dataset. For each category (colored red), given images containing this object category, we count the occurrence of composition mean score in four score bins.</p><p>batches of our annotations have consistent composition scores, and our dataset is qualified for scientific evaluation.</p><p>Moreover, we also adopt Spearman's ? to measure the rank correlation between the composition scores of pairwise raters and test its statistical significance at batch level via Benjamini-Hochberg procedure. The p-value for each batch is computed by averaging the pairwise p-values in the current batch following <ref type="bibr">[11]</ref>. At FDR level Q = 0.05, 98.04% batches have significant agreement, which further confirms the reliability of the composition quality annotations in our CADB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content Bias</head><p>In Section 3 of the main text, we briefly mention the content bias issue in our CADB dataset. Here we provide a detailed description of this concept. Intuitively, photos of any object category have chances to be of high or low composition quality, which means that composition mean score? should be approximately evenly distributed within each category. However, in our CADB dataset, as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, we observe that there are some categories whose score distributions are concentrated in a very narrow interval, and we refer to these categories as biased categories. For example, as shown in in <ref type="figure" target="#fig_1">Figure 4</ref>, most bird photos are rated with high scores, probably because bird photos are more likely to be taken by professional photographers rather than amateurs. In this case, the network may find a shortcut to simply rate images based on their contents, which is dubbed as content bias in this paper.</p><p>To identify the biased categories, we first leverage Faster R-CNN <ref type="bibr">[18]</ref> trained on Visual Genome <ref type="bibr">[12]</ref> to detect objects for all images. We divide the range of composition mean scor? y into M bins (M = 4) with the bin size equal to 1 (e.g., <ref type="bibr">[1,</ref><ref type="bibr">2)</ref>). For the images containing each object category, we count the occurrence of? in each bin and derive the score distribution over M bins. To measure the degree of bias, we compute the entropy of the score distribution for all categories. The category with an entropy below 0.1 is treated as a highly biased category whose associated images will be removed from our dataset. After this step, there are 9,497 images left. Then, we calculate the ratio of the maximum occurrence to the minimum non-zero occurrence in the M bins as r c for the c-th category. A category is defined as an unbiased category if r c ? 1.5 and otherwise a biased category. Furthermore, an image is defined as an unbiased image if its involving categories are all unbiased categories and otherwise a biased image.</p><p>For the test images in real-world applications, photos of any object category have chances to be of high or low composition quality. For better evaluation, we select 950 unbiased images to form the test set, which is closer to the test set in practice, and use the remaining 8,547 images (including both unbiased and biased ones) for training. <ref type="figure">Figure 5</ref>: Illustration of using weighted EMD loss to eliminate content bias. For each category (colored red), given images containing this object category, we count three type of occurrences of composition mean scores in four score bins: ground-truth, predicted by the model trained with EMD loss, predicted by the model trained with our weighted EMD loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Weighted EMD Loss</head><p>In our CADB dataset, each image is rated by five raters, so both composition mean score and composition score distribution can be computed. Considering the intrinsic orderliness of our composition rating scale (see Section 1.2), we train our model to predict composition score distribution and adopt the normalized EMD loss <ref type="bibr">[7]</ref>, which has been widely used in aesthetic assessment <ref type="bibr">[3,</ref><ref type="bibr">20]</ref>. We assume that the ground-truth and predicted composition score distribution are y and?, respectively. Then, the normalized EMD loss can be calculated by</p><formula xml:id="formula_2">L EMD (y,?) = 1 S S ? s=1 CDF y (s) ? CDF?(s) r 1/r ,<label>(1)</label></formula><p>where S = 5 is the scale of composition score in our dataset and r is a hyper-parameter. CDF y (s) = ? s i=1 y i denotes the cumulative distribution function. We set r = 2 following <ref type="bibr">[3,</ref><ref type="bibr">20]</ref>. The predicted composition mean score can be calculated as the expectation of the score distribution ? S i=1 i ?? i . As discussed in Section 3, we observe content bias in our dataset, that is, the images with certain object categories are more likely to have high or low composition scores. Training on such data, the network may find a shortcut to simply rate images based on their contents, leading to weak generalization ability to real-world photos. To eliminate the effect of content bias and prevent the model from learning a shortcut, we propose a strategy that assigns different weights to different samples when calculating EMD Loss. Specifically, as mentioned in Section 3, the range of composition mean score? is divided into M bins. We use T m,c to present the occurrence that the c-th category appears in m-th bin and calculate weights for each category via the strategy proposed in <ref type="bibr">[9]</ref>: ? m,c = ? M m=1 T m,c M?T m,c , which is inversely proportional to T m,c . Given an image that contains C object categories and has a? falling in the m-th bin, we take the minimum weight across all categories as its weight ? = min{? m,1 , ? m,2 , . . . , ? m,C }. Instead of minimum, we have also tried several other options (e.g., maximum, median, and mean), but minimum gives the best result. The weight ? is different for different training samples. We precompute ? for all training samples beforehand and assign sample-specific weight ? to EMD loss (1) during training.</p><p>In the ablation study in Section 5.2 of the main text, we have confirmed that using weighted EMD loss can benefit model performance by eliminating content bias. To take a closer look at the advantage of weighted EMD loss in eliminating content bias, for each category, we analyze the distribution of ground-truth/predicted composition mean scores of   images containing this object category. Specifically, we first employ the ResNet18 <ref type="bibr">[6]</ref> backbone trained with EMD loss (resp., weighted EMD loss) to estimate composition mean scores for images in the testing set. Then, for each category, we collect the ground-truth/predicted composition mean scores of images containing this object category. After that, we visualize the distribution of composition mean score for each category in a similar way to Section 3. As illustrated in <ref type="figure">Figure 5</ref>, for each example category, we show three types of composition mean scores: ground-truth, predicted by the model trained with EMD loss, predicted by the model trained with proposed weighted EMD loss. Comparing the results of EMD loss and weighted EMD loss, we can see that training with weighted loss produces a much more unbiased distribution of composition mean score, which also looks closer to the ground-truth distribution. For example, for the results on water images in <ref type="figure">Figure 5</ref>, the ground-truth composition mean scores are approximately evenly distributed across four bins, while the composition mean score distribution of using EMD loss is concentrated in the intervals of <ref type="bibr">[2,</ref><ref type="bibr">3)</ref> and <ref type="bibr">[3,</ref><ref type="bibr">4)</ref>. Differently, for the model trained with weighted EMD loss, the predicted composition mean score distribution is relatively balanced on all four bins, from which we can validate the advantages of the proposed weighted EMD loss on eliminating content bias qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation Details</head><p>We implement our model and conduct all experiments using Pytorch <ref type="bibr">[15]</ref>. During the training stage, the backbone weights are pretrained on ImageNet <ref type="bibr">[4]</ref> and other layers are randomly initialized. We adopt the Adam optimizer <ref type="bibr">[10]</ref> and set the batch size as 16. Then, the initial learning rate of the layers in the backbone and the layers in the additional modules (e.g., SAMP, AAFF, and prediction head) are set as 1e ?6 and 1e ?4 , respectively. This is because we noticed that using a small learning rate on the backbone results in easier and faster Pattern 0 means the simplest pattern with only one partition. Multi means using all 8 patterns. optimization in our experiments. Moreover, the learning rate of all layers is annealed by 0.1 every time the training loss plateaus. To prevent overfitting, a dropout rate of 0.5 is applied on each fully-connected layer of the additional modules, and we set weight decay as 5e ?5 for all layers in our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Hyper-parameter Analysis</head><p>There is a trade-off parameter ? before the attribute loss in Eq.(1) of the main text. We set the hyper-parameter according to cross-validation by splitting 20% training samples as validation set. We vary ? from 0 to 10 and present the results in <ref type="figure" target="#fig_4">Figure 6</ref>(a), in which we report Mean Squared Error (MSE) and Spearman's Rank Correlation Coefficient (SRCC). Comparing the result without attribute loss (? = 0) and the result with ? = 0.1, we can see a clear gap between their performance. Therefore, we set ? = 0.1 by default for all experiments. Moreover, the experimental results demonstrate that our method is robust when setting ? in the range of [0.01,10].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Different Training Set Size</head><p>As mentioned in Section 3 of the main text, we split the CADB dataset into training <ref type="bibr">(8,</ref><ref type="bibr">547)</ref> and test (950) sets. To study the correlation between the test performance of our model and the training set size, we randomly select a certain amount of samples from training set to train the model and evaluate on the same test set. We vary the number of training samples from 1,000 to 8,000 with the step length of 1,000 and report the results in <ref type="figure" target="#fig_4">Figure  6</ref>(b) using MSE and SRCC. When the training set size increases, the model performance improves significantly, yet the performance growth slows down. When the training size gets larger than 8,000, the performance gain becomes negligible, demonstrating that our model capacity is compatible with the CADB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Different Backbone Network</head><p>We evaluate our method with different backbones on the CADB dataset and report results in <ref type="table" target="#tab_4">Table 2</ref>. It can be see that our method achieves the best result using ResNet-34 and the performance drop using ResNet-50 or ResNet-101 might be caused by overfitting. Given that ResNet-18 is efficient and can already receive good results, we adopt ResNet-18 as the default backbone in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patterns</head><p>MSE? EMD? SRCC? LCC? Patterns 1? 8 0.3867 0.1798 0.6564 0.6709 Patterns 1? 11 0.3876 0.1800 0.6558 0.6701 <ref type="table">Table 3</ref>: Results of using more composition patterns. Based on the existing eight composition patterns, we add three more composition patterns (see <ref type="figure" target="#fig_6">Figure 10</ref>) in our model.  <ref type="table">Table 4</ref>: Comparison with human raters on the CADB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Effectiveness of Composition Pattern</head><p>Recall that we design eight composition patterns (see <ref type="figure" target="#fig_0">Figure 3</ref>(a) of the main text) for composition evaluation from different perspectives. To study the effectiveness of each pattern, we conduct experiments on our SAMP-Net with only a single pattern in SAMP. Moreover, we compare with the simplest pattern with only one partition (i.e., global pooling), which is referred to as pattern 0. The experimental results are summarized in <ref type="figure" target="#fig_5">Figure 7</ref>, where we report MSE and Linear Correlation Coefficient (LCC). It can be seen that all the models with the designed patterns, including single pattern and multi-pattern, perform better than pattern 0, which indicates that our designed patterns are meaningful and helpful for composition assessment. Among the results using a single pattern, we find that pattern 5 performs best in terms of MSE, which might because visually important objects are often placed at the center of images. Furthermore, the multi-pattern model beats all single-pattern models, which again demonstrates the effectiveness of our SAMP module.</p><p>10 Using More Composition Patterns Apart from existing eight patterns (see <ref type="figure" target="#fig_0">Figure 3</ref>(a) of the main text), to evaluate the effect of learning more diverse rules, we design three additional composition patterns in <ref type="figure" target="#fig_6">Figure 10</ref>. Pattern 9 is inspired by gloden ratio <ref type="bibr">[14]</ref>. Pattern 10 and pattern 11 concern more complex composition patterns. The results in <ref type="table">Table 3</ref> implies that using more composition patterns cannot achieve further improvement. We would like to explore other more composition patterns in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Comparison with Human Ratings</head><p>We have shown that the proposed method outperforms existing methods in the Section 5.3 of the main text. To further analyze the capability of our method, we evaluate the performance <ref type="figure">Figure 8</ref>: Visualization results of the proposed method on our CADB dataset. We show the estimated pattern weights and the largest weight is colored green. We also show the ground-truth/predicted composition mean score in blue/red. <ref type="figure">Figure 9</ref>: Visualization results of the proposed method on the PCCD dataset <ref type="bibr">[2]</ref>. We show the estimated pattern weights and the largest weight is colored green. We also show the predicted composition mean score in red.</p><p>of each individual raters by comparing with the ground-truth in the same way. Unlike our model which predicts a composition score distribution, each rater only has one score for each test image, resulting in an one-hot score distribution. We summarize the results in <ref type="table">Table 4</ref>. Interestingly, our method can outperform two of five human raters in terms of MSE and EMD. This may be due to the fact that our model is trained using the ratings of all raters and the prediction is close to the average distribution. However, considering SRCC and LCC, which indicate the ability to correctly rank different images according to their composition quality, we see that there is still a clear gap between our model and human raters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Our designed eight composition patterns and Saliency-augmented Multi-pattern Pooling (SAMP) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Attentional Attribute Feature Fusion (AAFF) module. fc means a fully-connected layer with sigmoid activation and e 1 , e 2 are attention coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The distribution of composition mean score in our CADB dataset. The dashed line indicates the fitted Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Permutation test on Kendall's W . Left: p(W ) vs. W . Right: p(W &lt; t) vs. t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Analysis of hyper-parameters and training set size on our CADB dataset. (a) Performance variation of our model with different hyper-parameter ? . (b) Performance variation of our model with different training set size. The dashed vertical line denotes the default value used in our paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Results of our model using each individual composition pattern (pattern 1? 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Three additional composition patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance of our method with different backbone networks.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is sponsored by National Natural Science Foundation of China (Grant No. 61902247) and Shanghai Sailing Program (19YF1424400).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Additional Visualization Results</head><p>Our SAMP-Net can facilitate composition assessment by integrating the information from multiple patterns and provide constructive suggestions for improving the composition quality. So we present additional examples in <ref type="figure">Figure 8</ref>, in which we show the input image, its saliency map, its ground-truth/predicted composition mean score, and its pattern weights. We refer to the composition pattern with the largest weight as the dominant pattern of the input image. For each pattern, we present two example images with this pattern as dominant pattern and draw this pattern on the image.</p><p>As discussed in Section 5.4 of the main text, the dominant pattern unveils from which perspective the input image is given a high or low score. For example, in <ref type="figure">Figure 8</ref>, in the right column of the second row, the vertical line of pattern 2 is parallel to the bird of the image, which looks more visually assuring to viewers. In the left column of the fourth row, pattern 4 implies that the knife is organised based on the diagonal line in the image. Since such images create a sense of visual balance and stability for viewer, the model estimates a relatively high score for them. On the contrary, in the left column of the second row in <ref type="figure">Figure  8</ref>, the carvings slightly deviate from their symmetrical axis under pattern 2. So the low score implies that maintaining horizontal symmetry may help to improve the composition quality. In the left column of the fifth row, per the relatively low score under pattern 5, the surfer is suggested to be moved towards the center. Those examples further validate the utility of our model for providing interpretable composition guidance.</p><p>Furthermore, we also test our model on some images outside the CADB dataset to show the generalization ability. Specifically, we test our model on some images collected from PCCD dataset [2]  and show the results in <ref type="figure">Figure 9</ref>. Although the PCCD dataset contains the overall composition score, they only present one reviewer's composition rating for each image and this reviewer (an anonymous website visitor) may be unprofessional, rendering the composition annotations of PCCD very noisy. Thus, we only report the composition score estimated by our model in <ref type="figure">Figure 9</ref>. We can see that our model can reasonably predict composition mean scores.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for photo-quality assessment and enhancement based on visual aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computational and experimental approaches to visual aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Redies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="119" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aesthetic critiques generation for photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive fractional dilated convolution network for image aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to compose with professional photographs on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an LSTM-based saliency attentive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image aesthetic assessment: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="80" to="106" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of smartphone photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The photographer&apos;s eye: Composition and design for better digital photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squared earth mover&apos;s distance-based loss for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno>abs/1611.05916</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning visual balance from largescale datasets of aesthetically highly rated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Vision and Electronic Imaging XX</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aesthetic attributes assessment of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritendra</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Tuan</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PAC-Net: Pairwise aesthetic comparison network for image aesthetic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semantic line detection and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Photographic composition classification and dominant geometric element detection for outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="105" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aesthetic quality assessment of consumer photos with faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Personalityassisted multi-task learning for generic and personalized image aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leida</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hancheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3898" to="3910" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel feature fusion method for computing image aesthetic quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianlin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="63043" to="63054" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composition-aware image aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing photo composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluation of visual balance for automated layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on Intelligent user interfaces</title>
		<meeting>the 9th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A-Lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Visual forces: an introduction to design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Block</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Pearson College Division</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The role of image composition in image aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Obrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Hackenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The fundamentals of creative photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pr?kel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Bloomsbury Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context-aware photography learning for smart mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Clicksmart: A context-aware viewpoint recommendation system for mobile photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A spring-electric graph model for socialized group photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="754" to="766" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Personalized image aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A holistic approach to aesthetic enhancement of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluation of image appeal in consumer photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Etz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human vision and electronic imaging V</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Will people like your image? learning the aesthetic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">Pa</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scenic photo quality assessment with bag of aesthetics-preserving features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Te</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07608</idno>
		<title level="m">Philip Andrew Mansfield, Lior Shapira, and Colvin Pitts. Camera view adjustment prediction for improving image composition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NIMA: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Instagram likes for architectural photos can be predicted by quantitative balance measures and curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Th?mmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>H?bner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1050" to="1067" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image cropping with composition and saliency aware aesthetic score map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling human perception for image aesthetic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural aesthetic image reviewer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="749" to="758" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Highlevel semantic photographic composition analysis and understanding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Tzu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Lun</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu-Chan</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chun</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The good, the bad, and the ugly: Predicting aesthetic image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Joon-Young Lee, and In So Kweon. Multi-scale pyramid pooling for deep convolutional representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunggyun</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to detect multiple photographic defects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fusion of multichannel local and global structural cues for photo aesthetics evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1419" to="1429" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint image and text representation for aesthetics analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Modeling perspective effects in photographic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The control of the false discovery rate in multiple testing under dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yekutieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1165" to="1188" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Aesthetic critiques generation for photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adaptive fractional dilated convolution network for image aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The photographer&apos;s eye: Composition and design for better digital photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Squared earth mover&apos;s distance-based loss for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno>abs/1611.05916</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Aesthetic attributes assessment of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Logistic regression in rare events data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="163" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The role of image composition in image aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Obrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Hackenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">The fundamentals of creative photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pr?kel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Bloomsbury Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Personalized image aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Evaluation of image appeal in consumer photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Etz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human vision and electronic imaging V</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">NIMA: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Neural aesthetic image reviewer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="749" to="758" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Joint image and text representation for aesthetics analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
