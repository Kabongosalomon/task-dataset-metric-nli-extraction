<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONTRASTIVE LEARNING OF GENERAL-PURPOSE AUDIO REPRESENTATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaqib</forename><surname>Saeed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CONTRASTIVE LEARNING OF GENERAL-PURPOSE AUDIO REPRESENTATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-self-supervised learning</term>
					<term>audio</term>
					<term>sound</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce COLA, a self-supervised pre-training approach for learning a general-purpose representation of audio. Our approach is based on contrastive learning: it learns a representation which assigns high similarity to audio segments extracted from the same recording while assigning lower similarity to segments from different recordings. We build on top of recent advances in contrastive learning for computer vision and reinforcement learning to design a lightweight, easy-toimplement self-supervised model of audio. We pre-train embeddings on the large-scale Audioset database and transfer these representations to 9 diverse classification tasks, including speech, music, animal sounds, and acoustic scenes. We show that despite its simplicity, our method significantly outperforms previous self-supervised systems. We furthermore conduct ablation studies to identify key design choices and release a library 1 to pre-train and fine-tune COLA models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Self-supervised pre-training has recently emerged as a successful technique to leverage unlabeled data to learn representations beneficial to supervised problems. This success spans a wide range of tasks and modalities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Among these methods, Discriminative Pre-Training (DPT) is particularly effective. This approach learns a representation from pairs of similar inputs from unlabeled data, exploiting e.g. temporal consistency <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> or data augmentation <ref type="bibr" target="#b6">[7]</ref> and trains a model to recognize similar elements among negative distractors. In contrast with generative encoder-decoder approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, DPT is computationally efficient as it avoids input reconstruction entirely.</p><p>Amidst DPT models for audio, <ref type="bibr" target="#b5">[6]</ref> used a metric learning approach with a triplet loss to minimize the distance between embeddings of anchor and positive pairs and maximize it among the negatives. The instance generation is achieved through noise injection, shifting along time-frequency di-mensions, and extracting samples in temporally close neighborhoods. Along similar lines, <ref type="bibr" target="#b12">[13]</ref> proposed a benchmark for comparing speech representations on non-semantic tasks. Through utilizing a triplet loss as an unsupervised objective with a subset of AudioSet <ref type="bibr" target="#b13">[14]</ref> for model training, they showed improved performance on several downstream speech classification tasks. Inspired from seminal work in NLP <ref type="bibr" target="#b14">[15]</ref>, the work in <ref type="bibr" target="#b15">[16]</ref> adopted a similar approach to learn audio representations (i.e. AUDIO2VEC) along with another "pretext" task of estimating temporal distance between audio segments. The pre-trained models are tested on several downstream tasks, from speaker identification to music recognition. Despite recent progress, most work on learning representations of audio focuses on speech tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> (with the exception of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>) and ignores other audio tasks such as acoustic scene detection or animal vocalizations. Moreover, triplet-based objectives heavily rely on the mining of negative samples, and the quality of learned features can vary significantly with the sample generation scheme.</p><p>In this work, we propose COLA (COntrastive Learning for Audio), a simple contrastive learning framework to learn general-purpose representations of sounds beyond speech. We build upon recent advances in contrastive learning <ref type="bibr" target="#b1">[2]</ref> for computer vision (SIMCLR <ref type="bibr" target="#b6">[7]</ref>, MOCO <ref type="bibr" target="#b19">[20]</ref>) and reinforcement learning (CURL <ref type="bibr" target="#b20">[21]</ref>). We generate similar pairs by simply sampling segments from the same audio clip, which avoids exploring augmentation strategies entirely unlike SIMCLR, MOCO, CURL and others <ref type="bibr" target="#b21">[22]</ref>. Our dissimilar pairs simply associate segments from different clips in the same batch, which does not require maintaining a memory bank of distractors as in MOCO. Our approach allows us to consider a large number of negatives for each positive pair in the loss function and bypass the need for a careful choice of negative examples, unlike triplet-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. COLA is also different from CPC <ref type="bibr" target="#b1">[2]</ref> as it does not predict future latent representations from past ones.</p><p>We demonstrate the effectiveness of COLA over challenging and diverse downstream tasks, including speech, music, acoustic scenes, and animal sounds. After pre-training on the large-scale AudioSet database <ref type="bibr" target="#b13">[14]</ref>, we show that a linear classifier trained over a COLA embedding gets close to the performance of a fully-supervised in-domain convolutional  network and exceeds it when using fine-tuning. Moreover, our system outperforms previous unsupervised approaches on most downstream tasks. These experiments demonstrate that COLA offers a simple, easy-to-implement method to learn general-purpose audio representations without supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>We learn general-purpose audio representations from unlabeled data by pre-training a neural network with a contrastive loss function. Our objective function maximizes an agreement between the latent embedding of segments extracted from the same audio clip while using different audio clips as negative classes, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. This objective pre-trains a convolutional feature extractor on unlabeled audio data. After pre-training, we combine our feature extractor with an additional classification layer for solving various audio understanding tasks across several datasets. Contrastive learning extracts a latent space in which the similarity between an anchor example and a related example should be greater than the similarity between the same anchor and unrelated examples. In our case, an anchor and its corresponding positive are audio segments from the same clip. This contrasts with approaches that generate positives as perturbations of the anchor <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. For negative examples, we take segments from different audio clips in the current training batch. This strategy allows to consider a large number of negatives and is efficient since batch examples are used both as positives and negatives without additional computation.</p><p>COLA computes the similarity between audio segments in two steps. First, an encoder f maps a log-compressed melfilterbanks x ? R N ?T , with N and T the number of frequency bins and time frames respectively, into a latent representation h = f (x) ? R d . This is the representation that we will transfer to downstream classification, after pre-training. Then, a shallow neural network g maps h onto a space z = g(h), where bilinear comparisons are performed. If we denote with W the bilinear parameters, the similarity between two segments (x, x ) is, therefore:</p><formula xml:id="formula_0">s(x, x ) = g(f (x)) W g(f (x )).<label>(1)</label></formula><p>Bilinear similarity has been used in the past <ref type="bibr" target="#b1">[2]</ref> but is less common than cosine similarity, e.g. SIMCLR and MOCO. In Section 3, we perform an ablation study on the choice of similarity measure. <ref type="table" target="#tab_2">Table 3</ref> shows that a bilinear similarity outperforms a simple cosine similarity</p><formula xml:id="formula_1">( g(f (x)) ?g(f (x )) g(f (x)) g(f (x ))</formula><p>) on all downstream tasks. In the rest of this paper, we use this method when not stated otherwise.</p><p>As an objective function, we rely on multi-class cross entropy applied to similarities, i.e.</p><formula xml:id="formula_2">L = ? log exp (s(x, x + )) x ? ?X ? (x)?{x + } exp (s(x, x ? ))<label>(2)</label></formula><p>where x + is the positive associated to anchor x, while X ? (x) refers to the set of negative distractors. This loss, unlike the triplet loss <ref type="bibr" target="#b23">[24]</ref>, leverages multiple distractors at a time.</p><p>As mentioned earlier, we train our model with positive segment pairs sampled from the same audio clip. For each pair, we use one segment as the anchor and the other element as the positive. Positive segments are used as negatives for all other anchors in the batch. This strategy is more efficient than keeping a memory bank of negatives <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref> since the representation of an example is paired with every anchor in the batch either as a positive or as a negative segment. In particular, we experiment with batch sizes varying from 256 to 2048, as shown in <ref type="table" target="#tab_3">Table 4</ref>. A large batch size allows the model to see many negative samples per anchor and helps accuracy on end tasks. It is important to note that we sample segment pairs on-the-fly and reshuffle the data at each training epoch to maximize the diversity of positive and negative pairs seen during training. The sample generation procedure is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We evaluate our method by pre-training COLA embeddings on a large-scale audio dataset and then transferring it to downstream tasks in the following ways: 1) training a linear classifier on top of a frozen embedding, used as a feature extractor and 2) fine-tuning the entire network on the end-task. Importantly, we assess the performance on several diverse datasets to determine the transferability of learned representations across audio domains and recording conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Tasks</head><p>We pre-train COLA embeddings on the diverse, large-scale Audioset <ref type="bibr" target="#b13">[14]</ref>. It contains 2 millions excerpts of 10 seconds audio from YouTube videos that are annotated in a multi-label fashion with over 500 classes. This dataset has been used by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13]</ref> for self-supervised pre-training. Since our method is self-supervised, we never use Audioset labels. As described earlier, we randomly sample audio clips to generate examples. Likewise, for the extraction of anchors and positives, segments of audio are selected uniformly at random inside a sequence. We perform downstream evaluation on a variety of tasks, including both speech and non-speech. To allow for comparison with previous methods, we rely on datasets that have been previously used by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13]</ref>. For speaker identification, we use a 100-hours subset of LibriSpeech (LBS) <ref type="bibr" target="#b25">[26]</ref> that contains audio of books read by 251 speakers, as well as the Voxceleb <ref type="bibr" target="#b26">[27]</ref> subset used in <ref type="bibr" target="#b12">[13]</ref>, with 1, 251 speakers. For keyword spotting, we use Speech Commands (SPC) <ref type="bibr" target="#b27">[28]</ref> V1 and V2 to recognize 11 and 35 spoken commands (classes) from one second of audio, respectively. For acoustic scene classification, we use TUT Urban Acoustic Scenes 2018 (TUT) <ref type="bibr" target="#b28">[29]</ref>, consisting of labeled audio segments from 10 different acoustic scenes. For animal vocalizations, we use the Bird Song Detection (BSD) dataset <ref type="bibr" target="#b29">[30]</ref> from DCASE 2018 Challenge to solve a binary classification problem. For music recognition, we use MUSAN <ref type="bibr" target="#b30">[31]</ref> that differentiates audio samples across 3 classes (speech, music and noise), as well as the NSynth dataset <ref type="bibr" target="#b31">[32]</ref> of musical notes, labeled with the family of the instrument (11 classes). For language identification, we use the Voxforge dataset <ref type="bibr" target="#b32">[33]</ref> to categorize audio clips into six classes based on the spoken language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Architecture and Implementation Details</head><p>Given an audio input sequence, we extract log-compressed mel-filterbanks with a window size of 25 ms, a hop size of 10 ms, and N = 64 mel-spaced frequency bins in the range 60-7800 Hz for T = 96 frames, corresponding to 960 ms. These features are passed through an encoder f based on EfficientNet-B0 <ref type="bibr" target="#b33">[34]</ref>, a lightweight and highly scalable convolutional neural network. Even though EfficientNet-B0 has been originally proposed for computer vision, the 2D structure of mel-filterbanks allows using this architecture without any adjustment. We apply a global max-pooling to the last layer of the encoder to get an embedding h of size 1280. During pre-training, we pass h through the projection head g, which contains a fully-connected layer with 512 units followed by a Layer Normalization <ref type="bibr" target="#b34">[35]</ref> and a tanh activation. We discard the projection head for the downstream tasks and train a linear classifier on top of the encoder directly. We pre-train all our models with ADAM <ref type="bibr" target="#b35">[36]</ref> and a learning rate of 10 ?4 , for 500 epochs. We explore the impact of the batch size and report the results in <ref type="table" target="#tab_3">Table 4</ref>. We train the downstream classifiers with a batch size of 64 and a learning rate of 10 ?3 , on randomly selected 960ms segments, as for pre-training. However, we evaluate downstream classifiers on entire sequences using the following procedure: we split the sequence into non-overlapping 960ms segments, pass them through the encoder and linear classifier, and average the predictions. <ref type="table" target="#tab_0">Table 1</ref> reports the accuracy on the 9 downstream datasets. We compare our approach against multiple baselines: a linear classifier trained on a randomly initialized fixed encoder and a fully-supervised model trained directly on downstream datasets which indicates the performance achievable with EfficientNet-B0 on these datasets. First, we evaluate pretrained COLA embeddings with a linear classifier on top of frozen representations, following the same procedure as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. This outperforms drastically the performance of a linear classifier trained on a random embedding (74.3% against 29.1% on average), showing that the encoder has learned useful representations. This is remarkable as we pre-train a single COLA embedding, which performs well across many tasks. Next, we also use a pre-trained COLA as initialization and fine-tune one model per downstream task. <ref type="table" target="#tab_0">Table 1</ref> shows that on all tasks but language identification, initializing a supervised model with COLA improves the performance over training from scratch (85.1% against 83.9% on average), which demonstrates the benefits of transferring COLA representations even in a fully supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>We then compare COLA to prior self-supervised methods proposed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>, including a standard triplet loss, AU-DIO2VEC (CBoW and SG) and temporal gap prediction models. Here, the CBoW and SG are generative models inspired from WORD2VEC, trained to reconstruct a randomly selected temporal slice of log-mel spectrograms given the rest or vice versa. Likewise, TemporalGap trains a model to predict the temporal distance between two pairs of audio segments. Ta- <ref type="table">Table 2</ref>. Test accuracy (%) of a linear classifier trained on top of COLA embeddings or baseline pre-trained representations.</p><p>CBoW <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> SG <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> TemporalGap <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> Triplet Loss <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>   ble 2 shows that COLA embeddings consistently outperform all these methods. In particular, on acoustic scene classification, we obtain a competitive accuracy of 94% compared to 73% achieved with a triplet loss in <ref type="bibr" target="#b15">[16]</ref>. We also considerably improve the performance on speech commands and musical instrument classification by an absolute 30% margin on both tasks. We also compare with the recent self-supervised learning framework TRILL <ref type="bibr" target="#b12">[13]</ref> on three speech-related tasks, benchmarking against TRILL-19 (the best self-supervised system of <ref type="bibr" target="#b12">[13]</ref>). Our general-purpose COLA embeddings are competitive with TRILL, despite the fact that TRILL is pre-trained specifically on the part of Audioset that contains speech, and is evaluated only across speech tasks, while we train and evaluate COLA across speech, music, acoustic scenes, and animal sounds.</p><p>To investigate the role of the similarity measure in the quality of learned representations, we perform an ablation study to compare model pre-training with cosine and bilinear similarity. With the cosine similarity, we use a temperature ? = 0.2 to normalize the scores before computing the loss. <ref type="table" target="#tab_2">Table 3</ref> reports the results obtained on downstream classifiers using encoders pre-trained with each of the similarity estima- tion techniques. We observe that the best results are obtained using bilinear similarity in all cases. We also conduct an experiment to measure the impact of pre-training batch size, as larger batch sizes result in more negative samples and facilitate convergence <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows that, on average, a batch size as large as 1024 provides better representations compared to smaller ones. However, increasing the batch size up to 2048 worsens the performance in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We introduce COLA , a simple, easy-to-implement, selfsupervised contrastive algorithm for general-purpose audio representation learning. Our approach achieves remarkable performance improvements over earlier unsupervised methods on a wide variety of challenging downstream tasks in a linear evaluation protocol as well as significantly improves results over supervised baselines through fine-tuning. We believe that the simplicity of our system, combined with its strong transferability across audio tasks, will pose it as a go-to baseline for future work in self-supervised learning for audio.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the contrastive self-supervised learning for audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Test accuracy (%) on downstream tasks.</figDesc><table><row><cell></cell><cell cols="2">Random Supervised</cell><cell cols="2">COLA</cell></row><row><cell>Task</cell><cell>Init.</cell><cell></cell><cell cols="2">Frozen Fine-tuned</cell></row><row><cell>Speaker Id. (LBS)</cell><cell>0.4</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Speech commands (V1)</cell><cell>62.9</cell><cell>97.2</cell><cell>71.7</cell><cell>98.1</cell></row><row><cell>Speech commands (V2)</cell><cell>4.0</cell><cell>94.3</cell><cell>62.4</cell><cell>95.5</cell></row><row><cell>Acoustic scenes</cell><cell>8.6</cell><cell>98.2</cell><cell>94.1</cell><cell>99.2</cell></row><row><cell>Speaker Id. (Voxceleb)</cell><cell>0.0</cell><cell>31.7</cell><cell>29.9</cell><cell>37.7</cell></row><row><cell>Birdsong detection</cell><cell>49.6</cell><cell>79.4</cell><cell>77.0</cell><cell>80.2</cell></row><row><cell>Music, Speech and Noise</cell><cell>56.8</cell><cell>99.3</cell><cell>99.1</cell><cell>99.4</cell></row><row><cell>Language Id.</cell><cell>59.1</cell><cell>85.0</cell><cell>71.3</cell><cell>82.9</cell></row><row><cell>Music instrument</cell><cell>20.8</cell><cell>70.7</cell><cell>63.4</cell><cell>73.0</cell></row><row><cell>Average</cell><cell>29.1</cell><cell>83.9</cell><cell>74.3</cell><cell>85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Test accuracy (%) with different similarity functions.</figDesc><table><row><cell></cell><cell cols="2">Cosine Similarity Bilinear Similarity</cell></row><row><cell>Speaker Id. (LBS)</cell><cell>99.9</cell><cell>100.0</cell></row><row><cell>Speech commands (V1)</cell><cell>64.5</cell><cell>71.7</cell></row><row><cell>Speech commands (V2)</cell><cell>42.4</cell><cell>62.4</cell></row><row><cell>Acoustic scenes</cell><cell>87.5</cell><cell>94.1</cell></row><row><cell>Speaker Id. (Voxceleb)</cell><cell>15.2</cell><cell>29.9</cell></row><row><cell>Birdsong detection</cell><cell>76.5</cell><cell>77.0</cell></row><row><cell>Music, Speech and Noise</cell><cell>99.0</cell><cell>99.1</cell></row><row><cell>Language Id.</cell><cell>62.3</cell><cell>71.3</cell></row><row><cell>Music instrument</cell><cell>58.3</cell><cell>63.4</cell></row><row><cell>Average</cell><cell>67.2</cell><cell>74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Impact of pre-training batch size on downstream test accuracy (%), using a bilinear similarity.</figDesc><table><row><cell></cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell></row><row><cell>Speaker Id. (LBS)</cell><cell cols="2">99.9 99.9</cell><cell>100.0</cell><cell>99.9</cell></row><row><cell>Speech commands (V1)</cell><cell cols="2">66.9 69.4</cell><cell>71.7</cell><cell>72.9</cell></row><row><cell>Speech commands (V2)</cell><cell cols="2">44.4 54.2</cell><cell>62.4</cell><cell>64.2</cell></row><row><cell>Acoustic scenes</cell><cell cols="2">86.4 90.7</cell><cell>94.1</cell><cell>90.2</cell></row><row><cell>Speaker Id. (Voxceleb)</cell><cell cols="2">17.6 21.6</cell><cell>29.9</cell><cell>22.8</cell></row><row><cell>Birdsong detection</cell><cell cols="2">75.9 76.9</cell><cell>77.0</cell><cell>76.4</cell></row><row><cell cols="2">Music, Speech and Noise 98.8</cell><cell>99.1</cell><cell>99.1</cell><cell>98.8</cell></row><row><cell>Language Id.</cell><cell cols="2">65.6 64.0</cell><cell>71.3</cell><cell>68.4</cell></row><row><cell>Music instrument</cell><cell cols="2">62.3 57.3</cell><cell>63.4</cell><cell>56.6</cell></row><row><cell>Average</cell><cell cols="2">68.6 70.3</cell><cell>74.3</cell><cell>72.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning robust and multilingual speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11128</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of semantic audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Proceedings of the ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00982</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Audio enhancing with dnn autoencoder for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aronowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Proceedings of the ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning for audio analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Google&apos;s next-generation real-time unit-selection synthesizer using sequence-to-sequence lstm-based autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1143" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning problem-agnostic speech representations from multiple self-supervised tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03416</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards learning a universal non-semantic representation of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haviv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12764</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised audio representation learning for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11796</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining transfers well across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data augmenting contrastive learning of speech representations in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00991</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01978</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-training audio representations with selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tut urban acoustic scenes 2018, development dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pamu?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Musan: A music, speech, and noise corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08484</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1068" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ken MacLean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maclean</surname></persName>
		</author>
		<ptr target="http://www.voxforge.org/home" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Voxforge. Acedido em 2012</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
