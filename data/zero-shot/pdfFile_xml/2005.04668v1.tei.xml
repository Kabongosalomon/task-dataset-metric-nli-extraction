<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptation for Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multispectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multispectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
							<email>rwq.renwenqi@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multispectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multispectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptation for Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image dehazing using learning-based methods has achieved state-of-the-art performance in recent years. However, most existing methods train a dehazing model on synthetic hazy images, which are less able to generalize well to real hazy images due to domain shift. To address this issue, we propose a domain adaptation paradigm, which consists of an image translation module and two image dehazing modules. Specifically, we first apply a bidirectional translation network to bridge the gap between the synthetic and real domains by translating images from one domain to another. And then, we use images before and after translation to train the proposed two image dehazing networks with a consistency constraint. In this phase, we incorporate the real hazy image into the dehazing training via exploiting the properties of the clear image (e.g., dark channel prior and image gradient smoothing) to further improve the domain adaptivity. By training image translation and dehazing network in an end-to-end manner, we can obtain better effects of both image translation and dehazing. Experimental results on both synthetic and real-world images demonstrate that our model performs favorably against the state-of-theart dehazing algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image dehazing aims to recover the clean image from a hazy input, which is essential for subsequent high-level tasks, such as object recognition and scene understanding. Thus, it has received significant attention in the vision community over the past few years. According to the physical scattering models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref>, the hazing process is usually formulated as I(x) = J(x)t(x) + A(1 ? t(x)).</p><p>(1) * Corresponding author. where I(x) and J(x) denote the hazy image and the clean image, A is the global atmospheric light, and t(x) is the transmission map. The transmission map can be represented as t(x) = e ??d(x) , where d(x) and ? denote the scene depth and the atmosphere scattering parameter, respectively. Given a hazy image I(x), most dehazing algorithms try to estimate t(x) and A. However, estimating the transmission map from a hazy image is an ill-posed problem generally. Early prior-based methods try to estimate the transmission map by exploiting the statistical properties of clear images, such as dark channel prior <ref type="bibr" target="#b8">[9]</ref> and color-line prior <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, these image priors are easily inconsistent with the practice, which may lead to inaccurate transmission approximations. Thus, the quality of the restored image is undesirable.</p><p>To deal with this problem, convolutional neural networks (CNNs) have been employed to estimate transmissions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> or predict clear images directly <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>. These methods are valid and superior to the priorbased algorithms with significant performance improvements. However, deep learning-based approaches need to rely on a large amount of real hazy images and their hazefree counterparts for training. In general, it is impractical to acquire large quantities of ground-truth images in the real world. Therefore, most dehazing models resort to training on synthetic hazy dataset. However, due to the domain shift problem, the models learned from synthetic data often fail to generalize well to real data.</p><p>To address this issue, we propose a domain adaptation framework for single image dehazing. The proposed framework includes two parts, namely an image translation module and two domain-related dehazing modules (one for synthetic domain and another for real domain). To reduce the discrepancy between domains, our method first employs the bidirectional image translation network to translate images from one domain to another. Since image haze is a kind of noise and nonuniform highly depending on the scene depth, we incorporate the depth information into the translation network to guide the translation of synthetic to real hazy images. Then, the domain-related dehazing network takes images of this domain, including the original and translated images, as inputs to perform image dehazing. Moreover, we use a consistency loss to ensure that the two dehazing networks generate consistent results. In this training phase, to further improve the generalization of the network in the real domain, we incorporate the real hazy images into the training. We hope that the dehazing results of the real hazy image can have some properties of the clear images, such as dark channel prior and image gradient smoothing. We train the image translation network and dehazing networks in an end-to-end manner so that they can improve each other. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model produces a cleaner image when compared with recent dehazing work of EPDN <ref type="bibr" target="#b24">[25]</ref>.</p><p>We summarize the contributions of our work as follows:</p><p>? We propose an end-to-end domain adaptation framework for image dehazing, which effectively bridges the gap between the synthetic and real-world hazy images. ? We show that incorporating real hazy images into the training process can improve the dehazing performance. ? We conduct extensive experiments on both synthetic datasets and real-world hazy images, which demonstrate that the proposed method performs favorably against the state-of-the-art dehazing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly discuss the single image dehazing approaches and domain adaptation methods, which are related to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image Dehazing</head><p>Prior-based methods. Prior-based methods estimate the transmission maps and atmospheric light intensity based on the statistics of clear images. Representative works in this regard include <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>. Specifically, Tan <ref type="bibr" target="#b28">[29]</ref> proposes a contrast maximization method for image dehazing since it observes that clear images tend to have higher contrast than their hazy counterparts. He et al. <ref type="bibr" target="#b8">[9]</ref> make use of dark channel prior (DCP) to estimate the transmission map, which is based on the assumption that pixels in hazefree patches are close to zero in at least one color channel. Follow-up works have improved the efficiency and performance of the DCP method <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref>. Besides, the attenuation prior is adopted in <ref type="bibr" target="#b38">[39]</ref> for recovering depth information of the hazy images. Fattal <ref type="bibr" target="#b7">[8]</ref> uses the color-line assumption to recover the scene transmission, which asserts that pixels of small image patches exhibit a one-dimensional distribution. Similarly, Berman et al. <ref type="bibr" target="#b1">[2]</ref> assume that several hundreds of distinct colors can well approximate colors of a clear image, and then perform image dehazing based on this prior. Though these methods have been shown effective for image dehazing, their performances are inherently limited since the assumed priors are not suited for all real-word images.</p><p>Learning-based Methods. With the advance in deep convolutional neural networks (CNNs) and the availability of large-scale synthetic datasets, data-driven approaches for image dehazing have received significant attention in recent years. Many methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b11">12]</ref> directly utilize deep CNNs to estimate the transmissions and atmospheric light, and then restore the clean image according to the degradation model (1). Cai et al. <ref type="bibr" target="#b3">[4]</ref> propose an end-to-end dehazing model, DehazeNet, to estimate the transmission map from hazy images. Ren et al. <ref type="bibr" target="#b25">[26]</ref> utilize a coarseto-fine strategy to learn the mapping between hazy inputs and transmission maps. Zhang and Patel <ref type="bibr" target="#b34">[35]</ref> propose a densely connected pyramid network to estimate the transmission maps. Li et al. <ref type="bibr" target="#b11">[12]</ref> propose an AOD-Net to estimate the parameter of the reformulated physical scattering model, which integrates the transmissions and atmospheric light. Also, some end-to-end methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> have been proposed to recover the clean image directly instead of estimating the transmission map and atmospheric light. Ren et al. <ref type="bibr" target="#b26">[27]</ref> adopt a gated fusion network to recover the clean image from a hazy input directly. Qu et al. <ref type="bibr" target="#b24">[25]</ref> transform the problem of image dehazing to an image-to-image translation problem, and propose an enhanced pix2pix dehazing network.</p><p>However, due to the domain gap between synthetic and real data, the CNN-based models trained on synthetic images tend to have a significant performance drop when applied to the real domain. To this end, Li et al. <ref type="bibr" target="#b13">[14]</ref> propose a semi-supervised dehazing model, which is trained on both synthetic and real haze image, and thus enjoys domain adaptivity between synthetic and real-world hazy images. However, only applying the real haze image for training does not really solve the problem of domain shift. Different from the above methods, our model first applies the image translation network to translate images from one domain to another, and then performs image dehazing on both synthetic and real domain using the translated images and their original images (synthetic or real). The proposed ap-proach can effectively solve the domain shift problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain Adaptation</head><p>Domain adaptation aims to reduce the discrepancy between different domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. Existing work either to perform feature-level or pixel-level adaptation. Featurelevel adaptation methods aim at aligning the feature distributions between the source and target domains through minimizing the maximum mean discrepancy <ref type="bibr" target="#b18">[19]</ref>, or applying adversarial learning strategies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> on the feature space. Another line of research focuses on pixel-level adaptation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7]</ref>. These approaches deal with the domain shift problem by applying image-to-image translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> learning, or style transfer <ref type="bibr" target="#b6">[7]</ref> methods to increase the data in the target domain.</p><p>Most recently, many methods perform feature-level and pixel-level adaptation jointly in many visual tasks, e.g., image classification <ref type="bibr" target="#b9">[10]</ref>, semantic segmentation <ref type="bibr" target="#b4">[5]</ref>, and depth prediction <ref type="bibr" target="#b36">[37]</ref>. These methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref> translate images from one domain to another with pixel-level adaptation via image-to-image translation networks, e.g., the Cycle-GAN <ref type="bibr" target="#b37">[38]</ref>. The translated images are then inputted to the task network with feature-level alignment. In this work, we take advantage of CycleGAN to adapt the real hazy images to our dehazing model trained on synthetic data. Moreover, since the depth information is closely related to the formulation of image haze, we incorporate the depth information into the translating network to better guide the real hazy image translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>This section presents the details of our domain adaptation framework. First, we provide an overview of our method. Then we describe the details of the image translation module and image dehazing module. Finally, we give the loss functions that are applied to train the proposed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>Given a synthetic dataset X S = {x s , y s } N l s=1 and a real hazy image set X R = {x r } Nu r=1 , where N l and N u denote the number of the synthetic and real hazy images, respectively. We aim to learn a single image dehazing model which can accurately predict the clear image from real hazy image. Due to the domain shift, the dehazing model trained only on the synthetic data can not generalize well to the real hazy image.</p><p>To deal with this problem, we present a domain adaptation framework, which consists of two main parts: the image translation network G S?R and G R?S , and two dehazing networks G S and G R . The image translation network translates images from one domain to another to bridge the gap between them. Then the dehazing networks perform image dehazing using both translated images and source images (e.g., synthetic or real).</p><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed model takes a real hazy image x r and a synthetic image x s along with its corresponding depth images d s as input. We first obtain the corresponding translated images x s?r = G S?R (x s , d s ) and x r?s = G R?S (x r ) using two image translators. And then, we pass x s and x r?s to G S , x r and x r?s to G R to perform image dehazing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Translation Module</head><p>The image translation module includes two translators: synthetic to real network G S?R and real to synthetic network G R?S . The G S?R network takes (X s , D s ) as inputs, and generates translated images G S?R (X s , D s ) with similar style to the real hazy images. Another translator G R?S performs image translation inversely. Since the depth information is highly correlated to the hazing formulation, we incorporate it into the generator G S?R to produce images with similar haze distribution in real cases.</p><p>We adopt the spatial feature transform (SFT) layer <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15]</ref> to incorporate the depth information into the translation network, which can fuse features from depth map and synthetic image effectively. As shown in <ref type="figure">Fig. 3</ref>, the SFT layer first applies three convolution layers to extract conditional maps ? from the depth map. The conditional maps are then fed to the other two convolution layers to predict the modulation parameters, ? and ?, respectively. Finally, we can obtain the output shifted features by:</p><formula xml:id="formula_0">SFT(F |?, ?) = ? F + ?.<label>(2)</label></formula><p>where is the element-wise multiplication. In the translator G S?R , we treat the depth map as the guidance and use the SFT layer to transform the features of the penultimate convolution layer. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the synthetic images are relative closer to the real-world hazy image after the translation. We show the detailed configurations of the translator G S?R in <ref type="table" target="#tab_1">Table 1</ref>. We also employ the architectures, provided by CycleGAN <ref type="bibr" target="#b37">[38]</ref>, for the generator G R?S and discriminators (D img R and D img S ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dehazing Module</head><p>Our method includes two dehazing modules G S and G R , which perform image dehazing on synthetic and real domains, respectively. G S takes the synthetic image x s and the translated image x r?s as inputs to perform image dehazing. And G R is trained on x r and x s?r . For these two image dehazing networks, we both utilize a standard encoderdecoder architecture with skip connections and side outputs as <ref type="bibr" target="#b36">[37]</ref>. The dehazing network in each domain shares the same network architecture but with different learned parameters.    <ref type="figure">Figure 3</ref>. Structure of the SFT layer. In the translator GS?R, we consider the depth map as the guidance to assist the image translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Losses</head><p>In the domain adaptation framework, we adopt the following losses to train the network. Image translation Losses. The aim of our translate module is to learn the translators G S?R and G R?S to reduce the discrepancy between the synthetic domain X S and the real domain X R . For translators G S?R , we expect the G S?R (x s , d s ) to be indistinguishable from the real hazy image x r . Thus, we employ an image-level discriminators D img R and a feature-level discriminators D f eat R , to perform a minmax game via an adversarial learning manner. The  </p><formula xml:id="formula_1">L img Gan (X R , (X S , D S ), D img R , G S?R ) = E xs?X S ,ds?D S [D img R (G S?R (x s , d s ))] +E xr?X R [D img R (x r ) ? 1].</formula><p>(3)</p><formula xml:id="formula_2">L f eat Gan (X R , (X S , D S ), D f eat R , G S?R , G R ) = E xs?X S ,ds?D S [D f eat R (G R (G S?R (x s , d s )))] +E xr?X R [D f eat R (G R (x r )) ? 1].<label>(4)</label></formula><p>Similar to G S?R , the translator G R?S has another image-level adversarial loss and feature-level adversarial loss, which are denoted as L img</p><formula xml:id="formula_3">Gan (X S , X R , D img S , G R?S ), L f eat Gan (X S , X R , D f eat S , G R?S , G S ),</formula><p>respectively. In addition, we utilize the cycle-consistency loss <ref type="bibr" target="#b37">[38]</ref> to regularize the training of translation network. Specifically, when passing an image x s to G S?R and G R?S sequentially, we expect the output should be the same image, and vice versa for x r . Namely, G R?S (G S?R (x s , d s )) ? x s and G S?R (G R?S (x r ), d r ) ? x r . The cycle consistency loss can be expressed as:</p><formula xml:id="formula_4">L c = E xs?X S ,ds?D S [||G R?S (G S?R (x s , d s ) ? x s || 1 ] + E xr?X R ,dr?D R [||G S?R (G R?S (x r ), d r ) ? x r || 1 ].</formula><p>(5) Finally, to encourage the generators to preserve content information between the input and output, we also utilize an identity mapping loss <ref type="bibr" target="#b37">[38]</ref>, which is denoted as:</p><formula xml:id="formula_5">L idt = E xs?X S [||G R?S (x s ) ? x s || 1 ] +E xr?X R ,dr?D R [||G S?R (x r , d r ) ? x r || 1 ].<label>(6)</label></formula><p>The full loss function for the the translating module is as follow:</p><formula xml:id="formula_6">L tran = L img Gan (X R , (X S , D S ), D img R , G S?R ) +L f eat Gan (X R , (X S , D S ), D f eat R , G S?R , G R ) +L img Gan (X S , X R , D img S , G R?S ) +L f eat Gan (X S , X R , D f eat S , G R?S , G S ) +? 1 L c + ? 2 L idt .<label>(7)</label></formula><p>Image dehazing Losses. We can now transfer the synthetic images X S and the corresponding depth images D S to the generator G S?R , and obtain a new dataset X S?R = G S?R (X S , D S ), which has a similar style with real hazy images. And then, we train a image dehazing network G R on X S?R and X R in a semi-supervised manner. For supervised branch, we apply the mean squared loss to ensure the predicted images J S?R is close to clean images Y S , which can be defined as:</p><formula xml:id="formula_7">L rm = J S?R ? Y S 2 2 .<label>(8)</label></formula><p>In the unsupervised branch, we introduce the total variation and dark channel losses, which regularize the dehazing network to produce images with similar statistical characteristics of the clear images. The total variation loss is an 1 -regularization gradient prior on the predicted images J R :</p><formula xml:id="formula_8">L rt = ? h J R 1 + ? v J R 1 .<label>(9)</label></formula><p>where ? h denotes the horizontal gradient operators, and ? v represents the vertical gradient operators. Furthermore, the recent work <ref type="bibr" target="#b8">[9]</ref> has proposed the concept of the dark channel, which can be expressed as:</p><formula xml:id="formula_9">D(I) = min y?N (x) min c?{r,g,b} I c (y) .<label>(10)</label></formula><p>where x and y are pixel coordinates of image I, I c represents c-th color channel of I, and N (x) denotes the local neighborhood centered at x. He et al. <ref type="bibr" target="#b8">[9]</ref> have also shown that most intensity of the dark channel image are zero or close to zero . Therefore, we apply the following dark channel (DC) loss to ensure that the dark channel of the predicted images are in consistence with that of clean image:</p><formula xml:id="formula_10">L rd = D(J R ) 1 .<label>(11)</label></formula><p>In addition, we also train a complementary image dehazing network G S on X S and X R?S . Similarly, we apply the same supervised loss and unsupervised loss to train the dehazing network G S , which are as follows:</p><formula xml:id="formula_11">L sm = J S ? Y S 2 2 ,<label>(12)</label></formula><formula xml:id="formula_12">L st = ? h J R?S 1 + ? v J R?S 1 ,<label>(13)</label></formula><formula xml:id="formula_13">L sd = D(J R?S ) 1 .<label>(14)</label></formula><p>Finally, considering that the outputs of the two dehazing networks should be consistency for real hazy images, i.e., G R (X R ) ? G S (G R?S (X R )), we introduce following consistency loss:</p><formula xml:id="formula_14">L c = G R (X R ) ? G S (G R?S (X R )) 1 .<label>(15)</label></formula><p>Overall Loss Function. The overall loss function are defined as follow:</p><formula xml:id="formula_15">L = L tran + ? m (L rm + L sm ) + ? d (L rd + L sd ) +? t (L rt + L st ) + ? c L c .<label>(16)</label></formula><p>where ? m , ? d , ? t and ? c are trade-off weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this part, we first present the implementation details of our framework. Then, we evaluate our domain adaptation method on synthetic datasets and the real images, respectively. Finally, ablation studies are conducted to analyze the proposed approach.</p><p>(a) Hazy image (b) NLD <ref type="bibr" target="#b1">[2]</ref> (c) DehazeNet <ref type="bibr" target="#b3">[4]</ref> (d) AOD-Net <ref type="bibr" target="#b11">[12]</ref> (e) DCPDN <ref type="bibr" target="#b34">[35]</ref> (f) GFN <ref type="bibr" target="#b26">[27]</ref> (g) EPDN <ref type="bibr" target="#b24">[25]</ref> (h) Ours (i) Clear image <ref type="figure">Figure 5</ref>. Visual comparisons on the SOTS <ref type="bibr" target="#b12">[13]</ref> dataset.</p><p>(a) Hazy image (b) NLD <ref type="bibr" target="#b1">[2]</ref> (c) DehazeNet <ref type="bibr" target="#b3">[4]</ref> (d) AOD-Net <ref type="bibr" target="#b11">[12]</ref> (e) DCPDN <ref type="bibr" target="#b34">[35]</ref> (f) GFN <ref type="bibr" target="#b26">[27]</ref> (g) EPDN <ref type="bibr" target="#b24">[25]</ref> (h) Ours (i) Clear image <ref type="figure">Figure 6</ref>. Visual comparisons on the HazeRD <ref type="bibr" target="#b35">[36]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Datasets. We randomly choose both synthetic and realword hazy images from the RESIDE dataset <ref type="bibr" target="#b12">[13]</ref> for training. The dataset is divided into five subsets, namely, ITS (Indoor Training Set), OTS (Outdoor Training Set), SOTS (Synthetic Object Testing Set), URHI (Unannotated real Hazy Images), and RTTS (real Task-driven Testing Set). For synthetic dataset, we choose 6000 synthetic hazy images for training, 3000 from the ITS and 3000 from the OTS. For real-world hazy images, we train the network by randomly selecting 1000 real hazy images from the URHI.</p><p>In the training phase, we randomly crop all the images to 256 ? 256 and normalize the pixel values to [?1, 1]. Training details. We implement our framework in Py-Torch <ref type="bibr" target="#b23">[24]</ref> and utilize ADAM <ref type="bibr" target="#b10">[11]</ref> optimizer with a batch size 2 to train the network. First, we train the image translation network G S?R and G R?S for 90 epochs with the momentum ? 1 = 0.5, ? 2 = 0.999, and the learning rate is set as 5 ? 10 ?5 . Then we train G R on {X R , G S?R (X s , D s )}, and G S on {X S , G R?S (X R )} for 90 epochs using the pretrained G S?R and G R?S models. The momentum and the learning rate are set as: ? 1 = 0.95, ? 2 = 0.999, lr = 10 ?4 . Finally, we fine tune the whole network using the above pre-trained models. When computing the DC loss, we set the patch as 35 ? 35. The trade-off weights are set as: ? tran = 1, ? m = 10, ? d = 10 ?2 , ? t = 10 ?3 and ? c = 10 ?1 .</p><p>Comparison methods. We evaluate the proposed method against the following state-of-the-art approaches: DCP <ref type="bibr" target="#b8">[9]</ref>, MSCNN <ref type="bibr" target="#b25">[26]</ref>, DehazeNet <ref type="bibr" target="#b3">[4]</ref>, NLD <ref type="bibr" target="#b1">[2]</ref>, AOD-Net <ref type="bibr" target="#b11">[12]</ref>,  <ref type="bibr" target="#b1">[2]</ref> (c) DehazeNet <ref type="bibr" target="#b3">[4]</ref> (d) AOD-Net <ref type="bibr" target="#b11">[12]</ref> (e) DCPDN <ref type="bibr" target="#b34">[35]</ref> (f) GFN <ref type="bibr" target="#b26">[27]</ref> (g) EPDN <ref type="bibr" target="#b24">[25]</ref> (h) Ours <ref type="figure">Figure 7</ref>. Visual comparisons on the real hazy images.</p><p>GFN <ref type="bibr" target="#b26">[27]</ref>, DCPDN <ref type="bibr" target="#b34">[35]</ref>, and EPDN <ref type="bibr" target="#b24">[25]</ref>. More image dehazing results and comparisons against other dehazing approaches are included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Synthetic Datasets</head><p>We use two synthetic datasets, namely SOTS <ref type="bibr" target="#b12">[13]</ref> and HazeRD <ref type="bibr" target="#b35">[36]</ref>, to evaluate the performance of our proposed method. The dehazed images of different methods on these two datasets are shown in <ref type="figure">Figure 5</ref> and 6. From <ref type="figure">Figure 5</ref> (b), we can observe that both NLD <ref type="bibr" target="#b1">[2]</ref> and GFN <ref type="bibr" target="#b26">[27]</ref> suffer from some color distortion where the result looks unrealistic. The dehazing results by EPDN <ref type="bibr" target="#b24">[25]</ref> are also darker than the ground-truths in some cases, as shown in <ref type="figure">Figure 5 (g)</ref>. Besides, there is still some remaining haze in the dehazed images by DehazeNet <ref type="bibr" target="#b3">[4]</ref>, AOD-Net <ref type="bibr" target="#b11">[12]</ref> and DCPDN <ref type="bibr" target="#b34">[35]</ref>. Compared with these methods, our algorithm restores images with sharper structures and details, which are closer to the ground-truths. Similar results can be found in the dehazed results on the HazeRD dataset in <ref type="figure">Figure 6</ref>, our algorithm generates results with better visual effect.</p><p>We also give the quantitative comparison of dehazed results in <ref type="table" target="#tab_2">Table 2</ref>. As shown, the proposed method obtains the highest PSNR and SSIM values on both datasets. Compared with the state-of-the-art EPDN <ref type="bibr" target="#b24">[25]</ref>, our approach achieves the gain with 3.94 dB and 0.04 in terms of PSNR and SSIM on the STOS dataset, respectively. For the HazeRD dataset, the PSNR and SSIM produced by our method are higher than EPDN <ref type="bibr" target="#b24">[25]</ref> by up to 0.7dB and 0.07, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Real Images</head><p>To evaluate the generalization of our method on real images, we compare the visual results of different approaches on real hazy images from the URHI dataset. As shown in <ref type="figure">Figure 7</ref>, NLD <ref type="bibr" target="#b1">[2]</ref> suffers from serious color distortions (see, e.g., the sky in <ref type="figure">Figure 7 (b)</ref>). <ref type="figure">From Fig 7 (f)</ref>, we can figure out that the GFN <ref type="bibr" target="#b26">[27]</ref> also suffers from color distortion in some cases, and the dehazed results look darker than our method. In addition, the dehazed results by De-hazeNet <ref type="bibr" target="#b3">[4]</ref>, AOD-Net <ref type="bibr" target="#b11">[12]</ref>, and DCPDN <ref type="bibr" target="#b34">[35]</ref> have some remaining haze artifacts as shown in the fifth row in <ref type="figure">Figure 7 (c-e)</ref>). Though EPDN <ref type="bibr" target="#b24">[25]</ref> has achieved a better visual effect than the above methods, the brightness of the dehazing results is lower than our method in general. Overall, our proposed method restores more details and obtains visually pleasing images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To verify the effectiveness of the image translation network and the unsupervised loss, we conduct a series of ablations to analyze our method. We construct the following dehazing models for comparison: 1) SYN: G S is only trained on X S ; 2) SYN+U: G S is trained on both X S and X R ; 3) R2S+U: G S is only trained on X S and G R?S (X R ); 4) S2R: G R is trained on G S?R (X s , D s ).</p><p>We compare the proposed domain adaptation method against these four dehazing models on both synthetic and real hazy images. The visual and quantitative results are shown in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure" target="#fig_4">Figure 8</ref>, which demonstrate that our approach achieves the best performance of image dehazing in terms of PSNR and SSIM as well as visual effects. As shown in <ref type="figure" target="#fig_4">Figure 8 (b)</ref>, due to the domain shift, the SYN method causes color distortion or darker artifacts (see, e.g., the sky part and the red rectangle). In contrast, the dehazing model G R trained on the translated images (S2R) achieves better quality image as shown in <ref type="figure" target="#fig_4">Figure 8</ref> (c), which demonstrates that the translators effectively reduces the discrepancy between synthetic data and real images. Moreover, <ref type="figure" target="#fig_4">Figure 8 (b) and (d)</ref> show that the dehazing model with unsupervised loss (SYN+U) can produce better results than SYN, which demonstrate the effectiveness of the unsupervised loss. Finally, we can observe that the proposed method with both translators and unsupervised loss generates cleaner and visually more pleasing results (e.g., the sky is brighter) in <ref type="figure" target="#fig_4">Figure 8</ref> (e). The quantitative results in <ref type="table" target="#tab_3">Table 3</ref> by applying image translation and unsupervised loss also agree with the qualitative results in <ref type="figure" target="#fig_4">Figure 8</ref>. As a conclusion, these ablations demonstrate that the image translation model and unsupervised loss are useful to reduce the domain gap between synthetic data and real-world images and improve the performance of image dehazing on both synthetic and real domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a novel domain adaptation framework for single image dehazing, which contains an image translation module and two image dehazing modules. we first use the image translation network to translate images from one domain to another to reduce the domain discrepancy. And then, the image dehazing networks take the translated images and their original images as inputs to perform image dehazing. To further improve the generalization, we incorporate the real hazy image into the dehazing training by exploiting the properties of clean images. Extensive experimental results on both synthetic datasets and real-world images demonstrate that our algorithm performs favorably against state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Dehazed results on a real world hazy image. Our method generates a cleaner image with fewer color distortion and brighter details. (a) Hazy image. (b) The result of EPDN [25]. (c) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the proposed domain adaptation framework for image dehazing. The framework consists of two parts, an image translation module and two image dehazing modules. The image translation module translates images from one domain to another to reduce the domain discrepancy. The image dehazing modules perform image dehazing on both synthetic and real domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Synthetic hazy image (b) Translated image (c) Real hazy image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Translated results on two synthetic hazy images. D img R aims at aligning the distributions between the real image x r and the translated image G S?R (x s , d s ). The discriminator D f eat R helps align the distributions between the feature map of x r and G S?R (x s , d s ). The adversarial losses are defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>(a) Real hazy image (b) Dehazed result of SYN (c) Dehazed result of S2R (d) Dehazed result of SYN+U (e) Dehazed result of Ours Comparison of dehazed results of several dehazing models on real hazy images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Synthetic hazy image Dehazed image Ground truth Real hazy image Dehazed image R2S image Ground truth Dehazed image Image translation module Dehazing module in synthetic domain Dehazing module in real domain Dehazed image Consistency loss Dehazing Network Unsupervised loss Dark Channel loss Total Variation loss S2R image Dehazing Network S2R Network GAN loss DR GAN loss Ds Sharing Weights Depth information S F T Dehazing Network R2S Network Dehazing Network Unsupervised loss Dark Channel loss Total Variation loss</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Layer</cell><cell></cell><cell></cell><cell>Conv1</cell><cell cols="2">Conv2</cell><cell>Conv3</cell><cell>Res4-12</cell><cell>Upconv13</cell><cell>Upconv14</cell><cell>SFT layer</cell><cell>Conv15</cell><cell>Tanh</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">In channels</cell><cell></cell><cell></cell><cell>3</cell><cell>64</cell><cell></cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>64</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Out channels</cell><cell></cell><cell>64</cell><cell cols="2">128</cell><cell>256</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>64</cell><cell>3</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Kernel size</cell><cell></cell><cell></cell><cell>7</cell><cell>3</cell><cell></cell><cell>3</cell><cell>-</cell><cell>3</cell><cell>3</cell><cell>-</cell><cell>7</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Stride</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell></cell><cell>2</cell><cell>-</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pad</cell><cell></cell><cell></cell><cell>-</cell><cell>1</cell><cell></cell><cell>1</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>?</cell><cell cols="4">Element-wise Product Summation</cell><cell cols="2">Input Features F</cell><cell></cell><cell></cell><cell cols="2">Scaled Features ?</cell><cell>?</cell><cell>Output Shifted Features ? +</cell></row><row><cell cols="2">Depth map</cell><cell>Conv</cell><cell>Conv</cell><cell>Conv</cell><cell>Features</cell><cell>Conv</cell><cell>Conv</cell><cell>Features</cell><cell>Conv</cell><cell>Conv</cell><cell>Features</cell></row></table><note>Configurations of image translation module. "Conv" denotes the convolution layer, "Res" denotes the residual block, "Upconv" denotes the up-sample layer by transposed convolution operator and "Tanh" denotes the non-linear Tanh layer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison (Average PSNR/SSIM) of the dehazing results on two synthetic datasets.</figDesc><table><row><cell></cell><cell>DCP [9]</cell><cell>NLD [2]</cell><cell>MSCNN [26]</cell><cell cols="3">DehazeNet [4] AOD-Net [12] DCPDN [35]</cell><cell>GFN [27]</cell><cell>EPDN [25]</cell><cell>Ours</cell></row><row><cell>SOTS</cell><cell cols="2">15.49/0.64 17.27/0.75</cell><cell>17.57/0.81</cell><cell>21.14/0.85</cell><cell>19.06/0.85</cell><cell>19.39/0.65</cell><cell>22.30/0.88</cell><cell>23.82/0.89</cell><cell>27.76/0.93</cell></row><row><cell>HazeRD</cell><cell cols="2">14.01/0.39 16.16/0.58</cell><cell>15.57/0.42</cell><cell>15.54/0.41</cell><cell>15.63/0.45</cell><cell>16.12/0.34</cell><cell>13.98/0.37</cell><cell>17.37/0.56</cell><cell>18.07/0.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results of different dehazing models on synthetic domain.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>SYN</cell><cell>25.67</cell><cell>0.8801</cell></row><row><cell>SYN+U</cell><cell>25.75</cell><cell>0.8699</cell></row><row><cell>R2S+U</cell><cell>25.91</cell><cell>0.8822</cell></row><row><cell>Ours</cell><cell>27.76</cell><cell>0.9284</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Abarghouei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Chunmei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain stylization: A strong, simple baseline for synthetic to real image domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zedlewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09384</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Benchmarking singleimage dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2766" to="2779" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring by depth guided model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single image dehazing via conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runde</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nighttime haze removal with glow and multiple light colors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Haze visibility enhancement: A survey and quantitative benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer sparse coding for robust image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optics of the atmosphere: scattering by molecules and particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccartney</surname></persName>
		</author>
		<idno>1976. 1</idno>
		<imprint>
			<date type="published" when="1976" />
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyong</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced pix2pix dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proximal dehaze-net: A prior learning-based deep network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hazerd: an outdoor scene dataset and benchmark for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
