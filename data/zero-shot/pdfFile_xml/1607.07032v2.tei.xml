<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Faster R-CNN Doing Well for Pedestrian Detection?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<email>kahe@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is Faster R-CNN Doing Well for Pedestrian Detection?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pedestrian Detection</term>
					<term>Convolutional Neural Networks</term>
					<term>Boosted Forests</term>
					<term>Hard-negative Mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN [1,2] have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN [2] for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative examples. Driven by these observations, we propose a very simple but effective baseline for pedestrian detection, using an RPN followed by boosted forests on shared, high-resolution convolutional feature maps. We comprehensively evaluate this method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting competitive accuracy and good speed. Code will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pedestrian detection, as a key component of real-world applications such as automatic driving and intelligent surveillance, has attracted special attention beyond general object detection. Despite the prevalent success of deeply learned features in computer vision, current leading pedestrian detectors (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>) are in general hybrid methods that combines traditional, hand-crafted features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and deep convolutional features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. For example, in <ref type="bibr" target="#b2">[3]</ref> a stand-alone pedestrian detector <ref type="bibr" target="#b10">[11]</ref> (that uses Squares Channel Features) is adopted as a highly selective proposer (&lt;3 regions per image), followed by R-CNN <ref type="bibr" target="#b11">[12]</ref> The corresponding author is Liang Lin.  for classification. Hand-crafted features appear to be of critical importance for state-of-the-art pedestrian detection. On the other hand, Faster R-CNN <ref type="bibr" target="#b1">[2]</ref> is a particularly successful method for general object detection. It consists of two components: a fully convolutional Region Proposal Network (RPN) for proposing candidate regions, followed by a downstream Fast R-CNN <ref type="bibr" target="#b0">[1]</ref> classifier. The Faster R-CNN system is thus a purely CNN-based method without using hand-crafted features (e.g., Selective Search <ref type="bibr" target="#b12">[13]</ref> that is based on low-level features). Despite its leading accuracy on several multi-category benchmarks, Faster R-CNN has not presented competitive results on popular pedestrian detection datasets (e.g., the Caltech set <ref type="bibr" target="#b13">[14]</ref>).</p><p>In this paper, we investigate the issues involving Faster R-CNN as a pedestrian detector. Interestingly, we find that an RPN specially tailored for pedestrian detection achieves competitive results as a stand-alone pedestrian detector. But surprisingly, the accuracy is degraded after feeding these proposals into the Fast R-CNN classifier. We argue that such unsatisfactory performance is attributed to two reasons as follows.</p><p>First, the convolutional feature maps of the Fast R-CNN classifier are of low solution for detecting small objects. Typical scenarios of pedestrian detection, such as automatic driving and intellegent surveillance, generally present pedestrian instances of small sizes (e.g., 28?70 for Caltech <ref type="bibr" target="#b13">[14]</ref>). On small objects ( <ref type="figure" target="#fig_1">Fig. 1(a)</ref>), the Region-of-Interest (RoI) pooling layer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1]</ref> performed on a low-resolution feature map (usually with a stride of 16 pixels) can lead to "plain" features caused by collapsing bins. These features are not discriminative on small regions, and thus degrade the downstream classifier. We note that this is in contrast to hand-crafted features that have finer resolutions. We address this problem by pooling features from shallower but higher-resolution layers, and by the hole algorithm (namely, "? trous" <ref type="bibr" target="#b15">[16]</ref> or filter rarefaction <ref type="bibr" target="#b16">[17]</ref>) that increases feature map size. Second, in pedestrian detection the false predictions are dominantly caused by confusions of hard background instances ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>). This is in contrast to general object detection where a main source of confusion is from multiple categories. To address hard negative examples, we adopt cascaded Boosted Forest (BF) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, which performs effective hard negative mining (bootstrapping) and sample re-weighting, to classify the RPN proposals. Unlike previous methods that use hand-crafted features to train the forest, in our method the BF reuses the deep convolutional features of RPN. This strategy not only reduces the computational cost of the classifier by sharing features, but also exploits the deeply learned features.</p><p>As such, we present a surprisingly simple but effective baseline for pedestrian detection based on RPN and BF. Our method overcomes two limitations of Faster R-CNN for pedestrian detection and gets rid of traditional hand-crafted features. We present compelling results on several benchmarks, including Caltech <ref type="bibr" target="#b13">[14]</ref>, INRIA <ref type="bibr" target="#b19">[20]</ref>, ETH <ref type="bibr" target="#b20">[21]</ref>, and KITTI <ref type="bibr" target="#b21">[22]</ref>. Remarkably, our method has substantially better localization accuracy and shows a relative improvement of 40% on the Caltech dataset under an Intersection-over-Union (IoU) threshold of 0.7 for evaluation. Meanwhile, our method has a test-time speed of 0.5 second per image, which is competitive with previous leading methods.</p><p>In addition, our paper reveals that traditional pedestrian detectors have been inherited in recent methods at least for two reasons. First, the higher resolution of hand-crafted features (such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>) and their pyramids is good for detecting small objects. Second, effective bootstrapping is performed for mining hard negative examples. These key factors, however, when appropriately handled in a deep learning system, lead to excellent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Integrate Channel Features (ICF) detector <ref type="bibr" target="#b6">[7]</ref>, which extends the Viola-Jones framework <ref type="bibr" target="#b22">[23]</ref>, is among the most popular pedestrian detectors without using deep learning features. The ICF detector involves channel feature pyramids and boosted classifiers. The feature representations of ICF have been improved in several ways, including ACF <ref type="bibr" target="#b7">[8]</ref>, LDCF <ref type="bibr" target="#b23">[24]</ref>, SCF <ref type="bibr" target="#b10">[11]</ref>, and many others, but the boosting algorithm remains a key building block for pedestrian detection.</p><p>Driven by the success of ("slow") R-CNN <ref type="bibr" target="#b11">[12]</ref> for general object detection, a recent series of methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> adopt a two-stage pipeline for pedestrian detection. In <ref type="bibr" target="#b2">[3]</ref>, the SCF pedestrian detector <ref type="bibr" target="#b10">[11]</ref> is used to propose regions, followed by an R-CNN for classification; TA-CNN <ref type="bibr" target="#b3">[4]</ref> employs the ACF detector <ref type="bibr" target="#b7">[8]</ref> to generate proposals, and trains an R-CNN-style network to jointly optimize pedestrian detection with semantic tasks; the DeepParts method <ref type="bibr" target="#b4">[5]</ref> applies the LDCF detector <ref type="bibr" target="#b23">[24]</ref> to generate proposals and learns a set of complementary parts by neural networks. We note that these proposers are stand-alone pedestrian detectors consisting of hand-crafted features and boosted classifiers.</p><p>Unlike the above R-CNN-based methods, the CompACT method <ref type="bibr" target="#b5">[6]</ref> learns boosted classifiers on top of hybrid hand-crafted and deep convolutional features. Most closely related to our work, the CCF detector <ref type="bibr" target="#b24">[25]</ref> is boosted classifiers on pyramids of deep convolutional features, but uses no region proposals. Our method has no pyramid, and is much faster and more accurate than <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our approach consists of two components (illustrated in <ref type="figure">Fig. 2</ref>): an RPN that generates candidate boxes as well as convolutional feature maps, and a Boosted Forest that classifies these proposals using these convolutional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region Proposal Network for Pedestrian Detection</head><p>The RPN in Faster R-CNN <ref type="bibr" target="#b1">[2]</ref> was developed as a class-agnostic detector (proposer) in the scenario of multi-category object detection. For single-category detection, RPN is naturally a detector for the only category concerned. We specially tailor the RPN for pedestrian detection, as introduced in the following.</p><p>We adopt anchors (reference boxes) <ref type="bibr" target="#b1">[2]</ref> of a single aspect ratio of 0.41 (width to height). This is the average aspect ratio of pedestrians as indicated in <ref type="bibr" target="#b13">[14]</ref>. This is unlike the original RPN <ref type="bibr" target="#b1">[2]</ref> that has anchors of multiple aspect ratios. Anchors of inappropriate aspect ratios are associated with few examples, so are noisy and harmful for detection accuracy. In addition, we use anchors of 9 different scales, starting from 40 pixels height with a scaling stride of 1.3?. This spans a wider range of scales than <ref type="bibr" target="#b1">[2]</ref>. The usage of multi-scale anchors waives the requirement of using feature pyramids to detect multi-scale objects.</p><p>Following <ref type="bibr" target="#b1">[2]</ref>, we adopt the VGG-16 net <ref type="bibr" target="#b9">[10]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b25">[26]</ref> as the backbone network. The RPN is built on top of the Conv5 3 layer, which is followed by an intermediate 3?3 convolutional layer and two sibling 1?1 convolutional layers for classification and bounding box regression (more details in <ref type="bibr" target="#b1">[2]</ref>). In this way, RPN regresses boxes with a stride of 16 pixels (Conv5 3). The classification layer provides confidence scores of the predicted boxes, which can be used as the initial scores of the Boosted Forest cascade that follows.</p><p>It is noteworthy that although we will use the "? trous" <ref type="bibr" target="#b15">[16]</ref> trick in the following section to increase resolution and reduce stride, we keep using the same RPN with a stride of 16 pixels. The? trous trick is only exploited when extracting features (as introduced next), but not for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>With the proposals generated by RPN, we adopt RoI pooling <ref type="bibr" target="#b0">[1]</ref> to extract fixed-length features from regions. These features will be used to train BF as introduced in the next section. Unlike Faster R-CNN which requires to feed these features into the original fully-connected (fc) layers and thus limits their dimensions, the BF classifier imposes no constraint on the dimensions of features. For example, we can extract features from RoIs on Conv3 3 (of a stride = 4 pixels) and Conv4 3 (of a stride = 8 pixels). We pool the features into a fixed resolution of 7?7. These features from different layers are simply concatenated without normalization, thanks to the flexibility of the BF classifier; on the contrast, feature normalization needs to be carefully addressed <ref type="bibr" target="#b26">[27]</ref> for deep classifiers when concatenating features.</p><p>Remarkably, as there is no constraint imposed to feature dimensions, it is flexible for us to use features of increased resolution. In particular, given the finetuned layers from RPN (stride = 4 on Conv3, 8 on Conv4, and 16 on Conv5), we can use the? trous trick <ref type="bibr" target="#b15">[16]</ref> to compute convolutional feature maps of higher resolution. For example, we can set the stride of Pool3 as 1 and dilate all Conv4 filters by 2, which reduces the stride of Conv4 from 8 to 4. Unlike previous methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref> that fine-tune the dilated filters, in our method we only use them for feature extraction, without fine-tuning a new RPN.</p><p>Though we adopt the same RoI resolution (7?7) as Faster R-CNN <ref type="bibr" target="#b1">[2]</ref>, these RoIs are on higher-resolution feature maps (e.g., Conv3 3, Conv4 3, or Conv4 3 a trous) than Fast R-CNN (Conv5 3). If an RoI's input resolution is smaller than output (i.e., &lt; 7 ? 7), the pooling bins collapse and the features become "flat" and not discriminative. This problem is alleviated in our method, as it is not constrained to use features of Conv5 3 in our downstream classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Boosted Forest</head><p>The RPN has generated the region proposals, confidence scores, and features, all of which are used to train a cascaded Boosted Forest classifier. We adopt the RealBoost algorithm <ref type="bibr" target="#b17">[18]</ref>, and mainly follow the hyper-parameters in <ref type="bibr" target="#b5">[6]</ref>. Formally, we bootstrap the training by 6 times, and the forest in each stage has {64, 128, 256, 512, 1024, 1536} trees. Initially, the training set consists of all positive examples (?50k on the Caltech set) and the same number of randomly sampled negative examples from the proposals. After each stage, additional hard negative examples (whose number is 10% of the positives, ?5k on Caltech) are mined and added into the training set. Finally, a forest of 2048 trees is trained after all bootstrapping stages. This final forest classifier is used for inference. Our implementation is based on <ref type="bibr" target="#b27">[28]</ref>.</p><p>We note that it is not necessary to handle the initial proposals equally, because our proposals have initial confidence scores computed by RPN. In other words, the RPN can be considered as the stage-0 classifier f 0 , and we set f 0 = 1 2 log s 1?s following the RealBoost form where s is the score of a proposal region (f 0 is a constant in standard boosting). The other stages are as in standard RealBoost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>We adopt single-scale training and testing as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, without using feature pyramids. An image is resized such that its shorter edge has N pixels (N =720 pixels on Caltech, 600 on INRIA, 810 on ETH, and 500 on KITTI). For RPN training, an anchor is considered as a positive example if it has an Intersectionover-Union (IoU) ratio greater than 0.5 with one ground truth box, and otherwise negative. We adopt the image-centric training scheme <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and each mini-batch consists of 1 image and 120 randomly sampled anchors for computing the loss. The ratio of positive and negative samples is 1:5 in a mini-batch. Other hyper-parameters of RPN are as in <ref type="bibr" target="#b1">[2]</ref>, and we adopt the publicly available code of <ref type="bibr" target="#b1">[2]</ref> to fine-tune the RPN. We note that in <ref type="bibr" target="#b1">[2]</ref> the cross-boundary anchors are ignored during fine-tuning, whereas in our implementation we preserve the cross-boundary negative anchors during fine-tuning, which empirically improves accuracy on these datasets.</p><p>With the fine-tuned RPN, we adopt non-maximum suppression (NMS) with a threshold of 0.7 to filter the proposal regions. Then the proposal regions are ranked by their scores. For BF training, we construct the training set by selecting the top-ranked 1000 proposals (and ground truths) of each image. The tree depth is set as 5 for the Caltech and KITTI set, and 2 for the INRIA and ETH set, which are empirically determined according to the different sizes of the data sets. At test time, we only use the top-ranked 100 proposals in an image, which are classified by the BF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We comprehensively evaluate on 4 benchmarks: Caltech <ref type="bibr" target="#b13">[14]</ref>, INRIA <ref type="bibr" target="#b19">[20]</ref>, ETH <ref type="bibr" target="#b20">[21]</ref> and KITTI <ref type="bibr" target="#b21">[22]</ref>. By default an IoU threshold of 0.5 is used for determining True Positives in these datasets.</p><p>On Caltech <ref type="bibr" target="#b13">[14]</ref>, the training data is augmented by 10 folds (42782 images) following <ref type="bibr" target="#b2">[3]</ref>. 4024 images in the standard test set are used for evaluation on the original annotations under the "reasonable" setting (pedestrians that are at least 50 pixels tall and at least 65% visible) <ref type="bibr" target="#b13">[14]</ref>. The evaluation metric is logaverage Miss Rate on False Positive Per Image (FPPI) in [10 ?2 , 10 0 ] (denoted as MR ?2 following <ref type="bibr" target="#b28">[29]</ref>, or in short MR). In addition, we also test our model on the new annotations provided by <ref type="bibr" target="#b28">[29]</ref>, which correct the errors in the original annotations. This set is denoted as "Caltech-New". The evaluation metrics in Caltech-New are MR ?2 and MR ?4 , corresponding to the log-average Miss Rate on FPPI ranges of [10 ?2 , 10 0 ] and [10 ?4 , 10 0 ], following <ref type="bibr" target="#b28">[29]</ref>. The INRIA <ref type="bibr" target="#b19">[20]</ref> and ETH <ref type="bibr" target="#b20">[21]</ref> datasets are often used for verifying the generalization capability of the models. Following the settings in <ref type="bibr" target="#b29">[30]</ref>, our model is trained on the 614 positive and 1218 negative images in the INRIA training set. The models are evaluated on the 288 testing images in INRIA and 1804 images in ETH, evaluated by MR ?2 .</p><p>The KITTI dataset <ref type="bibr" target="#b21">[22]</ref> consists of images with stereo data available. We perform training on the 7481 images of the left camera, and evaluate on the standard 7518 test images. KITTI evaluates the PASCAL-style mean Average Precision (mAP) under three difficulty levels: "Easy", "Moderate", and "Hard" 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Experiments</head><p>In this subsection, we conduct ablation experiments on the Caltech dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is RPN good for pedestrian detection?</head><p>In <ref type="figure" target="#fig_2">Fig. 3</ref> we investigate RPN in terms of proposal quality, evaluated by the recall rates under different IoU thresholds. We evaluate on average 1, 4, or 100 proposals per image 2 . <ref type="figure" target="#fig_2">Fig. 3</ref> shows that in general RPN performs better than three leading methods that are based on traditional features: SCF <ref type="bibr" target="#b10">[11]</ref>, LDCF <ref type="bibr" target="#b23">[24]</ref> and Checkerboards <ref type="bibr" target="#b30">[31]</ref>. With 100 proposals per image, our RPN achieves &gt;95% recall at an IoU of 0.7.</p><p>More importantly, RPN as a stand-alone pedestrian detector achieves an MR of 14.9% <ref type="table" target="#tab_1">(Table 1)</ref>. This result is competitive and is better than all but two stateof-the-art competitors on the Caltech dataset ( <ref type="figure">Fig. 4</ref>). We note that unlike RoI pooling that may suffer from small regions, RPN is essentially based on fixedsize sliding windows (in a fully convolutional fashion) and thus avoids collapsing bins. RPN predicts small objects by using small anchors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How important is feature resolution?</head><p>We first report the accuracy of ("slow") R-CNN <ref type="bibr" target="#b11">[12]</ref>. For fair comparisons, we fine-tune R-CNN using the VGG-16 network, and the proposals are from the same RPN as above. This method has an MR of 13.1% <ref type="table" target="#tab_1">(Table 1)</ref>, better than its proposals (stand-alone RPN, 14.9%). R-CNN crops raw pixels from images and warps to a fixed size (224?224), so suffers less from small objects. This result suggests that if reliable features (e.g., from a fine resolution of 224?224) can be extracted, the downstream classifier is able to improve the accuracy.</p><p>Surprisingly, training a Fast R-CNN classifier on the same set of RPN proposals actually degrades the results: the MR is considerably increased to 20.2% (vs. RPN's 14.9%, <ref type="table" target="#tab_1">Table 1</ref>). Even though R-CNN performs well on this task, Fast R-CNN presents a much worse result.</p><p>This problem is partially because of the low-resolution features. To show this, we train a Fast R-CNN (on the same set of RPN proposals as above) with th? a trous trick adopted on Conv5, reducing the stride from 16 pixels to 8. The problem is alleviated (16.2%, <ref type="table" target="#tab_1">Table 1</ref>), demonstrating that higher resolution can  But the BF classifier is more flexible and is able to take advantage of features of various resolutions. <ref type="table" target="#tab_2">Table 2</ref> shows the results of using different features in our method. Conv3 3 or Conv4 3 alone yields good results (12.4% and 12.6%), showing the effects of higher resolution features. Conv2 2 starts to show degradation (15.9%), which can be explained by the weaker representation of the shallower layers. BF on the concatenation of Conv3 3 and Conv4 3 features reduces the MR to 11.5%. The combination of features in this way is nearly cost-free. Moreover, unlike previous usage of skip connections <ref type="bibr" target="#b26">[27]</ref>, it is not necessary to normalize features in a decision forest classifier.</p><p>Finally, combining Conv3 3 with the? trous version of Conv4 3, we achieve the best result of 9.6% MR. We note that this is at the cost of extra computation ( <ref type="table" target="#tab_2">Table 2)</ref>, because it requires to re-compute the Conv4 features maps with the? trous trick. Nevertheless, the speed of our method is still competitive ( <ref type="table" target="#tab_7">Table 4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How important is bootstrapping?</head><p>To verify that the bootstrapping scheme in BF is of central importance (instead of the tree structure of the BF classifiers), we replace the last-stage BF classifier with a Fast R-CNN classifier. The results are in <ref type="table" target="#tab_4">Table 3</ref>. Formally, after the 6 stages of bootstrapping, the bootstrapped training set is used to train a Fast R-CNN classifier (instead of the final BF with 2048 trees). We perform this comparison using RoI features on Conv5 3 (? trous). The bootstrapped Fast R-CNN has an MR of 14.3%, which is closer to the BF counterpart of 13.7%, and better than the non-bootstrapped Fast R-CNN's 16.2%. This comparison indicates that the major improvement of BF over Fast R-CNN is because of bootstrapping, whereas the shapes of classifiers (forest vs. MLP) are less important.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-art Methods</head><p>Caltech <ref type="figure">Fig. 4 and 6</ref> show the results on Caltech. In the case of using original annotations ( <ref type="figure">Fig. 4)</ref>, our method has an MR of 9.6%, which is over 2 points better than the closest competitor (11.7% of CompactACT-Deep <ref type="bibr" target="#b5">[6]</ref>). In the case of using the corrected annotations ( <ref type="figure">Fig. 6)</ref>, our method has an MR ?2 of 7.3% and MR ?4 of 16.8%, both being 2 points better than the previous best methods. In addition, expect for CCF (MR 18.7%) <ref type="bibr" target="#b24">[25]</ref>, ours (MR 9.6%) is the only method that uses no hand-crafted features. Our results suggest that hand-crafted features are not essential for good accuracy on the Caltech dataset; rather, highresolution features and bootstrapping are the key to good accuracy, both of which are missing in the original Fast R-CNN detector. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the results on Caltech where an IoU threshold of 0.7 is used to determine True Positives (instead of 0.5 by default). With this more challeng-   <ref type="bibr" target="#b24">[25]</ref>, and that of CompactACT-Deep is reported in <ref type="bibr" target="#b5">[6]</ref>.</p><p>ing metric, most methods exhibit dramatic performance drops, e.g., the MR of CompactACT-Deep <ref type="bibr" target="#b5">[6]</ref>/DeepParts <ref type="bibr" target="#b4">[5]</ref> increase from 11.7%/11.9% to 38.1%/40.7%. Our method has an MR of 23.5%, which is a relative improvement of ?40% over the closest competitors. This comparison demonstrates that our method has a substantially better localization accuracy. It also indicates that there is much room to improve localization performance on this widely evaluated dataset. <ref type="table" target="#tab_7">Table 4</ref> compares the running time on Caltech. Our method is as fast as CompACT-Deep <ref type="bibr" target="#b5">[6]</ref>, and is much faster than CCF <ref type="bibr" target="#b24">[25]</ref> that adopts feature pyramids. Our method shares feature between RPN and BF, and achieves a good balance between speed and accuracy.</p><p>INRIA and ETH <ref type="figure" target="#fig_4">Fig. 7 and 8</ref>   KITTI <ref type="table" target="#tab_10">Table 5</ref> shows the performance comparisons on KITTI. Our method has competitive accuracy and fast speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>In this paper, we present a very simple but effective baseline that uses RPN and BF for pedestrian detection. On top of the RPN proposals and features, the BF classifier is flexible for (i) combining features of arbitrary resolutions from any layers, without being limited by the classifier structure of the pretrained network; and (ii) incorporating effective bootstrapping for mining hard negatives. These nice properties overcome two limitations of the Faster R-CNN  system for pedestrian detection. Our method is a self-contained solution and does not resort to hybrid features. Interestingly, we show that bootstrapping is a key component, even with the advance of deep neural networks. Using the same bootstrapping strategy and the same RoI features, both the tree-structured BF classifier and the regionwise MLP classifier (Fast R-CNN) are able to achieve similar results <ref type="table" target="#tab_4">(Table 3)</ref>. Concurrent with this work, an independently developed method called Online Hard Example Mining (OHEM) <ref type="bibr" target="#b31">[32]</ref> is developed for training Fast R-CNN for general object detection. It is interesting to investigate this end-to-end, online mining fashion vs. the multi-stage, cascaded bootstrapping one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1607.07032v2 [cs.CV] 27 Jul 2016 (a) Small positive instances (b) Hard negatives</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Two challenges for Fast/Faster R-CNN in pedestrian detection. (a) Small objects that may fail RoI pooling on low-resolution feature maps. (b) Hard negative examples that receive no careful attention in Fast/Faster R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of RPN and three existing methods in terms of proposal quality (recall vs. IoU) on the Caltech set, with on average 1, 4 or 100 proposals per image are evaluated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Comparisons on the Caltech set using an IoU threshold of 0.7 to determine True Positives (legends indicate MR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Comparisons on the ETH dataset (legends indicate MR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Our pipeline. RPN is used to compute candidate bounding boxes, scores, and convolutional feature maps. The candidate boxes are fed into cascaded Boosted Forests (BF) for classification, using the features pooled from the convolutional feature maps computed by RPN.</figDesc><table><row><cell>RPN</cell><cell>Boxes</cell><cell>Boosted Forest</cell></row><row><cell></cell><cell>Scores</cell><cell>?</cell></row><row><cell></cell><cell>Features</cell><cell></cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of different classifiers and features on the Caltech set. All methods are based on VGG-16 (including R-CNN). The same set of RPN proposals are used for all entries.</figDesc><table><row><cell>method</cell><cell cols="2">RoI features</cell><cell>MR (%)</cell></row><row><cell>RPN stand-alone</cell><cell>-</cell><cell></cell><cell>14.9</cell></row><row><cell>RPN + R-CNN</cell><cell cols="2">raw pixels</cell><cell>13.1</cell></row><row><cell>RPN + Fast R-CNN</cell><cell cols="2">Conv5 3</cell><cell>20.2</cell></row><row><cell cols="3">RPN + Fast R-CNN Conv5 3,? trous</cell><cell>16.2</cell></row><row><cell>RPN + BF</cell><cell cols="2">Conv5 3</cell><cell>18.2</cell></row><row><cell>RPN + BF</cell><cell cols="2">Conv4 3</cell><cell>12.6</cell></row><row><cell>RPN + BF</cell><cell cols="2">Conv5 3,? trous</cell><cell>13.7</cell></row><row><cell>RoI features</cell><cell cols="3">time/img MR (%)</cell></row><row><cell>Conv2 2</cell><cell></cell><cell>0.37s</cell><cell>15.9</cell></row><row><cell>Conv3 3</cell><cell></cell><cell>0.37s</cell><cell>12.4</cell></row><row><cell>Conv4 3</cell><cell></cell><cell>0.37s</cell><cell>12.6</cell></row><row><cell>Conv5 3</cell><cell></cell><cell>0.37s</cell><cell>18.2</cell></row><row><cell>Conv3 3, Conv4 3</cell><cell></cell><cell>0.37s</cell><cell>11.5</cell></row><row><cell cols="2">Conv3 3, Conv4 3, Conv5 3</cell><cell>0.37s</cell><cell>11.9</cell></row><row><cell cols="2">Conv3 3, (Conv4 3,? trous)</cell><cell>0.51s</cell><cell>9.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of different features in our RPN+BF method on the Caltech set. All entries are based on VGG-16 and the same set of RPN proposals.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of with/without bootstrapping on the Caltech set.</figDesc><table><row><cell>be helpful. Yet, this result still lags far behind the stand-alone RPN or R-CNN</cell></row><row><cell>(Table 1).</cell></row><row><cell>The effects of low-resolution features are also observed in our Boosted Forest</cell></row><row><cell>classifiers. BF using Conv5 3 features has an MR of 18.2% (Table 1), lower than</cell></row><row><cell>the stand-alone RPN. Using the? trous trick on Conv5 when extracting features</cell></row><row><cell>(Sec. 3.2), BF has a much better MR of 13.7%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Comparisons on the Caltech set (legends indicate MR).</figDesc><table><row><cell></cell><cell>.80 1</cell><cell></cell><cell></cell><cell>23.3% SCF+AlexNet</cell></row><row><cell></cell><cell>.64</cell><cell></cell><cell></cell><cell>22.5% Katamari</cell></row><row><cell>miss rate</cell><cell>.20 .50 .40 .30 .10</cell><cell></cell><cell></cell><cell>21.9% SpatialPooling+ 21.9% SCCPriors 20.9% TA-CNN 18.7% CCF 18.5% Checkerboards 17.3% CCF+CF 17.1% Checkerboards+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.9% DeepParts</cell></row><row><cell></cell><cell>.05</cell><cell></cell><cell></cell><cell>11.7% CompACT-Deep 9.6% RPN+BF [Ours]</cell></row><row><cell></cell><cell></cell><cell>10 -2</cell><cell>10 0</cell></row><row><cell></cell><cell></cell><cell cols="2">false positives per image</cell></row><row><cell>miss rate</cell><cell>.10 .20 .30 .40 .50 .64 .80 1 .05</cell><cell cols="2">10 -2 Fig. 4: false positives per image 10 0</cell><cell>68.6% CCF 49.0% Katamari 48.9% LDCF 48.6% SCCPriors 47.2% SCF+AlexNet 46.8% TA-CNN 46.7% SpatialPooling+ 41.8% Checkerboards 41.3% Checkerboards+ 40.7% DeepParts 38.1% CompACT-Deep 23.5% RPN+BF [Ours]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Comparisons on the Caltech-New set (legends indicate MR ?2 (MR ?4 )).</figDesc><table><row><cell></cell><cell>.64 1 .80</cell><cell></cell><cell>23.7(42.6)% CCF 23.7(38.3)% LDCF</cell></row><row><cell></cell><cell>.50</cell><cell></cell><cell cols="2">22.3(42.0)% CCF+CF</cell></row><row><cell></cell><cell>.40</cell><cell></cell><cell cols="2">22.2(34.6)% Katamari</cell></row><row><cell>miss rate</cell><cell>.20 .30 .10</cell><cell></cell><cell cols="2">21.6(34.6)% SCF+AlexNet 21.6(36.0)% SpatialPooling+ 19.2(34.0)% SCCPriors 18.8(34.3)% TA-CNN 16.3(28.7)% Checkerboards+ 15.8(28.6)% Checkerboards</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">12.9(25.2)% DeepParts</cell></row><row><cell></cell><cell>.05</cell><cell></cell><cell cols="2">9.2(18.8)% CompACT-Deep</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">7.3(16.8)% RPN+BF [Ours]</cell></row><row><cell></cell><cell>10 -2</cell><cell>10 0</cell><cell></cell></row><row><cell></cell><cell cols="2">false positives per image</cell><cell></cell></row><row><cell cols="2">Fig. 6: method</cell><cell>hardware</cell><cell cols="2">time/img (s) MR (%)</cell></row><row><cell></cell><cell>LDCF [24]</cell><cell>CPU</cell><cell>0.6</cell><cell>24.8</cell></row><row><cell></cell><cell>CCF [25]</cell><cell>Titan Z GPU</cell><cell>13</cell><cell>17.3</cell></row><row><cell></cell><cell cols="2">CompACT-Deep [6] Tesla K40 GPU</cell><cell>0.5</cell><cell>11.7</cell></row><row><cell></cell><cell>RPN+BF [ours]</cell><cell>Tesla K40 GPU</cell><cell>0.5</cell><cell>9.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of running time on the Caltech set. The time of LDCF and CCF is reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>show the results on the INRIA and ETH datasets. On the INRIA set, our method achieves an MR of 6.9%, considerably better than the best available competitor's 11.2%. On the ETH set, our result (30.2%) is better than the previous leading method (TA-CNN [4]) by 5 points. Comparisons on the INRIA dataset (legends indicate MR).</figDesc><table><row><cell></cell><cell cols="2">.80 1</cell><cell>16.0% VeryFast</cell></row><row><cell></cell><cell cols="2">.64</cell><cell>16.0% WordChannels</cell></row><row><cell>miss rate</cell><cell cols="2">.20 .50 .40 .30</cell><cell>15.4% RandForest 15.1% NAMC 14.5% SCCPriors 14.4% InformedHaar 13.8% LDCF 13.7% Franken</cell></row><row><cell></cell><cell cols="2">.10</cell><cell>13.5% Roerei</cell></row><row><cell></cell><cell></cell><cell></cell><cell>13.3% SketchTokens</cell></row><row><cell></cell><cell cols="2">.05</cell><cell>11.2% SpatialPooling 6.9% RPN+BF [Ours]</cell></row><row><cell></cell><cell></cell><cell>10 -2</cell><cell>10 0</cell></row><row><cell></cell><cell></cell><cell cols="2">false positives per image</cell></row><row><cell cols="2">miss rate</cell><cell cols="2">10 -2 Fig. 7: false positives per image 10 0 .05 .10 .20 .30 .40 .50 .64 .80 1</cell><cell>89.9% VJ 64.2% HOG 49.4% MLS 47.3% MF+Motion+2Ped 47.0% DBN-Isol 45.3% JointDeep 45.0% RandForest 45.0% LDCF 44.8% FisherBoost 43.5% Roerei 41.1% DBN-Mut 40.0% Franken 37.4% SpatialPooling 35.0% TA-CNN 30.2% RPN+BF [Ours]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on the KITTI dataset collected at the time of submission (Feb 2016). The timing records are collected from the KITTI leaderboard. ? : region proposal running time ignored (estimated 2s).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cvlibs.net/datasets/kitti/eval object.php 2 To be precise, "on average k proposals per image" means that for a dataset with M images, the top-ranked kM proposals are taken to evaluate the recall.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported in part by State Key Development Program under Grant 2016YFB1001000, in part by Guangdong Natural Science Foundation under Grant S2013050014548. This work was also supported by Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase). We thank the anonymous reviewers for their constructive comments on improving this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning complexityaware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quickly boosting decision trees-pruning underachieving features early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Piotr&apos;s Computer Vision Matlab Toolbox (PMT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="https://github.com/pdollar/toolbox" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakrapee</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
