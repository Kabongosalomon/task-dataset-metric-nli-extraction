<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Looking At The Body: Automatic Analysis of Body Gestures and Self-Adaptors in Psychological Distress</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Looking At The Body: Automatic Analysis of Body Gestures and Self-Adaptors in Psychological Distress</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-adaptors</term>
					<term>fidgeting</term>
					<term>psychological distress</term>
					<term>digital phenotyping</term>
					<term>behavioural sensing !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Psychological distress is a significant and growing issue in society. Automatic detection, assessment, and analysis of such distress is an active area of research. Compared to modalities such as face, head, and vocal, research investigating the use of the body modality for these tasks is relatively sparse. This is, in part, due to the limited available datasets and difficulty in automatically extracting useful body features. Recent advances in pose estimation and deep learning have enabled new approaches to this modality and domain. To enable this research, we have collected and analyzed a new dataset containing full body videos for short interviews and self-reported distress labels. We propose a novel method to automatically detect self-adaptors and fidgeting, a subset of self-adaptors that has been shown to be correlated with psychological distress. We perform analysis on statistical body gestures and fidgeting features to explore how distress levels affect participants' behaviors. We then propose a multi-modal approach that combines different feature representations using Multi-modal Deep Denoising Auto-Encoders and Improved Fisher Vector Encoding. We demonstrate that our proposed model, combining audio-visual features with automatically detected fidgeting behavioral cues, can successfully predict distress levels in a dataset labeled with self-reported anxiety and depression levels. . Her research focuses on computer vision and machine learning within the context of affective computing, behaviour analytics and human behaviour understanding. She is particularly interested in building inference models that tackle challenging realworld problems, usually characterised by data scarcity and noisy signals from multiple modalities. She applies her research in the areas of automotive applications, healthcare, and animal welfare.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P Sychological distress and mental disorders are significant threats to global health <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr" target="#b0">1</ref> According to the World Health Organization (WHO), an estimated 450 million people around the world suffer from neuropsychiatric conditions <ref type="bibr" target="#b2">[3]</ref>, with depression and anxiety being the most common mental disorders <ref type="bibr" target="#b3">[4]</ref>. Despite existing strategies for the treatment of distress, such as depression, it is estimated that nearly two-thirds of people suffering distress have never received help from a health professional <ref type="bibr" target="#b4">[5]</ref>. Early detection of distress is consistently noted as a key factor in treatment and positive outcomes. Early detection requires an ongoing assessment to identify distress when it begins. Self-evidently, ongoing assessment at scale is prohibitive when performed manually. As such, automatic detection of signs of psychological distress or specific mental disorders is an active area of research.</p><p>Currently, the most effective automated distress detection approaches utilize multi-modal machine learning. These modalities include facial, head, eye, linguistic (textual), vocal, and body.</p><p>There are significant challenges to body modality research, particularly within automatic distress detection, in-1. This work is an extension of the work in <ref type="bibr" target="#b1">[2]</ref>, originally published in the proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020 cluding the lack of relevant data, the inability to share much of the data, and the difficulty in gathering such data. Specifically, the combination of full-body data (either sensor-based or video-based) with psychological distress labels is rare. Compounding this rarity is the private and sensitive nature of the data, which means such datasets are rarely shared publicly.</p><p>Body expressions, and especially self-adaptors, have been shown to be correlated with human affect, depression and psychological distress <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Self-adaptors are self-comforting gestures, including any kind of touching on other parts of the body, either dynamically or statically <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Fidgeting, a subset of self-adaptors, is the act of moving about restlessly, playing with one's fingers, hair, or personal objects in a way that is not peripheral or nonessential to ongoing tasks or events <ref type="bibr" target="#b12">[13]</ref>. Patients with depression often engage in self-adaptors <ref type="bibr" target="#b13">[14]</ref>. Fidgeting has been seen and reported in both anxiety and depression <ref type="bibr" target="#b11">[12]</ref>; It is a sign of attention-deficit and hyperactivity disorder, also exhibited by individuals with autism <ref type="bibr" target="#b14">[15]</ref>. With manually annotated data, Scherer et al. <ref type="bibr" target="#b15">[16]</ref> reported a longer average duration of self-adaptors as well as fidgeting for distressed participants.</p><p>More recent advances in the state-of-the-art for pose estimation <ref type="bibr" target="#b16">[17]</ref> enable accurate pose data on a broader set of datasets and thus open the door for new approaches for body expression analysis and broader incorporation of body features in multi-modal systems.</p><p>In this paper, we propose to use a hierarchical model to automatically detect self-adaptors as well as fidgeting, which has been shown to be predictive of psychological distress. We analyzed body gestures and self adaptors in a arXiv:2007.15815v1 [cs.CV] 31 Jul 2020 dataset of video recordings that we collected, concentrating on symptoms of depression and anxiety because these are the most common mental disorders <ref type="bibr" target="#b3">[4]</ref>. We then present two methods to explore the body modality (especially fidgeting): First with a statistical linearity analysis with traditional linear regression, and second with a deep-learning-based pipeline. In the second method, a Multi-modal Deep Denoising Auto-Encoder (multi-DDAE) is utilized for encoding per-frame features. Improved Fisher Vector encoding <ref type="bibr" target="#b17">[18]</ref> is then used to generate per-sample representation. Finally, we demonstrate that these features are discriminative in psychological distress detection.</p><p>The contributions of this paper can be summarized as follows:</p><p>1) We introduce a new audio-visual dataset containing recordings of non-clinical interviews along with distress labels from established psychological evaluation questionnaires.</p><p>2) We propose a hierarchical model for automatic detection of self-adaptors (including fidgeting) from visual data and evaluate our approach on a publicly available fidgeting dataset with manual labels.</p><p>3) We present a statistical analysis of a set of statistical body gesture features as well as specific fidgeting features extracted from the body modality data and explore how distress levels affect participants' behavior in our dataset. 4) As proof of concept, we implement a multi-modal feature fusion framework to perform distress classification and demonstrate the importance of self-adaptors features, specifically fidgeting, in predicting symptoms of depression and anxiety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we focus on related work on automatic detection of signs of psychological distress, including studies that focus on separate modalities and multi-modal fusion frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Facial and head modality</head><p>Facial Action Coding System (FACS) <ref type="bibr" target="#b18">[19]</ref> has long been used to taxonomize human facial movements by their appearance on the face, which yields the concept of Facial Action Units (AUs). For example, the Audio/Visual Emotion Challenge (thereafter AVEC) used AUs features as a basic descriptor for its psychological distress detection tasks.</p><p>A big body of literature has been developed to analyze facial expressions and the head modalities in the context of depression and psychological distress. For example, Yang et al. <ref type="bibr" target="#b19">[20]</ref> proposed a "Histogram of Displacement Range (HDR)", which is a measurement of the amount of facial landmark movements to predict depression. Joshi et al. <ref type="bibr" target="#b20">[21]</ref> presented a categorization analysis framework which consists of "bag of facial dynamics" and "histogram of head movements". Dibeklioglu et al. <ref type="bibr" target="#b21">[22]</ref> [23] featureengineered dynamic representation (e.g., velocity, acceleration, and standard deviation of motion) for facial landmark movement and head motion and used them in a multimodal system to detect depression in a dataset of clinical interviews.</p><p>Psychomotor retardation refers to a slowing-down of thought and a reduction of physical movements in an individual. Sobin et al. <ref type="bibr" target="#b23">[24]</ref> demonstrated the correlation between psychomotor retardation and depression. Syed et al. <ref type="bibr" target="#b24">[25]</ref> handcrafted descriptors using craniofacial movements in order to capture the psychomotor retardation, and then made predictions of depression.</p><p>Some other features such as lower emotional expressivity <ref type="bibr" target="#b25">[26]</ref>, eye lid movement <ref type="bibr" target="#b24">[25]</ref>, reduced gaze activity <ref type="bibr" target="#b26">[27]</ref> [28], and averted gaze <ref type="bibr" target="#b25">[26]</ref> have been also used as predictive features of depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Audio modality</head><p>Acoustic features of speech can be predictive of distress irrespective of the speech content <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. For example, Ozdas et al. <ref type="bibr" target="#b28">[29]</ref> assessed the risk of suicide by detecting the fluctuations in the fundamental frequency of people's speech. Dibeklioglu et al. <ref type="bibr" target="#b21">[22]</ref> explored the use of vocal prosody for depression detection. Similarly, Syed et al. <ref type="bibr" target="#b24">[25]</ref> investigated the use of turbulence in speech patterns.</p><p>Besides, in AVEC challenges, low-level descriptors of voice signals, such as Mel-frequency Cepstral Coefficients (MFCCs), are provided, leading to many multi-modal methods incorporating these acoustic features for distress and mental illness detection <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Body modality</head><p>A few previous studies attempted to include the body modality in their models to predict psychological distress, mostly by extracting generic features from the video recordings related to the body. For example, Joshi et al. <ref type="bibr" target="#b20">[21]</ref> computed Histogram of Gradients (HOGs) and Histogram of Optical Flow (HOFs) around the generic Space-Time Interest Points (STIPs) extracted from the videos, and then generated a "Bag of Body Dynamics" feature that was used for depression classification. Some of the multi-modal work presented in the AVEC challenges <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> utilize the low-level descriptors of visual signals (such as latent CNN layer activation of ResNet <ref type="bibr" target="#b34">[35]</ref> and VGGNet <ref type="bibr" target="#b35">[36]</ref>) to predict on psychological distress.</p><p>More recent works also investigate the specific movement of body parts. In the past few years, the skeletal models, either using RGB such as OpenPose <ref type="bibr" target="#b16">[17]</ref> or RGBD such as Microsoft Kinect SDK skeleton tracker 2 , have gained popularity for action recognition tasks and were used to generate more specific and concrete features by feature engineering <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b36">[37]</ref>. For example, Jaiswal et al. <ref type="bibr" target="#b36">[37]</ref> extracted head movements using Kinect and performed multi-modal classification with other audiovisual features to predict ADHD and ASD. Though promising, the related work using such skeletal models on detecting psychological distress is still sparse.</p><p>In terms of automatic detection of self-adaptors, the only previous work that attempted to detect fidgeting behavior was presented by Mahmoud et al. <ref type="bibr" target="#b10">[11]</ref>. They developed a multi-modal framework for automatic detection of descriptors of rhythmic body movement by extracting Speeded-Up Robust Features (SURFs) interest points around Microsoft 2. https://developer.microsoft.com/en-us/windows/kinect/ Kinect pose points and then detected rhythmic behaviors from analyzing the trajectories of the interest points. However, there are two limitations in their proposed automated system when applied to distress detection: 1) Their dataset they used was based on acted data, so the behavior detected is not natural. For example, in more real interview scenarios, participants do not always fidget with a rhythmic pattern.</p><p>2) The trajectory data was noisy, and their method could not sufficiently handle the complexity of the detected body signal. As such, they were only able to achieve 59% recognition on their acted dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-modal Learning</head><p>Since psychological distress is expressed through all modalities, many of the state-of-the-art models that predict signs of psychological distress proposed multi-modal approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, combining low-level features extracted from the face, speech, and text, which are usually the features publicly available for the datasets. By only working with extracted features, most of these works focused on exploiting the given features, instead of analyzing the behavioral cues (e.g., specific gestures) of psychological distress. For example, the winner of AVEC 2019 <ref type="bibr" target="#b32">[33]</ref> proposed multi-layer attention fusion frameworks, but they did not explore the psychological basis of their models' decisions due to the lack of access to the raw data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>In this section, we describe the data collection, experimental design, and general characteristics of our collected dataset. This dataset is designed to enable investigation of the body modality for use in automatic detection of distress.</p><p>Currently, the corpus is not publicly available due to the sensitivity of the collected video. Longer-term, we intend to make some portions (such as the features) of the data more broadly available to the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview and design</head><p>Participants were recruited through the University of Cambridge email lists, student social media groups, and paper fliers posted around the town. We aimed to balance the sample with regards to distress levels, such that the database includes participants at the two distinct ends of the distress spectrum. To identify participants with high versus low levels of distress, we conducted an online screening with a total 106 people who signed up for the study. Participants completed standardized measures of depression (PHQ-8 <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and anxiety (GAD-7 <ref type="bibr" target="#b41">[42]</ref>), as well as demographics. In the selection, we balanced the participants according to the public norm shown in <ref type="table" target="#tab_2">Table 2</ref> (e.g., For depression, above 6.63 is marked as high, otherwise low). Given potential gender differences in nonverbal communication <ref type="bibr" target="#b42">[43]</ref>, we also balanced the final sample with regards to gender within each distress group <ref type="bibr" target="#b2">3</ref> . From the initial screening, 35 were 3. Non-binary/other was given as an option in the registration form. A number of people registered with this option. However, none of those people met the distress level criteria and were thus not selected for an interview.</p><p>invited to the face to face session, including 18 with high distress and 17 with low distress.</p><p>The participants completed the same measures of depression and anxiety immediately before the interview. This was meant to provide an assessment of distress closer in time to the interview and to increase the psychological salience of this information during the interview. We adopted a data collection methodology inspired by the DAIC dataset collection method <ref type="bibr" target="#b43">[44]</ref>, which consists of a human interviewer asking a series of open-ended conversational questions to elicit naturalistic behavior. The interviews were performed by a computer science researcher based on peer-support interview questions collected from the university support services. To achieve the conversational interview dynamic the interviewer asks general questions regarding the participant's life and further encourages the participant to elaborate. For example, the interviewer would ask "can you tell me about one time in your life you were particularly happy?" and then ask some follow up questions regarding the example the participant provided. The interviewer was blind to the distress level of participants during the interview.</p><p>To keep behaviors naturalistic, participants were not aware of the main goal of the study, which is an automatic analysis of behavioral cues. Instead, they were told that the experiment aimed at building models that can help in mental well-being. This ensured that their behavior would be as natural as possible. All participants got debriefed of the main aim of the data collection at the end of the session. Participants were not informed of the results of their questionnaires, and all of them were handed a small booklet with the list of peer support and mental well-being services provided by the university. It is worth mentioning that the interviewer was blind to whether participants were from high or low distress groups in order not to affect their behavior. They were also instructed to limit their body and facial expressions throughout the interview and keep their sitting posture constant through all the interviews in order to avoid any changes in participants' behavior due to mimicry effect <ref type="bibr" target="#b44">[45]</ref>.</p><p>The dataset is labeled with participant responses to selfevaluation questionnaires right before the interview for assessing distress and personality traits, as well as demographic labels such as gender. The distress questionnaires include the PHQ-8 for depression, GAD-7 for anxiety, SSS-8 <ref type="bibr" target="#b45">[46]</ref> for somatic symptoms, and the PSS <ref type="bibr" target="#b46">[47]</ref> for perceived stress. Personality traits are measured using the Big Five Inventory <ref type="bibr" target="#b47">[48]</ref>. In sum, each participant provided responses to 5 questionnaires, in which PHQ-8 and GAD-7 were measured twice, both at registration and before the faceto-face session.</p><p>As a result, the dataset includes videos of fully natural non-acted expressions, including facial expressions, body motion, gestures, and speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary Analysis</head><p>We collected videos of 35 interviewed participants with a total video duration of 07:50:08. General statistics regarding the questionnaire and demographic results within the dataset are provided in <ref type="table">Table 1</ref> normalized covariance values, also known as the correlation coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confounding Correlations</head><p>We assessed confounding correlations based on the depression label, as much of the related work focuses on depression. While the distress measures, anxiety, perceived stress, and somatic stress, were found to be strongly correlated with depression, the personality measures have below 50% covariance with the exception of neuroticism, which is a trait characterized by negative emotionality, with an 80% covariance. The demographic measures, gender, and age were negligibly correlated, with 9.47% and -11.09% covariance, respectively. Finally, the interview duration was found to be not correlated with any questionnaire result (less than 25% covariance with all labels). Thus, we can be confident that there are no confounding correlations with personality scores or demographics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published Norms</head><p>A comparison of the mean values for distress and personality measures between our dataset and the published norms is presented in <ref type="table" target="#tab_2">Table 2</ref>. While there are differences, the measures are generally in line with the published norms. The dataset has a substantially higher mean perceived stress score, but only slightly higher mean scores for anxiety and depression. Depression, extraversion, and neuroticism measures are particularly close to their published norms. While the dataset mean for agreeableness and openness are substantially higher than the published norms (over 10% over the technical range for those measures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Remarks</head><p>Participants completed the PHQ-8 and GAD-7 questionnaires twice: during registration and with the interview process. These questionnaires are temporal; specifically, they relate to the participant's mental state in the past two  weeks. Given this, some difference between registration and interview results was expected.</p><p>With the exception of a small number of outliers, participants were generally consistent in self-evaluation between registration and interview. PHQ-8 responses had a mean difference of 0.89, while GAD-7 responses had a mean difference of 0.63. As a result, we took the most recent response to self-evaluation questionnaires as the label for each participant's video recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>We used our collected dataset to study body gestures and self-adaptors. In this section, we demonstrate two different methods to analyze the body modality within the context of psychological distress. As a first step, we extract the most common audio-visual features. Then we describe a set of generic statistical body features that we extract to analyze general body gesture movement. To look specifically for self-adaptors, we then present an automatic approach to extract self-adaptors and fidgeting behavior in our dataset. We then perform a feature-based statistical analysis on the extracted body features -both generic and fidgeting features to understand what features are generally correlated with distress classification. Lastly, we move on to propose a multimodal approach to demonstrate further the effectiveness of body modality, where we incorporate and analyze the co-occurrence of multiple modalities to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Audiovisual Feature Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Visual Features</head><p>For each video, we used state-of-the-art tools, OpenPose <ref type="bibr" target="#b16">[17]</ref> and OpenFace 2.2 <ref type="bibr" target="#b50">[51]</ref>, to extract body pose features, facial Action Units (AUs), and gaze directions.</p><p>However, OpenPose and OpenFace do not take into account the consistency of the keypoints across time, causing the keypoints to usually fluctuate highly in many parts, introducing noise to the real continuous face and body motion. Besides, there are some frames where OpenPose or OpenFace fail to extract all pose points or gaze features, respectively. To overcome these problems, we infer the missing data via Cubic Spline Interpolation across the whole sequence. We then smooth the data using a Savitzky-Golay filter <ref type="bibr" target="#b51">[52]</ref> (window length is 11 and the order of the polynomial is 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Audio Features</head><p>Speaker diarization involves partitioning an audio stream into homogeneous segments according to the speaker's identity. In order to distinguish the speech of the interviewer and the participant, we use the open-source Speaker-Diarization project <ref type="bibr" target="#b52">[53]</ref> which utilizes an Unbounded Interleaved-State Recurrent Neural Network (UIS-RNN) <ref type="bibr" target="#b53">[54]</ref>, to extract speaker identities with respect to the time axis. We then conduct a manual check to assign correct diarization labels to the participant and the interviewer. We also use pyAudioAnalysis <ref type="bibr" target="#b54">[55]</ref> to extract MFCCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generic Body Features</head><p>To explore the body modality, we extract and analyze the set of generic statistical features that describe the body movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Feature Extraction</head><p>Two kinds of statistical features are computed and extracted: global features and localized features. In the global features, we care about the overall statistics of motion, while in the localized features (features that are within specific body parts, such as head, hands, and legs), we are interested in the statistics of the motion within the body parts, which we refer to as "localization". Our notation is summarized in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>We define a "gesture" as a period of sustained movement within a body localization. For example, waving hands is a gesture within "Hn (hand)" localization, and shaking legs continuously will register a gesture in "L (Legs)" localization.</p><p>To detect gestures within a localization, we scan the video using a moving window method.</p><p>First, the per-frame absolute movement (L 2 distance) is calculated for each pose point. The value is then averaged by the number of pose points in the localization. Formally,</p><formula xml:id="formula_0">F t = 1 |P | p?P ||P p,t ? P p,t?1 || 2<label>(1)</label></formula><p>where P p,t is the position vector of pose point p at time t, and F t is the averaged per-frame movement across all points. P are the collection of pose points in this localization. Second, a moving window is applied such that a small number of frames do not have a disproportionate effect on the detection. This process can be expressed by:</p><formula xml:id="formula_1">W i = 1 l t&lt;i?(l+1) t=i?l F t<label>(2)</label></formula><p>where W i is the windowed average at window index i, l is the length of the window, and F t is the average movement at frame t, from Equation 1. We experimentally chose l = 10, i.e. a second of movement is represented by 3 windows.  Third, the window moves until an average movement above a threshold is found, which is considered the beginning of the gesture. The gesture continues until n = 3 consecutive windows (30 frames, approximately 1 sec) are found below the movement threshold, which is thus considered the end of the gesture. <ref type="table" target="#tab_4">Table 3</ref> lists the set of body features we extract. Below we explain how we define each of these features for the overall body. Similarly, the localized features can be calculated for every localization/body part.</p><p>? Average frame movement -the per-frame average movement (moving distance) of every pose point of the body. This is the only feature that is not based on detected gestures. ? Proportion of total movement occurring during a gesture -the proportion of total movement that occurred while a gesture is happening (within some localizations). ? Average gesture surprise -defined as "fraction of frames with no gesture happening" ? "number of gestures". For example, if two gestures occurred within a sample such that 80% of the sample duration had no gesture occurring, the average gesture surprise would be 80% 2 = 40%. Whereas, if there were 100 gestures, the average surprise is 0.8%, even though both samples had the same proportion without any gesture occurring. This matches the intuition that each gesture within 100 evenly spaced gestures would be unsurprising as they were regularly occurring, whereas the 2 evenly spaced gestures would be surprising because nothing was happening in between.</p><p>? Average gesture movement standard deviation -the standard deviation of per-frame movement within a gesture is averaged across all detected gestures. This is intended to indicate the consistency of movement intensity through a gesture. ? Number of gestures -total number of detected gestures across all tracked localizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Feature Processing</head><p>All the movement data is extracted from smoothed Open-Pose data described in Section 4.1.1. All these body gesture features are concatenated (thereafter marked as BodyGesture, which has a feature vector of length 20 for each participant) and all features are normalized such that the length of the sample does not affect the results. Sum-based features (e.g., gesture length, gesture count, total movement, etc.) are normalized against the total number of frames in the sample. Gesture average features, such as gesture surprise, are again normalized against the total number of gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Self-adaptors and Fidgeting features</head><p>In addition to the generic body features, we were interested in analyzing the self-adaptors and fidgeting behavior. In this section, we present our fidgeting detection system in three subsections. We start by exploring the selfadaptors/fidgeting encoding and the overall hierarchical design. Then we show the methods of building the two essential detectors of our hierarchical model in the following two subsections. For each detector, we demonstrate the detector's design, and then present the labeling strategy which provides reliable labels for training and evaluation. In order to validate the effectiveness of our automated fidgeting detection approach before moving onto distress classification, we evaluate our model thoroughly both on an acted dataset and on our newly collected dataset of natural expressions. Given the lack of broad agreement on the definition of fidgeting so far, we utilize a two-step hierarchical model to identify fidgeting. As shown in <ref type="table" target="#tab_5">Table 4</ref>, we first identify selfadaptors, which we define as low-level location events (e.g. H2H, H2F). Secondly, action events (i.e. DYNAMIC, STATIC) of hand/leg are classified by the DYNAMIC/STATIC Classifier. Fidgeting is then defined as a combination of lowlevel self-adaptors and action events. Specifically, we define three types of fidgeting: cross hand fidgeting, single-hand fidgeting, and leg/feet fidgeting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Overall Design and Encoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-adaptors Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2H</head><p>Hand to Hand H2A</p><p>Hand to Arm H2L</p><p>Hand to Leg H2F</p><p>Hand to Face HF Hand Free (when not belong to any of above) L2G</p><p>Both Legs on Ground L2L</p><p>Leg on the other Leg (crossed legs)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Events Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DYNAMIC Moving obviously STATIC</head><p>No obvious movement is observed  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.1">Design:</head><p>Each body location is represented using a bounding box. Self-adaptors are defined as overlapping bounding boxes. We represent the hand and face using the smallest rectangular box bounding all corresponding hand or face keypoints. The forearms, upper arms, lower legs, and upper legs'bounding boxes' long sides are aligned with the connection between two joints from OpenPose, while the width is a free parameter tuned for the best automatic detection performance.</p><p>First, H2H self-adaptor events are detected (i.e., when the two hands' bounding boxes overlap). Then all other handbased self-adaptor events are detected, for all segments of the video not containing H2H segments.</p><p>All self-adaptors, except for H2F, must be longer than 100 frames (around 4 seconds with the frame rate of 26). This reduces noise from detected self-adaptor events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.2">Labeling and Evaluation:</head><p>In order to validate our self-adaptor detector, we manually labeled 4 participants' videos, a total duration of 59 minutes. The interlabeler agreement was checked using Krippendorff's alpha. Each frame was labeled with one of the self-adaptor codes from <ref type="table" target="#tab_5">Table 4</ref>. Within these videos, participants perform different self-adaptors and each event has a minimum total duration of 5 minutes, with the exception of H2F. <ref type="table" target="#tab_7">Table 5</ref>, the Krippendorffs alpha agreement for left-hand location is 0.823, for right-hand location is 0.888 and for leg location is 1.00. This suggests good agreement between the annotators and, thus, the reliability of the labels. The results show that our network is able to detect self-adaptor with excellent overall precision, and especially for the H2H, H2F, L2L and L2G events, the detector reached a very high accuracy respectively. Note that, 'NA' in <ref type="table" target="#tab_7">Table  5</ref> means that there is no corresponding gestures in the evaluation set of 4 labelled participants.   <ref type="figure" target="#fig_0">Fig. 1</ref>, the DY-NAMIC/STATIC Classifier operates on extracted optical flow from a sliding window across the video (size 100 frames, step 50 frames). To classify the action (DYNAMIC/STATIC), hand movements (especially fingers) and leg movements require optical flow to obtain smooth trajectories, given OpenPose estimations become unreliable when hands intersect or are occluded. We thus initialize the optical flow with the OpenPose estimations at the beginning of each slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>We choose Fast Fourier Transform (FFT), standard deviation (STD), and mean values (MEAN) of point trajectories as our input features (in this case, number of trajectories is 2 ? number of keypoints as we have 2-D data for each keypoint). For fidgeting, we are more interested in the cyclic motion with a frequency ranging from 0.5Hz to 2.5Hz <ref type="bibr" target="#b10">[11]</ref>. Therefore, we extracted the spectrum data within the range [0.5, 2.5] Hz. As we analyze slices of length 100, the dimension of FFT spectrum data that is within [0.5, 2.5] Hz is always fixed at 41? number of trajectories. We then average over the FFT values that have the same frequency to produce an FFT feature of length 41. As for the STD and MEAN features, we simply calculate along the time axis and give a vector with a length of the number of trajectories for each feature. 4.3.3.2 Labeling and Evaluation: To train and evaluate the DYNAMIC/STATIC Classifiers, accurate labeling is required. Three classifiers are required to cover the three categories of detected self-adaptors: {H2H}, {H2A, H2L, H2F, H2F}, and {L2G, L2L}.</p><p>We labelled DYNAMIC/STATIC on each of the three categories. We randomly sampled and labeled approximately 30% of slices for each category in every video.</p><p>Two researchers labeled the data independently. As shown in <ref type="table" target="#tab_9">Table 6</ref>, we first manually dropped the slices with a wrong category label (e.g. a slice is detected as H2H while it's in fact not). The number of slices that have a correct category label is shown as "Correct". Secondly, we labeled DYNAMIC/STATIC and dropped the slices that lack a consensus between two researchers. The number of slices with an agreement is shown as "Agreed". The high percentage of both "Correct" and "Agreed" suggests the good performance of our self-adaptor detection and also the high reliability of action labels.  As shown in <ref type="table">Table 7</ref>, the detector achieved generally high accuracy and F1 score with low standard deviations. Though the hand actions are difficult even for researchers to label, the detector can successfully classify more than 80% of slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Feature encoding</head><p>This section describes how we encoded low-level framelevel features described in Sec 4.1 and 4.3 in preparation for the final prediction step. The generic statistical BodyGesture will not need to be encoded since it represents global statistical features rather than time-series features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Fidgeting features processing</head><p>Having extracted low-level features from each frame, we combine them to form high-level descriptors of fidgeting behavior (CHF, CHF, and LFF as shown in <ref type="table" target="#tab_5">Table 4</ref>). The Fidget_pure feature group is formed by {HCF, SHF-L(left hand), SHF-L(right hand), SHF-A(left hand), SHF-A(right hand), SHF-F(left hand), SHF-F(right hand), LFF}. The Fidget_pure group is combined with a participant speaking feature array to form the full fidget feature group, enabling us to investigate whether fidgeting and speaking co-occurrence is relevant. This participant speaking feature array indicates whether the participant is speaking during a frame. This is calculated using the previously described diarization data.</p><p>After all the feature extraction, we have several feature groups shown in <ref type="table" target="#tab_11">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Per-frame representation</head><p>In order to capture more useful feature representations and reduce the dimensionality, and inspired by our previous work <ref type="bibr" target="#b55">[56]</ref>, different modalities are combined using a Multimodal Deep Denoising Auto-Encoder (multi-DDAE). As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, each modality is encoded through a dense layer and then all are concatenated to yield the last shared dense layer which provides the representation we use. The shared layer is then inversely decoded to generate each   modality. We optimized the hyper-parameters of the autoencoder via several experiments so that the dimensions of hidden layers are {0.5d, 0.25d, 0.5d} where d represents the input dimension of each node, and the noise applied at the input is 0.1 Gaussian noise. The training optimization target is the joint Mean Square Error (MSE) of the MSEs of the feature group at each node (later we fixed the loss weights to be 0.35 for the fidget feature group while 0.1 for others, as we are more interested in fidgeting in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Group Dimension Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Whole video representation</head><p>Due to varying lengths of the videos, it's necessary to unify the dimensionality of the per-video representation. Though Fisher Vector was originally proposed to aggregate visual features <ref type="bibr" target="#b17">[18]</ref>, it has become popular in social signal processing such as bipolar disorder <ref type="bibr" target="#b56">[57]</ref> and depression recognition <ref type="bibr" target="#b57">[58]</ref>. Inspired by these applications, we apply a Gaussian Mixture Model to cluster similar per-frame representations and then use an Improved Fisher Vector encoding to obtain a fixed-length representation. As a result, the feature is transformed from num_frames ? feature_dim to 2 ? GMM_Kernel_num ? feature_dim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Classification of signs of distress</head><p>We apply a Random Forest to select important features from the per-video representation. The selected features are used by the classifier. We experiment with two classifiers: 1) a logistic regression-based classifier (LR) using a binary threshold of 0.5; 2) a Multi-Layer Perception (MLP) with two softmax outputs for binary classification.</p><p>As the available samples are limited and the useful features vary across individual differences, label smoothing <ref type="bibr" target="#b58">[59]</ref> is applied to the MLP model in order to further boost the performance. More formally:</p><formula xml:id="formula_2">L new = L ? (1 ? s) + s n (3)</formula><p>where L is the one-hot label at softmax outputs, s is the smoothing parameter, and n is the number of classification classes. For example, when smoothing is 0.2, the one-hot label {0, 1} will become {0.1, 0.9}, which lowers the confidence on training samples but reduces overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STATISTICAL ANALYSIS OF BODY GESTURE</head><p>To better understand the effect of different body-related features, before moving to deep multimodal learning, we deploy a simple linear regression model to perform statistical analysis on the body gesture features (BodyGesture from Sec. 4.2) and fidgeting features (Fidget_pure from Sec. 4.4). The aim of this section is to shed some light on the effect of different movements of every part of the body and its correlation with depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Fidgeting features from Fidget_pure is processed by averaging along the time axis (9 ? N to 9 ? 1) to match the dimension of other features in BodyGesture (20 ?1). Reporting notation is defined as "[localization]-[feature type][linear polarity]". Localization and feature type token mappings are provided in <ref type="table" target="#tab_4">Table 3</ref>. Polarity is defined below:</p><p>? "+/?": A greater value (e.g. more activity) contributing to a positive/negative classification ? "/": A near-zero coefficient in linear model. ? "?": The polarity is observed inconsistent in different folds of cross-validation. With the linear model, we perform 3-fold crossvalidation on depression labels, which is more reliable than normal train-valid-test split for our small dataset. Crossvalidation also provides more confidence about the polarity of each feature, as only the features that show consistent polarity across all folds will be marked. All results are calculated as the mean of 3-fold cross-validation results. All experiments and cross-validation are participant-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>As shown in <ref type="table">Table 9</ref>, with only the global movement (O-FM), the F1 score is only 34.43%. This means that measuring the quantity of global motion in the body is not enough indicator of depression. While when combining all body gesture statistical features, the classifier achieves 66.81% F1 score.</p><p>Note that all body gesture statistical features include a large set of features representing statistics of different body parts as well as global body motion, as explained in Sec. 4.2. In order to filter out this large feature set, we performed an exhaustive feature search to obtain the combination of features that gives the best performance, repsresented in <ref type="table">Table 9</ref> as "Searched BodyGesture". It reaches a good F1 score at 82.70%.</p><p>As shown in <ref type="table">Table 9</ref>, when we combine specific fidgeting features (Fidget_pure) with BodyGesture, and perform feature search on the concatenated feature, the F1-score reaches the best at 83.38%. The resulted best feature combination includes: {O-FM?, O-GM+, O-GN?, Hn-GN?, Hn-GS?, He-GL+, He-GN+, He-GT+, He-GA+, He-GS+, L-GL+, L-GN+, L-GA+, SHF-L(Right)+, SHF-A(Right)+, SHF-F(Right)+, SHF-F(Left)+}. Looking deeply into this list of features we could infer some interesting insights into the overall body movements in our dataset, which we explain below.</p><p>For example, the O-GM+ token suggests that more movement within gestures relative to all other movement is indicative of depression, and especially, total movement within head gestures (He-GT+) is positively correlated with depression. The localized features suggest that the length of gestures in the head and legs (He-GL+, L-GL+) has a correlation with depression. It's clear that gesture statistics in hands (Hn-* ) are generally not interesting in prediction, while the classifier pays head and leg motions more attention. However, Hn-GS? suggests that more regular (thus less surprising) hand gestures (e.g. constant fidgeting) show a positive contribution to depression.</p><p>We can also conclude that a higher quantity of right hand fidgeting on the leg, arm, and face (SHF-* (Right)+) have a positive contribution to the higher depression level, and left hand fidgeting on the face (SHF-F(Left)+) is also positively correlated with high depression level. The difference in left and right might be because most participants are right-handed and therefore, their left hands exhibit less useful motions that are predictive of depression. The conclusion is not surprising, as, in our observations, people perform hand to hand fidgeting regardless of their depression label. Combining the results from above, we can conclude that, in our dataset, more regular hand gestures and more fidgeting on the leg, arm, and face are indicative of depression. Depressed participants also have more frequent motions in the head and leg region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION OF MULTIMODAL DEEP LEARNING</head><p>In this section, we evaluate and demonstrate the validity and potential of fidgeting features as complementing modality with other features to predict the signs of psychological distress.</p><p>First, we present some baseline distress classification results on our dataset. Next, we present results for our full multi-modal classifier pipeline, where we investigate the effects of hyper-parameters on the performance given the small size of our dataset. Finally, we apply our automatic fidgeting detection approach to a publicly available dataset <ref type="bibr" target="#b10">[11]</ref> to demonstrate its accuracy and generalisability beyond our dataset.</p><p>As in Sec. 5, all results are calculated as the mean of 3-fold cross-validation results. All experiments and crossvalidation are participant-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines</head><p>As a baseline, we used Gaussian kernel Support Vector Machines (SVMs) classifiers applied on each individual feature group used in our multi-modal model (listed in <ref type="table" target="#tab_11">Table 8</ref>). Unlike in Sec. 5, non-linearity can be considered in these baseline models. They are evaluated for a binary depression label and a binary anxiety label. These models provide a simple and common baseline for our dataset. For the baseline SVM, we use the mean value for each feature over the whole sample, thus providing a normalized representation with mean values of all the features. Results are presented in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>These baseline models demonstrate two points: first, the behaviors we are attempting to classify in our dataset are complex; and second, our fidgeting features by themselves are not trivially predictive of distress, but rather require learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multi-modal distress classification</head><p>As presented in the previous baseline section, single modalities are not enough to capture the complexity of signs of psychological distress. Therefore we experiment with our proposed multi-modal classification framework. We encode different modalities through multi-DDAE and Improved Fisher Vector encoding (Sec. 4.4), and classify distress labels using either LR or MLP classifier after Random Forest feature selection (Sec. 4.5).</p><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we present the best performance of different feature group combinations using our multi-modal fusion framework. We use a Random Forests (RF) for feature selection. As RFs take in labels to find the most discriminative features, this feature selection is only performed on the training set and selected features are then applied to the test set, which prevents label leaking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Effects of some hyper-parameters</head><p>As shown in <ref type="figure">Fig. 3</ref>, when other hyperparamters are fixed, label smoothing makes great effects on classification performance. <ref type="figure">Fig. 3</ref> presents the great effect of label smoothing on classification performance when other hyperparameters are fixed. Though some turbulences exist, the performance increases with higher label smoothing but starts to decrease when smoothing is too much. This is intuitively reasonable because when smoothing is above 0.5, there is less allowed space for model to learn features well. The results in <ref type="figure">Fig. 3</ref> shows that label smoothing parameter at 0.4 generally provides good performance, and thus we fixed this value in follow experiments.</p><p>We test different numbers of features selected by RF (RF num), and different GMM kernel sizes. <ref type="figure">Fig. 4</ref> shows that the performance is generally worse when RF num is low (&lt; 100) as it results in insufficient information. However, when RF num is high (? 250), redundant features bias the classifier, decreasing performance.</p><p>Using 32 GMM kernels achieves better performance than 16 kernels. We believe this is due to the way GMM clusters similar per-frame features. More kernels mean more clusters and thus more predictive information. However, when kernel size is above 32, the fitting score is large (in GMM lower is better) and therefore increasing beyond 32 will not further improve performance. <ref type="figure" target="#fig_4">From Fig. 5</ref>, it is clear that fidget features improve most configurations' performance, but performance decreases slightly without the participant speaking event (presented as "Pure Fidgeting" in figure). This leads us to conclude that the co-occurrence of speaking and fidgeting is relevant for distress detection. <ref type="figure" target="#fig_4">Fig. 5</ref> also demonstrates our ablation analysis to help us understand better the important factors in distress classification. We remove one or two feature groups from our framework and conduct the same experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Effects of feature groups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Ablation Analysis</head><p>Without MFCCs features, the performance generally doesn't drop too much in depression and even increases in anxiety. This may suggest that MFCCs are not very important in depression and even distractive in anxiety detection.</p><p>AUs have long been proved to be predictive of distress, and, as expected, we see a significant performance reduction when omitting them.</p><p>It is interesting to note that fidgeting, with the LR configuration, does not consistently improve performance, but in anxiety, it always boosts the classification results. This allows us to conclude that fidgeting is certainly important in anxiety, but is also predictive in depression when combined with other feature configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Fidget detector cross-dataset validation</head><p>To further validate our automatic fidgeting detection approach, we evaluate it on a publicly available dataset from Mahmoud et al. <ref type="bibr" target="#b10">[11]</ref> that has videos of fidgeting behavior along with manual fidgeting labels.</p><p>In this dataset, actors perform specific fidgets. While these fidgets are overemphasized compared to natural fidgets, their core movement is similar.</p><p>Segments of the video containing fidgeting are manually labeled in an action-exclusive manner. That is, the co-occurrence of fidgeting is not labeled. Given this, we measure the accuracy of our approach in two phases: first, we check that fidgeting, regardless of location, is detected during the periods of manually labeled fidgeting; and second, we calculate the recall for location-specific fidgeting. Precision would not make sense for location-specific fidgeting, because the detected location may also be fidgeting, while the ground truth only considers one location.</p><p>Detected fidgeting segments shorter than 100 frames are excluded to reduce noise. As shown in <ref type="table" target="#tab_13">Table 10</ref>, the recall of the non-fidget label is around 50%, but this due to the fact that the labels are generally assigned to a long continuous segment and do not accurately reflect the actions occurring per-frame. However, the recall of the fidget label is good, achieving 80%.</p><p>Our fidgeting detection approach outperforms the stateof-the-art presented by Mahmoud et al. <ref type="bibr" target="#b10">[11]</ref> for each fidget type, achieving a recall above 75% for all fidgeting types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We introduced a novel audio-visual distress dataset comprising recorded interviews and distress labels based on psychological questionnaires.</p><p>We then presented an automated self-adaptor and fidgeting detection system to extract different fidgeting behaviors from real interview videos. We validated our automated approach by evaluating it on a manually-labeled publiclyavailable fidgeting dataset as well as our newly collected dataset of natural expressions.</p><p>Statistical analysis with generic gesture features was carried out, providing interesting insights into the effect of different generic body movements and their correlation with depression labels.</p><p>We also presented a deep learning method that doesn't require a feature search and utilizes the co-occurrence of different multi-modal features. We combined our detected fidgeting features with three other modalities, AUs, gaze, and MFCCs, in a multi-modal distress classification pipeline. This pipeline utilized a Multi-modal Deep Denoising Auto-Encoder to compactly represent the modalities per-frame, a GMM to FV step to represent the features across a whole video compactly, and a random forest to select important features. Finally, we tested the binary classification of depression/anxiety labels using LR and MLP classifiers. An ablation study has been carried out to demonstrate the effect of detected fidgeting behaviors in predicting signs of psychological distress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LIMITATIONS AND FUTURE WORK</head><p>Given the limitations of the small dataset we used, more work is required to utilize the fidgeting features as a complementary modality for classification and prediction of psychological distress. Though recruiting participants and interviewing are time-consuming and costly, we are planning to extend our dataset with more videos. In our multimodal classification experiments, we treated all fidgeting features as a whole. When more data is available, it will be interesting to evaluate the importance of each fidget behavior (e.g., hand to arm fidget and hand to hand fidget). In our work, we only focused on depression and anxiety disorders. However, our automatic approach to detecting self-adaptors and fidgeting opens the door for more work to explore the presence of these non-verbal behaviors and measure them quantitatively in other psychological disorders.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Hierarchical self-adaptor detection workflow. (1) First, detect hand/leg location; (2) Classify motion using DYNAMIC/STATIC Classifier and then finally combine location and motion to give high-level fidgeting event. The figure shows the detection of H2H (Hand to hand) fidget. The same principle applies to other fidgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>SHF (Single Hand Fidgeting) {H2A, H2L, H2F, H2F} + DYNAMIC SHF-L (to Leg only) H2L + DYNAMIC SHF-F (to Face only) H2F + DYNAMIC SHF-A (to Arm only) H2A + DYNAMIC LFF (Leg/Feet Fidgeting) {L2G, L2L} + DYNAMIC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Multi-modal fusion &amp; classification pipeline. The dashed arrow represents a fully connected neural network between dense layers. Pose estimation, gaze, Action Units, and MFCC data are extracted from videos. Fidget features are computed using the method described in Section 4. (1) All features are fed into a Multi-modal Deep Denoising Auto-Encoder (multi-DDAE) to generate a compact per-frame encoded representation. (2) These per-frame features are then compressed into a whole video representation using a Gaussian Mixture Model (GMM) and Fisher Vector combination. (3) Random Forest feature selection is performed. (4) Finally, a classifier predicts a given label. We experiment with two classifiers, a logistic regression classifier and a Multi-layer Perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Effects of label smoothing. In general, smoothing can boost performance. (error bar extends by the standard deviation in either side and best performance in bold) Effects of hyper-parameters. Red denotes models incorporating fidget features and blue for non-fidget models. In general, models with fidget features perform better. (Error bars are not shown for better visualization; best performance of each model is in bold). RF+number denotes the number of features selected by Random Forest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Effects of feature groups and ablation analysis (error bars extend by the standard deviation in either side; best performance is in bold).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>General statistics regarding the questionnaire and demographic results within the dataset. This table demonstrates there are no confounding correlations with the depression label.</figDesc><table><row><cell cols="3">Label Range Mean</cell><cell>Covariance with Depression</cell></row><row><cell>Distress</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Depression</cell><cell>0-19</cell><cell>7.43</cell><cell>-</cell></row><row><cell>Anxiety</cell><cell>0-19</cell><cell>7.00</cell><cell>86.15%</cell></row><row><cell>Perceived stress</cell><cell>1-30</cell><cell>18.17</cell><cell>84.00%</cell></row><row><cell>Somatic symptoms</cell><cell>1-27</cell><cell>9.06</cell><cell>74.16%</cell></row><row><cell>Personality</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Extraversion</cell><cell>3-31</cell><cell>16.37</cell><cell>-30.49%</cell></row><row><cell>Agreeableness</cell><cell>12-34</cell><cell>25.67</cell><cell>-42.21%</cell></row><row><cell>Openness</cell><cell>7-39</cell><cell>27.29</cell><cell>4.29%</cell></row><row><cell>Neuroticism</cell><cell>1-31</cell><cell>16.86</cell><cell>80.00%</cell></row><row><cell>Conscientiousness</cell><cell>10-36</cell><cell>21.46</cell><cell>-46.41%</cell></row><row><cell>Demographic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gender</cell><cell cols="2">18 M &amp; 17 F</cell><cell>9.47%</cell></row><row><cell>Age</cell><cell>18-52</cell><cell>25.40</cell><cell>-11.09%</cell></row><row><cell></cell><cell cols="2">TABLE 1</cell><cell></cell></row></table><note>. Covariance is presented as</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparison of the mean questionnaire values within our dataset to the published norms. This shows that the population distribution, with regards to these distress and personality measures, is generally in line with the broader population.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Feature notation Abbrs. of BodyGesture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell>Self-adaptor and fidgeting encoding book</cell></row><row><cell>4.3.2 Self-adaptor Detector</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell>Self-adaptor Detection Evaluation</cell></row><row><cell>4.3.3 Fidgeting Detector</cell></row><row><cell>4.3.3.1 Design: As shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Hand/Leg action labelling overviewHaving reliable slice labels, we then partitioned participants into 5 folds and performed slice-level cross-validation. For evaluation, we calculated accuracy, F1 score, and their respective standard deviations.</figDesc><table><row><cell>Category</cell><cell>Acc.</cell><cell>Acc. Std.</cell><cell>F1</cell><cell>F1 Std.</cell></row><row><cell>BOTH: H2H</cell><cell>0.833</cell><cell>0.019</cell><cell>0.834</cell><cell>0.019</cell></row><row><cell>LEFT:{H2A, H2L, H2F, H2F}</cell><cell>0.884</cell><cell>0.025</cell><cell>0.884</cell><cell>0.026</cell></row><row><cell cols="2">RIGHT:{H2A, H2L, H2F, H2F} 0.895</cell><cell>0.026</cell><cell>0.894</cell><cell>0.026</cell></row><row><cell>{L2G, L2L}</cell><cell>0.875</cell><cell>0.022</cell><cell>0.871</cell><cell>0.021</cell></row><row><cell></cell><cell>TABLE 7</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">DYNAMIC/STATIC Classifier evaluation (LEFT means left hand, RIGHT</cell></row><row><cell cols="4">means right hand, BOTH means both hands)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Feature Groups. N is number of frames in each recording of participants.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10</head><label>10</label><figDesc>Results of fidget detection on Mahmoud et al.'s dataset [11].</figDesc><table><row><cell cols="3">Step 1: Detect fidget only</cell><cell></cell><cell></cell></row><row><cell cols="5">fidget precision recall f1-score support</cell></row><row><cell>0</cell><cell>0.51</cell><cell>0.49</cell><cell>0.50</cell><cell>29440</cell></row><row><cell>1</cell><cell>0.79</cell><cell>0.80</cell><cell>0.80</cell><cell>69517</cell></row><row><cell cols="4">Step 2: Detect specific fidgeting</cell><cell></cell></row><row><cell cols="3">(evaluated with recall)</cell><cell></cell><cell></cell></row><row><cell cols="2">Fidget type</cell><cell></cell><cell cols="2">Recall Support</cell></row><row><cell>leg</cell><cell></cell><cell></cell><cell>0.784</cell><cell>32430</cell></row><row><cell cols="2">hand to face</cell><cell></cell><cell>0.865</cell><cell>10594</cell></row><row><cell cols="2">hand to arm</cell><cell></cell><cell>0.787</cell><cell>12794</cell></row><row><cell cols="2">hand cross</cell><cell></cell><cell>0.768</cell><cell>13699</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global, regional, and national incidence, prevalence, and years lived with disability for 328 diseases and injuries for 195 countries, 1990-2016: a systematic analysis for the global burden of disease study 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Abajobir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Abbafati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abd-Allah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Abdulkader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Abdulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Abebo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Abera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">10100</biblScope>
			<biblScope unit="page" from="1211" to="1259" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic detection of self-adaptors for psychological distress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The World Health Report</title>
	</analytic>
	<monogr>
		<title level="m">Mental health : new understanding, new hope</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depression and other common mental disorders: global health estimates. World Health Organization(WHO)</title>
	</analytic>
	<monogr>
		<title level="j">WHO</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Home -mental health foundation</title>
		<idno>2020/07/04</idno>
	</analytic>
	<monogr>
		<title level="j">Mental Health Foundation</title>
		<imprint/>
	</monogr>
	<note>Mental Health Foundation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why bodies? Twelve reasons for including bodily expressions in affective neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1535</biblScope>
			<biblScope unit="page" from="3475" to="3484" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dont scratch! Self-adaptors reflect emotional stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Toothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E F</forename><surname>Tree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Intelligent Virtual Agents (IVA)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="398" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards automatic analysis of gestures and body expressions in depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PervasiveHealth</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="276" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic processing of self-adaptors, emblems, and iconic gestures: An erp study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurolinguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="105" to="122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">To behave like a liar: Nonverbal cues to deception in an asian sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Police and Criminal Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="172" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic multimodal descriptors of rhythmic body movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonverbal interaction of patients and therapists during psychiatric interviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Fairbanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An analysis of fidgeting and associated individual differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="406" to="429" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The repertoire of nonverbal behavior: Categories, origins, usage, and coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonverbal Communication, Interaction, and Gesture</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">57106</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Home literacy, television viewing, fidgeting and adhd in young children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Froiland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1337" to="1353" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic behavior descriptors for psychological disorder analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facial action coding system: a technique for the measurement of facial movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Palo Alto</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal measurement of depression using deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Oveneke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Can body expressions contribute to automatic depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal detection of depression in clinical interviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="307" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic multimodal measurement of depression severity using deep autoencoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Psychomotor symptoms of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Sackeim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychiatry</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="17" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Depression severity prediction based on biomarkers of psychomotor retardation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic audiovisual behavior descriptors for psychological disorder analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="648" to="658" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-cultural detection of depression from nonverbal behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alghowinem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting depression severity by interpretable representations of motion dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zakia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="739" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analysis of fundamental frequency for near term suicidal risk assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Shiavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1853" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An end-to-end model for detection and assessment of depression levels using speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srimadhur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lalitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-modality hierarchical recall based on gbdts for bipolar disorder classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bipolar disorder recognition with histogram features of arousal and body gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Oveneke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multilevel attention network using text, audio and video for depression prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A multi-modal hierarchical recurrent neural network for depression detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="65" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic detection of adhd and asd from expressive behaviour in rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gillott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="762" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AVEC 2018: Bipolar disorder and cross-cultural affect recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">AVEC 2017: Real-life depression, and affect recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mozgai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The PHQ-8 as a measure of current depression in the general population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Strine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Mokdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Affective Disorders</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The PHQ-9: validity of a brief depression severity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B W</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of General Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="606" to="613" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A brief measure for assessing generalized anxiety disorder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>L?we</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Nonverbal communication. Transaction Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The distress analysis interview Corpus of human and computer interviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The Social Context of Nonverbal Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Philippot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blairy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="213" to="241" />
		</imprint>
	</monogr>
	<note>Mimicry: Facts and fiction</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The somatic symptom scale-8 (SSS-8)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gierk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spangenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Br?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>L?we</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="407" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Measuring Stress: A Guide for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamarck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health and Social Scientists</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Perceived stress scale</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The big five trait taxonomy: history, measurement, and theoretical perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of personality: Theory and research</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="102" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">National study of chronic disease self-management: six-month outcome findings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Ory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lorig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Aging and Health</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1258" to="1274" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Development of personality in early and middle adulthood: Set like plaster or persistent change?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Gosling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1041" to="1053" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Openface 2.0: Facial behavior analysis toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">What is a savitzky-golay filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="111" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Speaker diarizationn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<idno>2020/07/04</idno>
		<ptr target="https://github.com/taylorlu/Speaker-Diarization" />
		<imprint/>
	</monogr>
	<note>Github repository</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fully supervised speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6301" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">pyAudioAnalysis: an spen-source python library for audio signal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal deep learning framework for mental disorder recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automated screening for bipolar disorder from audio/visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop on Audio/Visual Emotion Challenge (AVEC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A temporally piece-wise fisher vector approach for depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="255" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">When does label smoothing help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">He studied Computer Science at Hong Kong University for one year, and then transferred to Cambridge for a 4-year BA/MEng course in Engineering. He was awarded a silver medal in</title>
	</analytic>
	<monogr>
		<title level="m">His research interest spans Natural Language Processing</title>
		<imprint/>
		<respStmt>
			<orgName>Information Engineering at University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Hyperspectral Image Processing, and Affective Computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
