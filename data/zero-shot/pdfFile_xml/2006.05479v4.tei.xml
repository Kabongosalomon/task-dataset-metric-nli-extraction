<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>B?hm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Berkeley Center for Cosmological Physics Department of Physics</orgName>
								<orgName type="laboratory">Lawrence Berkeley National Laboratory</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uro?</forename><surname>Seljak</surname></persName>
							<email>useljak@berkley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Berkeley Center for Cosmological Physics Department of Physics</orgName>
								<orgName type="laboratory">Lawrence Berkeley National Laboratory</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Reviewed on OpenReview: https: // openreview .net/ forum? id= AEoYjvjKVA</note>
					<note>Published in Transactions on Machine Learning Research (09/2022)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Principal Component Analysis (PCA) minimizes the reconstruction error given a class of linear models of fixed component dimensionality. Probabilistic PCA adds a probabilistic structure by learning the probability distribution of the PCA latent space weights, thus creating a generative model. Autoencoders (AE) minimize the reconstruction error in a class of nonlinear models of fixed latent space dimensionality and outperform PCA at fixed dimensionality. Here, we introduce the Probabilistic Autoencoder (PAE) that learns the probability distribution of the AE latent space weights using a normalizing flow (NF). The PAE is fast and easy to train and achieves small reconstruction errors, high sample quality, and good performance in downstream tasks. We compare the PAE to Variational AE (VAE), showing that the PAE trains faster, reaches a lower reconstruction error, and produces good sample quality without requiring special tuning parameters or training procedures. We further demonstrate that the PAE is a powerful model for performing the downstream tasks of probabilistic image reconstruction in the context of Bayesian inference of inverse problems for inpainting and denoising applications. Finally, we identify latent space density from NF as a promising outlier detection metric.</p><p>Despite their popularity, variational autoencoders have well known practical limitations. Successful VAE training requires to find a delicate balance between the two contributing terms to the ELBO: The distortion term, which encourages high quality reconstructions, and the rate term, which controls the sample quality by matching the aggregate posterior with a chosen prior distribution <ref type="bibr" target="#b2">(Alemi et al., 2018)</ref>. Whether the VAE training process succeeds in striking this balance depends on a number of factors, including the network architectures, the chosen prior and the class of allowed posterior distributions <ref type="bibr" target="#b23">(Hoffman &amp; Johnson, 2016)</ref>. In some cases, too powerful decoders can decouple the latent space from the input <ref type="bibr" target="#b6">(Bowman et al., 2016;</ref><ref type="bibr" target="#b9">Chen et al., 2017)</ref> and lead to posterior collapse (van den Oord et al., 2017).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep generative models are powerful machine learning models that can learn complex, high-dimensional data likelihoods and generate samples from them. Because of their probabilistic formulation, generative models are becoming an indispensable tool for scientific data analysis in a range of domains including particle physics <ref type="bibr" target="#b40">(Paganini et al., 2018;</ref><ref type="bibr" target="#b50">Stein et al., 2020)</ref> and cosmology <ref type="bibr" target="#b52">(Thorne et al., 2021;</ref><ref type="bibr" target="#b44">Reiman et al., 2020)</ref>.</p><p>Variational Autoencoders (VAEs) <ref type="bibr" target="#b27">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b46">Rezende et al., 2014)</ref> are among the most popular generative models. VAEs project the data to a lower dimensional latent space and reformulate the data likelihood estimation as a variational inference problem. Their training objective is the Evidence Lower BOund (ELBO), which approximates the true data likelihood with a variational ansatz from below. VAEs can be built with expressive architectures, enjoy the benefits of regularization through data compression and have a firm theoretical foundation. Different to generative adversarial networks <ref type="bibr">(Goodfellow et al., 2014)</ref>, another popular class of generative models, VAEs provide an estimator for the data likelihood and a posterior distribution for the latent variables.</p><p>A long list of works have dissected and studied the training behavior of VAEs <ref type="bibr" target="#b2">(Alemi et al., 2018;</ref><ref type="bibr" target="#b23">Hoffman &amp; Johnson, 2016)</ref> and suggested modifications to remedy common issues. Many fixes add complexity to the VAE model, e.g. by modifying or annealing the ELBO objective <ref type="bibr" target="#b6">(Bowman et al., 2016;</ref><ref type="bibr" target="#b1">Alemi et al., 2017;</ref><ref type="bibr" target="#b21">Higgins et al., 2017;</ref><ref type="bibr" target="#b35">Makhzani et al., 2015)</ref>, choosing more expressive posterior distributions <ref type="bibr" target="#b28">(Kingma et al., 2016;</ref><ref type="bibr" target="#b45">Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b48">Salimans et al., 2015;</ref><ref type="bibr" target="#b57">Tran et al., 2016)</ref>, or using more flexible priors <ref type="bibr" target="#b3">(Bauer &amp; Mnih, 2019;</ref><ref type="bibr" target="#b9">Chen et al., 2017;</ref><ref type="bibr" target="#b56">Tomczak &amp; Welling, 2018)</ref>.</p><p>In this work we take a different approach. We give up on the variational ansatz that lies at the heart of VAEs and instead suggest a conceptually very simple model with very stable training properties. The Probabilistic Autoencoder (PAE) is motivated by probabilistic principal component analysis <ref type="bibr" target="#b53">(Tipping &amp; Bishop, 1999)</ref> and consists of an Autoencoder (AE), which is interpreted probabilistically after training by means of a Normalizing Flow (NF). Both of these components are comparably easy to set up and train and this two-stage set up allows the practitioner to optimize their hyper-parameters (model architecture, training procedure, etc.) independently. We claim that the PAE is a viable alternative to VAEs despite its conceptual simplicity. We back this claim empirically through ablation studies. Specifically, we compare the performance of the PAE to that of equivalent VAEs in a number of tasks which we think are specifically relevant for practical applications: data compression (reconstruction quality), data generation, anomaly detection and probabilistic data denoising and imputation.</p><p>Our primary contributions are: 1) a simple generative model designed with ease-of-use and training in mind 2) a quantitative comparison of this model to variational autoencoders, showing that it performs relevant tasks at comparable quality and accuracy without variational inference 3) a new anomaly detection metric through NF density estimation in latent space, which is a byproduct of the PAE, but can also be used within the VAE framework. We make all of our code publicly available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation: Probabilistic PCA</head><p>The probabilistic autoencoder is motivated by linear Principal Component Analysis (PCA) and its probabilistic interpretation, probabilistic principal component analysis <ref type="bibr" target="#b53">(Tipping &amp; Bishop, 1999)</ref>, which provides a PCA-based data likelihood estimate.</p><p>A principal component analysis of data x ? R N at fixed latent space dimensionality K (K&lt;N ) finds the orthogonal linear transformation, O,</p><formula xml:id="formula_0">O : R K ? R N , z ? Oz, OO T =1 N<label>(1)</label></formula><p>that maximizes the data variance in the latent space. Maximizing the variance of the transformed data is equivalent to minimizing the average reconstruction error (the residual variance in data space). The PCA problem can be solved analytically and the principal components are given by the eigenvectors of the data covariance matrix.</p><p>A suitable latent space dimensionality, K, is chosen by inspecting the eigenvalues, ? i , of the data covariance and keeping only the eigenvectors that correspond to the largest eigenvalues. The average reconstruction error that originates from the discarded eigenvalues is ? 2 recon = N i=K+1 ? i . The data model under a PCA is</p><formula xml:id="formula_1">x = Oz + ,<label>(2)</label></formula><p>where O is constructed from the eigenvectors that correspond to the largest eigenvectors and is the residual not captured by the PCA transformation.</p><p>In probabilistic PCA (PPCA) the residuals are assumed to follow a Gaussian distribution. The implicit likelihood is then,</p><formula xml:id="formula_2">lnp(x|z) = ? 1 2 N ln(2?) + ln det ? + (x ? Oz) T ? ?1 (x ? Oz) .<label>(3)</label></formula><p>Under the approximation that the reconstruction error is uncorrelated and isotropic, ? is a diagonal matrix with ? 2 recon along its diagonal, ? = ? 2 recon 1 N . The implicit likelihood in equation 3 alone is not yet a probabilistic model for the data. A fully probabilistic structure requires a prior over the latent space. PPCA <ref type="bibr" target="#b53">(Tipping &amp; Bishop, 1999)</ref> assumes that the latent variables follow a Gaussian distribution with mean zero and covariance ?, where ? is a diagonal matrix with the rank-ordered eigenvalues ? i along its diagonal.</p><p>The Gaussian prior allows us to analytically compute the marginal,</p><formula xml:id="formula_3">lnp(x) = ? 1 2 N ln(2?) + ln det C + x T C ?1 x ,<label>(4)</label></formula><p>with C = O?O T + ?.</p><p>In summary, probabilistic PCA constructs a probabilistic model by first finding a basis which minimizes the reconstruction error in a class of models, followed by using the probability distribution of the latent variables as the prior. With the probabilistic autoencoder we generalize this approach to non-linear models. The PCA is replaced by an autoencoder trained to minimize the reconstruction error, and the Gaussian ansatz for the prior is replaced by a normalizing flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The probabilistic autoencoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PAE training</head><p>In analogy to PPCA the PAE is constructed in two stages. Stage 1 is an autoencoder with encoder f and decoder g, both deep neural networks with respective trainable parameters ? and ?,</p><formula xml:id="formula_4">f ? : R N ? R K , x ? f ? (x), g ? : R K ? R N , z ? g ? (z).<label>(5)</label></formula><p>The training objective of the AE is the reconstruction error or L 2 -distance,</p><formula xml:id="formula_5">L AE = E p(x) ||x ? g ? (f ? (x))|| 2 2 .<label>(6)</label></formula><p>The autoencoder is not a probabilistic model. To construct the PAE, we interpret it probabilistically with a second stage. We approximate the latent space prior, p(z), by performing a density estimation on the AE-encoded training data. In PPCA the latent space density is modeled with a Gaussian. The PAE employs a more flexible density estimator for modeling the prior, a normalizing flow.</p><p>Normalizing flows <ref type="bibr" target="#b47">(Rippel &amp; Adams, 2013;</ref><ref type="bibr" target="#b12">Dinh et al., 2015;</ref><ref type="bibr" target="#b26">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b19">Grathwohl et al., 2019)</ref> address the task of modeling the density distribution p(z) of input data z by introducing a bijective mapping, b ? (z) = u, from the data z to an underlying latent representation u,</p><formula xml:id="formula_6">b ? : R K ? R K , z ? u=b ? (z).<label>(7)</label></formula><p>Requiring the latent variables to follow a given prior distribution q(u), one can write the modeled data probability density using conservation of probability,</p><formula xml:id="formula_7">p ? (z) = q(u)|? z b ? (z)|.<label>(8)</label></formula><p>Here, q(u) is some simple normalized latent space probability density, usually a Gaussian N (0, I), and |? z b ? (z)| is the Jacobian determinant of the transformation b ? (z). The NF is parametrized by parameters ? and training takes the form of maximizing the data likelihood p ? (z) with respect to ?. Equation <ref type="formula" target="#formula_7">8</ref> requires an evaluation of the Jacobian. NF architectures have forms for which this is simple and fast. The architectural constraints originating from this requirement also introduce beneficial regularizing properties and prevent overfitting. A schematic diagram of the PAE is shown in figure 1.</p><p>In the PAE, an NF is trained as a mapping from the latent space of the AE, z, to the Gaussian latent space of the normalizing flow, u. The training objective of the NF is the negative log likelihood of the encoded samples, z = f ? (x),</p><formula xml:id="formula_8">L NF = Ep (z) [? ln p ? (z)]= Ep (z) ? ln p(u) ? ln det ?b ?1 ? (u) ?u u=b?(z) .<label>(9)</label></formula><p>The normalizing flow maps the potentially very irregular latent space distribution of the AE to a Gaussian distribution.</p><p>To sample from the PAE we draw a sample, u?N (0, 1), from the NF latent distribution and pass it through both the NF and AE generators (left panel in <ref type="figure" target="#fig_0">figure 1</ref>),</p><formula xml:id="formula_9">x = g ? (b ?1 ? (u)).<label>(10)</label></formula><p>Just as in PPCA, density estimation on the encoded data only provides an approximate prior. Formally, for a fully probabilistic model, a prior would be given by the aggregate posterior,</p><formula xml:id="formula_10">p model (z) = dx p(x) p model (z|x) = E p(x) [p model (z|x)] ,<label>(11)</label></formula><p>meaning that the density estimation should be conducted on samples from the posteriors. Since our first stage is a non-probabilistic autoencoder, there is no notion of a posterior. Our choice to fit an approximate prior on the encoded data, however, is not completely unjustified: we know that small reconstruction errors (which can be easily achieved with an AE) generally result in very narrow posteriors centered on latent space points which are well determined through the projection of the data into the latent space. The position of these points are hardly influenced by the prior. By replacing samples from the posterior with the encoded samples, we replace the narrow posteriors by Dirac delta distributions. By fitting on the AE encoded data we approximate the maximum a posteriori (MAP) solution with the AE encoded position. This is not a formal mathematical derivation, but can serve as a reasoning for why the PAE is able to compete with fully probabilistic models even in probabilistic tasks.</p><p>The generalization and regularization properties of NFs are another important ingredient for enabling the use of approximate MAP positions instead of samples from the posterior. NFs are unlikely to fit delta functions to their training points, but smoothly interpolate between them. The success of NF-based density estimation is proof of this: on many datasets NFs achieve the highest validation log data likelihoods <ref type="bibr" target="#b15">(Durkan et al., 2019)</ref>. If the NF does not provide sufficient regularization, it will become apparent as overfitting (lower density estimates on validation data), which can be controlled by simplifying the architecture or early stopping.</p><p>The AE latent space is usually of relatively low dimensionality, K N , which allows for computationally tractable density estimation: the NF models do not require complex deep architectures and are fast to train, which enables efficient hyper-parameter optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to VAE</head><p>Different to the PAE a variational autoencoder is trained on a fully probabilistic objective, the Evidence Lower BOund (ELBO),</p><formula xml:id="formula_11">ELBO = ?L VAE = E p(x) E q ? (z|x) [ln p ? (x|z)] ? D KL [q ? (z|x)||p(z)] ,<label>(12)</label></formula><p>where q ? is an approximate, parametrized variational posterior, usually a Gaussian with diagonal covariance. Typical choices for the parametrized implicit likelihood p ? (x|z) are a Bernoulli distribution for binary valued data or a diagonal Gaussian distribution for continuous data. The ELBO is guaranteed to bound the true evidence p(x) from below. During the VAE training equation 12 is evaluated stochastically on samples from the approximate posterior. Equation 12 shows that the VAE objective balances the average reconstruction error (the likelihood term or distortion) with the sample quality (the KL term or rate). If the former dominates the loss during training, the encoded distribution and prior do not match well. If this is the case, samples from the prior can land outside of the encoded domain resulting in low sample quality. If the KL term dominates, some latent dimensions will solely be used to satisfy the second term and not encode any information about the input data, a problem known as posterior collapse <ref type="bibr" target="#b2">(Alemi et al., 2018)</ref>. Balancing the two terms can be controlled by an additional parameters ?,</p><formula xml:id="formula_12">L ??VAE = ? E p(x) E q ? (z|x) [ln p ? (x|z)] ? ? D KL [q ? (z|x)||p(z)] .<label>(13)</label></formula><p>This and related modification are known as ?-VAEs <ref type="bibr" target="#b6">(Bowman et al., 2016;</ref><ref type="bibr" target="#b1">Alemi et al., 2017;</ref><ref type="bibr" target="#b21">Higgins et al., 2017;</ref><ref type="bibr" target="#b35">Makhzani et al., 2015)</ref>. Training on equation 13 usually involves a grid search in order to find an optimal value for ? and annealing schedules.</p><p>The PAE optimizes the reconstruction and sample quality individually. Training of stage 1 reaches an optimal reconstruction error. The latter is then left unchanged in the training of stage 2, which can focus entirely on matching the latent space distribution. We test in our experiments whether this procedure results in an advantage in reconstruction error and sample quality. A practical advantage of this procedure is that it facilitates the hyper-parameter search over model architecture and training schedule. Instead of having to iterate over encoder/decoder and flow architecture and the balance between rate and distortion term, each step can be optimized individually and towards a single objective.</p><p>For our comparisons between VAE and PAE to be fair, we allow the VAE prior, which is typically a standard normal distribution, to be more flexible. In analogy to the PAE, we model it with a normalizing flow,</p><formula xml:id="formula_13">L flow?VAE = ? E p(x) E q ? (z|x) [ln p ? (x|z)] ? D KL [q ? (z|x)||p ? (z)] .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Downstream Tasks</head><p>In our experiments, we test the PAE performance not only in terms of sample and reconstruction quality, but also in terms of anomaly detection, a highly relevant downstream task of generative models. In appendix E, we further show how the PAE can be used for posterior-based probabilistic image inputation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Anomaly detection</head><p>One application of generative models is anomaly or out-of-distribution (OoD) detection. This is often based on the assumption that a density estimator should return smaller probability densities for out-of-distribution data than in-distribution (iD) data. However, this is assumption is not always satisfied and generative model based density estimators have been reported to perform poorly in some OoD detection problems. OoD detection with VAEs, NFs <ref type="bibr" target="#b26">(Kingma &amp; Dhariwal, 2018)</ref> and PixelCNNs (van den Oord et al., 2016) can exhibit catastrophic outlier detection failures <ref type="bibr" target="#b39">(Nalisnick et al., 2019)</ref>.</p><p>The PAE model is not trained to maximize the data likelihood, nor does it provide an estimate of it. While we could attempt to perform a marginalization over the latent space (after introducing an approximate implicit likelihood) in order to obtain such an estimate, we suggest a much simpler outlier detection metric: the estimated density in latent space. We find in our experiments that the NF estimated latent space density is an excellent OoD detection metric for outlier detection problems that have been identified as problematic in the literature.</p><p>Our OoD metric is again motivated by PPCA: The PPCA model is a useful toy model to understand how dimensionality reduction prior to density estimation can be beneficial. The PPCA density estimate in latent space is given by</p><formula xml:id="formula_14">lnp(z) = ? 1 2 K ln(2?) + K i ln ? i + K i z i ? ?1 i z i .<label>(15)</label></formula><p>Equation <ref type="formula" target="#formula_0">15</ref> diverges for ? i ? 0. This suggests thatp(z) and henceforthp(x) (Equation 4) can be dominated by small eigenvalues. These small eigenvalue components are well known to be difficult to estimate from a limited amount of data, i.e. they are prone to overfitting. This has led to the development of special covariance estimation regularization techniques known as shrinkage methods in the statistics literature <ref type="bibr" target="#b10">(Chen et al., 2010;</ref><ref type="bibr" target="#b32">Ledoit &amp; Wolf, 2004)</ref>. Dimensionality reduction keeping the largest eigenvalues removes dimensions with vanishing variance and hence cures the estimator's sensitivity to small and likely mis-estimated eigenvalues. In PPCAp(x), the data covariance C = O T ?O + ? is regularized by the noise covariance, ?, which reinterprets the discarded and mis-estimated eigenvalues as noise. Noise is not informative for anomaly detection, which suggests that PPCA data space density estimation has no advantage over density estimation in latent space for this task.</p><p>We illustrate this on an outlier detection problem between the FashionMNIST <ref type="bibr" target="#b62">(Xiao et al., 2017)</ref> and MNIST <ref type="bibr" target="#b31">(Lecun et al., 1998)</ref> data sets in figure 2. Both of these data sets have a data covariance matrix with a high condition number. Only 86 PCA components are required to capture 90% of the data variance in MNIST (compared to N =784) and the smallest eigenvalues are evidently singular. Similar applies to F-MNIST, where a PCA captures 90% of the data variance with 83 components. These data are known to produce catastrophic failures in OoD detection, specifically when presenting samples from the MNIST data set to models trained on F-MNIST <ref type="bibr" target="#b39">(Nalisnick et al., 2019)</ref>. We construct a probabilistic PCA model for F-MNIST (from training data) and use equation 4 and equation 15 as outlier detection metrics to separate F-MNIST in-Distribution (iD) test data from MNIST Out-of-Distribution (OoD) data. In figure 2 we show the Area Under Receiver Operator Curve for this outlier detection task as a function of the number of PCA components. The highest outlier detection accuracy based on latent space density estimation (equation 15) is AUROC = 0.980 and it is reached at a relatively low number of PCA components of 127. The highest accuracy for OoD detection based on data space density estimation (equation 4) is AUROC = 0.974 at 37 components. We argue analogously that the PAE latent space density is better for OoD detection than using a full dimensionality NF. In our experiments, we find that the PAE latent space density is a superior anomaly detector than the ELBO or full dimensionality NF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Generative moment matching networks <ref type="bibr" target="#b33">(Li et al., 2015)</ref> have been proposed to be used in a 2-stage PAE-like set up, consisting of an autoencoder and a mapping of a Gaussian to the encoded distribution. The second stage is non-invertible and trained with a moment-matching objective. Wasserstein autoencoders (Tolstikhin et al., 2018) employ a training objective based on the Wasserstein distance to match the encoded distribution to a given prior. WAEs achieve high sample quality, but do not provide a density estimate. Generative latent flows <ref type="bibr" target="#b63">(Xiao et al., 2019)</ref> are a similar conjunction of AE and NF and achieve high sample quality, but the authors do not perform controlled ablation studies to compare their approach with ELBO-based training objectives, nor do they explore the probabilistic interpretation for downstream tasks.</p><p>Giving more flexibility to the prior distribution has also been suggested in the context of VAEs, e.g. by modeling it with an NF <ref type="bibr" target="#b3">(Bauer &amp; Mnih, 2019;</ref><ref type="bibr" target="#b9">Chen et al., 2017;</ref><ref type="bibr" target="#b56">Tomczak &amp; Welling, 2018)</ref>. This has sometimes been deemed prone to overfitting (Tomczak &amp; Welling, 2018) and many works suggest using more expressive variational distributions instead <ref type="bibr" target="#b28">(Kingma et al., 2016;</ref><ref type="bibr" target="#b45">Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b48">Salimans et al., 2015;</ref><ref type="bibr" target="#b57">Tran et al., 2016)</ref>. In our experiments, we compare our PAE with a VAE with an NF prior because it allows us to compare models with the same architecture. We pay special attention to overfitting, but do not observe it to be a problem.</p><p>Other approaches that improve the VAE sample quality include ?-VAEs <ref type="bibr" target="#b54">(Tishby et al., 2000;</ref><ref type="bibr" target="#b1">Alemi et al., 2017;</ref><ref type="bibr" target="#b22">Hled?k et al., 2019;</ref><ref type="bibr" target="#b21">Higgins et al., 2017)</ref> and 2-Stage-VAEs <ref type="bibr" target="#b11">(Dai &amp; Wipf, 2019)</ref>. ?-VAEs balance rate and distortion by means of an additional scalar parameters (?). 2-stage-VAEs combine two consecutive VAE's, one for the purpose of data compression, where the KL term in the ELBO is suppressed and a second one for latent space density estimation. This two-stage approach achieves high quality samples. With the PAE we demonstrate that this success does not rely on the ELBO objective in the first stage and that the first stage can be replaced by an AE and the second stage by a powerful NF density estimator. High sample quality is also achieved by VQ-VAE (van den <ref type="bibr">Oord et al., 2017;</ref><ref type="bibr" target="#b43">Razavi et al., 2019)</ref>, another model that requires 2-stage training. It combines a more complicated first stage which includes hyper-parameter tuning and discretization with a second stage in which an autoregressive model is trained to learn the prior. The successes of 2-stage models indicate that separating the tasks of learning a lower-dimensional representation and learning its distribution is beneficial for generative model performance.</p><p>Other non-ELBO approaches that address density estimation for data that is confined to a lower dimensional manifold include M (e) -flows <ref type="bibr" target="#b7">(Brehmer &amp; Cranmer, 2020)</ref> and relaxed injective probability flows <ref type="bibr" target="#b29">(Kumar et al., 2020)</ref>. Instead of separating the tasks of compression and density estimation in the lower dimensional manifold, these models regularize a flow itself. The authors of M (e) -flows compare their model with the PAE and find that the PAE reaches the same low reconstruction error and a higher accuracy in anomaly detection. The recently introduced regularized autoencoder is another deterministic VAE alternative based on an autoencoder, that produces comparable or even better sample quality than VAEs <ref type="bibr" target="#b16">(Ghosh et al., 2020)</ref>.</p><p>Downstream tasks: Out-of-distribution detection with generative models has recently attracted a lot of attention, triggered by the finding that state-of-the art generative models such as VAE, GLOW <ref type="bibr" target="#b26">(Kingma &amp; Dhariwal, 2018)</ref> and MAF <ref type="bibr" target="#b41">(Papamakarios et al., 2017)</ref> fail in this task on a number of standard data sets <ref type="bibr" target="#b39">(Nalisnick et al., 2019)</ref>. Our finding that dimensionality reduction combined with latent space density estimation results in reliable anomaly detection is in line with other works: e.g. <ref type="bibr" target="#b0">Abati et al. (2019)</ref> use a single stage training with a free parameter that controls the relative contribution of reconstruction error and NF latent space density and requires tuning of the free parameter. They apply it to OoD, but not to other PAE tasks such as generating samples, denoising and inpainting. Other proposed solutions include the use likelihood ratios as an OoD metric instead of the likelihood itself <ref type="bibr" target="#b16">(Ghosh et al., 2020)</ref>. More recently, a reliable OoD detection was reported with density-of-states, a method which leverages another density estimator on top of the density estimation <ref type="bibr" target="#b38">(Morningstar et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The aim of our experiments is to test whether training on the ELBO offers any measurable advantage over the PPCA-inspired PAE training approach. For this test, we construct and train equivalent PAE and VAE models and compare them in terms of reconstruction error, sample quality, outlier detection accuracy and probabilistic inpainting and denoising ability. We conduct our detailed comparisons between PAE and VAE models on the FashionMNIST data set. For anomaly detection we perform an additional comparison on MNIST and CIFAR10 with the anomaly detection method by <ref type="bibr" target="#b0">Abati et al. (2019)</ref> (Appendix C). We further train a PAE model on the higher dimensional Celeb-A <ref type="bibr" target="#b34">(Liu et al., 2015)</ref> data set (Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation studies</head><p>We compare the PAE to two ELBO-based alternatives:  . The value of the scale parameter, ?, of this implicit likelihood was set to ? = 0.1. We found this to be to be the optimal value in a small ablation study (appendix G). We also found that the model performance is not overly sensitive to this choice. In addition to the experiments presented here, we trained a vanilla ?-VAE and show results in appendix B. We did not include this model in main text because it does not use a flow prior and requires parameter fine-tuning. We also find that it performs worse than all alternatives studied in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data sets and preprocessing</head><p>We perform our ablation studies on the Fashion-MNIST <ref type="bibr" target="#b62">(Xiao et al., 2017)</ref> data set, which we split into 50,000 training examples, 10,000 validation and 10,000 test samples. As outlier data sets we use MNIST <ref type="bibr" target="#b31">(Lecun et al., 1998)</ref>   <ref type="table" target="#tab_0">Table 1</ref>: Overview of the different models used in the ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Parameter choice and training procedure</head><p>Training of the encoder/decoder pair: We used the same encoder and decoder architecture for all of our experiments. We further fixed the latent space dimensionality to 40, the number of training steps to 300,000 and used a learning rate schedule in which we keep the learning rate constant at the initial value up to training step 100,000, then reduce it linearly down to 1/10 of the initial rate over 50,000 steps. For the remaining 150,000 steps we restart the learning rate and repeat the annealing scheme. We did not find our final loss to depend on the details of this annealing scheme, but found that annealing and restarting was beneficial. We used the ADAM optimizer (Kingma &amp; Ba, 2015) with parameters ? 1 = 0.9, ? 2 = 0.999, = 10 ?7 in all of our trainings (including for the normalizing flow).</p><p>The batch size, initial learning rate, sample size in the stochastic evaluation of the ELBO as well as the drop out rate were optimized to yield the best possible reconstruction error on validation data on the ? 0 -VAE model: we ran around 30 shorter trainings (100,000 steps) with different combinations of these parameters and used a Gaussian Process surrogate to determine the combination of parameter values that optimized the reconstruction error on the validation data. The values we obtained through this procedure are outlined in table 2 and were used in all experiments. The only parameter that we adapted in some of our experiments is the dropout rate. The dropout layer is a necessary regularization to prevent overfitting in autoencoder models and models with ? = 0. This regularization is not necessary in VAE models with a flow prior and was not used in these models. We chose the ? 0 -VAE for parameter optimization, because we think that it is a fair middle ground between PAE and flow-VAE sharing some properties with both. It was also chosen for convenience, because it allowed us to optimize the encoder/decoder pair and and normalizing flow separately, without having to worry about the feedbacks of changing one on the other. We point out that optimizing on the flow-VAE would have suffered from this complication. To not disadvantage the flow-VAE, we also train a flow-VAE with a different (simpler) NF architecture.  <ref type="table">Table 2</ref>: Training parameters that were kept constant in all encoder/decoder pair trainings. The parameters were optimized to achieve the lowest reconstruction error on validation data for the ? 0 -VAE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training of normalizing flows:</head><p>We construct normalizing flows from realNVP, NSF and GLOW building blocks. We optimized the number of building blocks of each kind to achieve maximal log p ? (z) on the validation data encoded with the ? 0 -VAE model. In models where the NF was trained separately, we used a learning schedule consisting of 120 epochs during which both learning rate and batch size were annealed and restarted. We found that a restarting learning rate was especially helpful when using NSF transformation layers, but that the final loss did not strongly depend on the details of the annealing and restarting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training of VAE with normalizing flow prior:</head><p>In the flow-VAEs, encoder, decoder and NF are trained jointly on the ELBO objective with the same training parameters and architectures that were used for ? 0 -VAE and PAE. The only parameter that we adapted was the dropout rate, which we set to zero. To test the dependence on the flow architecture, we ran two experiments. One in which we adapted the flow architecture we had found to work best on the ? 0 -VAE encoded data, and another experiment, in which we simplified the flow architecture (flow-VAE (s)). We found the deeper architecture to be superior. All NF architectures are detailed in appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of results:</head><p>For every model (PAE, ? 0 -VAE, flow-VAE) we repeated every training run three times starting from different initial network parameter values. We report results for the trained models that achieved the lowest training loss on validation data out of these three runs. We found very little scatter between different trainings of the same model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction Quality:</head><p>We measure the reconstruction quality in all models by means of the average reconstruction error on the test data set,</p><formula xml:id="formula_15">? 2 recon = 1 M 1 N M j N i x ji ? g ? (f ? (x j )) i 2 = 1 M M j ? 2 recon,j ,<label>(16)</label></formula><p>where j labels the image and i the pixel. In the VAE models we evaluate the reconstruction error using the mean of the variational posterior. We also report the 95 percentile of the image-wise reconstruction errors P 95% (? 2 recon ), which better characterizes the large error tail of the distribution. The results are listed in  We find that the PAE model has a consistently and significantly lower mean and 95 percentile reconstruction error than ELBO-based models. In <ref type="figure" target="#fig_3">figure 3</ref>, we show reconstructions of test data points for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Quality:</head><p>How to best measure image generation quality in a way that quantifies both image quality (are the samples visually compelling?) and diversity (is the model sampling from the full distribution?), is still a question of active research. Here, we measure the sample quality in terms of the well established Frechet-Inception Distance (FID) score <ref type="bibr" target="#b20">(Heusel et al., 2017)</ref>, which is known to correlate well with human perception of image quality. The FID score takes a sample of generated images and a sample of real images, passes each of them through the pre-trained Inception network <ref type="bibr" target="#b51">(Szegedy et al., 2016)</ref> and extracts as features the outputs of one of the last layers. Each set of features is then fitted with a multivariate Gaussian distribution, N (x|?, ?). The FID score is defined as the Fr?chet distance between these two Gaussians</p><formula xml:id="formula_16">FID(x orig , x gen ) = ||? orig ? ? gen || 2 2 + Tr ? orig + ? gen ? 2(? orig ? gen ) 1 2 .<label>(17)</label></formula><p>In <ref type="figure" target="#fig_4">figure 4</ref> we show generated images from each model. While visually comparable, we get the lowest FID scores for the flow-VAE model <ref type="table" target="#tab_5">(table 4)</ref>. A concern sometimes raised with models that use flow priors is  that the added expressiveness of the model results in overfitting or memorization of training data. We pay special attention to overfitting during training by monitoring the loss on validation data. Outlier Detection Accuracy: For outlier detection, we use the outlier detection metrics listed in table 1 and report the outlier detection accuracy for each model in terms of the Area Under Receiver Operator Curve (AUROC). An AUROC of 1 corresponds to a perfect separation between the out-of-distribution and in-distribution data. For the latter we use the F-MNIST test data set. The ELBO of the VAE is evaluated using 10 samples from the variational posterior and we verified that using more samples does not achieve better AUROC values. The results are listed in 5. The latent space density estimation of the PAE model outperforms the ELBO of our VAE models in all but one of our experiments. However, we do not observe a catastrophic failure of OoD detection with the ELBO as it has been reported for this data set <ref type="bibr" target="#b39">(Nalisnick et al., 2019)</ref>. Following our discussion in 4.1, which suggests that latent space density estimation is superior</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OoD detection accuracy measured in terms of AUROC</head><formula xml:id="formula_17">? OoD data ? 0 -VAE PAE (AE+flow) flow-VAE flow-VAE (s) MNIST</formula><p>0.9086 ? 0.0023 0.9970 ? 0.0003 0.9633 ? 0.0014 0.9230 ? 0.0022 Omniglot 0.8668 ? 0.0025 0.9736 ? 0.0011 0.9221 ? 0.0020 0.8753 ? 0.0021 F-MNIST hor. 0.6790 ? 0.0036 0.6883 ? 0.0038 0.6758 ? 0.0036 0.6773 ? 0.0035 F-MNIST vert. 0.8891 ? 0.0022 0.8789 ? 0.0024 0.8994 ? 0.0022 0.8964 ? 0.0019 to data space density estimation in many cases, we disseminate the ELBO further and take a look at the different contributing terms of the ELBO. Specifically, we split the ELBO into three terms, which we identify as distortion (first term), entropy (second term) and cross entropy (last term),</p><formula xml:id="formula_18">ELBO = E p(x) E q ? (z|x) [ln p ? (x|z) ? q ? (z|x) + p ? (z)] ,<label>(18)</label></formula><p>and measure the outlier detection accuracy with each of these terms individually. The results are listed in table 6.</p><p>We find that the cross entropy term, which measures a stochastic mean over the latent space density and is therefore closely related to the latent space density, is a very reliable outlier detection metric for models trained on the ELBO objective.</p><p>OoD detection with flow-VAE, AUROC(?) OoD data distortion rate entropy cross entropy MNIST 0.9620 ? 0.0014 0.9965 ? 0.0003 0.9659 ? 0.0012 0.9963 ? 0.0004 Omniglot 0.9176 ? 0.0023 0.9786 ? 0.0009 0.8965 ? 0.0023 0.9795 ? 0.0009 F-MNIST hor. 0.6751 ? 0.0042 0.6728 ? 0.0038 0.6274 ? 0.0041 0.6619 ? 0.0038 F-MNIST vert. 0.8977 ? 0.0023 0.8981 ? 0.0022 0.7415 ? 0.0041 0.8921 ? 0.0024 <ref type="table">Table 6</ref>: Dissecting out-of-distribution detection with the ELBO. Error estimates obtained through bootstrapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and conclusion</head><p>We have introduced the probabilistic autoencoder, a simple generative model with a lower dimensional latent space that is motivated by probabilistic PCA. Different to variational autoencoders, it builds the probabilistic structure after the first stage of training, but has the advantage of being simple and straightforward to set up and train. Because it is first trained to achieve optimal reconstruction error and then, in a second stage, to produce optimal samples, it performs both tasks reliably and well. We further test its performance in two additional downstream tasks, which we think are particularly relevant for practical applications: anomaly detection and probabilistic image denoising and inputation. We find that the PAE performs all considered tasks at comparable quality as equivalent VAE models, suggesting that ELBO-based variational optimization is not an essential component of this class of models. We find that our proposed OoD metric of NF density in latent space, while a natural byproduct of PAE, can also be used in VAEs if they are complemented with an NF prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PAE on Celeb-A</head><p>To demonstrate the feasibility of the PAE approach on higher dimensional, more complex data, we train a PAE on Celeb-A. The celeb-A samples are cropped to the central 128x128 pixels and then downsampled to 64x64 pixels. We used the same preprocessing, architecture and training procedure as for the F-MNIST experiments and a latent space dimensionality of K = 64. Samples from this model are shown in <ref type="figure" target="#fig_6">figure 5</ref> on the left. On the right we show interpolations between images, produced by projecting two images from the test data set into the latent space of the NF, connecting them in the NF latent space by linear interpolation and sampling along the connecting line in equally spaced intervals. The samples then get forward modeled into data space to produce the images shown. The smooth transitions indicates that the PAE produces a continuous latent space without any holes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison to vanilla VAE</head><p>For completeness, we include here results for a conventional VAE, which shares the same architecture and training parameters as the models used in the other experiments, but uses a standard normal distribution as a prior. We trained the VAE on equation 13 with initial value of ? = 100, which was then linearly annealed during the first 100,000 steps. We show samples and reconstructions of this model in <ref type="figure" target="#fig_7">Fig 6 and</ref> list reconstruction error and FID scores in <ref type="table">Table 7</ref>. In all cases the results and visual images are inferior to flow-VAE and PAE described in the main text.</p><formula xml:id="formula_19">? 2 recon [?10 ?3 ] (?) P 95% (? 2 recon ) [?10 ?3 ] (?) FID Score (?) 12.90 ? 0.10</formula><p>68.00 ? 0.60 32.3 ? 0.3 <ref type="table">Table 7</ref>: Reconstruction errors and FID scores of vanilla ?-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison of anomaly detection with Abati et al.</head><p>We compare anomaly detection with the PAE to the results presented in <ref type="bibr" target="#b0">Abati et al. (2019)</ref>. Their set up is similar, but instead of a two-stage training, they propose a joint training of autoencoder and density estimator. Their loss function is a combination of reconstruction error and estimated density of encoded data. The contribution of the density term to the loss function is controlled by a tunable scalar parameter ?.</p><p>Using the same autoencoder architecture latent space dimension, and batch size 2 , we perform anomaly detection experiments as <ref type="bibr" target="#b0">Abati et al. (2019)</ref> but with a PAE-style two-stage training: We train models on each class of the CIFAR10 and MNIST datasets separately and then evaluate the outlier detection accuracy when each of these models is applied to the full test dataset containing all classes (considering all other classes outliers). Results of these experiments and the results from <ref type="bibr" target="#b0">Abati et al. (2019)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Interpolation studies</head><p>We expand on the image interpolation results shown in Appendix A by comparing PAE image interpolation with AE image interpolations and adding pixel-level interpolation (linear interpolation between pixel values) as a baseline. Results are shown in figure 7. While both AE and PAE produce fairly well interpolated images of high quality, PAE tends to produce more natural looking interpolations with fewer artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PAE posterior and application to data inputation</head><p>Large-scale data acquisition often results in noisy and incomplete data. In most applications, e.g. when one is interested in finding certain rare features in an image, the aim of data restoration is not only to obtain the most probable uncorrupted image, but also to obtain an estimate of its fidelity. A plethora of generative model based approaches for image reconstruction have been suggested in the literature <ref type="bibr" target="#b46">(Rezende et al., 2014;</ref><ref type="bibr" target="#b36">Mattei &amp; Frellsen, 2018;</ref><ref type="bibr" target="#b14">Dong et al., 2016;</ref><ref type="bibr" target="#b24">Jin et al., 2017;</ref><ref type="bibr" target="#b42">Putzky &amp; Welling, 2017;</ref><ref type="bibr" target="#b58">Ulyanov et al., 2020;</ref><ref type="bibr" target="#b5">Bora et al., 2018;</ref><ref type="bibr" target="#b37">Mattei &amp; Frellsen, 2019)</ref>, but few of them enable uncertainty quantification <ref type="bibr" target="#b4">(B?hm et al., 2019)</ref>. With the PAE, we can perform sound posterior-based data restoration. This does not only enable uncertainty quantification, it also provides a framework for consistently including analytical data models, such as physics models.</p><p>A latent space posterior for a corrupted data pointx = M x+n, where M is a pixel-wise mask and n denotes the noise, consists of an implicit likelihood and a prior. The form of the implicit likelihood is determined by the noise properties. For Gaussian noise, n, with noise covariance ? noise (typically a diagonal matrix) and a generative model g ? , trained on uncorrupted data, the implicit likelihood is given by</p><formula xml:id="formula_20">p ? (x|z, M , ? noise ) = N x|M g ? (z), ? 2 recon + ? 2 noise .<label>(19)</label></formula><p>Note that the covariance of this Gaussian likelihood is composed of the generative model's reconstruction error, ? 2 recon , and the noise level in the corrupted data, ? 2 noise . For sufficiently high latent space dimensionalities the latter dominates, ? recon,i ? noise,i , ensuring that the likelihood is well approximated by a Gaussian. By replacing x by its generative process, g ? (z), we bring the inference problem to the low dimensional latent space of the generative model. Posterior analysis of high-dimensional data becomes computationally tractable in this lower dimensional space.  </p><p>To denoise and inpaint a corrupted image one performs latent space posterior analysis. A point estimate is given by the MAP, z , the maximum of equation 20, which forward modeled into data space, x recon = g ? (z ), yields the most likely underlying image. A full posterior analysis can be performed with many techniques, including Laplace approximation, VI or MCMC sampling. Given the multi-modal posterior of some of our examples we fit a full rank Gaussian mixture model to equation 20 following <ref type="bibr" target="#b49">(Seljak &amp; Yu, 2019)</ref> in our experiments. We can then sample from this model to obtain other solutions that are compatible with the data.</p><p>We examine probabilistic reconstruction with both PAE and flow-VAE on three examples created by corrupting test data with uncorrelated Gaussian noise (? n =0, ? n =0.1) and masks. The true underlying images are shown in the first column of <ref type="figure" target="#fig_10">figure 8</ref> and the corrupted input data to the reconstructions in the second column. The masked areas were chosen to allow for several plausible inpainting solutions. To obtain reconstructions we minimize the negative of the log posterior in equation 20. Since we expect multimodal posteriors we run 20 minimizations starting from random points which we draw from the prior. We keep only those minimization results that are associated with a positive definite Hessian (true minima and no saddle points). In the third and fourth column we show the forward modeled deepest minimum found by this procedure for the PAE (third column) and flow-VAE model (fourth column). The reconstructed examples are well denoised, but flow-VAE and PAE find slightly different inpainting solutions for the masked areas. This is most visible for the first example, where the flow-VAE prefers a clasp on the bag, while the VAE reconstructs a dint at the top. While this suggests that the models have learned slightly different priors and forward models, we find that all reconstructions are very plausible and that the PAE does not seem to perform worse at this task than the flow-VAE. In fact, in the first example, the PAE seems to better reconstruct the area outside of the masked area (e.g. the ribbon on the left), a consequence of the lower reconstruction error of the PAE model.</p><p>To obtain uncertainty estimates we fit a Gaussian mixture model using the local minima associated with the largest posterior mass. We use the thus constructed posterior approximation for uncertainty estimation by generating samples from this posterior. The samples are shown in <ref type="figure" target="#fig_11">figure 9</ref>, with samples from the PAE posterior in the top row and samples from the flow-VAE in the bottom row. We observe some variety in these samples, which we encouraged by posing problems that should have multiple plausible solutions. For example the PAE finds that handbags with different dint depths are compatible with the data. The flow-VAE mostly prefers handbags with clasps, but it also produces two samples without a clasp. It seems that neither of the two models explores the full variety of potential solutions (the flow-VAE missing the dints and the PAE missing the clasps). In the second and third example the PAE model seems to provide some greater variety in the samples. This could be an indicator that the flow-VAE posterior is more complex and wasn't explored properly by our poterior analysis. It is not in scope of this work to explore these possibilities in great detail, instead, we note that flow-VAE and PAE perform comparably well at this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Model architectures and training procedures</head><p>We use the same encoder and decoder architecture in all experiments. Details are given in <ref type="table" target="#tab_0">table 10 and  table 11</ref>. The only model parameter that is varied is the dropout rate, which was set to 0.15 in models where it was necessary to avoid overfitting and to zero otherwise. The choice of hyperparameters is described in detail in section 6.3.</p><p>The normalizing flow(s) we use are composed of the same building blocks in all experiments but vary in how often the building blocks are repeated. All blocks have the same structure and only differ in the type of transformation that is performed. A block is made up of a transformation of the first half of data variables, a swapping of the first half of data variables with the second half, a transformation on the other half of variables and finally a trainable permutation of the variables. Block 1 applies a neural spline flow transformation, Block 2 a realNVP transformation with shifting and rescaling, Block 3 a realNVP transformation with only       <ref type="table" target="#tab_0">Table 13</ref>: Networks used to determine bin widths and slopes in the rational quadratic spline of the neural spline flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Flow-VAE ablation studies</head><p>We ran several flow-VAE models with different values of ?, with ? being the scale parameter in the implicit ELBO likelihood, a multivariate Gaussian with ?=f ? (z) and ?=?1, p ? (x|z) = G(x|f ? (z), ?1).</p><p>The objective of this ablation study is to test whether the flow-VAE performance depends on this parameter. Similar to the ?-parameter, ? could have an influence on the relative contribution of distortion and rate term to satisfying the ELBO objective during training. The results of this ablation study (in terms of reconstruction error and sample quality) are listed in table 15. We do not find a strong dependence of neither reconstruction error nor sample quality on ?. Somewhat counter-intuitively we get slightly higher reconstruction errors for lower values of ?, an indication that the model is starting to overfit. A test run with ? = 0.05 (not listed in the table) resulted in catastrophic overfitting. A slightly higher value, ? = 0.1, seems to be a sweet spot, with the lowest reconstruction error, no signs of overfitting and good FID score. This is the model we use in the result section. We note that the other values of ? we tried would not change our results in a way that invalidates our comparison or conclusions. We also note that such an ablation study/ parameter optimization is not needed for the PAE training, which highlights an advantage of the PAE approach.   <ref type="table" target="#tab_0">Table 15</ref>: Flow-VAE ablation study: reconstruction error and sample quality as a function of ?, the width of the Gaussian likelihood entering the distortion term in the ELBO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic diagram of the PAE (left panel) and an illustration of the sampling procedure from the PAE (right panel). The autoencoder networks are depicted as gray trapezia, the normalizing flow is represented by black arrows and the latent spaces of the autoencoder and normalizing flow are shown in red and blue, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Principal component analysis of FashionMNIST and MNIST data sets and outlier detection accuracy (in-distribution: Fashion MNIST, out-of-distribution: MNIST) with equation 4 and equation 15 as a function of included number of PCA components. A higher AUROC value corresponds to a better separation between in-and out-of-distribution data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1. A VAE with a normalizing flow prior, where the encoder/decoder pair is trained on equation 13 with ?=0, i.e. without the KL-Divergence term and the variance of q(z) is kept constant. The normalizing flow is trained in a second stage on the encoded distribution. The ELBO with ?=1 and the normalizing flow prior is used as a density estimator. We call this model ? 0 -VAE. The difference to the PAE lies in the noisy estimation of the likelihood during training. Comparing the PAE to the ? 0 -VAE tests whether training on the reconstruction error instead of the distortion term offers any advantage. 2. A VAE with a normalizing flow prior that is trained on the ELBO with ?=1. We call this model flow-VAE. The difference to the PAE lies in the training procedure. In the flow-VAE, the normalizing flow, encoder and decoder are trained jointly under the ELBO training objective. This means that the normalizing flow is trained on samples from the approximate posterior instead of the encoded samples, which are used in the PAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparison of F-MNIST reconstructions with different models. From left to right: original data, ? 0 -VAE, PAE, flow-VAE, flow-VAE(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison of random samples. From left to right: ? 0 -VAE, PAE, flow-VAE, flow-VAE (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>score (?) 32.8 ? 0.4 28.4 ? 0.3 22.8 ? 0.3 33.9 ? 0.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>PAE performance on Celeb-A at K=64. Samples (left) reach FID=49.2 (reconstructions FID=44.0). Right: Interpolations between samples from the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>From left to right: Original test images, their reconstructions and samples generated with the vanilla ?-VAE described in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Linear interpolations between images. in each panel we show in the top row the reconstructions from linear interpolations in PAE latent space, in the middle row from AE latent space, and in the bottom row linear interpolations in data space (linear interpolation between pixel values).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The prior is modeled by the generative models' latent space distribution. Combining the two, we obtain the log posterior ln p ?,? (z|x, M , ? noise ) = ln p ? (x|z, M , ? noise ) + ln p ? (z) ? const.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Underlying true data (left column), corrupted data (second column) and most likely reconstructions with the PAE (third column) and flow-VAE (fourth column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Samples from the reconstruction posteriors (PAE: top row, flow-VAE: bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>summarizes the different models and the parameters in which they differ. To allow for a fair com- parison, we use the same model architecture and training parameters for all experiments, except in cases where fixing them might disadvantage the VAE. Encoder and decoder networks are loosely based on the infoGAN architecture (Chen et al., 2016). Normalizing flows are constructed from RealNVP transforma- tions (Dinh et al., 2017), Neural Spline Flow (NSF) transformations (Durkan et al., 2019) and trainable permutations (Kingma &amp; Dhariwal, 2018) (GLOW). The exact model architectures are listed in table F and the choice of parameters is detailed below in table 6.3. Because we use non-binarized data, we use a Gaussian implicit likelihood in our VAE models (instead of a Bernoulli likelihood which would be the suitable choice for binarized data)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>L ??VAE , L NF L AE ,L NF L flow?VAE</figDesc><table><row><cell></cell><cell cols="2">model parameters</cell><cell></cell></row><row><cell>model name</cell><cell>? 0 -VAE</cell><cell>PAE</cell><cell>flow-VAE</cell></row><row><cell>training objective(s) flow prior</cell><cell>TRUE</cell><cell>TRUE</cell><cell>TRUE</cell></row><row><cell>2 stage training</cell><cell>TRUE</cell><cell>TRUE</cell><cell>FALSE</cell></row><row><cell>?</cell><cell>0</cell><cell>N/A</cell><cell>1</cell></row><row><cell>dropout rate</cell><cell>0.15</cell><cell>0.15</cell><cell>N/A</cell></row><row><cell>OoD metric</cell><cell>ELBO</cell><cell>log p(z)</cell><cell>ELBO</cell></row></table><note>and Omniglot (Lake et al., 2015), as well as horizontal and vertical flips of F-MNIST test data. We preprocess the data by dequantizing (adding uniform noise ? [?1/256, 1/256]) before rescaling pixel values to the interval [-0.5,0.5].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table 3 ,</head><label>3</label><figDesc>the reported errors were obtained through bootstrapping on the test data.</figDesc><table><row><cell></cell><cell cols="2">reconstruction quality</cell><cell></cell><cell></cell></row><row><cell>model name</cell><cell>? 0 -VAE</cell><cell>PAE</cell><cell>flow-VAE</cell><cell>flow-VAE(s)</cell></row><row><cell cols="5">? 2 recon [?10 ?3 ] (?) P 95% (? 2 recon ) [?10 ?3 ] (?) 29.6 ? 0.3 6.24 ? 0.06 5.87?0.06 6.22 ? 0.07 6.66 ? 0.06 27.6 ? 0.3 35.7 ? 0.4 31.9 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of F-MNIST models in terms of reconstruction quality. The PAE model achieves the lowest reconstruction errors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Sample quality, measured in terms of the FID score. Lower values are better. Error estimates obtained through bootstrapping.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Out of distribution detection with different models. The outlier detection accuracy is measured in terms of the AUROC? [0, 1]. Higher values are better. Error estimates obtained through bootstrapping.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison of MNIST anomaly detection results.</figDesc><table><row><cell>Class</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>mean</cell></row><row><cell cols="3">Abati et al. 0.735 0.58</cell><cell cols="9">0.69 0.542 0.761 0.546 0.751 0.535 0.717 0.548 0.608</cell></row><row><cell>PAE</cell><cell cols="11">0.737 0.472 0.684 0.52 0.749 0.506 0.758 0.529 0.682 0.446 0.604</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison of CIFAR anomaly detection results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>The layout of the encoder network used in all experiments.</figDesc><table><row><cell></cell><cell>decoder</cell></row><row><cell>layer</cell><cell>details</cell></row><row><cell>input</cell><cell>encoded data, latent size=40</cell></row><row><cell>linear</cell><cell>output size=1024</cell></row><row><cell>batch norm</cell><cell>momentum=0.999, =1e-5</cell></row><row><cell>leakyReLU</cell><cell>?= 0.2</cell></row><row><cell>linear</cell><cell>output size=128x28/4x28/4</cell></row><row><cell>batch norm</cell><cell>momentum=0.999, =1e-5</cell></row><row><cell>leakyReLU</cell><cell>?=0.2</cell></row><row><cell>dropout</cell><cell>dropout rate dependent on model, see 2</cell></row><row><cell>reshape</cell><cell>output size=[28/4,28/4,128]</cell></row><row><cell cols="2">transpose convolution output shape=[28/2,28/2,64], kernel size =[4,4] , strides=[2,2]</cell></row><row><cell>batch norm</cell><cell>momentum=0.999, =1e-5</cell></row><row><cell>leakyReLU</cell><cell>?=0.2</cell></row><row><cell>dropout</cell><cell>dropout rate dependent on model, see 2</cell></row><row><cell cols="2">transpose convolution output shape=[28,28,1], kernel size =[4,4] , strides = [2,2]</cell></row><row><cell>sigmoid</cell><cell>subtract 0.5 to match input data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>The layout of the decoder network used in all experiments. shifting. The architectural details are given in table 12. We use two different layouts in our experiments, a deeper flow with [2x block 1, 4x block 2 and 4x block 3] and a simpler flow with [1x block 1, 2x block 2 and 1x block 3]. For our Celeb-A experiments, we use a normalizing flow consisting of [1x block 1, 2x block 2 and 2x block 3]. Every flow also features a re-scale operation that ensures that the encoded training samples lie within the range z i ? [?1, 1].The neural spline flow uses two networks to determine the bin widths and slopes of the rational quadratic splines. The layouts of these networks are listed in table 13.</figDesc><table><row><cell></cell><cell></cell><cell>normalizing flow</cell></row><row><cell>blocks</cell><cell>layer</cell><cell>details</cell></row><row><cell></cell><cell>rescale</cell><cell>scale dependent on model, see 14</cell></row><row><cell>1</cell><cell cols="2">neural spline transform bins=36, bins: bin network, slopes: slope network</cell></row><row><cell></cell><cell>swap permutation</cell><cell>swap first half of dimensions with second half</cell></row><row><cell></cell><cell cols="2">neural spline transform bins=36, bins: bin network, slopes: slope network</cell></row><row><cell></cell><cell>trainable permutation</cell><cell>GLOW-style LU decomposition of orthogonal matrix</cell></row><row><cell>2</cell><cell>real NVP transform</cell><cell>trainable shift and rescale</cell></row><row><cell></cell><cell>swap permutation</cell><cell>swap first half of dimensions with second half</cell></row><row><cell></cell><cell>real NVP transform</cell><cell>trainable shift and rescale</cell></row><row><cell></cell><cell>trainable permutation</cell><cell>GLOW-style LU decomposition of orthogonal matrix</cell></row><row><cell>3</cell><cell>real NVP transform</cell><cell>trainable shift</cell></row><row><cell></cell><cell>swap permutation</cell><cell>swap first half of dimensions with second half</cell></row><row><cell></cell><cell>real NVP transform</cell><cell>trainable shift</cell></row><row><cell></cell><cell>trainable permutation</cell><cell>GLOW-style LU decomposition of orthogonal matrix</cell></row><row><cell></cell><cell>rescale</cell><cell>1/scale dependent on model, see table 14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Building blocks of the normalizing flows used in all experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>bin network</cell></row><row><cell></cell><cell>slope network</cell><cell>layers</cell><cell>details</cell></row><row><cell>layers</cell><cell>details</cell><cell>dense</cell><cell>output size=latent size/2</cell></row><row><cell>dense</cell><cell>output size=latent size//2</cell><cell cols="2">leakyReLU ?=0.2</cell></row><row><cell cols="2">leakyReLU ?=0.2</cell><cell>dense</cell><cell>output size=latent size/2</cell></row><row><cell>dense</cell><cell>output size=bins-1</cell><cell cols="2">leakyReLU ?=0.2</cell></row><row><cell>reshape</cell><cell>[latent size/2, bins-1]</cell><cell>dense</cell><cell>output size=bins</cell></row><row><cell>softplus</cell><cell>softplus(x)+1e-2</cell><cell>reshape</cell><cell>[latent size/2, bins]</cell></row><row><cell></cell><cell></cell><cell>softmax</cell><cell>softmax(x)(2-1e-2 bins)+1e-2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Normalizing flow layouts of different models. Blocks are described in table 12.</figDesc><table><row><cell></cell><cell>?-ablation study</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.08</cell><cell>0.1</cell><cell>0.12</cell></row><row><cell cols="4">? 2 recon [?10 ?3 ] (?) P 95% (? 2 recon ) [?10 ?3 ] (?) 36.5 ? 0.3 7.85 ? 0.09 6.22 ? 0.07 8.35 ? 0.09 35.7 ? 0.4 39.9 ? 0.5</cell></row><row><cell>sample FID score (?)</cell><cell cols="2">22.5 ? 0.2 22.8 ? 0.3</cell><cell>22.8 ? 0.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/VMBoehm/PAE-ablation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">obtained from https://github.com/aimagelab/novelty-detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published in Transactions onMachine Learning Research (09/2022)   </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00057</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Abati_Latent_Space_Autoregression_for_Novelty_Detection_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxQzBceg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fixing a broken ELBO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/alemi18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Resampled priors for variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v89/bauer19a.html" />
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<editor>Kamalika Chaudhuri and Masashi Sugiyama</editor>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Uncertainty quantification with generative models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Lanusse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uros</forename><surname>Seljak</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.10046" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ambientgan: Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hy7fDog0b" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/K/K16/K16-1002.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flows for simultaneous manifold learning and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Cranmer</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/051928341be67dcba03f0e04104d9047-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BysvGP5ee" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shrinkage algorithms for mmse covariance estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<idno>1941-0476. doi: 10.1109/ TSP.2010.2053029</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5016" to="5029" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diagnosing and enhancing VAE models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1e0X3C9tQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NICE: non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.8516" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkpbnH9lx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2439281</idno>
		<idno>doi: 10.1109/ TPAMI.2015.2439281</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2015.2439281" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From variational to deterministic autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1g7tpEYDS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sherjil Ozair, Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FFJORD: free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxgknCcK7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
	<note>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy2fzU9gl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tight upper bound on mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Hled?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Sokolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gasper</forename><surname>Tkacik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ITW44776.2019.8989292</idno>
		<ptr target="https://doi.org/10.1109/ITW44776.2019.8989292" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Information Theory Workshop, ITW 2019</title>
		<meeting><address><addrLine>Visby, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ELBO surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Approximate Bayesian Inference, NIPS 2016 Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Kyong Hwan Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unser</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2713099</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2713099" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Nicol? Cesa-Bianchi, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
	<note>Samy Bengio,</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving variational inference with inverse autoregressive flow. CoRR, abs/1606.04934</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.04934" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized autoencoders via relaxed injective probability flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v108/kumar20a.html" />
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
		<editor>Silvia Chiappa and Roberto Calandra</editor>
		<meeting><address><addrLine>Online [Palermo, Sicily, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-28" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4292" to="4301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno>0036-8075. doi: 10.1126/ science.aab3050</idno>
		<ptr target="https://science.sciencemag.org/content/350/6266/1332" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A well-conditioned estimator for large-dimensional covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0047-259X(03)00096-4</idno>
		<ptr target="https://doi.org/10.1016/S0047-259X(03)00096-4" />
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="411" />
			<date type="published" when="2004-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/li15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1511.05644</idno>
		<ptr target="http://arxiv.org/abs/1511.05644" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Leveraging the exact likelihood of deep latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Alexandre</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/0609154fa35b3194026346c9cac2a248-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Nicol? Cesa-Bianchi, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="3859" to="3870" />
		</imprint>
	</monogr>
	<note>Samy Bengio,</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MIWAE: deep generative modelling and imputation of incomplete data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Alexandre</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes</forename><surname>Frellsen</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/mattei19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4413" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Density of states estimation for out of distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><forename type="middle">R</forename><surname>Morningstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cusuh</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v130/morningstar21a.html" />
	</analytic>
	<monogr>
		<title level="m">The 24th International Conference on Artificial Intelligence and Statistics, AIS-TATS 2021</title>
		<editor>Arindam Banerjee and Kenji Fukumizu</editor>
		<imprint>
			<publisher>Virtual Event</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="3232" to="3240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1xwNhCcYm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multilayer Calorimeters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Luke De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nachman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.120.042003</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42003</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Putzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04008</idno>
		<title level="m">Recurrent Inference Machines for Solving Inverse Problems. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fully probabilistic quasar continua predictions near Lyman-alpha with conditional neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tamanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Xavier</forename><surname>Prochaska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominika</forename><surname>?urov??kov?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00615</idno>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/rezende15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/rezende14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">High-dimensional probability estimation with deep density models. CoRR, abs/1302</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Prescott</forename><surname>Adams</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1302.5125" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/salimans15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uros</forename><surname>Seljak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonghee</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04454</idno>
		<title level="m">Posterior inference unchained with EL_2O. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uros</forename><surname>Seljak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwei</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11638</idno>
		<title level="m">Unsupervised in-distribution anomaly detection of new physics through conditional density estimation. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.308" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A generative model of galactic dust emission using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lloyd</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Prabhu</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stab1011</idno>
	</analytic>
	<monogr>
		<title level="j">MNRAS</title>
		<imprint>
			<biblScope unit="volume">504</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2603" to="2613" />
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">https:/rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00196</idno>
		<ptr target="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00196" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The information bottleneck method. CoRR, physics/0004057</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/physics/0004057" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkL7n1-0b" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">VAE with a vampprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v84/tomczak18a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics, AISTATS</title>
		<editor>Amos J. Storkey and Fernando P?rez-Cruz</editor>
		<meeting><address><addrLine>Playa Blanca, Lanzarote, Canary Islands, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational gaussian process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06499" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-020-01303-4</idno>
		<ptr target="https://doi.org/10.1007/s11263-020-01303-4" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1867" to="1888" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<ptr target="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html" />
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.07747" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Generative latent flow: A framework for nonadversarial image generation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-An</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.10485" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
