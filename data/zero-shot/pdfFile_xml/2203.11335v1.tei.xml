<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Matching with Overlapping Attention for Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Global Matching with Overlapping Attention for Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical flow estimation is a fundamental task in computer vision. Recent direct-regression methods using deep neural networks achieve remarkable performance improvement. However, they do not explicitly capture long-term motion correspondences and thus cannot handle large motions effectively. In this paper, inspired by the traditional matching-optimization methods where matching is introduced to handle large displacements before energy-based optimizations, we introduce a simple but effective global matching step before the direct regression and develop a learning-based matching-optimization framework, namely GMFlowNet. In GMFlowNet, global matching is efficiently calculated by applying argmax on 4D cost volumes. Additionally, to improve the matching quality, we propose patch-based overlapping attention to extract large context features. Extensive experiments demonstrate that GM-FlowNet outperforms RAFT, the most popular optimizationonly method, by a large margin and achieves state-ofthe-art performance on standard benchmarks. Thanks to the matching and overlapping attention, GMFlowNet obtains major improvements on the predictions for textureless regions and large motions. Our code is made publicly available at https://github.com/xiaofeng94/ GMFlowNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimation is a key computer vision task, which benefits various applications, including video interpolation <ref type="bibr" target="#b24">[25]</ref>, deblurring <ref type="bibr" target="#b53">[53]</ref>, video segmentation <ref type="bibr" target="#b44">[44]</ref> and action recognition <ref type="bibr" target="#b37">[38]</ref>. Prevalent work in this area has been largely dominated by either matching-optimization or direct-regression methods. Previous energy-based optimization methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref> usually fail to handle large displacements due to their inability to capture long-term motion correspondences. To remedy this, matchingoptimization methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">48]</ref> introduce a matching step <ref type="bibr">Figure 1</ref>. Main frameworks for optical flow estimation. (a) Traditional matching-optimization methods first build a sparse matching to get a coarse flow and then exploit energy-based optimization to refine the flow. (b) Direct-regression methods mimic the energybased optimization with learned parameters. They can be regarded as learning-based optimizations without matching. (c) Our framework introduces matching before the learning-based optimization and further improves the performance. before the optimization, which aims to find correspondences between pixels or patches across frames. However, their matching process depends on complicated handcrafted features and is time-consuming and inaccurate.</p><p>Recent direct-regression methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">55]</ref> regard optical flow estimation as a regression task and achieve considerable improvements especially in predicting small changes in optical flow. These methods typically calculate 4D cost volumes representing the similarity between pixels and then directly regress flows from cost volumes by neural networks. Similar to energy-based optimization, directregression methods cannot capture long-term motion correspondences in an explicit way and thus suffer from a perfor-mance drop in areas with large motions.</p><p>In this paper, we incorporate a matching step to explicitly handle large displacements for direct-regression methods, inspired by the improvement matching-optimization methods brought to energy-based optimization approaches. Based on this idea, we develop a novel framework for optical flow estimation, namely Global Matching Flow Network (GMFlowNet), where global matching is introduced before the direct regression. Unlike traditional methods, GMFlowNet provides an efficient and accurate matching step. For efficiency, we apply argmax to the typical 4D cost volume to build the global matching since it results in minor computational overhead. For accuracy, we propose a Patch-based OverLapping Attention (POLA) block to extract large context features to diminish regional ambiguities in matching, e.g., repeated patterns and textureless regions. Specifically, POLA divides input feature maps into patches and attends each patch with itself and its neighboring patches. Since direct-regression methods mimic the traditional energy-based optimizations in a data-driven manner <ref type="bibr" target="#b41">[42]</ref>, they can be interpreted as learning-based optimizations. Thus, our method can be regarded as a learning-based matching-optimization framework. <ref type="figure">Fig. 1</ref> illustrates differences between previous related frameworks and ours.</p><p>We evaluate GMFlowNet on standard datasets for optical flow estimation. Extensive experiments demonstrate that GMFlowNet significantly outperforms the most popular optimization-only model RAFT <ref type="bibr" target="#b41">[42]</ref> and achieves stateof-the-art performance. As expected, GMFlowNet provides better flow estimations especially for large motion areas and textureless regions. Besides, we thoroughly investigate our global matching and POLA, showing that they are both effective and efficient.</p><p>Our contributions are summarized as follows: 1) We introduce a global matching step to explicitly handle large displacement optical flow estimations for direct-regression methods. With typical 4D cost volumes, our global matching is effective and efficient. 2) We propose a welldesigned Patch-based OverLapping Attention (POLA) to address local ambiguities in matching and demonstrate its effectiveness via extensive experiments. 3) Following traditional matching-optimization frameworks, we propose a learning-based matching-optimization framework named GMFlowNet that achieves state of the art performance on standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Optical flow as energy optimization. Previous methods formulated the optical flow as a continuous global energy function optimization problem <ref type="bibr" target="#b17">[18]</ref>. Black and Anandan <ref type="bibr" target="#b4">[5]</ref> introduced a robust estimation framework to address outliers caused by occlusions or significant brightness variations. Later research made further improvements by using better regularization terms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">54]</ref> or additional robust optimization terms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. However, these approaches lack the ability to compute long-term dependencies and, thus, only work well for small displacements. To handle large displacements, later methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> introduced the coarse-tofine strategy where large and small displacements are handled at different levels of an image pyramid.</p><p>However, coarse-to-fine approaches can neither handle small and fast-moving objects that disappear at coarse levels nor remedy mistakes made in the early stages. To address those issues, Brox and Malik <ref type="bibr" target="#b6">[7]</ref> introduced feature matching to the energy-based optimization framework, which was further improved in later works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b50">50]</ref>. Following studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">49]</ref> widely adopted this approach. However, all these studies consider global matching highly timeconsuming, so they only conducted local matching for computation efficiency, e.g., EpicFlow <ref type="bibr" target="#b35">[36]</ref>. Contrary to previous methods, we calculate global matching efficiently by applying the argmax operator on widely adopted 4D cost volumes and achieve better performance.</p><p>Optical flow as network regression. More recently, the community has been motivated by the success of CNNs on high-level vision tasks <ref type="bibr" target="#b27">[28]</ref> to exploit learning-based solutions for optical flow estimation. Relevant studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56]</ref> typically formulate optical flow estimation as regression instead of matching. In regression, cost volumes are the critical component that represents the similarity between pixels. For example, Sun et al. <ref type="bibr" target="#b38">[39]</ref> designed a network using stacked image pyramids, feature warping, and cost volumes. Hofinger et al. <ref type="bibr" target="#b16">[17]</ref> employed a sampling-based strategy to improve the calculation of cost volumes. Teed and Deng <ref type="bibr" target="#b41">[42]</ref> built 4D cost volumes for all pairs of pixels. However, due to the high cost of memory and time, they did not aggregate the cost volumes to involve the global information. Separable Flow <ref type="bibr" target="#b55">[55]</ref> proposed a separable cost volume module for efficient aggregations. In this work, we sidestep the high-cost global aggregation and leverage global information by constructing global matching using existing 4D cost volumes.</p><p>Attention mechanism in vision. This work extracts large context information for matching via leveraging recent advances in Vision Transformers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. Methods leveraging Transformers' ability of modeling long-term dependencies have outperformed convolutional neural networks in various high-level computer vision tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">43]</ref>. Inspired by these, Jiang et al. <ref type="bibr" target="#b25">[26]</ref> introduced an attentionbased module to resolve occlusions for optical flow estimation. Furthermore, LoFTR <ref type="bibr" target="#b40">[41]</ref> adopted the self-and crossattention to extract better descriptors for feature matching. Prevailing Vision Transformer architectures, e.g., Swin Transformer <ref type="bibr" target="#b28">[29]</ref>, conduct indirect inter-patch information exchange with shifted windows. We propose POLA to exchange information across patches directly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching to Flow</head><formula xml:id="formula_0">(1/8) 2 HW (1/8) 2 HW 4D Cost Volume i j 1/8 i j h k F 1 F 2 F 1 F 2 I 1 , I 2 f 0 1!2 f T 1!2 1/8 A ? N Patch-based Overlapping Attention (POLA) 9M 2 ? d M 2 ? d Figure 2.</formula><p>Overview of GMFlowNet. GMFlowNet has three components: 1) The large context feature extraction module generates initial features from 3 convolutional layers and adopts the proposed POLA to extract large context information. N refers to the number of attention blocks.</p><p>2) The global matching module adopts large context features and constructs a 4D cost volume. Then, a global matching is built by applying argmax on the cost volume and refined by mutual matching. A coarse flow f 0 1?2 is generated from the matching.</p><p>3) The optimization module takes f 0 1?2 as the initial state and updates the flow estimation iteratively. T refers to the number of iterations. We employ the off-the-shelf optimization from RAFT <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We propose a novel framework GMFlowNet where a simple and effective global matching is introduced before the learning-based optimization. Our GMFlowNet consists of three modules, namely, large context feature extraction, global matching, and learning-based optimization. <ref type="figure">Fig. 2</ref> provides an overview of GMFlowNet, and each module is elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Large Context Feature Extraction</head><p>Large context information is the key to handle matching in locally ambiguous locations, e.g. repeated patterns and textureless regions. GMFlowNet first employs 3 convolutional layers (3-Convs) to extract initial features and then adopts Transformer blocks to include long-term dependency information. Due to the large dimension of image features, it's computationally prohibitive to apply vanilla self-attention <ref type="bibr" target="#b45">[45]</ref> on whole feature maps. To reduce the computation cost, we propose a well-designed local attention module POLA for optical flow estimation. In this section, we first describe attention in Tranformer and then we introduce POLA. In the end, we compare POLA with other feature extractors and discuss why ours is better for our task.</p><p>Attention in Transformer. Given query vectors Q ? R Nq?d , key vectors K ? R N k ?d , and value vectors V ? R Nv?d , where d is the feature dimension, attention module attends Q with V by the similarity between Q and K. Additionally, Ramachandran et al. <ref type="bibr" target="#b33">[34]</ref> suggest a learned relative position bias B ? R Nq?N k for better performance, and the attention is calculated as,</p><formula xml:id="formula_1">Attention(Q, K, V ) = softmax(QK T / ? d + B) ? V. (1)</formula><p>For more details about Transformers, please refer to <ref type="bibr" target="#b45">[45]</ref>.</p><p>Patch-based overlapping attention. Our POLA divides features into M ? M non-overlapping patches and attends every patch with itself and its eight neighboring patches. <ref type="figure" target="#fig_1">Fig. 3a</ref> illustrates our POLA with M = 2. Following prior work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">45]</ref>, we adopt multi-head attentions in our attention block, as well. Given a patch vectorized as P ? R M 2 ?d and its surrounding 3 ? 3 patches vectorized as S ? R 9M 2 ?d , for the i-th head of our attention, we first project P and S into d k dimensions by learned linear projections and denote the projected results as P i and S i , respectively. Then, we perform attention with P i and S i and get the output h i . Finally, we concatenate h i from all heads as H and project H to d dimensions as the final result O ? R M 2 ?d . Our multi-head patch-based overlapping attention can be formulated as,</p><formula xml:id="formula_2">h i = Attention(L Q i (P ), L K i (S), L V i (S)). H = Concat([h 1 , h 2 , . . . , h n ]) O = L O (H).<label>(2)</label></formula><p>Here n is the number of heads,</p><formula xml:id="formula_3">L Q i , L K i , L V i and L O are linear projection functions.</formula><p>In the experiments, we set n = 8 and d k = d/n.</p><p>Why POLA is an improved attention method. Swin Transformer <ref type="bibr" target="#b28">[29]</ref> provides a general local attention mechanism for vision tasks with windows and shifted windows as shown in <ref type="figure" target="#fig_1">Fig. 3b</ref>. However, the shifted window scheme requires two individual attention blocks to propagates interpatch features, leading to information loss. Such loss is especially detrimental to matching because matching heavily depends on context information to reduce local ambiguities. By contrast, our POLA involves inter-patch features within one block and propagates information directly with less information loss. Moreover, POLA can be viewed as a generalization of per pixel overlapping attention that has been explored in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>. Compared with the per-pixel one, POLA enjoys at least three advantages: 1) consumes less memory, 2) can be efficiently implemented in existing deep learning platforms, and 3) arranges features by patch, which may provide better performance as suggested in recent research <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">43</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Matching</head><p>We extract the context features F 1 and F 2 for the first input image I 1 and the second input image I 2 , respectively. Then, a 4D cost volume is constructed on F 1 and F 2 . After that, a global matching is computed from the cost volume and outputs a coarse flow f 0 1?2 for I 1 and I 2 , which is taken as the initial state of the later optimization.</p><p>4D cost volume calculation. We follow prior work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref> to construct the 4D cost volume on 1/8 of the input resolution. The cost volume C is calculated as,</p><formula xml:id="formula_4">C(i, j, u, v) = F 1 (i, j) ? F 2 (u, v),<label>(3)</label></formula><p>where (i, j) and (u, v) refer to locations in F 1 and F 2 .</p><p>Matching confidence calculation. We adopt a dualsoftmax operator <ref type="bibr" target="#b36">[37]</ref> to convert the cost volume into matching confidence. This operator is efficient and enables the supervision of matching. In our case, the matching confidence P c is computed by,</p><formula xml:id="formula_5">P c (i, j, u, v) = softmax(C(i, j, ?)) softmax(C(?, u, v)),<label>(4)</label></formula><p>where C(i, j, ?) means all (u, v) for given (i, j). C(?, u, v) is similar. Pixel-wise production is denoted as .</p><p>Matching selection and flow generation. Based on P c , we obtain the matching for I 1 at (i, j) as</p><formula xml:id="formula_6">M 1?2 (i, j) = arg max u,v P c (i, j, u, v).<label>(5)</label></formula><p>The matching for I 2 , M 2?1 (u, v), is attained similarly. Then, we pick robust matches that satisfy both M 1?2 (i, j) and M 2?1 (u, v) and define the matching set M c as,</p><formula xml:id="formula_7">M c = {(?,?)|(?,?) = M 2?1 (M 1?2 (?,?))}.<label>(6)</label></formula><p>The coarse flow is computed as,</p><formula xml:id="formula_8">f 0 1?2 = M 1?2 (i, j) ? (i, j) (i, j) ? M c (0, 0) Otherwise .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>We use the off-the-shelf update operator from RAFT <ref type="bibr" target="#b41">[42]</ref> as our optimization. This optimization predicts a delta flow and adds it to the current flow estimation. It iterates on such additions and outputs a series of flow predictions</p><formula xml:id="formula_9">{f 1 1?2 , f 2 1?2 , . . . , f T 1?2 },</formula><p>where T is the total number of iterations and f T 1?2 is used as the final prediction. We initialize the optimization with our coarse flow f 0 1?2 instead of the zero flow used in <ref type="bibr" target="#b41">[42]</ref>. The optimization part in GMFlowNet is replaceable. We adopt RAFT's because it achieves the best performance. Any future optimization may be applied here for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Supervision</head><p>Matching loss. We round the ground truth optical flow f gt 1?2 to the pixel level and collect the ground truth matching set M gt c . We consider regions as matched if they appear in both frames and set occlusion areas as unmatched. As the supervision in feature matching <ref type="bibr" target="#b40">[41]</ref>, we minimize the negative log-likelihood of P c in matched regions as,</p><formula xml:id="formula_10">L M = ? 1 |M gt c | (?,?)?M gt c log P c (?,?)<label>(8)</label></formula><p>Optimization loss. We follow RAFT <ref type="bibr" target="#b41">[42]</ref> and supervise the optimization with 1 distance between the predicted flow and f gt . The optimization loss is defined as,</p><formula xml:id="formula_11">L O = T i=1 ? (i?T ) ||f gt 1?2 ? f i 1?2 || 1 .<label>(9)</label></formula><p>The overall loss function of GMFlowNet is,</p><formula xml:id="formula_12">L = L O + ?L M<label>(10)</label></formula><p>where ? balances different loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section elaborates on the experimental results to demonstrate the effectiveness of GMFlowNet. We show that GMFlowNet improves optical flow estimation when large motions and textureless regions are present based on both quantitative and qualitative evaluations. We also discuss the improvements in the results. An ablation study and an efficiency evaluation finalize the evaluation.</p><p>We implemented GMFlowNet in PyTorch <ref type="bibr" target="#b32">[33]</ref> and followed the training setting of RAFT <ref type="bibr" target="#b41">[42]</ref>. We first train our model on FlyingChairs <ref type="bibr" target="#b23">[24]</ref> (C) for 120k iterations (batch size of 10) and then finetune it on FlyingThings <ref type="bibr" target="#b29">[30]</ref> (T) for 160k iterations (batch size of 6). After that, our model is further finetuned on a combination of data from Fly-ingThings (T), Sintel with both clean and final passes <ref type="bibr" target="#b9">[10]</ref> (S), KITTI <ref type="bibr" target="#b30">[31]</ref> (K), and/or HD1K <ref type="bibr" target="#b26">[27]</ref> (H). In the following sections, C+T refers to FlyingChairs and FlyingThings. C+T+S/K means C+T with either Sintel or KITTI. C+T+S+K+H refers to all training datasets. We set the patch size to M = 7 for POLA and the feature dimension to d = 256. When evaluating on Sintel, we improve the model by replacing 4 heads of our POLA blocks with 2 vertical and 2 horizontal axial-attention heads that are proposed by Wang et al. <ref type="bibr" target="#b46">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluations</head><p>Evaluations on different displacements. Our global matching aims at addressing large motions explicitly. To evaluate its performance, we divide all regions of the Sintel training set (both clean and final passes) into different subsets, i.e., s10, s10-40, s40+, based on displacements. s10 refers to regions with displacements between 0 and 10, s10-40 for 10 and 40, and s40+ for larger than 40. Then, we train the optimization-only baseline model RAFT <ref type="bibr" target="#b41">[42]</ref> and GMFlowNet on C+T and evaluate them on the different subsets. on s10 and s10-40 but outperforms RAFT on s40 by 4.7%.</p><p>Those results indicate that GMFlowNet enjoys great improvements on regions with extremely large displacements, which demonstrates that the global matching with large context information is beneficial to handle large motions.</p><p>Cross-domain evaluations. Following previous studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">55]</ref>, we trained the proposed GMFlowNet on C+T and evaluated it on the training sets of Sintel and KITTI as cross-domain evaluations. <ref type="table">Table 2</ref> displays the results of GMFlowNet and other competitive approaches. As a common practice, AEPE is reported for Sintel. Fl-epe and Fl-all are reported for KITTI.</p><p>As shown, GMFlowNet is close to the best method Separable Flow <ref type="bibr" target="#b55">[55]</ref> on Sintel Final and achieves better performance on the other datasets. Our method achieves an AEPE of 1.14 on Sintel Clean, a Fl-all of 15.4 on KITTI, which are 19.6% and 11.5% better than the optimizationonly baseline, RAFT. Those results demonstrate that GM-FlowNet boasts a better generalization ability than RAFT as well as other methods. Considering that GMFlowNet and RAFT share the same optimization stage, we attribute the huge improvement in generalization to our global matching. We believe this is a fair claim because RAFT exploits regression but GMFlowNet considers both matching and regression. Since regression is more likely to overfit specific datasets than matching, GMFlowNet generalizes better.</p><p>Evaluations on standard benchmarks. We evaluate GMFlowNet on standard online benchmarks, i.e., Sintel <ref type="bibr" target="#b9">[10]</ref> and KITTI <ref type="bibr" target="#b30">[31]</ref>. For a fair comparison, we follow previous methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">55]</ref> and train GMFlowNet on C+T+S/K and C+T+S+K+H, respectively. <ref type="table">Table 2</ref> exhibits the evaluation results. GMFlowNet adopts the optimization process of RAFT, but outperforms RAFT by a large margin. Moreover, GMFlowNet outperforms the state-ofthe-art method Separable Flow <ref type="bibr" target="#b55">[55]</ref> on Sintel, but achieves slightly lower performance on KITTI. This is probably be-  <ref type="table">Table 2</ref>. Quantitative results on Sintel and KITTI datasets. "C+T": We test the generalization ability on Sintel and KITTI training sets after training on FlyingChairs (C) and FlyingThing (T). "C+T+S/K": We train models on C+T and finetune them on either Sintel (S) or KITTI (T) and evaluate on the test set of S or T. "C+T+S+K+H": Our training set contains training samples from C, T, S, K and HD1K (H). Parentheses denote results on the training set. The best and runner up results are highlighted in bold and underlined, respectively. *We report results of the 2-view setting that is adopted by other methods.</p><p>cause GMFlowNet adopts attention blocks to extract large context features. However, KITTI only provides 200 training images that are far from enough to train high quality attention blocks. We assume that with more training data, GMFlowNet may result in larger improvements compared to CNN-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluations</head><p>We visualize the estimated flows and cost volumes to illustrate the exact aspects that GMFlowNet improves. The supplementary document provides additional visualizations for ours coarse flow from matching.</p><p>Visualizations of estimated flows. <ref type="figure" target="#fig_2">Fig. 4</ref> provides several test samples from KITTI and the corresponding flow estimations of RAFT and GMFlowNet. As we can see, compared with RAFT, GMFlowNet provides better predictions on locally ambiguous regions like textureless regions. For example, there are two white cars moving forward side by side in the last row of <ref type="figure" target="#fig_2">Fig. 4</ref>. Since the two cars share similar colors and shapes, RAFT interprets them as one car. In contrast, our method succeeds in estimating the difference between the two cars and predicts the flow correctly. For more results, please refer to The supplementary Sect 7. These improvements are strong evidence of the effectiveness of the introduced global matching and POLA.</p><p>Visualizations of cost volumes. <ref type="figure">Fig. 5</ref> visualizes the average and normalized cost volumes of both RAFT and GMFlowNet for large displacement regions (&gt; 20 pixels). The supplementary Sect 7 provides more details about the visualization. For a fair comparison, we trained RAFT and (a) Input images (b) RAFT <ref type="bibr" target="#b41">[42]</ref> (c) Ours  <ref type="figure">Figure 5</ref>. Visualizations of cost volumes for large motions. The peak of our cost volume is twice higher than that of RAFT's, which demonstrates that our method handles large displacements better.</p><p>GMFlowNet on C+T and drew the figure using the training set of Sintel. As shown, the peak of our cost volume is much higher than that of RAFT, which clearly demonstrates that GMFlowNet is better at handling large displacements. This is plausible because our matching is designed to handle large motions and the proposed POLA extracts large context information that is crucial to overcome regional ambiguities for matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We perform a set of ablation studies to show the importance and effectiveness of each component in GMFlowNet. All models in the experiments are trained on C+T and tested on Sintel and KITTI training sets. <ref type="table">Table 3</ref> provides the results for various ablation experiments. In each section of the table, we study a specific component of our approach in isolation and underline the settings used in our final model. Initial feature extraction. We tried three different modules, i.e., None, ResNet <ref type="bibr" target="#b15">[16]</ref>, and 3-Convs, to extract initial features for the following POLA blocks. None means no initial features. For this setting, we use the Swin Transformer architecture as the overall feature extractor, but we replace its attention blocks with POLA blocks. As shown in <ref type="table">Table 3</ref>, 3-Convs achieve the best performance. This is likely because, on the one hand, attention blocks have more difficulties than CNNs to learn rich features from raw images. On the other hand, ResNet is much deeper than 3-Convs and may extract more high level features that are less useful for matching.</p><p>Large context feature extraction. To verify the effectiveness of our POLA, we compare it with ResNet, Swin Transformer <ref type="bibr" target="#b28">[29]</ref>, and ViT <ref type="bibr" target="#b13">[14]</ref>. For ViT, we further reduce the feature maps by 4x. Otherwise, ViT will run out of memory because it takes global attentions instead of local attentions used in POLA. As shown in <ref type="table">Table 3</ref>, POLA outperforms others by a large margin.</p><p>The number of our attention blocks. A simple way to expand GMFlowNet is to increase the number of attention blocks. <ref type="table">Table 3</ref> shows that more attention blocks achieve better performance, which is probably because more blocks provide larger receptive fields and better context information. However, more blocks increases the computation and memory costs. As a trade-off, we take 6 blocks finally.</p><p>Overlapping type. Our POLA can be viewed as a generalization of per pixel overlapping attention proposed in <ref type="bibr" target="#b33">[34]</ref>. As shown in <ref type="table">Table 3</ref>, POLA shares the same amount of parameters with the per pixel attention and outperforms it.</p><p>Global matching. Our key motivation is to introduce global matching into direct-regression methods. We remove the global matching in GMFlowNet and observe a significant performance drop on Sintel Final and KITTI shown in <ref type="table">Table 3</ref>. Those results clearly demonstrate the effectiveness of the global matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sintel <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Efficiency</head><p>Running time cost of our global matching. Running time cost is a major concern to adopt global matching. To address this concern, we compare the running time of the widely adopted RAFT and RAFT+GM. RAFT+GM refers to RAFT with our global matching step. As shown in <ref type="table">Table 4</ref>, the global matching is very efficient and only takes 0.002s or 0.52% of extra time. Moreover, compared with RAFT, GMFlowNet runs slightly slower with 4M more parameters but significantly improves the performance. Therefore, the main benefit of our method is the performance improvement.</p><p>Running time cost of our overlapping attention. Our overlapping attention introduces more calculations but is not necessarily inefficient. To demonstrate this, we com-pare our model with +SWIN in <ref type="table">Table 4</ref>. +SWIN is a variant model where the POLA blocks are replaced with local attention blocks from Swin Transformer <ref type="bibr" target="#b28">[29]</ref>. As shown, compared with +Swin, GMFlowNet requires 0.078s of extra time and improves the performance by 13.5% on Sintel clean pass and by 24.9% on KIITI. We believe that the overhead is acceptable given the performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have shown that matching improves the performance of direct-regression optical flow estimation methods in handling large displacements. We proposed a novel framework, GMFlowNet, where a global matching step is introduced before learning-based optimization. To improve the matching, we proposed a patch-based overlapping attention that extracts large context features to diminish regional ambiguities. GMFlowNet significantly improves predictions for large motions and textureless regions and achieves state-ofart performance on standard benchmark datasets. Future work may focus on addressing GMFlowNet's limitations on running time cost and number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Architecture Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Large Context Feature Extraction</head><p>In the paper, we exploit Transformer blocks to extract large context features to improve the matching step in GM-FlowNet. In the original Transformer block <ref type="bibr" target="#b45">[45]</ref>, input features are updated by a Multi-head Self-Attention (MSA) followed by a Multilayer perceptron (MLP). MSA is able to extract the long-term dependency, and MLP projects the features to the required dimension. Both MSA and MLP calculate residuals that are added to the input features as the output features. The update in a transformer block can be formulated as,x l = MSA(LN(x l?1 )) +x l</p><formula xml:id="formula_13">x l = MLP(LN(x l )) +x l ,<label>(11)</label></formula><p>where LN refers to layer norm, and x l?1 and x l represent output features of the previous block and the current block, respectively. The MSA is originally designed for language tasks and takes the whole 1D features as input, but it is computationally prohibitive to apply it on 2D feature maps for optical flow estimation. To extract the long-term dependency with an acceptable computation cost, we propose the patch-based overlapping attention (POLA) to replace MSA of the original attention block and call our attention block as multi-head POLA (M-POLA).</p><p>In our large context feature extraction module (Section 3.1), we take 3 convolutional layers (3-Convs) to extract initial features and 6 M-POLA blocks to extract large context information based on initial features. The detailed structure of this module is listed in <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Optimization Network</head><p>We adopt the iterative update operator proposed in RAFT <ref type="bibr" target="#b41">[42]</ref> as the optimization step of GMFlowNet. As stated in <ref type="bibr" target="#b41">[42]</ref>, this operator mimics the steps of an optimization algorithm and iteratively outputs a series of flow predictions {f 1?2 is calculated by a Convolutional GRU <ref type="bibr" target="#b12">[13]</ref> (ConvGRU) as,</p><formula xml:id="formula_14">x (t) = [f (t?1) 1?2 , F 1 , lookup(C, f (t?1) 1?2 , r)], r (t) = ?(Conv([h (t?1) , x (t) ])),</formula><formula xml:id="formula_15">h (t) = ?(Conv([r (t) h (t?1) , x (t) ])), z (t) = ?(Conv([h (t?1) , x (t) ])), h (t) = (1 ? z (t) ) h (t?1) + z (t) h (t) , ?f (t) 1?2 = Conv(h (t) ), f (t) 1?2 = f (t?1) 1?2 + ?f (t) 0?1<label>(12)</label></formula><p>where F 1 is the context features, C is the 4D cost volume (See Section 3.2 of the paper), Conv(?) refers to a con-  volution layer, ?(?) means sigmoid, and ?(?) means tanh. lookup(?) represents the cost volume within the range of r.</p><p>For each location x in I 1 , lookup(?) is defined as,</p><formula xml:id="formula_16">lookup(?) = {C(x, f (t?1) 1?2 (x) + ?x) | r &gt; ?x 1 }. (13)</formula><p>Different iterations share the weights in the ConvGRU.</p><p>7. More Visualizations 7.1. Attention maps <ref type="figure" target="#fig_4">Fig. 6</ref> visualizes full attention score maps of the first POLA for three pixels highlighted in white. The more red a pixel is, the higher the score is. Yellow dash boxes indicate the local regions that are used in POLA. As shown, a pixel is more likely to attend to those that are visually similar to the pixel. <ref type="figure">Figure 7</ref> displays the coarse flows from our matching step as well as the final flow estimation for samples from Sintel <ref type="bibr" target="#b9">[10]</ref> and KITTI <ref type="bibr" target="#b30">[31]</ref> datasets. We compare our GM-FlowNet with RAFT <ref type="bibr" target="#b41">[42]</ref> because they share the same optimization architecture. For Sintel, both models are trained on C+T. For KITTI, they are trained on all the training data. As shown, the coarse flow results in better predictions especially in large motion areas and textureless regions. For example, the hand of the character in <ref type="figure">Fig. 7b</ref> moves fast, leading to failures of RAFT. On the contrary, our matching step finds the optical flow for the hand and improves the final prediction. <ref type="figure">Figure 8</ref> provides the qualitative evaluation of GM-FlowNet and RAFT on the Sintel test set. We highlight with white arrows and red dash boxes the regions where our method outperforms RAFT. <ref type="figure">Fig. 9</ref> exhibits the visualization of more samples from the KITTI test set. Red dash boxes highlights the regions where our method outperforms RAFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Coarse Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">More Visual Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">How We Visualize Cost Volumes</head><p>In order to compare the 4D cost volumes C of RAFT <ref type="bibr" target="#b41">[42]</ref> and our method, we extract the matrix F x,y as the matching matrix for the point (x, y), </p><p>where ?x and ?y are indicated by the ground truth flow at (x, y). The symbol C[?] means to fetch values from C within a given range. Then, we average F x,y on all points within a specific displacement range for all images in Sintel and visualize the averaged matching matrix. We visualize the cost volume for different ranges of displacements in <ref type="figure">Fig. 10</ref>. The larger the value at the center of the averaged matching matrix is, the higher quality the cost volume has. As shown, GMFlowNet outperforms RAFT in all displacement ranges, which indicates that our approach provides better cost volumes not only for small displacements but also for large ones.  <ref type="figure">Figure 8</ref>. Qualitative evaluation on the Sintel test set <ref type="bibr" target="#b9">[10]</ref>. White arrows in (a) and red dash boxes in (b) highlight the differences between our method and RAFT. Ground-truth optical flows are not available and are not shown. Models are trained on the same training data.  <ref type="figure">Figure 9</ref>. Qualitative evaluation on the KITTI test set <ref type="bibr" target="#b30">[31]</ref>. Red dash boxes highlight the differences between our method and RAFT. Models are trained on the same training data.  <ref type="figure">Figure 10</ref>. Visualization of cost volumes in different range of displacements. The first row is for RAFT <ref type="bibr" target="#b41">[42]</ref>, and the second row is ours. s10 refers to regions with displacements below 10 pixels, s10?20 for displacements between 10 and 20 pixels, s20?30 for displacements between 20 and 30 pixels, and s30+ for displacements larger than 30 pixels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Local attention. (a) The proposed POLA. (b) Window partitions in Swin Transformer<ref type="bibr" target="#b28">[29]</ref>. Red boxes highlight windows, and blue boxes highlight shifted windows. Two blocks are required to propagate information between windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative evaluations for four samples from KITTI test set. (b) Results of the widely adopted optimization-only baseline model RAFT<ref type="bibr" target="#b41">[42]</ref>. (c) Results of our GMFlowNet. Regions with significant improvements are highlighted by red dash boxes. GMFlowNet works better especially in textureless regions, because our overlapping attention provides more context information to diminish regional ambiguities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For the t-th iteration, the flow prediction f (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of attention scores. The more red a pixel is, the higher the score is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc>x,y = softmax(C[x, y,(x + ?x ? 40) : (x + ?x + 40), (y + ?y ? 40) : (y + ?y + 40)])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Table 1 .</head><label>11</label><figDesc>provides the evaluation results in terms of average end-point-error (AEPE). As shown, for the clean pass, GMFlowNet improves RAFT by 22.4% (from 8.80 from 6.83) on s40+ and 18.3% (from 1.38 from 1.69) on s10-40. For the final pass, GMFlowNet is close to RAFT Sintel RAFT<ref type="bibr" target="#b41">[42]</ref> Ours Rel. Impr. Quantitative results on different displacements. Models are trained on C+T. Rel. Impr. refers to relative improvement. Our method improves more on regions with extremely large motions (s40+) than on s10-40.</figDesc><table><row><cell cols="2">Dataset Type</cell><cell cols="2">(AEPE) (AEPE)</cell><cell>(%)</cell></row><row><cell></cell><cell>s0-10</cell><cell>0.37</cell><cell>0.28</cell><cell>24.3</cell></row><row><cell cols="2">Clean s10-40</cell><cell>1.69</cell><cell>1.38</cell><cell>18.3</cell></row><row><cell cols="2">(train) s40+</cell><cell>8.80</cell><cell>6.83</cell><cell>22.4</cell></row><row><cell></cell><cell>All</cell><cell>1.47</cell><cell>1.14</cell><cell>22.4</cell></row><row><cell>Final</cell><cell>s0-10 s10-40</cell><cell>0.53 3.11</cell><cell>0.54 3.09</cell><cell>?1.9 0.6</cell></row><row><cell cols="2">(train) s40+</cell><cell>18.11</cell><cell>17.25</cell><cell>4.7</cell></row><row><cell></cell><cell>All</cell><cell>2.78</cell><cell>2.71</cell><cell>2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Ablation experiments. Settings used in the final model are underlined. See Sec. 4.3 for details. Comparisons of parameters and inference time. All models are trained on C+T and tested on S and K. Speed measurements are evaluated on Sintel with the same platform.</figDesc><table><row><cell>train) Clean Final F1-epe F1-all KITTI-15 (train) Parameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Large context feature extraction. The arguments in Conv(?) are the input channel number, the output channel number, the kernel size, and the convolution stride, respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Samuel Schulter from NEC Laboratories America for helpful discussions. This research has been partially funded by the following grants, NSF IUCRC CARTA, ARO MURI 805491, NSF IIS-1793883, NSF CNS-1747778, NSF IIS 1763523, DOD-ARO ACC-W911NF, NSF OIA-2040638 to Dimitris Metaxas.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RAFT Ours   Ground-truth flows for KITTI are unavailable and thus are not shown. With the coarse flow, our method outperforms the most popular optimization-only method RAFT <ref type="bibr" target="#b41">[42]</ref>. Red dash boxes highlight the main differences between RAFT's predictions and ours.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow Fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4015" to="4023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CNNbased patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3250" to="3259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ScopeFlow: Dynamic scene scoping for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviram</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="75" to="104" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining the advantages of local and global optic flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schn?rr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="454" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lucas/kanade meets horn/schunck: Combining local and global optic flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schn?rr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4706" to="4714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>ICLR, 2021. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving optical flow on a pyramid level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="770" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient coarseto-fine patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5704" to="5712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lite-FlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A lightweight optical flow CNN -revisiting data fidelity and regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2555" to="2569" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Super SloMo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to estimate hidden motions with global motion aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The HCI benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10012" to="10019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS. IEEE</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-local total generalized variation for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="439" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Models matter, so does training: An empirical study of CNNs for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1408" to="1423" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LoFTR: Detector-free local feature matching with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">RAFT: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792</idno>
		<title level="m">Patches are all you need? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Axial-DeepLab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Displacement-invariant matching cost learning for accurate optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="794" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3555" to="3564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Separable Flow: Learning motion cost volumes for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">Adrian</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10807" to="10817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Asymmetric feature matching with learnable occlusion mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
