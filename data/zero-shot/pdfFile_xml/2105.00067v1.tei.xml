<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Discriminative Embedding for Sub-Action Learning in Complex Activities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirnam</forename><surname>Swetha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT-IBM Watson Lab</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Discriminative Embedding for Sub-Action Learning in Complex Activities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action recognition and detection in the context of long untrimmed video sequences has seen an increased attention from the research community. However, annotation of complex activities is usually time consuming and challenging in practice. Therefore, recent works started to tackle the problem of unsupervised learning of sub-actions in complex activities. This paper proposes a novel approach for unsupervised sub-action learning in complex activities. The proposed method maps both visual and temporal representations to a latent space where the sub-actions are learnt discriminatively in an end-to-end fashion. To this end, we propose to learn sub-actions as latent concepts and a novel discriminative latent concept learning (DLCL) module aids in learning sub-actions. The proposed DLCL module lends on the idea of latent concepts to learn compact representations in the latent embedding space in an unsupervised way. The result is a set of latent vectors that can be interpreted as cluster centers in the embedding space. The latent space itself is formed by a joint visual and temporal embedding capturing the visual similarity and temporal ordering of the data. Our joint learning with discriminative latent concept module is novel which eliminates the need for explicit clustering. We validate our approach on three benchmark datasets and show that the proposed combination of visual-temporal embedding and discriminative latent concepts allow to learn robust action representations in an unsupervised setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen a great progress in video activity analysis. However, most of this research is focused on the classification of short video clips with atomic or shortrange actions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. This is a relatively easier task when compared with analysis of untrimmed and complex video sequences <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. In untrimmed video analysis, the focus is either on the problem of temporal action localization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, where only a set of key actions is considered in untrimmed videos; or on the task of <ref type="figure">Figure 1</ref>: Overview of the proposed approach. Given videos of a complex activity, the proposed model learns sub-actions as latent concepts in an end-to-end manner. The latent concept assignment for each input video segment feature forms sub-action prediction shown as 'Initial Predictions', which is then refined using Viterbi to generate 'Final Predictions'. Sample results for activity 'Make Chocolate Milk', it can be seen that the latent concepts are able to group sub-actions. The sub-action 'pour-milk' includes lifting bottle and pouring milk; the jitter can be associated to the confusion when either a chocolate/milk bottle is lifted. temporal action segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, where each frame of the video is associated with a respective subaction class as it requires to identify sub-actions and also temporally localize them.</p><p>Existing works on temporal action segmentation mainly explore supervised approaches where frame-level annotations are required for all the sub-actions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. However, complex activities are usually longranged and obtaining frame-level annotation is arduous and expensive. A new line of research focuses on learning these sub-actions from videos of a complex activity in an unsupervised setting <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> . In the unsupervised setting, the problem is even more challenging as it requires (i) breaking down a complex activity video into semantically meaningful sub-actions; and (ii) capturing the temporal relationship between the sub-actions. Most approaches tackle this problem in two stages, where during the first stage an embedding based on visual and/or temporal information is learned, and in the second stage clustering is applied on this embedding space. This limits the learning ability by preventing the embedding to actually learn from clustering. At the same time, performing explicit clustering which is independent of embedding learning makes the model less efficient and does not utilize end-to-end learning.</p><p>To address this problem, we propose an end-to-end approach where sub-actions are learned by combining embedding and latent concepts. Here, the embedding space is trained jointly with the latent concepts leading to an effective sub-action discovery as shown in <ref type="figure">Figure 1</ref>. To allow for such a joint training, we propose a novel discriminative latent concept learning (DLCL) module which combines latent concept learning with a contrastive loss to ensure that the sub-actions learnt in the latent embedding are distant. The resulting latent concept vectors can be interpreted as cluster centers, removing the need for explicit clustering at a later stage.</p><p>As the sub-actions are softly bound to the temporal position of each activity, incorporating temporal ordering is crucial. Recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> incorporated temporal embedding either by predicting the discrete temporal entities or by learning continuous temporal embedding with shallow MLP architectures. In those cases, the temporal information is only given by a discrete or continuous scalar value and the joint embedding space is constructed by predicting this value from the input. To learn better spatio-temporal representations, we propose to use temporal position encoding <ref type="bibr" target="#b24">[25]</ref> instead of scalar values and learn the respective embedding space by jointly reconstructing both visual and temporal representations. This embedding is further trained jointly with constrastive loss of the latent concept module, so that the embedding is also guided by and contributes to overall clustering.</p><p>We evaluate our method on three benchmark datasets: Breakfast <ref type="bibr" target="#b7">[8]</ref>, 50Salads <ref type="bibr" target="#b25">[26]</ref> and YouTube Instructions <ref type="bibr" target="#b4">[5]</ref>. For the evaluation at test time, we follow the protocol from <ref type="bibr" target="#b18">[19]</ref> and employ the Viterbi algorithm to decode the initial sub-action predictions into coherent segments based on the ordered clustering of the sub-action latent concepts. A detailed analysis shows the impact of the proposed elements, the reconstruction and well as the latent concept learning.</p><p>In summary, we propose a novel end-to-end unsupervised approach for sub-action learning in complex activities. We make the following contributions in this work:</p><p>? We propose an unsupervised end-to-end approach for sub-action learning in complex activities by jointly learning an embedding which simultaneously incorporates visual and temporal information.</p><p>? We learn discriminative latent concepts using contrastive loss, thus integrating clustering as part of latent embedding learning.</p><p>? Our method improves the state-of-the-art on three benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, there has been a lot of interest in learning with less supervision. This is essential for both action <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> and complex activity understanding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref>, as supervised approaches require a large number of frames to be annotated in videos, which is expensive, tedious and cannot be scaled to large datasets. Weakly supervised approaches use a video and readily available information like accompanying text narration or audio. Some works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> use associated text narrations or scripts for learning actions in the video. Another line of work with weak-supervision include the works where it is assumed that the order of sub-actions is known <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, however the per-frame annotations between video and sub-actions are not known during training. Authors in <ref type="bibr" target="#b32">[33]</ref> propose to use combination of audio, text and video to identify steps in instructional videos in kitchen setting. The performance of the above methods is highly dependent on both the availability and quality of the text/audio alignment to video, which is not guaranteed and heavily limit their application.</p><p>There have been some works, in which the assumption of weak supervision have been removed in learning of action classes. One of the first works with no supervision addressed the problem of human motion segmentation <ref type="bibr" target="#b33">[34]</ref> based on sensory-motor data, and proposed an application of a parallel synchronous grammar, to learn simple action representations similar to words in language. Later, a Bayesian non-parametric approach to concurrently model multiple sets of time series data was proposed in <ref type="bibr" target="#b34">[35]</ref>. However, this work only focuses on motion capture data. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> benefit from the temporal structure of videos to fine-tune networks without any labels. Additionally, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> also leverage the temporal structure of videos to learn feature representation to learn actions.</p><p>Recently, unsupervised approaches have been proposed to learn sub-actions in complex activity. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref> propose unsupervised approaches for temporal segmentation of complex activity into sub-actions. While <ref type="bibr" target="#b3">[4]</ref> proposes to solve a variant of the problem where the goal is to detect event boundaries, i.e. event boundary segmentation for complex activities. This does not focus on identifying subactions, instead it learns to identify boundaries across multiple sub-actions in long videos. A self-supervised predictive learning framework is proposed to solve by utilizing the difference between observed and predicted frame features to determine event boundaries in complex activities.</p><p>In this work, we focus on solving the temporal segmentation of complex activity into sub-actions as shown in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. In <ref type="bibr" target="#b9">[10]</ref>, an iterative approach is proposed that alternates between discriminative learning and gener- <ref type="figure">Figure 2</ref>: Overview of the proposed model. Given videos for a complex activity, we extract visual features (Xnm) and compute positional encoding vectors (?nm) which are fed to the encoder to map them to a joint latent embedding for learning sub-action clusters. To learn these sub-action clusters as latent concepts Y , an attention block (D) is used which takes in randomly initialized vectors (Y ) along with ?nm and learns the latent concepts. We use contrastive loss to learn Y discriminatively in B. Here ?, znm and Y k represent attention, latent vector for input Xnm and k th latent concept respectively. ative modeling. For discriminative learning, they map the visual features into latent space using a 'linear' transformation and compute the weight matrix which minimizes the pairwise ranking loss. For temporal ordering they use Generalized Mallows Model which models distributions over permutations as they formulate complex activity as a sequence of permutable sub-actions. In <ref type="bibr" target="#b18">[19]</ref>, the model incorporates the continuous temporal ordering of frames in a joint embedding space. This is achieved by training a regressor to predict the timestamp of frames in a video. The hidden layer representations are used as the embedding for clustering and the clusters are ordered with respect to their time stamp. We refer to this model as CTE (Continuous Temporal Embedding). In <ref type="bibr" target="#b23">[24]</ref>, two-stage embedding pipeline is proposed where a next frame prediction U-Net model in stage one is combined with with temporal discriminator in stage two followed by clustering. The temporal embedding model employed is similar to <ref type="bibr" target="#b18">[19]</ref>.</p><p>Latent embedding learning is crucial for unsupervised learning, recently <ref type="bibr" target="#b6">[7]</ref> formulated learning graph based latent embedding using latent concepts for supervised classification of complex activities. The intuition was to model long range videos using latent concepts as graphical nodes for complex activity recognition. Inspired by their ideology of latent concept learning to model latent space, we propose DLCL as an unsupervised latent learning module with joint embedding learning to model sub-actions.</p><p>Most of the above works in unsupervised learning involve two stage process which does not utilize end-to-end learning making them less efficient as clustering is inde-pendent of embedding learning. In this work, we present an end-to-end model where clustering is incorporated in embedding learning using a constrastive loss. To incorporate temporal ordering we propose to use positional encodings and we also propose an effective way to unify visual and temporal representations to learn a visual-temporal embedding by jointly reconstructing visual and temporal representations. The proposed latent embedding is not only better at capturing visual &amp; temporal representations but also clustering friendly. We demonstrate later in this paper the usefulness of the proposed model both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given a set of N videos, {V n } N n=1 , for a complex activity, we divide each video into segments and for each segment we extract I3D features <ref type="bibr" target="#b0">[1]</ref> , and compute positional encoding vectors <ref type="bibr" target="#b24">[25]</ref> as described in Section 3.2. Each video is represented by M n features where X nm represent the m th feature in the n th video, and its corresponding positional encoding is represented by ? nm . The task is to learn the sub-actions and their ordering for each activity, i.e., by predicting sequence of a sub-action labels l nm ? {1, 2, ..., K} for each feature X nm for each video. The number of sub-actions labels K for each activity is the maximum number of possible sub-actions as they occur in that activity.</p><p>Overview of our proposed model is shown in <ref type="figure">Figure 2</ref>.</p><p>First we learn an encoded representation of X nm and ? nm shown as ? nm , which is passed as input feature to the 'Attention Block' (shown as D in <ref type="figure">Figure 2</ref>) to learn the latent concepts/clusters which are representative of subactions. The attention block learns the latent concepts</p><formula xml:id="formula_0">Y ? { Y 1 , Y 2 , ..., Y K } discriminatively, where each input feature (? nm )</formula><p>is assigned to only one latent concept. We use a combination of reconstruction loss and constrastive loss to learn the embedding(shown as B in <ref type="figure">Figure 2</ref>). We believe that using a combination of both visual and temporal information in conjunction with latent concept learning to learn a latent embedding (shown as block B in <ref type="figure">fig. 2</ref>) makes our model more robust. We evaluate the performance of our model based on latent concept assignments for each input feature which forms 'Initial Predictions'. Then, we model the sub-action transitions and perform Viterbi decoding to estimate optimal sub-action ordering.</p><p>Note that unlike previous works <ref type="bibr" target="#b18">[19]</ref>, we do not perform explicit clustering, instead our model learns to cluster features in latent space as discussed in Section 3.2 &amp; 3.3. Thus eliminating the need for all the data to be available at once, our model can learn the latent concepts incrementally. The resulting sub-action latent concepts are temporally ordered and then each video is decoded w.r.t the above ordering given initial sub-action probability assignments for each clip to each latent concept as described in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Visual and Temporal Latent Embedding</head><p>In unsupervised learning, the approach to learn clusters in latent space plays a critical role in learning semantically meaningful clusters. We employ an encoder-decoder model to obtain the latent representation. Skip-connections are included between encoder and decoder (shown as C in <ref type="figure">Figure</ref> 2), as they help to preserve commonality of an action and reduce redundant information like background in latent representation.</p><p>For incorporating temporal ordering in our model, we employ the positional encodings inspired by <ref type="bibr" target="#b24">[25]</ref>. We divide the video segment sequence into g equal groups and then use the ordering index to compute positional encoding vectors. Quantizing temporal index of the video clip and using a positional encoding not only captures relative positioning but also makes it easy to generalize for highly varying video lengths. The idea of learning a mapping from features to joint visual and temporal embedding with an encoder-decoder aids in grouping clips into sub-actions in the latent space. The reconstruction loss for the autoencoder is composed of visual features and positional encoding as shown below,</p><formula xml:id="formula_1">Loss r = L(X nm , X nm ) + ? * L(? nm , ? nm ),<label>(1)</label></formula><p>where, X nm , X nm respectively represent input and reconstructed visual feature; ? nm , ? nm respectively represent in-put and reconstructed positional encoding; ? is a hyperparameter and L is a loss function penalizing X nm and ? nm for being dissimilar from X nm and ? nm respectively, namely mean squared error. A combination of latent visual feature representation and the positional encodings becomes input to the 'Attention Block'. In order to ensure that the learnt clusters are representative of sub-actions, the clusters have to be distant in the latent space, which is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discriminative Latent Concept Learning</head><p>The idea behind having this module is to learn the subaction clusters discriminatively in the latent space in an endto-end fashion, eliminating the need for explicit clustering. The attention block is inspired by <ref type="bibr" target="#b6">[7]</ref>, which takes an input feature (? nm ) and randomly initialized latent vectors (Y ) which is analogous to cluster center initialization as shown in <ref type="figure">Figure 2</ref>. The latent concepts ( Y ) are learnt using an MLP with weight (w) and bias (b) i.e., it transforms the random latent vector initializations (Y ) to latent concepts</p><formula xml:id="formula_2">( Y ) as Y = w * Y + b. Though latent vector initializa- tion (Y ) is fixed, w &amp; b</formula><p>are learnable parameters making the latent concepts ( Y ) learnable in the latent space. These latent concepts which represent cluster centers are learned by minimizing the contrastive loss by moving features in the latent space closer to the latent concepts. The similarity between input feature (? nm ) and latent concepts ( Y ) is measured with the dot product ?. Then, activation function ? is applied on the similarities to compute activation values ? i.e., ? = ?(? nm * Y T ). Finally, the attended latent vector representation is computed as Z nm = ? Y , which captures how much each latent concept is related to the given input feature. However, these latent concepts tend to learn similar/overlapping concepts, which is not what we intend to learn. Our objective in learning the latent concepts is to cluster the latent representations discriminatively. We achieve this with a contrastive loss, where the similarity between latent vectors of the same sub-action with the maximum confident latent concept is maximized, while the similarity w.r.t other latent concepts is minimized as shown in Eq 2.</p><formula xml:id="formula_3">Loss d (Z nm , Y ) = ?log e sim(Znm, Y k * ) k =k * e sim(Znm, Y k )<label>(2)</label></formula><p>where, Y k represents the latent concept associated with k th sub-action, sim denotes cosine similarity and k * represents the latent concept with maximum confidence probability for Z nm as shown below k * = argmax k P (k|Z nm )</p><p>where P (k|Znm) = ?(sim(Znm, Y k )) represents the confidence probability of latent vector Z nm for the latent concept Y k , ? is softmax activation. viterbi) and after Viterbi predictions of our approach for activity 'Make Tea'. It can be seen that our model ('Init') is able to group sub-actions and also learn the ordering of sub-actions for an activity. The jitter in sub-action predictions occurs during transition from one sub-action to next, which is expected during transition. Finally, using transition modeling Viterbi decoding smoothness the jitter between sub-action transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overall Loss.</head><p>Total loss for learning the proposed embedding is composed of losses from Section 3.2 and 3.3 as Loss = ? * Loss r + ? * Loss d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Temporal Segmentation</head><p>Initial Predictions At test time, we first assign each feature in video to its respective closest latent concept vector using Eq 3. This gives initial predictions directly based on the embedding (shown as predictions in <ref type="figure">Figure 1</ref>). For ease of understanding, we refer to those as latent sets, analogous to clusters, from here on. Transition modeling and Viterbi decoding <ref type="figure" target="#fig_1">Figure 4</ref> represents a brief outline for transition modeling and Viterbi decoding. To allow for a temporal decoding, the global ordering of the latent sets needs to be estimated. We follow here the protocol proposed by <ref type="bibr" target="#b18">[19]</ref> and compute the mean timestamp for each set (shown as T in <ref type="figure" target="#fig_1">Figure 4</ref>) and sort them in ascending order. The last set in the sorted ordering becomes the terminal state and using this ordering the sub-action state transition probabilities from sub-action i to j are defined as P (j|i) given:</p><formula xml:id="formula_5">P (j|i) = ? ? ? ? ? ? ? ? ? 0.5, if j immediately follows i 0.5, if i = j 1.0, if i = j &amp; j is terminal state 0, otherwise<label>(4)</label></formula><p>Decoding Finally, we use the ordering and transition probabilities to compute the best path for the set ordering given the input features X nm and ? nm . Using Eq. 3 we compute the probability of each embedded input feature (Z nm ) belonging to the latent set k. We maximize the probability of the input sequence following the order defined by Eq. 4 to get consistent latent set assignments in a video by maximizing,</p><formula xml:id="formula_6">l Mn 1 = argmax l1,...,l Mn Mn m=1 P (l m |l m?1 ) * P (l m |Z nm ),<label>(5)</label></formula><p>where l 1 , ..., l m ? {1, 2, ..., K} represent the set label sequence for n th video, P (l m = k|Z nm ) is the probability that Z nm belongs to the k th latent set (as described in Section 3.3),l Mn 1 is the set label sequence for the maximum likelihood for n th video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>For our experiments, we define a segment as a sequence of 8 frames. The video segment sequence is divided into 128 equal groups and then the ordering index is used to compute positional encoding <ref type="bibr" target="#b24">[25]</ref> for each segment. We extract I3D features (layer 'mixed 5c') which is fed to the encoder. Our embedding dimension is 1024. We use a 3-layer encoder-decoder with Adam optimizer and the learning rate is set to 1 ? 10 ?4 . We evaluate our approach on 3 datasets. Breakfast Dataset comprises of 10 complex activities of humans performing common kitchen activities. There are a total of 48 sub-activities in 1, 712 videos with varying lengths based on activity and preparation style with variations in sub-action orderings. 50Salads Dataset contains videos of duration 4.5 hours for a single complex activity 'making mixed salad'. It is a multimodal dataset, as it includes RGB frames, depth maps and accelerometer data. However, we only use RGB frames. The videos in this dataset are much longer with average frame length of 10k frames and provides annotations at multiple granularity levels. YouTube Instructions Dataset has 5 activities and 150 videos with 47 sub-actions. These videos are taken from (a) Make Cereals (b) Make Chocolate Milk <ref type="figure">Figure 5</ref>: Illustrative comparison with state-of-the-art. By comparing with CTE (Init) and Ours (Init), we show that our approach learns to model sub-actions with very few intermittent sub-action transitions leading to effective grouping of sub-actions. Then, Viterbi decoding helps to smoothen the intermittent jitters in predictions. We show that our method provides coherent sub-action predictions and is able to capture the orderings for sub-actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1-score MoF</head><p>Weakly Supervised RNN-FC <ref type="bibr" target="#b41">[42]</ref> 33.3% TCFPN <ref type="bibr" target="#b16">[17]</ref> 38.4% NN-Vit <ref type="bibr" target="#b31">[32]</ref> 43% D3TW <ref type="bibr" target="#b42">[43]</ref> 45.7% CDFL <ref type="bibr" target="#b43">[44]</ref> 50.2% YouTube directly and have background segments where there is no sub-action. The frequency and spread of background varies based on activity as well as on the person performing the task. Hence, the background segments neither have similar appearance nor have a temporal ordering. Therefore the background segments would be assigned to the latent concepts with very less confidence probability. Following protocol in <ref type="bibr" target="#b18">[19]</ref>, we consider ? percent of clips with least confidence as background. Only the foreground labeled segments along with latent concepts assignments form our initial predictions. We report results for background ratio of 60%.  Metrics Our model predicts a sequence of cluster labels ? {1, 2, ..., K} for each video without any correspondence to the K ground-truth class labels. To map ground-truth and prediction label correspondences, inline with <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>, for each activity we use the Hungarian method to find a oneto-one mapping for each cluster to exactly one sub-action and report performance after this mapping. In this work, we use Mean over Frames (MoF) as used by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> as well as F1-score used by <ref type="bibr" target="#b4">[5]</ref>. In addition, we report Mean over class (MoC) accuracy, as it averages the accuracy for each activity class, therefore giving equal weights to all classes irrespective of the underlying data distribution. MoF is the percentage of correct predictions computed across all activity classes together, which can be affected by the underlying activity classes distribution and biased towards dominant activity class. For F1-score, similar to previous methods, we report the mean score over all activities. For state-ofthe-art comparisons, we also evaluate our method for the task of event boundary segmentation following the protocol in <ref type="bibr" target="#b3">[4]</ref> and compare our method to [4] -indicated as video-based Hungarian matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison to state-of-the-art</head><p>Here, we compare the proposed method to state-of-theart approaches. We present the accuracy comparison with recent works on Breakfast dataset in <ref type="table" target="#tab_0">Table 1</ref> and present the performance on new metric MoC in <ref type="table" target="#tab_2">Table 2</ref>. Our approach achieves 47.4% MoF and 31.9% F1-score which is 2% gain over state-of-the-art as shown in table 1. We show qualitative evaluation of the proposed approach in <ref type="figure" target="#fig_0">Figure 3</ref> &amp; 5. In <ref type="figure">Figure 5</ref>, we show that our approach models the sub-actions coherently with very less intermittent sub-action transitions along with learning ordering of sub-actions for complex activity. For example, in <ref type="figure" target="#fig_0">Figure 3</ref> our model predicts 'stir-tea' with intermittent transitions after 'pour-water', this occurs when the person dips the tea bag in water which closely resembles to the sub-action 'stir-tea' (as shown in last image in <ref type="figure" target="#fig_0">Figure 3</ref>) and then it correctly predicts background once the dip action ends (there is no annotation for 'dipping tea-bag' in ground truth) indicating the goodness of the proposed sub-action learning. The intermittent transitions indicate that the model confuses to assign latent concept  <ref type="table">Table 3</ref>: Comparison of the proposed method to state-of-the-art unsupervised approaches on 50Salads dataset at granularity 'eval'.</p><p>Here, * denotes results with video-based Hungarian matching for the task event boundary segmentation.</p><p>based on single feature and Viterbi aids in generating more coherent sub-action segments for the sequence as shown. Additionally, we evaluate our method for the task of event boundary segmentation and compare with the state-of-theart approaches. Our approach out-performs the state-of-theart MoF by a margin of 31% on Breakfast dataset indicating the effectiveness of the proposed method to temporally segment meaningful sub-actions. For 50Salads dataset, we perform evaluation on granularity level 'eval' and provide state-of-the-art comparison in <ref type="table">Table 3</ref>. Our method out-performs <ref type="bibr" target="#b18">[19]</ref> by 6.67% and <ref type="bibr" target="#b23">[24]</ref> by 11.6% with an F1-score of 34.37%. We further evaluate our method for the task of event boundary segmentation and perform state-of-the-art comparison in <ref type="table">Table 3</ref>. We show 10% gain over state-of-the-art <ref type="bibr" target="#b3">[4]</ref> MoF, indicating our method is effective in sub-action learning for complex events.</p><p>For YouTube Instructions dataset, we follow protocol in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref> and report the performance of our approach without considering the background frames. We achieve 42% MoC &amp; 43.8% MoF (as shown in <ref type="table" target="#tab_5">Table 4</ref>). This is a 4.8% gain in MoF over state-of-the-art method with comparable F1-score. Note that <ref type="bibr" target="#b3">[4]</ref> reported F1-score with background frames included on YouTube Instructions Dataset. We follow the same procedure and compare our method to <ref type="bibr" target="#b3">[4]</ref> in <ref type="table" target="#tab_5">Table 4</ref> (indicated with *). It can be seen that our method outperforms the state-of-the-art for event boundary segmentation task showing the sub-action learning capability to identify better event boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of the Embedding.</head><p>To demonstrate the impact of the proposed embedding, we compare our Joint Embedding with Continuous Temporal Embedding in <ref type="bibr" target="#b18">[19]</ref> in <ref type="table" target="#tab_2">Table 2</ref>  performance is due to the effectiveness of the approach and not with using I3D features, we train <ref type="bibr" target="#b18">[19]</ref> using I3D features by keeping the embedding dimension same as ours and compare the performance. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the MoC w/o Viterbi improves by 4% by using I3D features on CTE, while the MoC with Viterbi drops by 3% with 1% increase in F1-score. However, our approach still outperforms the baseline (with same embedding dimension) by huge margin indicating our approach effectiveness. Besides dataset level comparisons, we also show activity level comparison with CTE <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure">Figure 6</ref> (a) shows that our joint embedding outperforms CTE on all activities indicating the significance of our joint embedding. We see a drop in performance for activity 'making cereals' after Viterbi decoding (from <ref type="figure">Figure 6(b)</ref>), this can be attributed to the ordering of the sub-actions 'take-bowl' and 'pour-cereals'. For many samples in 'making cereals', the sub-action 'takebowl' does not occur impacting the ordering of both subactions leading to drop in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Experiments</head><p>We perform the below ablation studies on the breakfast dataset. Effect of Loss Components. To begin with, we first examine the influence of Loss r and Loss d on our model and the performances are presented in <ref type="table" target="#tab_7">Table 5</ref>. It can be seen that having all loss components leads to best performance. Effect of Discriminative learning. The use of constrastive loss (Loss d ) helps the clusters to move apart in the latent space. This helps in obtaining more discrete boundaries in the latent space. As shown in <ref type="table" target="#tab_7">Table 5</ref>, the accuracy drastically reduces to 35.8% (11% ?) indicating the importance of discriminative learning. Effect of Positional Encoding. Positional Encoding plays a crucial role in our model. It helps to temporally group the video clips in the latent space. As sub-actions are softly bound to the temporal position for each activity, removing   reconstruction loss for positional encoding is expected to deteriorate the model performance. We observe the similar trend in <ref type="table" target="#tab_7">Table 5</ref>. Additionally, we perform an ablation by removing the PE component branch and train our model end-to-end. As expected, there is a significant reduction in accuracy and F1-score (as shown in <ref type="table" target="#tab_6">Table 6</ref>) indicating the significance of using positional encoding. Effect of Skip-Connections. To assess the effectiveness of skip-connections, we report performance by removing the skip-connections and train model end-to-end. We re- port the performance in <ref type="table" target="#tab_6">Table 6</ref>, it can be seen that w/o skip-connections, the accuracy drops considerably indicating that the skip-connections help in learning better representations.  Effect of Sub-actions Cluster Size. For all the above evaluations, the sub-action cluster size (K) is defined as mentioned in Section 3.1. To analyze the impact of sub-action cluster size, we vary the number of sub-actions from K ? 2 to K+2 where K is the number of sub-actions as per ground truth and evaluate performance. <ref type="figure" target="#fig_3">Figure 7</ref> shows the MoF vs number of sub-actions for each activity in Breakfast dataset. For 6 out of 10 activities we see that having K sub-actions leads to best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we proposed an end-to-end approach for unsupervised learning of sub-actions in complex activities. The main motivation behind this approach is to design a latent space to incorporate visual as well as positional encoding together. This latent space is learned via jointly training this embedding space in conjunction with a contrastive learning for clustering. We show that this allows for a robust learning that on it's own already results in a reasonable clustering of sub-actions. We then predict optimal subaction sequence by employing the Viterbi algorithm which outperforms all the other methods. Our evaluation shows the impact of the proposed ideas and how they are able to improve the performance on this task compared to existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison of initial predictions (w/o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Brief overview of transition modeling and Viterbi decoding. Each latent concept is color coded (best viewed in color). The latent concepts are ordered w.r.t the mean time (shown as T ) and each video is decoded into coherent segments using Viterbi algorithm based on the ordered sub-action latent concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) MoF w/o Viterbi (b) Final MoF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>MoF vs. #sub-actions for all activities in Breakfast dataset. k represents the number of sub-actions from ground-truth; we vary the sub-actions for each activity and report MoF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the proposed method to state-of-the-art on Breakfast dataset. Here, * denotes results with video-based Hungarian matching for the task event boundary segmentation.</figDesc><table><row><cell>Mallow [10]</cell><cell>-</cell><cell>34.6%</cell></row><row><cell>CTE [19]</cell><cell>26.4%</cell><cell>41.8%</cell></row><row><cell>JVT [24]</cell><cell>29.9%</cell><cell>48.1%</cell></row><row><cell>Ours</cell><cell>31.9%</cell><cell>47.4%</cell></row><row><cell>LSTM-AL [4]*</cell><cell>-</cell><cell>42.9%*</cell></row><row><cell>Ours*</cell><cell>-</cell><cell>74.6%*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of MoC (Mean over class) of all activities on Breakfast dataset before and after applying Viterbi. FV represents Fisher Vectors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. FromTable 2(MoC w/o viterbi), it can be seen that the proposed joint embedding outperforms the continuous temporal embedding by a huge margin of 16.6%. It can be seen that our 'MoC w/o viterbi' is closer to the CTE 'MoC w Viterbi' suggesting that our embedding is very effective. To emphasize that our gain in</figDesc><table><row><cell>Method</cell><cell>F1-score</cell><cell>MoF</cell></row><row><cell>Frank-Wolfe [5]</cell><cell>24.4%</cell><cell>34.6%</cell></row><row><cell>Mallow [10]</cell><cell>27.0%</cell><cell>27.8%</cell></row><row><cell>CTE [19]</cell><cell>28.3%</cell><cell>39.0%</cell></row><row><cell>JVT [24]</cell><cell>29.9%</cell><cell>28.2%</cell></row><row><cell>Ours</cell><cell>29.6%</cell><cell>43.8%</cell></row><row><cell>LSTM-AL [4]*</cell><cell>39.7%*</cell><cell>-</cell></row><row><cell>Ours*</cell><cell>45.4%*</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the proposed method to state-of-the-art unsupervised methods on YouTube Instructions dataset. Here, * denotes results with video-based Hungarian matching for the task event boundary segmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Figure 6 :</head><label>6</label><figDesc>Activity level MoF comparison on Breakfast dataset with CTE [19]. Last column represents the average (MoC) for all activities. (a) represents MoF for each activity without Viterbi i.e, the MoF is computed based on the learnt cluster assignments. Our method outperforms the baseline on all activities. (b) represents MoF for each activity after applying Viterbi.</figDesc><table><row><cell>Lossr</cell><cell></cell><cell>Loss d</cell><cell>MoC</cell></row><row><cell>L f</cell><cell>Lp</cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>25.7%</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>33.6%</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>35.8%</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>40.2%</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell>40.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>46.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation experiments for the loss components are performed on the Breakfast dataset. Lossr and Loss d represents reconstruction loss and contrastive loss respectively. L f and Lp denote the reconstruction loss for feature and positional encoding respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablations experiments to evaluate the effect of PE and SC on Breakfast dataset (w/o: without, PE: Positional Encoding, SC: skip-connections).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A perceptual prediction framework for self supervised event segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sathyanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Aakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Videograph: Recognizing minutes-long human activities in videos. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An end-toend generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning and segmentation of complex activities from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation attention for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<title level="m">Sub-graph localization for temporal action detection. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of action classes with continuous temporal embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kukleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hybrid rnn-hmm approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint visual-temporal embedding for unsupervised learning of actions in untrimmed sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosaura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidalmata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kukleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuehne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UBICOMP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing of video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What&apos;s cookin&apos;? interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A language for human action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gutemberg</forename><surname>Guerra-Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint modeling of multiple time series via the beta process with application to motion capture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lstm self-supervision for detailed behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna-Sophia</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">E</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning temporal embeddings for complex video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized rank pooling for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly supervised energy-based learning for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
