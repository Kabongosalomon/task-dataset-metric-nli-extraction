<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD &amp; CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Platform</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD &amp; CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Lin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Software Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD &amp; CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD &amp; CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Platform</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers do not yet possess the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the selfattention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks. 1 1 The code has been released: https://github.com/cheerss/CrossFormer 2 In this paper, we also use "embeddings" to represent the input of each layer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It turns out that transformer <ref type="bibr" target="#b30">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b10">Devlin et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref> has achieved great success in the field of natural language processing (NLP). Benefitting from its selfattention module, transformer is born with the key ability to build long-distance dependencies. Since long-distance dependencies are also needed by a number of vision tasks <ref type="bibr" target="#b39">(Zhang &amp; Yang, 2021;</ref><ref type="bibr" target="#b7">Chu et al., 2021)</ref>, a surge of research work <ref type="bibr">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b29">Touvron et al., 2021;</ref> has been conducted to explore various transformer-based vision architectures.</p><p>A transformer requires a sequence of embeddings 2 (e.g., word embeddings) as input. To adapt this requirement to typical vision tasks, most existing vision transformers <ref type="bibr">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b29">Touvron et al., 2021;</ref><ref type="bibr" target="#b22">Liu et al., 2021b)</ref> produce embeddings by splitting an input image into equal-sized patches. For example, a 224 ? 224 image can be split into 56 ? 56 patches of size 4 ? 4, and these patches are projected through a linear layer to yield an embedding sequence. Inside a certain transformer, self-attention is engaged to build the interactions between any two embeddings. Thus, the computational or memory cost of the self-attention module is O(N 2 ), where N is the length of an embedding sequence. Such a cost is too big for a visual input because its embedding sequence is much longer than that of NLP. Therefore, the recently proposed vision transformers <ref type="bibr" target="#b22">Liu et al., 2021b;</ref><ref type="bibr" target="#b18">Lin et al., 2021)</ref> develop multiple substitutes to approximate the vanilla self-attention module with a lower cost.</p><p>Though the aforementioned vision transformers have made some progress, they suffer from an issue that restricts their performance -They fail to build the interactions among features of different scales, whereas such an ability is very vital for a lot of vision tasks. For example, an image often contains many objects of different scales, and to fully understand the image, building the interactions among those objects is helpful. Besides, some particular tasks such as instance segmentation need the interactions between large-scale (coarse-grained) features and small-scale (fine-grained) ones. Existing vision transformers fail to deal with the above cases due to two reasons: (1) The embeddings are generated from equal-sized patches, so they only own features of one single scale. Moreover, their scales are kept unchanged or enlarged uniformly through operations like average pooling in the following layers. Hence, embeddings in the same layer are always equal-scale. (2) Inside the selfattention module, adjacent embeddings are often grouped together and merged <ref type="bibr" target="#b7">Chu et al., 2021)</ref>. Since the number of groups is smaller than that of embeddings, such behavior can reduce the computational budget of the self-attention. In this case, however, even if embeddings have both small-scale and large-scale features, merging operations will lose the small-scale (fine-grained) features of each individual embedding, thereby disabling the cross-scale attention.</p><p>To enable the building of cross-scale interactions, we co-design a novel embedding layer and selfattention module as follows. 1) Cross-scale Embedding Layer (CEL) -Following , we also employ a pyramid structure for our transformer, which naturally splits the vision transformer model into multiple stages. CEL appears at the start of each stage, which receives last stage's output (or an input image) as input and samples patches with multiple kernels of different scales (e.g., 4 ? 4 or 8 ? 8). Then, each embedding is constructed by projecting and concatenating these patches as opposed to solely using one single-scale patch, which endows each embedding with cross-scale features. 2) Long Short Distance Attention (LSDA) -We propose a substitute of the vanilla selfattention, but to preserve small-scale features, the embeddings will not be merged. In contrast, we split the self-attention module into Short Distance Attention (SDA) and Long Distance Attention (LDA). SDA builds the dependencies among neighboring embeddings, while LDA takes charge of the dependencies among embeddings far away from each other. The proposed LSDA can also reduce the cost of the self-attention module like previous studies <ref type="bibr" target="#b7">Chu et al., 2021)</ref>, but different from them, LSDA does not undermine either small-scale or large-scale features. As a consequence, attention with cross-scale interactions is enabled.</p><p>Besides, following prior work <ref type="bibr" target="#b26">(Shaw et al., 2018;</ref><ref type="bibr" target="#b22">Liu et al., 2021b)</ref>, we employ a relative position bias for embeddings' position representations. The Relative Position Bias (RPB) only supports fixed image/group size 3 . However, image size for many vision tasks such as object detection is variable, so does group size for many architectures, including ours. To make the RPB more flexible, we further introduce a trainable module called Dynamic Position Bias (DPB), which receives two embeddings' relative distance as input and outputs their position bias. The DPB module is optimized end-to-end in the training phase, inducing an ignorable cost but making RPB apply to variable image/group size.</p><p>All our proposed modules can be implemented with about ten lines of code. Based on them, we construct four versatile vision transformers of different sizes, dubbed CrossFormers. Other than image classification, the proposed CrossFormer can handle a variety of tasks with variable-sized inputs such as object detection. Experiments on four representative vision tasks (i.e., image classification, object detection, instance segmentation, and semantic segmentation) demonstrate that CrossFormer outperforms the other state-of-the-art vision transformers on all the tasks. Remarkably, the performance gains brought by CrossFormer are substantially significant on dense prediction tasks, e.g., object detection and instance/semantic segmentation.</p><p>It is worth highlighting our contributions as follows:</p><p>? We propose cross-scale embedding layer (CEL) and long short distance attention (LSDA), which together compensate for existing transformers' incapability of building cross-scale attention.</p><p>? The dynamic position bias module (DPB) is further proposed to make the relative position bias more flexible, i.e., accommodating variable image size or group size.</p><p>? Multiple CrossFormers with different sizes are constructed, and we corroborate their effectiveness through sufficient experiments on four representative vision tasks.  </p><formula xml:id="formula_0">CEL (4?4, 8?8, 16?16, 32?32) CrossFormer Block ?" ! CEL (2?2, 4?4) CrossFormer Block ?" " CEL (2?2, 4?4) CrossFormer Block ?" # CEL (2?2, 4?4) CrossFormer Block ?" $ Classification Head Stage-1 Stage-2 Stage-3 Stage-4 ! ! 4 ? $ ! 4 ?% ! ! 8 ? $ ! 8 ?2% ! ! 16 ? $ ! 16 ?4% ! ! 32 ? $ ! 32<label>?8%</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Vision Transformers. Motivated by the transformers developed for NLP, researchers design specific visual transformers for vision tasks to take full advantage of their powerful attention mechanism. In particular, ViT and DeiT transfer the original transformer <ref type="bibr" target="#b30">Vaswani et al. (2017)</ref> to vision tasks <ref type="bibr" target="#b29">(Touvron et al., 2021;</ref><ref type="bibr">Dosovitskiy et al., 2021)</ref>, achieving impressive performance. Later, PVT , HVT <ref type="bibr" target="#b24">(Pan et al., 2021)</ref>, Swin <ref type="bibr" target="#b22">(Liu et al., 2021b)</ref>, and ViTAE  introduce a pyramid structure into the visual transformers, greatly decreasing the number of patches in the later layers of a respective model. They also extend the visual transformers to other vision tasks like object detection and segmentation <ref type="bibr" target="#b22">Liu et al., 2021b)</ref>.</p><p>Substitutes of Self-attention. As the core component of transformers, the self-attention module incurs the O(N 2 ) computational/memory cost, where N is the length of an embedding sequence. Though such a cost may be acceptable for image classification, it is not the case for other tasks with much larger input images (e.g., object detection and segmentation). To alleviate the cost, Swin <ref type="bibr" target="#b22">(Liu et al., 2021b)</ref> restricts the attention in a certain local region, giving up long-distance dependencies. PVT  and Twins <ref type="bibr" target="#b7">(Chu et al., 2021)</ref> make adjacent embeddings share the same key/value to reduce the cost. Likewise, other vision transformers such as <ref type="bibr" target="#b4">(Chen et al., 2021a;</ref> also employ a divide-and-conquer method and approximate the vanilla self-attention module with a lower cost.</p><p>Position Representations. Transformer is combination-invariant. That is, shuffling input embeddings does not change the output of a transformer. Nevertheless, the position of embeddings also contains important information. To make the respective model aware of this, many different position representations of embeddings <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> are proposed. For example, <ref type="bibr">Dosovitskiy et al. (2021)</ref> directly add the embeddings with the vectors that contain absolute position information. In contrast, Relative Position Bias (RPB) <ref type="bibr" target="#b26">(Shaw et al., 2018)</ref> resorts to position information indicating the relative distance of two embeddings. Much recent work <ref type="bibr" target="#b22">(Liu et al., 2021b;</ref> shows that RPB performs better than other position representations. Motivated by this finding, our proposed position representation DPB also uses relative distance, but different from RPB that only handles fixed-sized images, our DPB applies to images with dynamic sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CROSSFORMER</head><p>The overall architecture of CrossFormer is plotted in <ref type="figure" target="#fig_1">Figure 1</ref>. Following <ref type="bibr" target="#b22">Liu et al., 2021b;</ref><ref type="bibr" target="#b18">Lin et al., 2021)</ref>, CrossFormer also employs a pyramid structure, which naturally splits the transformer model into four stages. Each stage consists of a cross-scale embedding layer (CEL, Section 3.1) and several CrossFormer blocks (Section 3.2). A CEL receives last stage's output (or an input image) as input and generates cross-scale embeddings. In this process, CEL (except that in Stage-1) reduces the number of embeddings to a quarter while doubles their dimensions for a pyramid structure. Then, several CrossFormer blocks, each of which involves long short distance attention (LSDA) and dynamic position bias (DPB), are set up after CEL. A specialized head (e.g., the classification head in <ref type="figure" target="#fig_1">Figure 1</ref>) follows after the final stage accounting for a specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CROSS-SCALE EMBEDDING LAYER (CEL)</head><p>Cross-scale embedding layer (CEL) is leveraged to generate input embeddings for each stage. so that they generate the same number of embeddings 4 . As we can observe in <ref type="figure">Figure 2</ref>, every four corresponding patches have the same center but different scales, and all these four patches will be projected and concatenated as one embedding. In practice, the process of sampling and projecting can be fulfilled through four convolutional layers.</p><p>For a cross-scale embedding, one problem is how to set the projected dimension of each scale. The computational budget of a convolutional layer is proportional to K 2 D 2 , where K and D represent kernel size and input/output dimension, respectively (assuming the input dimension equals to the output dimension). Therefore, given the same dimension, a large kernel consumes more budget than a smaller one. To control the total budget of the CEL, we use a lower dimension for large kernels while a higher dimension for small kernels. <ref type="figure">Figure 2</ref> provides the specific allocation rule in its subtable, and a 128 dimensional example is given. Compared with allocating the dimension equally, our scheme saves much computational cost but does not explicitly affect the model's performance. The cross-scale embedding layers in other stages work in a similar way. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, CELs for Stage-2/3/4 use two different kernels (2 ? 2 and 4 ? 4). Further, to form a pyramid structure, the strides of CELs for Stage-2/3/4 are set as 2 ? 2 to reduce the number of embeddings to a quarter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CROSSFORMER BLOCK</head><p>Each CrossFormer block consists of a long short distance attention module (i.e., LSDA, which involves a short distance attention (SDA) module or a long distance attention (LDA) module) and a multilayer perceptron (MLP). As shown in <ref type="figure" target="#fig_1">Figure 1b</ref>, SDA and LDA appear alternately in different blocks, and the dynamic position bias (DPB) module works in both SDA and LDA for obtaining embeddings' position representations. Following the prior vision transformers, residual connections are used in each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">LONG SHORT DISTANCE ATTENTION (LSDA)</head><p>We split the self-attention module into two parts: short distance attention (SDA) and long distance attention (LDA). For SDA, every G ? G adjacent embeddings are grouped together. <ref type="figure" target="#fig_2">Figure 3a</ref> gives an example where G = 3. For LDA with input of size S ? S, the embeddings are sampled with a fixed interval I. For example in <ref type="figure" target="#fig_2">Figure 3b</ref> (I = 3), all embeddings with a red border belong to a group, and those with a yellow border comprise another group. The group's height or width for LDA is computed as G = S/I (i.e., G = 3 in this example). After grouping embeddings, both SDA and LDA employ the vanilla self-attention within each group. As a result, the memory/computational cost of the self-attention module is reduced from O(S 4 ) to O(S 2 G 2 ), and G S in most cases.</p><p>It is worth noting that the effectiveness of LDA also benefits from cross-scale embeddings. Specifically, we draw all the patches comprising two embeddings in <ref type="figure" target="#fig_2">Figure 3b</ref>. As we can see, the smallscale patches of two embeddings are non-adjacent, so it is difficult to judge their relationship without the help of the context. In other words, it will be hard to build the dependency between these two embeddings if they are only constructed by small-scale patches (i.e., single-scale feature). On the contrary, adjacent large-scale patches provide sufficient context to link these two embeddings, which makes long-distance cross-scale attention easier and more meaningful. We provide the pseudo-code of LSDA in the appendix (A.1). Based on the vanilla multi-head selfattention, LSDA can be implemented with only ten lines of code. Further, only reshape and permute operations are used, so no extra computational cost is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">DYNAMIC POSITION BIAS (DPB)</head><p>Relative position bias (RPB) indicates embeddings' relative position by adding a bias to their attention. Formally, the LSDA's attention map with RPB becomes:</p><formula xml:id="formula_1">Attention = Softmax(QK T / ? d + B)V ,<label>(1)</label></formula><p>where Q, K, V ? R G 2 ?D represent query, key, value in the self-attention module, respectively.</p><p>? d is a constant normalizer, and B ? R G 2 ?G 2 is the RPB matrix. In previous works <ref type="bibr" target="#b22">(Liu et al., 2021b)</ref>, B i,j =B ?xij ,?yij , whereB is a fixed-sized matrix, and (?x ij , ?y ij ) is the coordinate distance between the i th and j th embeddings. It is obvious that the image/group size is restricted in case that (?x ij , ?y ij ) exceeds the size ofB. In contrast, we propose an MLP-based module called DPB to generate the relative position bias dynamically, i.e.,</p><formula xml:id="formula_2">B i,j = DP B(?x ij , ?y ij ).</formula><p>(2)</p><p>The structure of DPB is displayed in <ref type="figure" target="#fig_2">Figure 3c</ref>. Its non-linear transformation consists of three fullyconnected layers with layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> and ReLU <ref type="bibr" target="#b23">(Nair &amp; Hinton, 2010)</ref>. The input dimension of DPB is 2, i.e., (?x ij , ?y ij ), and intermediate layers' dimension is set as D/4, where D is the dimension of embeddings. The output B ij is a scalar, encoding the relative position feature between the i th and j th embeddings. DPB is a trainable module optimized along with the whole transformer model. It can deal with any image/group size without worrying about the bound of (?x ij , ?y ij ). In the appendix (A.2), we prove that DPB is equivalent to RPB if the image/group size is fixed. In this case, we can transform a trained DPB to RPB in the test phase. We also provide an efficient O(G 2 ) implementation of DPB when image/group size is variable (the complexity is O(G 4 ) in a normal case because B ? R G 2 ?G 2 ). <ref type="table" target="#tab_1">Table 1</ref> lists the detailed configurations of CrossFormer's four variants (-T, -S, -B, and -L for tiny, small, base, and large, respectively) for image classification. To re-use the pre-trained weights, the models for other tasks (e.g., object detection) employ the same backbones as classification except that they may use different G and I. Specifically, besides the configurations same to classification, we also test with G 1 = G 2 = 14, I 1 = 16, and I 2 = 8 for the detection (and segmentation) models' first two stages to adapt to larger images. The specific configurations are described in the appendix (A.3). Notably, the group size or the interval (i.e., G or I) does not affect the shape of weight tensors, so the backbones pre-trained on ImageNet can be readily fine-tuned on other tasks even though they use different G or I. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VARIANTS OF CROSSFORMER</head><formula xml:id="formula_3">(S 1 = 56) SDA/LDA ? ? ? D 1 = 64 H 1 = 2 G 1 = 7 I 1 = 8 ? ? ? ? 1 ? ? ? D 1 = 96 H 1 = 3 G 1 = 7 I 1 = 8 ? ? ? ? 2 ? ? ? D 1 = 96 H 1 = 3 G 1 = 7 I 1 = 8, ? ? ? ? 2 ? ? ? D 1 = 128 H 1 = 4 G 1 = 7 I 1 = 8 ? ? ? ? 2 MLP Stage-2 28 ? 28</formula><p>Cross Embed. Kernel size: 2 ? 2, 4 ? 4, Stride=2</p><formula xml:id="formula_4">(S 2 = 28) SDA/LDA ? ? ? D 2 = 128 H 2 = 4 G 2 = 7 I 2 = 4 ? ? ? ? 1 ? ? ? D 2 = 192 H 2 = 6 G 2 = 7 I 2 = 4 ? ? ? ? 2 ? ? ? D 2 = 192 H 2 = 6 G 2 = 7 I 2 = 4 ? ? ? ? 2 ? ? ? D 2 = 256 H 2 = 8 G 2 = 7 I 2 = 4 ? ? ? ? 2 MLP Stage-3 14 ? 14</formula><p>Cross Embed. Kernel size: 2 ? 2, 4 ? 4, Stride=2</p><formula xml:id="formula_5">(S 3 = 14) SDA/LDA ? ? ? D 3 = 256 H 3 = 8 G 3 = 7 I 3 = 2 ? ? ? ? 8 ? ? ? D 3 = 384 H 3 = 12 G 3 = 7 I 3 = 2 ? ? ? ? 6 ? ? ? D 3 = 384 H 3 = 12 G 3 = 7 I 3 = 2 ? ? ? ? 18 ? ? ? D 3 = 512 H 3 = 16 G 3 = 7 I 3 = 2 ? ? ? ? 18 MLP Stage-4 7 ? 7</formula><p>Cross Embed. Kernel size: 2 ? 2, 4 ? 4, Stride=2 Results. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. As we can see, CrossFormer achieves the highest accuracy with parameters and FLOPs comparable to other state-of-the-art vision transformer structures. In specific, compared against strong baselines DeiT, PVT, and Swin, our CrossFormer outperforms them at least absolute 1.2% in accuracy on small models. Further, though RegionViT achieves the same accuracy (82.5%) as ours on a small model, it is 0.7% (84.0% vs. 83.3%) absolutely lower than ours on the large model.</p><formula xml:id="formula_6">(S 4 = 7) SDA/LDA ? ? ? D 4 = 512 H 4 = 16 G 4 = 7 I 4 = 1 ? ? ? ? 6 ? ? ? D 4 = 768 H 4 = 24 G 4 = 7 I 4 = 1 ? ? ? ? 2 ? ? ? D 4 = 768 H 4 = 24 G 4 = 7 I 4 = 1 ? ? ? ? 2 ? ? ? D 4 = 1024 H 4 = 32 G 4 = 7 I 4 = 1 ? ? ? ? 2 MLP Head 1 ? 1<label>Avg</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OBJECT DETECTION AND INSTANCE SEGMENTATION</head><p>Experimental Settings. The experiments on object detection and instance segmentation are both done on the COCO 2017 dataset <ref type="bibr" target="#b19">(Lin et al., 2014)</ref>, which contains 118K training and 5K val images.  We use MMDetection-based <ref type="bibr" target="#b6">(Chen et al., 2019)</ref> RetinaNet <ref type="bibr" target="#b20">(Lin et al., 2020)</ref> and Mask R-CNN <ref type="bibr" target="#b13">(He et al., 2017)</ref> as the object detection and instance segmentation head, respectively. For both tasks, the backbones are initialized with the weights pre-trained on ImageNet. Then the whole models are trained with batch size 16 on 8 V100 GPUs, and an AdamW optimizer with an initial learning rate of 1 ? 10 ?4 is used. Following previous works, we adopt 1? training schedule (i.e., the models are trained for 12 epochs) when taking RetinaNets as detectors, and images are resized to 800 pixels for the short side. While for Mask R-CNN, both 1? and 3? training schedules are used. It is noted that multi-scale training <ref type="bibr" target="#b3">(Carion et al., 2020)</ref> is also employed when taking 3? training schedules.</p><p>Results. The results on RetinaNet and Mask R-CNN are shown in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, respectively. As we can see, the second-placed architecture changes along with the experiment, that is, these architectures may perform well on one task but poorly on another task. In contrast, our CrossFormer outperforms all the others on both tasks (detection and segmentation) with both model sizes (small and base). Further, CrossFormer's performance gain over the other architectures gets sharper when enlarging the model, indicating that CrossFormer enjoys greater potentials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SEMANTIC SEGMENTATION</head><p>Experimental Settings. ADE20K <ref type="bibr" target="#b42">(Zhou et al., 2017)</ref> is used as the benchmark for semantic segmentation. It covers a broad range of 150 semantic categories, including 20K images for training  and 2K for validation. Similar to models for detection, we initialize the backbones with weights pretrained on ImageNet, and MMSegmentation-based (Contributors, 2020) semantic FPN and UPer-Net <ref type="bibr" target="#b33">(Xiao et al., 2018)</ref> are taken as the segmentation head. For FPN <ref type="bibr" target="#b17">(Kirillov et al., 2019)</ref>, we use an AdamW optimizer with learning rate and weight deacy of 1 ? 10 ?4 . Models are trained for 80K iterations with batch size 16. For UPerNet, an AdamW optimizer with an initial learning rate of 6 ? 10 ?5 and a weight decay of 0.01 is used, and models are trained for 160K iterations.</p><p>Results. All results are shown in <ref type="table" target="#tab_5">Table 5</ref>. Similar to object detection, CrossFormer exhibits a greater performance gain over the others when enlarging the model. For example, CrossFormer-T achieves 1.4% absolutely higher on IOU than Twins-SVT-B, but CrossFormer-B achieves 3.1% absolutely higher on IOU than Twins-SVT-L. Totally, CrossFormer shows a more significant advantage over the others on dense prediction tasks (e.g., detection and segmentation) than on classification, implying that cross-scale interactions in the attention module are more important for dense prediction tasks than for classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDIES</head><p>Cross-scale Embeddings vs. Single-scale Embeddings. We conduct the experiments by replacing cross-scale embedding layers with single-scale ones. As we can see in <ref type="table" target="#tab_6">Table 6</ref>, when using single-scale embeddings, the 8 ? 8 kernel in Stage-1 brings 0.4% (81.9% vs. 81.5%) absolute improvement compared with the 4 ? 4 kernel. It tells us that overlapping receptive fields help improve the model's performance. Besides, all models with cross-scale embeddings perform better than those with single-scale embeddings. In particular, our CrossFormer achieves 1% (82.5% vs. 81.5%) absolute performance gain compared with using single-scale embeddings for all stages. For cross-scale embeddings, we also try several different combinations of kernel sizes, and they all show similar performance (82.3% ? 82.5%). In summary, cross-scale embeddings can bring a large performance gain, yet the model is relatively robust to different choices of kernel size.</p><p>LSDA vs. Other Self-attentions. Two self-attention modules used in PVT and Swin are compared. Specifically, PVT sacrifices the small-scale features when computing the self-attention, while Swin restricts the self-attention in a local region, giving up the long-distance attention. As we can observe in <ref type="table" target="#tab_7">Table 7a</ref>, compared against the PVT-like and Swin-like self-attention mechanisms, our Cross-Former outperforms them at least absolute 0.6% accuracy (82.5% vs. 81.9%). The results show that performing the self-attention in a long-short distance manner is most conducive to improving the model's performance.</p><p>DPB vs. Other Position Representations. We compare the parameters, FLOPs, throughputs, and accuracies of the models among absolute position embedding (APE), relative position bias (RPB), and DPB. The results are shown in <ref type="table" target="#tab_7">Table 7b</ref>. DPB-residual means DPB with residual connections. Both DPB and RPB outperform APE for absolute 0.4% accuracy, which indicates that relative position representations are more beneficial than absolute ones. Further, DPB achieves the same accuracy (82.5%) as RPB with an ignorable extra cost; however, as we described in Section 3.2.2, it is more flexible than RPB and applies to variable image size or group size. The results also show that residual connection in DPB does not help improve or even degrades the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We proposed a novel transformer-based vision architecture, namely CrossFormer. Its core ingredients are Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA), thereby yielding the cross-attention module. We further proposed a dynamic position bias, making the relative position bias apply to any input size. Extensive experiments show that CrossFormer achieves superior performance over other state-of-the-art vision transformers on several representative vision tasks. Particularly, CrossFormer is demonstrated to gain great improvements on object detection and segmentation, which indicates that CEL and LSDA are together essential for dense prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A CROSSFORMER</head><p>A.1 PSEUDO CODE OF LSDA The pseudo code for LSDA is shown in Algorithm 1. As we can see, based on the vanilla selfattention module, both SDA and LDA are implemented with only ten lines of code, and only reshape and permute operations are used.  <ref type="figure">Figure 4</ref> gives an example of computing (?x ij , ?y ij ) with G = 5 in the DPB module. For a group of size G ? G, it is easy to deduce that:</p><formula xml:id="formula_7">0 ? x, y &lt; G 1 ? G ? ?x ij ? G ? 1 1 ? G ? ?y ij ? G ? 1.</formula><p>(3) Thus, motivated by the relative position bias, we construct a matrix B ? R (2G?1)?(2G?1) , wher? B i,j = DP B(1 ? G + i, 1 ? G + j), 0 ? i, j &lt; 2G ? 1. <ref type="formula">(4)</ref> The complexity of computingB is O(G 2 ). Then, the bias matrix B in DPB can be drawn fromB, i.e.,</p><formula xml:id="formula_8">B i,j =B ?xij ,?yij .<label>(5)</label></formula><p>When the image/group size (i.e., G) is fixed, bothB and B will be also unchanged in the test phase. Therefore, we only need to computeB and B once, and DPB is equivalent to relative position bias in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 VARIANTS OF CROSSFORMER FOR DETECTION AND SEGMENTATION</head><p>We test two different backbones for dense prediction tasks. The variants of CrossFormer for dense prediction (object detection, instance segmentation, and semantic segmentation) are in <ref type="table" target="#tab_8">Table 8</ref>. The architectures are the same as those for image classification except that different G and I in the first two stages are used. Notably, group size (i.e., G and I) does not affect the shape of weight tensors, so backbones pre-trained on ImageNet can be fine-tuned directly on other tasks even if they use different G and I.    <ref type="table" target="#tab_9">Table 9</ref> provides more results on object detection with RetinaNet and Mask-RCNN as detection heads. As we can see, a smaller (G, I) achieves a higher AP than a larger one, but the performance gain is marginal. Considering that a larger (G, I) can save more memory cost, we think (G 1 = 14, I 1 = 16, G 2 = 14, I 2 = 8), which accords with configurations in <ref type="table" target="#tab_8">Table 8</ref>, achieves a better trade-off between the performance and cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>architecture of CrossFormer for image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) The architecture of CrossFormer for classification. The input size is H 0 ? W 0 , and the size of feature maps in each stage is shown on the top. Stage-i consists of a CEL and n i CrossFormer blocks. Numbers in CELs represent kernels' sizes used for sampling patches. (b) The inner structure of two consecutive CrossFormer blocks. SDA and LDA appear alternately in different blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Short distance attention (SDA). Embeddings (blue cubes) are grouped by red boxes. (b) Long distance attention (LDA). Embeddings with the same color borders belong to the same group. Large patches of embeddings in the same group are adjacent. (c) Dynamic position bias (DBP). The dimensions of intermediate layers are D/4, and the output is a scalar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>carried out on four challenging tasks: image classification, object detection, instance segmentation, and semantic segmentation. To entail a fair comparison, we keep the same data augmentation and training settings as the other vision transformers as far as possible. The competitors are all competitive vision transformers, including DeiT<ref type="bibr" target="#b29">(Touvron et al., 2021)</ref>, PVT, T2T-ViT, TNT<ref type="bibr" target="#b12">(Han et al., 2021)</ref>, CViT<ref type="bibr" target="#b4">(Chen et al., 2021a)</ref>, Twins<ref type="bibr" target="#b7">(Chu et al., 2021)</ref>, Swin<ref type="bibr" target="#b22">(Liu et al., 2021b)</ref>, NesT, CvT, ViL, CAT<ref type="bibr" target="#b18">(Lin et al., 2021)</ref>, ResT<ref type="bibr" target="#b39">(Zhang &amp; Yang, 2021)</ref>, TransCNN<ref type="bibr" target="#b21">(Liu et al., 2021a)</ref>, Shuffle<ref type="bibr" target="#b15">(Huang et al., 2021)</ref>, BoTNet(Srinivas et al., 2021), and RegionViT.4.1 IMAGE CLASSIFICATIONExperimental Settings. The experiments on image classification are done with the ImageNet (Russakovsky et al., 2015) dataset. The models are trained on 1.28M training images and tested on 50K validation images. The same training settings as the other vision transformers are adopted. In particular, we use an AdamW<ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> optimizer training for 300 epochs with a cosine decay learning rate scheduler, and 20 epochs of linear warm-up are used. The batch size is 1,024 split on 8 V100 GPUs. An initial learning rate of 0.001 and a weight decay of 0.05 are used. Besides, we use drop path rate of 0.1, 0.2, 0.3, 0.5 for CrossFormer-T, CrossFormer-S, CrossFormer-B, CrossFormer-L, respectively. Further, Similar to Swin<ref type="bibr" target="#b22">(Liu et al., 2021b)</ref>,RandAugment (Cubuk  et al., 2020), Mixup<ref type="bibr" target="#b37">(Zhang et al., 2018)</ref>, Cutmix<ref type="bibr" target="#b36">(Yun et al., 2019)</ref>, random erasing<ref type="bibr" target="#b41">(Zhong et al., 2020)</ref>, and stochastic depth<ref type="bibr" target="#b14">(Huang et al., 2016)</ref> are used for data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 Figure 4 :</head><label>14</label><figDesc>LSDA code (PyTorch-like) # H: height, W: width, G: group size of SDA/LDA # x: input tensor (H, W, D) class LSDA(): def forward(x, type): ## group the embeddings if type == "SDA": x = x.reshaspe(H // G, G, W // G, G, D).permute(0, 2, 1, 3, 4) elif type == "LDA": x = x.reshaspe(G, H // G, G, W // G, D).permute(1, 3, 0, 2, 4) x = x.reshape(H * W // (G ** 2), G ** 2, D) ## the vanilla self-attention module x = Attention(x) ## un-group the embeddings x = x.reshaspe(H // G, W // G, G, G, D) if type == "SDA": x = x.permute(0, 2, 1, 3, 4).reshaspe(H, W, D) elif type == "LDA": x = x.permute(2, 0, 3, 1, 4).reshaspe(H, W, D) return x A.2 DYNAMIC POSITION BIAS (DPB)? !" = ?1 ? !" = ?2 An example of computing (?x ij , ?y ij ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure2 takes the first CEL, which is ahead of Stage-1, as an example. It receives an image as input, then sampling patches using four kernels of different sizes. The stride of four kernels is kept the same</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Embedding</cell></row><row><cell></cell><cell></cell><cell>Embed and Concatenate</cell><cell>64 dims</cell></row><row><cell>4 ? 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 ? 8</cell><cell></cell><cell></cell><cell>32 dims</cell></row><row><cell></cell><cell>16 ? 16</cell><cell></cell><cell>16 dims</cell></row><row><cell></cell><cell>32 ? 32</cell><cell></cell><cell>16 dims</cell></row><row><cell></cell><cell cols="2">Embedding Layer</cell><cell></cell></row><row><cell>Type</cell><cell>Kernel</cell><cell>Stride</cell><cell>Dim</cell></row><row><cell>Conv.</cell><cell>4 ? 4</cell><cell>4 ? 4</cell><cell>Dt 2</cell></row><row><cell>Conv.</cell><cell>8 ? 8</cell><cell>4 ? 4</cell><cell>Dt 4</cell></row><row><cell>Conv.</cell><cell>16 ? 16</cell><cell>4 ? 4</cell><cell>Dt 8</cell></row><row><cell>Conv.</cell><cell>32 ? 32</cell><cell>4 ? 4</cell><cell>Dt 8</cell></row></table><note>Figure 2: Illustration of the CEL layer. The in- put image is sampled by four different kernels (i.e., 4 ? 4, 8 ? 8, 16 ? 16, 32 ? 32) with same stride 4 ? 4. Each embedding is constructed by projecting and concatenating the four patches. D t means the total dimension of the embedding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Variants of CrossFormer for image classification. The example input size is 224 ? 224. S represents the feature maps' height (and width) of each stage. D and H mean embedding dimensions and the number of heads in the multi-head self-attention module, respectively. G and I are group size and interval for SDA and LDA, respectively.</figDesc><table><row><cell></cell><cell>Output Size</cell><cell>Layer Name</cell><cell>CrossFormer-T</cell><cell>CrossFormer-S</cell><cell>CrossFormer-B</cell><cell>CrossFormer-L</cell></row><row><cell></cell><cell></cell><cell>Cross Embed.</cell><cell></cell><cell cols="3">Kernel size: 4 ? 4, 8 ? 8, 16 ? 16, 32 ? 32, Stride=4</cell></row><row><cell>Stage-1</cell><cell>56 ? 56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the ImageNet validation set. The input size is 224 ? 224 for most models, while is 384 ? 384 for the model with a ? . Results of other architectures are drawn from original papers.</figDesc><table><row><cell>Architectures</cell><cell>#Params</cell><cell>FLOPs</cell><cell>Acc.</cell><cell>Architectures</cell><cell>#Params</cell><cell>FLOPs</cell><cell>Acc.</cell></row><row><cell>PVT-S</cell><cell>24.5M</cell><cell>3.8G</cell><cell>79.8%</cell><cell>BoTNet-S1-59</cell><cell>33.5M</cell><cell>7.3G</cell><cell>81.7%</cell></row><row><cell>RegionViT-T</cell><cell>13.8M</cell><cell>2.4G</cell><cell>80.4%</cell><cell>PVT-L</cell><cell>61.4M</cell><cell>9.8G</cell><cell>81.7%</cell></row><row><cell>Twins-SVT-S</cell><cell>24.0M</cell><cell>2.8G</cell><cell>81.3%</cell><cell>CvT-21</cell><cell>32.0M</cell><cell>7.1G</cell><cell>82.5%</cell></row><row><cell>CrossFormer-T</cell><cell>27.8M</cell><cell>2.9G</cell><cell>81.5%</cell><cell>CAT-B</cell><cell>52.0M</cell><cell>8.9G</cell><cell>82.8%</cell></row><row><cell>DeiT-S T2T-ViT CViT-S PVT-M TNT-S</cell><cell>22.1M 21.5M 26.7M 44.2M 23.8M</cell><cell>4.6G 5.2G 5.6G 6.7G 5.2G</cell><cell>79.8% 80.7% 81.0% 81.2% 81.3%</cell><cell>Swin-S RegionViT-M Twins-SVT-B NesT-S CrossFormer-B</cell><cell>50.0M 41.2M 56.0M 38.0M 52.0M</cell><cell>8.7G 7.4G 8.3G 10.4G 9.2G</cell><cell>83.0% 83.1% 83.1% 83.3% 83.4%</cell></row><row><cell>Swin-T</cell><cell>29.0M</cell><cell>4.5G</cell><cell>81.3%</cell><cell>DeiT-B</cell><cell>86.0M</cell><cell>17.5G</cell><cell>81.8%</cell></row><row><cell>NesT-T</cell><cell>17.0M</cell><cell>5.8G</cell><cell>81.5%</cell><cell>DeiT-B  ?</cell><cell>86.0M</cell><cell>55.4G</cell><cell>83.1%</cell></row><row><cell>CvT-13</cell><cell>20.0M</cell><cell>4.5G</cell><cell>81.6%</cell><cell>ViL-B</cell><cell>55.7M</cell><cell>13.4G</cell><cell>83.2%</cell></row><row><cell>ResT</cell><cell>30.2M</cell><cell>4.3G</cell><cell>81.6%</cell><cell>RegionViT-B</cell><cell>72.0M</cell><cell>13.3G</cell><cell>83.3%</cell></row><row><cell>CAT-S</cell><cell>37.0M</cell><cell>5.9G</cell><cell>81.8%</cell><cell>Twins-SVT-L</cell><cell>99.2M</cell><cell>14.8G</cell><cell>83.3%</cell></row><row><cell>ViL-S</cell><cell>24.6M</cell><cell>4.9G</cell><cell>81.8%</cell><cell>Swin-B</cell><cell>88.0M</cell><cell>15.4G</cell><cell>83.3%</cell></row><row><cell>RegionViT-S</cell><cell>30.6M</cell><cell>5.3G</cell><cell>82.5%</cell><cell>NesT-B</cell><cell>68.0M</cell><cell>17.9G</cell><cell>83.8%</cell></row><row><cell>CrossFormer-S</cell><cell>30.7M</cell><cell>4.9G</cell><cell>82.5%</cell><cell>CrossFormer-L</cell><cell>92.0M</cell><cell>16.1G</cell><cell>84.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Object detection results on COCO 2017 val set with RetinaNets as detectors. Results for Swin are drawn from Twins as Swin does not report results on RetinaNet. Results in blue fonts are the second-placed ones. CrossFormers with ? use different group sizes from classification models.</figDesc><table><row><cell cols="4">(More details are put in the appendix (A.3))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>#Params</cell><cell>FLOPs</cell><cell>AP b</cell><cell>AP b 50</cell><cell>AP b 75</cell><cell>AP b S</cell><cell>AP b M</cell><cell>AP b L</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>37.7M</cell><cell>234.0G</cell><cell>36.3</cell><cell>55.3</cell><cell>38.6</cell><cell>19.3</cell><cell>40.0</cell><cell>48.8</cell></row><row><cell></cell><cell>CAT-B</cell><cell>62.0M</cell><cell>337.0G</cell><cell>41.4</cell><cell>62.9</cell><cell>43.8</cell><cell>24.9</cell><cell>44.6</cell><cell>55.2</cell></row><row><cell></cell><cell>Swin-T</cell><cell>38.5M</cell><cell>245.0G</cell><cell>41.5</cell><cell>62.1</cell><cell>44.2</cell><cell>25.1</cell><cell>44.9</cell><cell>55.5</cell></row><row><cell></cell><cell>PVT-M</cell><cell>53.9M</cell><cell>?</cell><cell>41.9</cell><cell>63.1</cell><cell>44.3</cell><cell>25.0</cell><cell>44.9</cell><cell>57.6</cell></row><row><cell></cell><cell>ViL-M</cell><cell>50.8M</cell><cell>338.9G</cell><cell>42.9</cell><cell>64.0</cell><cell>45.4</cell><cell>27.0</cell><cell>46.1</cell><cell>57.2</cell></row><row><cell></cell><cell>RegionViT-B</cell><cell>83.4M</cell><cell>308.9G</cell><cell>43.3</cell><cell>65.2</cell><cell>46.4</cell><cell>29.2</cell><cell>46.4</cell><cell>57.0</cell></row><row><cell></cell><cell>TransCNN-B</cell><cell>36.5M</cell><cell>?</cell><cell>43.4</cell><cell>64.2</cell><cell>46.5</cell><cell>27.0</cell><cell>47.4</cell><cell>56.7</cell></row><row><cell>RetinaNet 1? schedule</cell><cell>CrossFormer-S CrossFormer-S  ?</cell><cell>40.8M 40.8M</cell><cell>282.0G 272.1G</cell><cell>44.4 (+1.0) 44.2 (+0.8)</cell><cell>65.8 65.7</cell><cell>47.4 47.2</cell><cell>28.2 28.0</cell><cell>48.4 48.0</cell><cell>59.4 59.1</cell></row><row><cell></cell><cell>ResNet101</cell><cell>56.7M</cell><cell>315.0G</cell><cell>38.5</cell><cell>57.8</cell><cell>41.2</cell><cell>21.4</cell><cell>42.6</cell><cell>51.1</cell></row><row><cell></cell><cell>PVT-L</cell><cell>71.1M</cell><cell>345.0G</cell><cell>42.6</cell><cell>63.7</cell><cell>45.4</cell><cell>25.8</cell><cell>46.0</cell><cell>58.4</cell></row><row><cell></cell><cell>Twins-SVT-B</cell><cell>67.0M</cell><cell>322.0G</cell><cell>44.4</cell><cell>66.7</cell><cell>48.1</cell><cell>28.5</cell><cell>48.9</cell><cell>60.6</cell></row><row><cell></cell><cell>RegionViT-B+</cell><cell>84.5M</cell><cell>328.2G</cell><cell>44.6</cell><cell>66.4</cell><cell>47.6</cell><cell>29.6</cell><cell>47.6</cell><cell>59.0</cell></row><row><cell></cell><cell>Swin-B</cell><cell>98.4M</cell><cell>477.0G</cell><cell>44.7</cell><cell>65.9</cell><cell>49.2</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>Twins-SVT-L</cell><cell>110.9M</cell><cell>455.0G</cell><cell>44.8</cell><cell>66.1</cell><cell>48.1</cell><cell>28.4</cell><cell>48.3</cell><cell>60.1</cell></row><row><cell></cell><cell>CrossFormer-B</cell><cell>62.1M</cell><cell>389.0G</cell><cell>46.2 (+1.4)</cell><cell>67.8</cell><cell>49.5</cell><cell>30.1</cell><cell>49.9</cell><cell>61.8</cell></row><row><cell></cell><cell>CrossFormer-B  ?</cell><cell>62.1M</cell><cell>379.1G</cell><cell>46.1 (+1.3)</cell><cell>67.7</cell><cell>49.0</cell><cell>29.5</cell><cell>49.9</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Object detection and instance segmentation results on COCO val 2017 with Mask R-CNNs as detectors. AP b and AP m are box average precision and mask average precision, respectively.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">#Params FLOPs AP b</cell><cell>AP b 50</cell><cell>AP b 75</cell><cell>AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell></row><row><cell></cell><cell>PVT-M</cell><cell>63.9M</cell><cell>? 42.0</cell><cell>64.4</cell><cell>45.6</cell><cell>39.0</cell><cell>61.6</cell><cell>42.0</cell></row><row><cell></cell><cell>Swin-T</cell><cell cols="2">47.8M 264.0G 42.2</cell><cell>64.6</cell><cell>46.2</cell><cell>39.1</cell><cell>61.6</cell><cell>42.0</cell></row><row><cell></cell><cell>Twins-PCPVT-S</cell><cell cols="2">44.3M 245.0G 42.9</cell><cell>65.8</cell><cell>47.1</cell><cell>40.0</cell><cell>62.7</cell><cell>42.9</cell></row><row><cell></cell><cell>TransCNN-B</cell><cell>46.4M</cell><cell>? 44.0</cell><cell>66.4</cell><cell>48.5</cell><cell>40.2</cell><cell>63.3</cell><cell>43.2</cell></row><row><cell></cell><cell>ViL-M</cell><cell cols="2">60.1M 261.1G 43.3</cell><cell>65.9</cell><cell>47.0</cell><cell>39.7</cell><cell>62.8</cell><cell>42.0</cell></row><row><cell></cell><cell>RegionViT-B</cell><cell cols="2">92.2M 287.9G 43.5</cell><cell>66.7</cell><cell>47.4</cell><cell>40.1</cell><cell>63.4</cell><cell>43.0</cell></row><row><cell></cell><cell>RegionViT-B+</cell><cell cols="2">93.2M 307.1G 44.5</cell><cell>67.6</cell><cell>48.7</cell><cell>41.0</cell><cell>64.4</cell><cell>43.9</cell></row><row><cell></cell><cell>CrossFormer-S</cell><cell cols="3">50.2M 301.0G 45.4 (+0.9) 68.0</cell><cell>49.7</cell><cell cols="2">41.4 (+0.4) 64.8</cell><cell>44.6</cell></row><row><cell>Mask R-CNN</cell><cell>CrossFormer-S  ?</cell><cell cols="3">50.2M 291.1G 45.0 (+0.5) 67.9</cell><cell>49.1</cell><cell cols="2">41.2 (+0.2) 64.6</cell><cell>44.3</cell></row><row><cell>1? schedule</cell><cell>CAT-B</cell><cell cols="2">71.0M 356.0G 41.8</cell><cell>65.4</cell><cell>45.2</cell><cell>38.7</cell><cell>62.3</cell><cell>41.4</cell></row><row><cell></cell><cell>PVT-L</cell><cell cols="2">81.0M 364.0G 42.9</cell><cell>65.0</cell><cell>46.6</cell><cell>39.5</cell><cell>61.9</cell><cell>42.5</cell></row><row><cell></cell><cell>Twins-SVT-B</cell><cell cols="2">76.3M 340.0G 45.1</cell><cell>67.0</cell><cell>49.4</cell><cell>41.1</cell><cell>64.1</cell><cell>44.4</cell></row><row><cell></cell><cell>ViL-B</cell><cell cols="2">76.1M 365.1G 45.1</cell><cell>67.2</cell><cell>49.3</cell><cell>41.0</cell><cell>64.3</cell><cell>44.2</cell></row><row><cell></cell><cell>Twins-SVT-L</cell><cell cols="2">119.7M 474.0G 45.2</cell><cell>67.5</cell><cell>49.4</cell><cell>41.2</cell><cell>64.5</cell><cell>44.5</cell></row><row><cell></cell><cell>Swin-S</cell><cell cols="2">69.1M 354.0G 44.8</cell><cell>66.6</cell><cell>48.9</cell><cell>40.9</cell><cell>63.4</cell><cell>44.2</cell></row><row><cell></cell><cell>Swin-B</cell><cell cols="2">107.2M 496.0G 45.5</cell><cell>?</cell><cell>?</cell><cell>41.3</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>CrossFormer-B</cell><cell cols="3">71.5M 407.9G 47.2 (+1.7) 69.9</cell><cell>51.8</cell><cell cols="2">42.7 (+1.4) 66.6</cell><cell>46.2</cell></row><row><cell></cell><cell>CrossFormer-B  ?</cell><cell cols="3">71.5M 398.1G 47.1 (+1.6) 69.9</cell><cell>52.0</cell><cell cols="2">42.7 (+1.4) 66.5</cell><cell>46.1</cell></row><row><cell></cell><cell>PVT-M</cell><cell>63.9M</cell><cell>? 44.2</cell><cell>66.0</cell><cell>48.2</cell><cell>45.0</cell><cell>63.1</cell><cell>43.5</cell></row><row><cell></cell><cell>ViL-M</cell><cell cols="2">60.1M 261.1G 44.6</cell><cell>66.3</cell><cell>48.5</cell><cell>40.7</cell><cell>63.8</cell><cell>43.7</cell></row><row><cell></cell><cell>Swin-T</cell><cell cols="2">47.8M 264.0G 46.0</cell><cell>68.2</cell><cell>50.2</cell><cell>41.6</cell><cell>65.1</cell><cell>44.8</cell></row><row><cell></cell><cell>Shuffle-T</cell><cell cols="2">48.0M 268.0G 46.8</cell><cell>68.9</cell><cell>51.5</cell><cell>42.3</cell><cell>66.0</cell><cell>45.6</cell></row><row><cell>Mask R-CNN</cell><cell>CrossFormer-S  ?</cell><cell cols="3">50.2M 291.1G 48.7 (+1.9) 70.7</cell><cell>53.7</cell><cell cols="2">43.9 (+1.6) 67.9</cell><cell>47.3</cell></row><row><cell>3? schedule</cell><cell>PVT-L</cell><cell cols="2">81.0M 364.0G 44.5</cell><cell>66.0</cell><cell>48.3</cell><cell>40.7</cell><cell>63.4</cell><cell>43.7</cell></row><row><cell></cell><cell>ViL-B</cell><cell cols="2">76.1M 365.1G 45.7</cell><cell>67.2</cell><cell>49.9</cell><cell>41.3</cell><cell>64.4</cell><cell>44.5</cell></row><row><cell></cell><cell>Shuffle-S</cell><cell cols="2">69.0M 359.0G 48.4</cell><cell>70.1</cell><cell>53.5</cell><cell>43.3</cell><cell>67.3</cell><cell>46.7</cell></row><row><cell></cell><cell>Swin-S</cell><cell cols="2">69.1M 354.0G 48.5</cell><cell>70.2</cell><cell>53.5</cell><cell>43.3</cell><cell>67.3</cell><cell>46.6</cell></row><row><cell></cell><cell>CrossFormer-B  ?</cell><cell cols="3">71.5M 398.1G 49.8 (+1.3) 71.6</cell><cell>54.9</cell><cell cols="2">44.5 (+1.2) 68.8</cell><cell>47.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Semantic segmentation results on the ADE20K validation set. "MS IOU" means testing with variable input size.</figDesc><table><row><cell cols="2">Semantic FPN (80K iterations)</cell><cell></cell><cell cols="2">UPerNet (160K iterations)</cell><cell></cell></row><row><cell>Backbone</cell><cell>#Params FLOPs IOU</cell><cell>Backbone</cell><cell>#Params</cell><cell>FLOPs IOU</cell><cell>MS IOU</cell></row><row><cell>PVT-M</cell><cell>48.0M 219.0G 41.6</cell><cell>Swin-T</cell><cell>60.0M</cell><cell>945.0G 44.5</cell><cell>45.8</cell></row><row><cell>Twins-SVT-B</cell><cell>60.4M 261.0G 45.0</cell><cell>Shuffle-T</cell><cell>60.0M</cell><cell>949.0G 46.6</cell><cell>47.6</cell></row><row><cell>Swin-S</cell><cell>53.2M 274.0G 45.2</cell><cell>CrossFormer-S</cell><cell>62.3M</cell><cell cols="2">979.5G 47.6 (+1.0) 48.4</cell></row><row><cell>CrossFormer-S</cell><cell>34.3M 220.7G 46.0 (+0.8)</cell><cell>CrossFormer-S  ?</cell><cell>62.3M</cell><cell cols="2">968.5G 47.4 (+0.8) 48.2</cell></row><row><cell>CrossFormer-S  ?</cell><cell>34.3M 209.8G 46.4 (+1.2)</cell><cell>Swin-S</cell><cell cols="2">81.0M 1038.0G 47.6</cell><cell>49.5</cell></row><row><cell>PVT-L</cell><cell>65.1M 283.0G 42.1</cell><cell>Shuffle-S</cell><cell cols="2">81.0M 1044.0G 48.4</cell><cell>49.6</cell></row><row><cell>CAT-B</cell><cell>55.0M 276.0G 43.6</cell><cell>CrossFormer-B</cell><cell cols="3">83.6M 1089.7G 49.7 (+1.3) 50.6</cell></row><row><cell>CrossFormer-B</cell><cell>55.6M 331.0G 47.7 (+4.1)</cell><cell>CrossFormer-B  ?</cell><cell cols="3">83.6M 1078.8G 49.2 (+0.8) 50.1</cell></row><row><cell>CrossFormer-B  ?</cell><cell>55.6M 320.1G 48.0 (+4.4)</cell><cell>Swin-B</cell><cell cols="2">121.0M 1088.0G 48.1</cell><cell>49.7</cell></row><row><cell>Twins-SVT-L</cell><cell>103.7M 397.0G 45.8</cell><cell>Shuffle-B</cell><cell cols="2">121.0M 1096.0G 49.0</cell><cell>?</cell></row><row><cell>CrossFormer-L</cell><cell>95.4M 497.0G 48.7 (+2.9)</cell><cell>CrossFormer-L</cell><cell cols="3">125.5M 1257.8G 50.4 (+1.4) 51.4</cell></row><row><cell>CrossFormer-L  ?</cell><cell>95.4M 482.7G 49.1 (+3.3)</cell><cell>CrossFormer-L  ?</cell><cell cols="3">125.5M 1243.5G 50.5 (+1.5) 51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on the ImageNet validation set. The baseline model is CrossFormer-S (82.5%). We test with different kernel sizes of CELs.</figDesc><table><row><cell>Stage-1</cell><cell>CEL's Kernel Size Stage-2</cell><cell>Stage-3</cell><cell>Stage-4</cell><cell cols="2">#Params FLOPs</cell><cell>Acc.</cell></row><row><cell>4 ? 4</cell><cell>2 ? 2</cell><cell>2 ? 2</cell><cell>2 ? 2</cell><cell>28.3M</cell><cell cols="2">4.5G 81.5%</cell></row><row><cell>8 ? 8</cell><cell>2 ? 2</cell><cell>2 ? 2</cell><cell>2 ? 2</cell><cell>28.3M</cell><cell cols="2">4.5G 81.9%</cell></row><row><cell>4 ? 4, 8 ? 8</cell><cell>2 ? 2, 4 ? 4</cell><cell cols="2">2 ? 2, 4 ? 4 2 ? 2, 4 ? 4</cell><cell>30.6M</cell><cell cols="2">4.8G 82.3%</cell></row><row><cell>4 ? 4, 8 ? 8, 16 ? 16, 32 ? 32</cell><cell>2 ? 2, 4 ? 4</cell><cell cols="2">2 ? 2, 4 ? 4 2 ? 2, 4 ? 4</cell><cell>30.7M</cell><cell cols="2">4.9G 82.5%</cell></row><row><cell cols="3">4 ? 4, 8 ? 8, 16 ? 16, 32 ? 32 2 ? 2, 4 ? 4, 8 ? 8 2 ? 2, 4 ? 4</cell><cell>2 ? 2</cell><cell>29.4M</cell><cell cols="2">5.0G 82.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Experimental results of ablation studies.</figDesc><table><row><cell cols="2">(a) Ablation studies on cross-scale embeddings</cell><cell cols="4">(b) Comparisons between different position representa-</cell></row><row><cell cols="2">(CEL) and long short distance attention (LSDA).</cell><cell cols="4">tions. The base model is CrossFormer-S. Throughput is</cell></row><row><cell cols="2">The base model is CrossFormer-S (82.5%).</cell><cell cols="2">tested on 1? V100 GPU.</cell><cell></cell><cell></cell></row><row><cell>PVT-like Swin-like LSDA CEL</cell><cell>Acc.</cell><cell>Method</cell><cell>#Params/FLOPs</cell><cell>Throughput</cell><cell>Acc.</cell></row><row><cell></cell><cell>81.3%</cell><cell>APE</cell><cell cols="3">30.9342M/4.9061G 686 imgs/sec 82.1%</cell></row><row><cell></cell><cell>81.9%</cell><cell>RPB</cell><cell cols="3">30.6159M/4.9062G 684 imgs/sec 82.5%</cell></row><row><cell></cell><cell>82.5%</cell><cell>DPB</cell><cell cols="3">30.6573M/4.9098G 672 imgs/sec 82.5%</cell></row><row><cell></cell><cell>81.5%</cell><cell cols="4">DPB-residual 30.6573M/4.9098G 672 imgs/sec 82.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>CrossFormer-based backbones for object detection and semantic/instance segmentation. The example input size is 1280 ? 800. D and H mean embedding dimension and the number of heads in the multi-head self-attention module, respectively. G and I are group size and interval for SDA and LDA, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Object detection results on COCO val 2017. "Memory" means the allocated memory per GPU reported by torch.cuda.max memory allocated(). ? indicates that models use different (G, I) from classification models. I 1 G 2 I 2 Memory #Params FLOPs AP b AP b</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>G 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Semantic segmentation results on ADE20K validation set with semantic FPN or UPerNet as heads.</figDesc><table><row><cell>Backbone</cell><cell cols="4">G 1 I 1 G 2 I 2</cell><cell cols="6">Semantic FPN (80K iterations) Memory #Params FLOPs IOU Memory #Params UPerNet (160K iterations) FLOP IOU MS IOU</cell></row><row><cell>CrossFormer-S</cell><cell>7</cell><cell>8</cell><cell>7</cell><cell>4</cell><cell>20.9G</cell><cell>34.3M 220.7G 46.0</cell><cell>?</cell><cell>62.3M</cell><cell>979.5G 47.6</cell><cell>48.4</cell></row><row><cell cols="4">CrossFormer-S  ? 14 16 14</cell><cell>8</cell><cell>20.9G</cell><cell>34.3M 209.8G 46.4</cell><cell>14.6G</cell><cell>62.3M</cell><cell>968.5G 47.4</cell><cell>48.2</cell></row><row><cell>CrossFormer-B</cell><cell>7</cell><cell>8</cell><cell>7</cell><cell>4</cell><cell>14.6G</cell><cell>55.6M 331.0G 47.7</cell><cell>15.8G</cell><cell cols="2">83.6M 1089.7G 49.7</cell><cell>50.6</cell></row><row><cell cols="4">CrossFormer-B  ? 14 16 14</cell><cell>8</cell><cell>14.6G</cell><cell>55.6M 320.1G 48.0</cell><cell>15.8G</cell><cell cols="2">83.6M 1078.8G 49.2</cell><cell>50.1</cell></row><row><cell>CrossFormer-L</cell><cell>7</cell><cell>8</cell><cell>7</cell><cell>4</cell><cell>25.3G</cell><cell>95.4M 497.0G 48.7</cell><cell cols="3">18.1G 125.5M 1257.8G 50.4</cell><cell>51.4</cell></row><row><cell cols="4">CrossFormer-L  ? 14 16 14</cell><cell>8</cell><cell>25.3G</cell><cell>95.4M 482.7G 49.1</cell><cell cols="3">18.1G 125.5M 1243.5G 50.5</cell><cell>51.4</cell></row><row><cell cols="3">B EXPERIMENTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">B.1 OBJECT DETECTION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Some vision transformers split input embeddings into several groups. Group size means the number of embeddings in a group.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The image will be padded if necessary.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<editor>Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno>abs/2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Regionvit: Regional-to-local attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<idno>abs/2106.02689</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twins: Revisiting spatial attention design in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno>abs/2104.13840</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics, NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcheng</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/2106.03650</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CAT: cross attention in vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yuan</surname></persName>
		</author>
		<idno>abs/2106.05786</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection. Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transformer in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajad</forename><surname>Chhatkuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/2106.03180</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<editor>Johannes F?rnkranz and Thorsten Joachims</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scalable visual transformers with hierarchical pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<idno>abs/2103.10619</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparse R-CNN: end-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2103.15808</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11209</biblScope>
			<biblScope unit="page" from="432" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/2106.03348</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/2103.15358</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rest: An efficient transformer for visual recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing-Long</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Aggregating nested transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno>abs/2105.12723</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5122" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SEMANTIC SEGMENTATION Similar to object detection, we test two different configurations of (G, I) for semantic segmentation&apos;s backbones. The results are shown in Table 10. As we can see, the memory costs of the two configurations are almost the same, which is different from experiments on object detection. Further, when taking semantic FPN as the detection head</title>
		<idno>46.4 vs. 46.0</idno>
	</analytic>
	<monogr>
		<title level="m">CrossFormers ? show advantages over CrossFormers on both IOU</title>
		<imprint/>
	</monogr>
	<note>) and FLOPs. e.g., 209.8G vs. 220.7G). When taking UPerNet as the segmentation head, a smaller (G, I) achieves higher performance like object detection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
