<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">SGOUWS@GOOGLE</orgName>
								<orgName type="institution">COM Google Inc</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. IRO</orgName>
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Canadian Institute for Advanced Research Greg Corrado Google Inc</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Raw text data is freely available in many languages, yet labeled data -e.g. text marked up with parts-of-speech or named-entities -is expensive and mostly available for English. Although several techniques exist that can learn to map hand-crafted features from one domain to another <ref type="bibr" target="#b3">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b6">Daum? III, 2009;</ref><ref type="bibr" target="#b18">Pan &amp; Yang, 2010)</ref>, it is in general non-trivial to come up with good features which generalize well across tasks, and even harder across different languages. It is therefore very desirable to have unsupervised techniques which can learn useful syntactic and semantic features that are invariant to the tasks or lan-Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copyright 2015 by the author(s). guages that we are interested in. Unsupervised distributed representations of words capture important syntactic and semantic information about languages and these techniques have been succesfully applied to a wide range of tasks <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b20">Turian et al., 2010)</ref>, across many different languages <ref type="bibr" target="#b0">(Al-Rfou' et al., 2013)</ref>. Traditionally, inducing these representations involved training a neural network language model <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref> which was slow to train. However, contemporary word embedding models are much faster in comparison, and can scale to train on billions of words per day on a single desktop machine <ref type="bibr" target="#b16">(Mnih &amp; Teh, 2012;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b19">Pennington et al., 2014)</ref>. In all these models, words are represented by learned, realvalued feature vectors referred to as word embeddings and trained from large amounts of raw text. These models have the property that similar embedding vectors are learned for similar words during training. Additionally, the vectors capture rich linguistic relationships such as male-female relationships or verb tenses, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a) and (b). These two properties improve generalization when the embedding vectors are used as features on word-and sentence-level prediction tasks.</p><p>Distributed representations can also be induced over different language-pairs and can serve as an effective way of learning linguistic regularities which generalize across languages, in that words with similar distributional syntactic and semantic properties in both languages are represented using similar vectorial representations (i.e. embed nearby in the embedded space, as shown in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>). This is especially useful for transferring limited label information from high-resource to low-resource languages, and has been demonstrated to be effective for document classification <ref type="bibr" target="#b11">(Klementiev et al., 2012)</ref>, outperforming a strong machine-translation baseline; as well as namedentity recognition and machine translation <ref type="bibr" target="#b22">(Zou et al., 2013;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013a)</ref>. Since these techniques are fundamentally data-driven techniques, the quality of the learned representations improves as the size of the training data improves <ref type="bibr" target="#b15">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b19">Pennington et al., 2014)</ref>. However, as we will discuss in more detail in ?2, there are two significant drawbacks associated with current bilingual embedding methods: they are either very slow to train or they can only exploit parallel training data. The former limits the large-scale application of these techniques, while the latter severely limits the amount of available training data, and furthermore introduces a big domain bias into the learning process, since parallel data is typically only easily available for certain narrow domains (such as parliamentary discussions). This paper introduces BilBOWA (Bilingual Bag-of-Words without Word Alignments), a simple, scalable technique for inducing bilingual word embeddings with a trivial extension to multilingual embeddings. The model is able to leverage essentially unlimited amounts of monolingual raw text. It furthermore does not require any word-level alignments, but instead extracts a bilingual signal directly from a limited sample of sentence-aligned, raw-text parallel data (e.g. Europarl) which it uses to align embeddings as they are learned over monolingual training data. Our contributions are the following:</p><p>? We introduce a novel, computationally-efficient sampled cross-lingual objective ("BilBOWA-loss") which is employed to align monolingual embeddings as they are being trained in an online setting. The monolingual models can scale to large-scale training sets, thereby avoiding training bias, and the BilBOWAloss only considers sampled bag-of-words sentencealigned data at each training step, which scales extremely well and also avoids the need for estimating word-alignments ( ?3.2);</p><p>? we experimentally evaluate the induced cross-lingual embeddings on a document-classification ( ?5.1) and lexical translation task ( ?5.2), where the method outperforms current state-of-the-art methods, with training time reduced to minutes or hours compared to several days for prior approaches;</p><p>? finally, we make available our efficient Cimplementation 1 to hopefully stimulate further research on cross-lingual distributed feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning Cross-lingual Word Embeddings</head><p>Monolingual word embedding algorithms <ref type="bibr" target="#b15">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b19">Pennington et al., 2014)</ref> learn useful features about words from raw text (e.g. <ref type="figure" target="#fig_0">Fig 1 (a)</ref> &amp; (b)). These algorithms are trained over large datasets to be able to predict words from the contexts in which they appear. Their working can intuitively be understood as mapping each word to a learned vector in an embedded space, and updating these vectors in an attempt to simultaneously minimize the distance from a word's vector to the vectors of the words with which it frequently co-occurs. The result of this optimization process yields a rich geometrical encoding of the distributional properties of natural language, where words with similar distributional properties cluster together. Due to their general nature, these features work well for several NLP prediction tasks <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b20">Turian et al., 2010)</ref>.</p><p>In the cross-lingual setup, the goal is to learn features which generalize well across different tasks and different languages. The goal is therefore to learn features (embeddings) for each word such that similar words in each lan-guage are assigned similar embeddings (the monolingual objectives), but additionally we also want similar words across languages to have similar representations (the crosslingual objective). The latter property allows one to use the learned embeddings as features for training a discriminative classifier to predict labels in one language (e.g. topics, parts-of-speech, or named-entities) where we have labelled data, and then directly transfer it to a language for which we do not have much labelled data. From an optimization perspective, there are several approaches to how one can optimize these two objectives (our classification):</p><p>OFFLINE ALIGNMENT: The simplest approach is to optimize each monolingual objective separately (i.e. train embeddings on each language separately using any of the several available off-the-shelve toolkits), and then enforce the cross-lingual constraints as a separate, disjoint, 'alignment' step. The alignment step consists of learning a transformation for projecting the embeddings of words onto the embeddings of their translation pairs, obtained from a dictionary. This was shown to be a viable approach by <ref type="bibr" target="#b14">(Mikolov et al., 2013a)</ref> who learned a linear projection from one embedding space to the other. It was extended by <ref type="bibr" target="#b8">(Faruqui &amp; Dyer, 2014)</ref>, who simultanteously projected source and target language embeddings into a joint space using canonical correlation analysis. The advantage of this approach is that it is very fast to learn the embedding alignments. The main drawback of this approach is that it is not clear that a single transformation (whether linear or nonlinear) can capture the relationships between all words in the source and target languages, and our improved results on the translation task seem to point to the contrary ( ?5.2). Furthermore, an accurate dictionary is required for the language-pair and the method considers only one translation per word, which ignores the rich multi-sense polysemy of natural languages.</p><p>PARALLEL-ONLY: Alternatively, one may leverage purely sentence-aligned parallel data and train a model to learn similar representations for the aligned sentences. This is the approach followed by the BiCVM <ref type="bibr" target="#b10">(Hermann &amp; Blunsom, 2013</ref>) and the bilingual auto-encoder (BAE, (Chandar et al., 2014)). The advantage of this approach is that it can be fast when using an efficient noise-contrastive training criterion like that of the BiCVM. The main drawbacks of this method are that it can only train on limited parallel data, which is expensive to obtain and not necessarily written in the same style or register as the domain where the features might be applied (i.e. there is a strong domain bias).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JOINTLY-TRAINED MODELS:</head><p>Another approach is to jointly optimize the monolingual objectives L(?), with the cross-lingual objective enforced as a cross-lingual regularizer (see <ref type="figure" target="#fig_1">Figure 2</ref> for a schematic). To do this, we define a cross-lingual regularization term ?(?), and use it to con- strain monolingual models as they are jointly being trained over the context h and target word w t training pairs in the dataset D, e.g.</p><formula xml:id="formula_0">L = min ? e ,? f l?{e,f } wt,h?D l L l (w t , h; ? l ) feature learning +? ?(? e , ? f ) alignment .</formula><p>(1)</p><p>This formulation captures the intuition that we want to learn representations which model their individual languages well (the first term) while the ?(?) regularizer encourages representations to be similar for words that are related across the two languages. Conceptually, this regularizer consists of minimizing a distance function between the vector representations r i learned for words w i in the two domains, weighted by how similar they are, i.e.</p><formula xml:id="formula_1">?(R e , R f ) = wi?V e wj ?V f sim(w i , w j ) ? distance(r e i , r f j ).</formula><p>(2) where we use R to denote learned embedding representations, and r i to denote the embedding learned for word w i . In other words, when this weighted sum (and hence its contribution to the total objective) is low, one can be sure that words across languages that are similar (i.e. high sim(w i , w j )) will be embedded nearby each other.</p><p>This approach was shown to be useful by <ref type="bibr" target="#b11">(Klementiev et al., 2012)</ref>. Crucially, the advantages of this formulation are that it enables one to train on any available monolingual data, which is both more abundant and less biased than the parallel-only approach, since one can train on data which resembles the data one will be applying the learned features to. The disadvantage is that the original model of <ref type="bibr">Klementiev et al.</ref> is extremely slow to train. The training complexity stems both from how the authors imple-ment their monolingual and cross-lingual objectives. For the monolingual objective, they train a standard neural language model for which the complexity of the output softmax layer grows with the output vocabulary size. Therefore, in order to evaluate the model the authors had to reduce the output vocabulary to only the 3000 most frequent words. The second reason for the slow training time is that the cross-lingual objective considers the interactions between all pairs of words (in the worst case) between the source and target vocabulary at each training step, which scales as the product of the two vocabularies 2 . In this work, we address these two issues individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The BilBOWA Model</head><p>As discussed in ?2, the primary challenges with existing bilingual embedding models are their computational complexity (due to an expensive softmax or an expensive regularization term, or both), but most importantly, the strong domain bias that is introduced by models that train only on parallel data such as Europarl.</p><p>The BilBOWA model is designed to overcome these issues in order to enable computationally-efficient cross-lingual distributed feature learning over large amounts of monolingual text. A schematic overview of the model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The two main aspects (discussed in the following sections) are 1. First, similar to <ref type="bibr" target="#b22">(Zou et al., 2013)</ref>, we leverage advances in monolingual feature learning algorithms by replacing the softmax objective with a more efficient noise-contrastive objective ( ?3.1), allowing monolingual training updates to scale independently of the vocabulary size.</p><p>2. Second, we introduce a novel computationallyefficient cross-lingual loss which only considers sampled, bag-of-words sentence-aligned data for the cross-lingual objective ( ?3.2). This avoids the need for estimating word alignments, but moreover, the computation of the regularization term reduces to only the words in the observed sample (compared to considering the O(V 2 ) worst-case possible interactions at each training step in the naive case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Monolingual Features: The L term</head><p>Since we do not care about language modelling, but more about feature learning, an alternative to the softmax is to use a noise-contrastive approach to score valid, observed combinations of words against randomly sampled, unlikely combinations of words. This idea was introduced by Collobert and Weston <ref type="bibr" target="#b5">(Collobert et al., 2011)</ref> where they opti-mized a margin between the observed score and the noise scores. In their formulation, scores were computed on sequences of words, but in <ref type="bibr" target="#b14">(Mikolov et al., 2013a)</ref> this idea was taken one step further and successfully applied to bagof-word representations of contexts in their continuous bagof-words (CBOW) and skipgram models trained using the negative sampling training objective (a simplified version of noise-contrastive estimation <ref type="bibr" target="#b16">(Mnih &amp; Teh, 2012)</ref>). Any of these objectives would yield comparable speedup and could be used in our architecture. In this work we opted for the skipgram model trained using negative sampling since it has been shown to learn high-quality monolingual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Cross-lingual Features: The</head><p>BilBOWA-loss (? term)</p><p>Besides learning how words in one language relate to each other, it is equally important for the representations to capture how words between the two languages relate to each other, which we enforce using the ? term in equation 1. In the general bilingual setting, word similarities can be expressed as a matrix A where a ij encodes the translation "score" of word i in one language with word j in the other.</p><p>In the rest of our discussion we will refer to English and French, and denote all English-specific parameters using e superscript, and all French-specific parameters with f superscript.</p><p>If the K-dimensional word embedding row-vectors r i are stacked to form a (V, K)-dimensional matrix R, then we can express what we will refer to as the exact cross-lingual objective as follows:</p><formula xml:id="formula_2">? A (R e , R f ) = i j a ij ||r e i ? r f j || 2 (3) = (R e ? R f ) A(R e ? R f ).<label>(4)</label></formula><p>where subscript A indicates that the alignments are fixed (given). A captures the relationships between all V e words in English with respect to all V f words in French, and is indeed also the source of the two main challenges in this formulation, namely:</p><p>1. how to derive or learn which words to pair as translation pairs (i.e. deriving/learning A);</p><p>2. how to efficiently evaluate ?(?) during training, since naively evaluating it scales as the product of the two vocabulary sizes O(|V e | ? |V f |) at each training step.</p><p>The cross-lingual objective therefore penalizes the Euclidian distance between words in the two embedding spaces (R e and R f ) proportional to their alignment frequency. Previous work approached this step by performing a word- alignment step prior to training to learn the alignment matrix A. However, performing word alignment requires running Giza++ <ref type="bibr" target="#b17">(Och &amp; Ney, 2003)</ref> or FastAlign <ref type="bibr" target="#b7">(Dyer et al., 2013)</ref> software and training HMM word-alignment models. This is both computationally costly and also noisy. We would like to learn the translation correspondences without utilizing word alignments. In order to do that, we directly exploit the parallel training data. The main contribution of this work is to approximate the costly ?(?) term, defined in equation 3 in terms of the global word-alignment statistics, using cheaply-obtained local word co-occurrence statistics obtained from raw-text parallel sentence-pairs (i.e. without running a word-alignment step). The main concept is illustrated schematically in <ref type="figure" target="#fig_2">Figure 3</ref>, and discussed in more detail below.</p><p>As a first step, notice that since the alignment weights can be normalized to sum to one, we can interpret the alignment weights as a distribution and write equation 3 as an expectation over the distribution of English and French wordalignment probabilities a ij = P (w e i , w f j )</p><p>,</p><formula xml:id="formula_3">? A (R e , R f ) = E (i,j)?P (w e ,w f ) ||r e i ? r f j || 2 (5)</formula><p>Since the parallel data is paired at the sentence level, we know that translation pairs for the en sentence occur in the fr sentence, but we do not know where. We therefore need a word-alignment model. A naive assumption is to assume that each observed en word can potentially be aligned with each observed fr word (i.e. to assume a uniform wordalignment model), for each word in the observed sentence pairs. Under this assumption, one can then approximate equation 5 by sampling S m-length English and n-length French sentence-pairs (s e , s f ) from the parallel training data:</p><formula xml:id="formula_4">? A (R e , R f ) ? 1 S (s e ,s f )?S 1</formula><p>mn wi?s e wj ?s f ||r e i ? r f j || 2 (6) It is important to note that the lengths of the sampled English and French parallel sentences m and n need not be equal, and more importantly m |V e | and n |V f |. Furthermore, notice that under a uniform alignment model, at each training step, each word in the sampled English sentence s e will be updated towards all words in the French sentence s f . We can precompute this by simply updating each English word towards the mean French bag-of-words sentence-vector. Specifically, we implement equation 6 by sampling only one parallel sentence-pair per training step (i.e. S = 1), and at each training step t we optimize the following sampled, approximate cross-lingual objective:</p><formula xml:id="formula_5">? (t) A (R e , R f ) || 1 m m wi?s e r e i ? 1 n n wj ?s f r f j || 2 (7)</formula><p>where s * denotes the English or French sampled sentencepair drawn from the parallel corpus. In other words, the BilBOWA-loss minimizes a sampled L 2 -loss between the mean bag-of-words sentence-vectors of the parallel corpus. On its own, this objective is degenerate since all embeddings would converge to the trivial solution (by collapsing all embeddings to the same value), but coupled as a regu-larizer with the monolingual losses, we find that it works very well in practice. By sampling training sentences from the parallel document distribution, this objective efficiently approximates equation 3 (the more two words are observed together in a parallel sentence-pair, the stronger the embeddings for the two words will be pushed together, i.e. proportional to a ij ), without having to actually compute the word alignment weights a ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parallel subsampling for better results</head><p>Equation <ref type="formula">7</ref> is an approximation of equation 3. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, we are really interested in estimating the global word-alignment statistics for a word-pair, i.e. a ij . However, by sampling words at the sentence-level, the local alignment statistics are skewed by the words' unigram frequencies of occurrence in a given sentence (i.e. regardless of alignment). Since language has a very strong Zipfian distribution we therefore find in practice that equation 7 over-regularizes the frequent words. A simple solution to this is to subsample (discard) words from the parallel sentences proportional to their unigram probabilities of occurrence. In other words, we discard each word from the parallel sentences with a probability that depends on its unigram frequency of occurrence. This heavily discards frequent words and effectively flattens the unigram distribution to a uniform distribution. This idea is closely related to the monolingual subsampling employed in the word2vec models, although it is motivated for a different reason in the cross-lingual setting to obtain a better approximation of the global word-alignment statistics from the local sentencelevel co-occurrence statistics (see <ref type="figure" target="#fig_2">Figure 3</ref>).</p><p>In practice we found this useful to learn finer-grained crosslingual embeddings for the frequent words. To better illustrate the effect this has on training, we jointly visualized the top-25 most frequent words in English and German using the t-SNE algorithm. This is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. We show in red the embeddings trained without subsampling and in blue the embeddings for the same words trained using parallel subsampling. As the visualization shows, without subsampling frequent words are over-regularized and cluster near the origin. This effect is largely reduced by the proposed subsampling scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation and Training Details</head><p>We implemented our model in C by building on the popular open-source word2vec toolkit 3 . The implementation launches a monolingual skipgram model as a separate thread for each language, as well as a cross-lingual thread. All threads access the shared embedding parameters asynchronously. For training the model, we make use of online asynchronous stochastic gradient descent (ASGD), where at time step t, parameter ? is updated as</p><formula xml:id="formula_6">? (t) = ? (t?1) ? ? ?L ?? (8)</formula><p>Our initial implementation synchronized updates between threads, but we found that simply clipping individual updates to [?0.1, 0.1] per thread was sufficient to ensure training stability and considerably improved training speed. For monolingual training data, we use the freely available, pretokenized Wikipedia datasets <ref type="bibr" target="#b0">(Al-Rfou' et al., 2013)</ref>. For cross-lingual training we use the freely-available Europarl v7 corpus <ref type="bibr" target="#b12">(Koehn, 2005)</ref>. Unlike the approach of (Klementiev et al., 2012) however, we do not need to perform a word-alignment step first. Instead our implementation trains directly on the raw parallel text files obtained after applying the standard preprocessing scripts that come with the data to tokenize, recase and remove all empty sentence-pairs. Embedding matrices were initialized by drawing from a zero mean, unit-variance gaussian distribution. The skipgram negative sampling objectives (a simplified form of noise-contrastive estimation) require us to sample k noise words per training pair from the unigram P (w) en and fr distributions, and we set k = 15 which has been shown to give good results.</p><p>Doing each training update therefore occurs asynchronously across the threads. Monolingual threads each select a context-target (h,w t )-pair for each language and sample k noise words according to their unigram noise distributions. The cross-lingual thread samples a random pair of parallel sentences from the parallel data. Finally, each thread makes an update to all parameters asynchronously according to equation 8, for which gradients are easy to compute due to the square-loss of the BilBOWA-loss and the log-linear nature of the skipgram models. The learning rate was set to 0.1, with linear decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we present experiments which evaluate the utility of the induced representations. We evaluate the embeddings in a cross-lingual document classification task which tests semantic transfer of information across languages, as well as a word-level translation task which tests fine-grained lexical transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Cross-lingual Document Classification</head><p>We use an exact replication of the cross-lingual document classification (CLDC) setup introduced by Klementiev et al. <ref type="bibr" target="#b11">(Klementiev et al., 2012)</ref> to evaluate their cross-lingual embeddings. The CLDC task setup is as follows: The goal is to classify documents in a target language using only labelled documents in a source language. Specifically, we train an averaged perceptron classifier on the labelled training data in the source language and then attempt to apply the classifier as-is to the target data (known as "direct transfer"). Documents are represented as the tf-idf-weighted sum of the embedding vectors of the words that appear in the documents.</p><p>Similar to Klementiev, we induce cross-lingual embeddings for the English-German language pair, and use the induced representations to classify a subset of the English and German sections of the Reuters RCV1/RCV2 multilingual corpora <ref type="bibr" target="#b13">(Lewis et al., 2004)</ref> as pertaining to one of four categories: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets).</p><p>For the classification experiments, 15,000 documents (for each language) were randomly selected from the RCV1/2 corpus, with one third (5,000) used as the test set and the remainder divided into training sets of sizes between 100 and 10,000, and a separate, held-out validation set of 1,000 documents used during the development of our models. Since our setup exactly mirrors Klementiev et al, we use the same baselines, namely: the majority class baseline, glossed (replacing words in the target document by their most frequently aligned words in the source language), and a stronger MT baseline (translating target documents into the source language using an SMT system).</p><p>Results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. In order to make all results comparable, results for all methods reported here were obtained using the same embedding dimensionality of 40 and the same training data. We use the first 500K lines of the English-German Europarl data both as monolingual and parallel training data. We use a vocabulary size of 46, 678 for English and 47, 903 for German. Since our method is motivated as a faster version of the model proposed by <ref type="bibr">Klementiev et al.,</ref> we note that we significantly improve upon their results, while training in 6 minutes versus the original 10 days (14,400 minutes). This yields a total factor 2, 400 speedup. This demonstrates that the Bil-BOWA loss (equation 7) is both a computationally-efficient and an accurate approximation of the full cross-lingual objective implemented by Klementiev (equation 3).</p><p>Next, we compare our method to the current state-of-the-art bilingual embedding methods. The current state-of-the-art on this task is 91.8 (en2de) and 72.8 (de2en) reported using the Bilingual Auto-encoder (BAE) model by <ref type="bibr" target="#b4">(Chandar et al., 2014)</ref>. Hermann et al. <ref type="bibr" target="#b10">(Hermann &amp; Blunsom, 2013)</ref> report 83.7 and 71.4 with the BiCVM model. As shown, our model outperforms the BiCVM on both tasks, and outperforms BAEs on German to English to yield a current state-of-the-art result on that task of 75%. The runtime of our method also compares very favorably to other methods. Note that even though the BiCVM method should in principle be as fast or faster than our method, its reported training time here is slightly higher since it was trained for more iterations over the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">WMT11 Word Translation</head><p>We also evaluated the induced cross-lingual embeddings on the word translation task used by <ref type="bibr" target="#b14">Mikolov et al. (Mikolov et al., 2013a)</ref> using the publicly-available WMT11 data 4 . In this task, the authors extracted the 6K most frequent words from the WMT11 English-Spanish data, and then used the online Google Translate service to derive dictionaries by translating these source words into the target language (individually for English and Spanish). Since their method requires translation-pairs for training, they used the first 5K most frequent words to learn the "translation matrix", and then evaluated their method on the remaining 1K words used as a test set. To translate a source word, one finds its k nearest neighbours in the target language embedding space, and then evaluate the translation precision P @k as the fraction of target translations that are within the top-k words returned using the specific method. Our method does not require translation-pairs for training, so we simply test on the same 1K test-pairs.</p><p>We use as baselines the same two methods described in <ref type="bibr" target="#b14">(Mikolov et al., 2013a)</ref>. Edit Distance ranks words based on their edit-distance. Word Co-occurrence is based on distributional similarity: For each word w, one first constructs a word co-occurrence vector which counts the words with which w co-occurs within a 10-word window in the corpus. The word-count vectors are then mapped from the source to the target language using the dictionary. Finally, for each test word, the word with the most similar vector in the tar-  <ref type="bibr" target="#b11">Klementiev et al. (Klementiev et al., 2012)</ref>, Bilingual Auto-encoders <ref type="bibr" target="#b4">(Chandar et al., 2014)</ref>, and the BiCVM model <ref type="bibr" target="#b10">(Hermann &amp; Blunsom, 2013)</ref>, on an exact replica of the Reuters cross-lingual document classification task. These methods were all used to induce 40-dimensional embeddings using the same training data. Baseline results are from Klementiev.  <ref type="table">Table 2</ref>. Results for the translation task measured as word translation accuracy (out of 100, higher is better) evaluated on the top-1 and top-5 words as ranked by the method. Cross-lingual embeddings are induced and distance in the embedded space are used to select word translation pairs. +x indicates improvement in absolute precision over the previous state-of-the-art on this task <ref type="bibr" target="#b14">(Mikolov et al., 2013a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>get language is selected as its translation.</p><p>The results on the English-Spanish translation tasks are summarized in <ref type="table">Table 2</ref>. We induced 40-dimensional embeddings using the English and Spanish Wikipedias and Europarl as parallel data. Our model improves on both the baselines and on Mikolov et al.'s method on both tasks and gives a noticeable improvement in accuracy for the P @1 prediction. For the English to Spanish translation, we improve absolute word translation accuracy by 6 percent. For the Spanish to English task, we improve absolute word translation accuracy by 9 percent. This indicates that our model is able to learn fine-grained translation equivalences from the monolingual data by using only the raw-text, sentence-aligned parallel data, despite the lack of word-level alignments or training dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The BilBOWA model as introduced in this paper utilizes a sampled L 2 bag-of-words cross-lingual loss. This is the main source of the significant speedup over the Klementiev model, allowing training to scale to much larger datasets, which in turn yields more accurate features. We found that the asynchronous implementation significantly speeds up training with no noticeable impact on the quality of the learned embeddings, and the parallel subsampling improves the accuracy of the learned features. Getting asynchronous training to work required clipping the updates, especially as the dimensionality of the embeddings gets larger. Parallel subsampling makes training more accurate, especially for the frequent words, and therefore turns out to be important both in the monolingual and crosslingual setting. We have motivated the reason for the crosslingual setting as helping to uncover a better approximation of the global alignment statistics from the observed local, sentence-level co-occurrences.</p><p>Despite the speedup, the model is still much slower to use than offline methods like translation matrix <ref type="bibr" target="#b14">(Mikolov et al., 2013a)</ref> or multilingual CCA <ref type="bibr" target="#b8">(Faruqui &amp; Dyer, 2014)</ref>. However, results on the translation task suggest BilBOWA can learn finer-grained cross-lingual relationships than these methods, and can train over much larger monolingual datasets than parallel-only methods. If the goal is to learn high-quality general purpose bilingual embeddings, it is always beneficial to leverage more training data, and hence a hybrid model like BilBOWA might be a better choice than a parallel-only technique like BiCVM or BAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduce BilBOWA, a computationally-efficient model for inducing bilingual distributed word representations directly from monolingual raw text and a limited amount of parallel data, without requiring word-alignments or dictionaries. BilBOWA combines advances in training monolingual word embeddings with a particularly efficient novel sampled cross-lingual objective. The result is that the required computations per training step scales only with the number of words in the sentences, thereby enabling efficient large-scale cross-lingual training. We achieve stateof-the-art results for English-German cross-lingual document classification whilst obtaining up to three orders of magnitude speedup, and improve upon the previous state of the art in an English-Spanish word-translation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a &amp; b)  Monolingual embeddings have been shown to capture syntactic and semantic features such as noun gender (blue) and verb tense (red). (c) The (idealized) goal of crosslingual embeddings is to capture these relationships across two or more languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic of the proposed BilBOWA model architecture for inducing bilingual word embeddings. Two monolingual skipgram models are jointly trained while enforcing a sampled L2-loss which aligns the embeddings such that translation-pairs are assigned similar embeddings in the two languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Using global word-alignments for aligning cross-lingual embeddings (equation 3) is costly and scales as the product of the two vocabulary sizes. In contrast, the BilBOWA-loss (equation 7) approximates the global loss by averaging over implicit local co-occurrence statistics in a limited sample of parallel sentence-pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>A joint t-SNE visualization of the same 25 most frequent English and German words trained without (red, left) and with parallel subsampling (blue, right), illustrating the effect that occurs without parallel subsampling, as frequent words are overregularized towards the origin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Methoden ? de de ? en Training Time (min) Classification accuracy and training times for the proposed BilBOWA method compared to</figDesc><table><row><cell>Majority Baseline</cell><cell>46.8</cell><cell>46.8</cell><cell>-</cell></row><row><cell>Glossed Baseline</cell><cell>65.1</cell><cell>68.6</cell><cell>-</cell></row><row><cell>MT Baseline</cell><cell>68.1</cell><cell>67.4</cell><cell>-</cell></row><row><cell>Klementiev et al.</cell><cell>77.6</cell><cell>71.1</cell><cell>14,400</cell></row><row><cell>Bilingual Auto-encoders (BAEs)</cell><cell>91.8</cell><cell>72.8</cell><cell>4,800</cell></row><row><cell>BiCVM</cell><cell>83.7</cell><cell>71.4</cell><cell>15</cell></row><row><cell>BilBOWA (this work)</cell><cell>86.5</cell><cell>75</cell><cell>6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/gouwsmeister/bilbowa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If we limit each word to align to k words this is still O(V k).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://code.google.com/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.statmt.org/wmt11/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou&amp;apos;</forename><surname>Rami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-3520" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria, Au</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="http://ukpmc.ac.uk/abstract/CIT/412956" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-S</forename><surname>Senecal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanislas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravidran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2014</title>
		<meeting>NIPS 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2014</title>
		<meeting>EACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">word2vec explained: Deriving mikolov et al.s negative-sampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6173</idno>
		<title level="m">Multilingual distributed representations without word alignment</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Josef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinno</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jialin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An efficient method for generating discrete random variables with general distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="256" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
