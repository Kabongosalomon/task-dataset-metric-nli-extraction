<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view 3D Reconstruction with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-24">24 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinrui</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Septimiu</forename><surname>Salcudean</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>IEEE</roleName><forename type="first">Rabab</forename><forename type="middle">Ward</forename><surname>Fellow</surname></persName>
						</author>
						<title level="a" type="main">Multi-view 3D Reconstruction with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-24">24 Mar 2021</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep CNN-based methods have so far achieved the state of the art results in multi-view 3D object reconstruction. Despite the considerable progress, the two core modules of these methods -multi-view feature extraction and fusion, are usually investigated separately, and the object relations in different views are rarely explored. In this paper, inspired by the recent great success in self-attention-based Transformer models, we reformulate the multi-view 3D reconstruction as a sequenceto-sequence prediction problem and propose a new framework named 3D Volume Transformer (VolT) for such a task. Unlike previous CNN-based methods using a separate design, we unify the feature extraction and view fusion in a single Transformer network. A natural advantage of our design lies in the exploration of view-to-view relationships using self-attention among multiple unordered inputs. On ShapeNet -a large-scale 3D reconstruction benchmark dataset, our method achieves a new state-of-the-art accuracy in multi-view reconstruction with fewer parameters (70% less) than other CNN-based methods. Experimental results also suggest the strong scaling capability of our method. Our code will be made publicly available. ? Corresponding author: Xinrui Cui (xinruic@ece.ubc.ca).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning 3D object representation from multi-view images is a fundamental and challenging problem in 3D modeling, virtual reality, and computer animation. Recently, deep learning approaches have greatly promoted the research in multiview 3D reconstruction problem, where the deep convolutional neural network (CNN) based approaches have so far achieved state-of-the-art accuracy in this task <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>To learn effective 3D representation from multiple input views, most recent CNN-based approaches follow the design principle of divide-and-conquer, where a common practice is to introduce a CNN for feature extraction and fusion module for integrating the features or reconstruction results from multiple views. Despite the strong connection between the two modules, their methodology designs are investigated separately. Also, during the CNN feature extraction stage, the object relations in different views are rarely explored. Although some recent approaches have introduced recurrent neural network (RNN) to learn object relationships between different views <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, such a design lacks computational efficiency and the input views to the RNN model are permutationsensitive <ref type="bibr" target="#b5">[6]</ref>, which is difficult to be compatible with a set of unordered input views. It is also shown in recent researches that CNN-based reconstruction methods may suffer from the model scaling problem. For example, when the number of model inputs exceeds a certain scale (e.g. <ref type="bibr" target="#b3">4)</ref>, the accuracy of the model will be saturated, showing the difficulty of learning complementary knowledge through a large set of independent CNN feature extraction units <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Considering the above challenges, we propose a new framework named "3D Volume Transformer (VolT)", and explore the potential of recent great success of self-attention based language model for the multi-view 3D object reconstruction task. We reformulate the multi-view 3D reconstruction as a sequence-to-sequence prediction problem and unify the feature extraction and view fusion in a single Transformer network. On one hand, in multi-view 3D reconstruction, from a particular view, we can only see part of the underlying 3D structure. On the other hand, in a Transformer model, the self-attention mechanism has recently shown its great power on learning complex semantic abstractions within an arbitrary number of input tokens <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and is naturally suitable for exploring the view-to-view relationships of a 3D object's different semantic parts. Given all this, the structure of Transformer <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> becomes a natural and attractive solution for the multi-view 3D reconstruction.</p><p>Our Transformer-based framework contains a 2D-view Transformer encoder and a 3D-volume Transformer decoder, as presented in <ref type="figure" target="#fig_0">Figure 1</ref>. In the proposed framework, the 2Dview encoder encodes and fuses the multiple 2D-view information by exploring their "2D-view ? 2D-view" relationships of the different inputs. The 3D-volume decoder decodes and fuses the multi-view features from the encoder and generate a 3D probabilistic voxel output for each of the spatial query tokens. The attention layers in the decoder learns "2D-view ? 3Dvolume" relationships between each of the output voxel grids and input views. Meanwhile, volume attention layers in the decoder further learn "3D-volume ? 3D-volume" relationships by exploiting correlations amongst different 3D locations. By using the above unified design, the "2D-view ? 2D-view", "3D-volume ? 3D-volume", and "2D-view ? 3D-volume" relationships can be jointly explored by multiple attention layers in both the encoder and decoder networks.</p><p>On basis of the above encoder-decoder structure design, we further investigate the "attention uniformity" problem in a Transformer model and propose a effective solution for enhancing the effectiveness of a Transformer model in the multi-view reconstruction task. In Transformers, self-attention possesses a solid inductive bias towards "token uniformity" <ref type="bibr" target="#b10">[11]</ref>, which encourages feature representations of input tokens converge. However, this convergence may further cause the problem of "attention uniformity" in deeper layers, which makes a Transformer model loses expressive power speedily with respect to network depth <ref type="bibr" target="#b10">[11]</ref>. We show that in the multi-view 3D reconstruction task, this problem is particu- larly prominent and will limit the Transformer's capability of exploring and abstracting multi-view associations at a deeper level. In our experiment, we found that when directly adopting the vanilla Transformer <ref type="bibr" target="#b8">[9]</ref> as our backbone for multi-view 3D reconstruction, the increase in depth will cause degradation of reconstruction accuracy when the model exceeds a certain depth. To tackle it, we further propose the divergenceenhanced Transformer that can slow down the divergence decay in the self-attention layers by enhancing the discrepancy of the embeddings from different views.</p><p>The contributions can be summarized as follows:</p><p>? We propose a brand new framework VolT for multi-view 3D object reconstruction. Different from the previous CNN based methods that using a separate design of feature extraction + view fusion, we unify these two stages into a single Transformer network and re-frame the 3D reconstruction as a "sequence-to-sequence" prediction problem. ? The proposed method can jointly and naturally explore multi-level correspondence and associations between the 2D input views and 3D output volume with in a single unified framework. ? We investigate the problem of "divergence decay" in the proposed 3D Volume Transformer layers and propose a view-divergence enhancing operation in our self-attention layers to avoid such degradation. ? Our method achieves a new state-of-the-art for multi-view 3D reconstruction on ShapeNet with only 30% amount of parameters of other recent CNN-based methods. Our method also shows better scaling capability on the number of input views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-view 3D Reconstruction</head><p>Reconstructing an object's 3D shape from multi-view images has long been a research hot-spot in both computer vision and computer graphics. Traditional methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> of this field are typically designed based on hand-crafted geometric features. Some representatives of early methods like Structure from Motion (SfM) <ref type="bibr" target="#b11">[12]</ref>, Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b12">[13]</ref> need extrinsic camera parameters, which are not always feasible to obtain. Recently, CNN-based approaches, without requiring viewpoint labels or camera calibration, have quickly become the main stream of multiview 3D reconstruction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and have achieved the state of the art reconstruction accuracy.</p><p>In CNN-based methods, a 2D-CNN view encoder, a 3D-CNN view decoder, and a multi-view fusion model are usually separately designed for 3D reconstruction. Among them, the fusion plays an central role in the integration of multi-view feature information. Previous multi-view fusion methods can be roughly grouped into three categories, i.e., pooling-based fusion, learnable weighted-sum fusion and RNN-based fusion. The The pooling-based fusion, including max-pooling fusion and average-pooling fusion, only learns partial information of multiple views and ignores the view associations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The learnable weighted-sum fusion models are introduced to resolve these problems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The RNN-based fusion methods like 3D-R2N2 <ref type="bibr" target="#b3">[4]</ref> and LSM <ref type="bibr" target="#b4">[5]</ref> can learn effective view-to-view relations but are computational expensive and permutation-variant <ref type="bibr" target="#b5">[6]</ref>. In this paper, different from the above CNN-based methods, we propose a Transformer-based 3D reconstruction method, which unifies the feature extraction and view fusion in a single model and naturally explore the relationship between different input views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transformer</head><p>In natural language processing, Transformer models have achieves great success in a variety of tasks such as machine translation, text classification, and question answering <ref type="bibr" target="#b15">[16]</ref>.</p><p>The key to the Transformer is the multi-head self-attention operation, which aggregates features among every token pairs of the embedding sequence. Recently, Transformer has been also successfully adapted to the computer vision field <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref> and shows promising application prospective. DETR <ref type="bibr" target="#b16">[17]</ref> provides a new framework for object detection that combines a 2D CNN with a Transformer, and directly predicts (in parallel) the final object detection as a sequence of language tokens. ViT <ref type="bibr" target="#b9">[10]</ref> applies Transformer directly to sequences of image patches for the image classification task without using CNNs features and have achieves comparable and even higher image classification accuracy when pretrained on large-scale dataset. In CNN-based multi-view 3D reconstruction methods, it is still a difficult task to design a fusion model that can explore the deep relationship between views while maintaining the permutation-invariant capability. A natural advantage of the Transformer in multi-view 3D reconstruction is that its token embedding can be abstracted and learned layer by layer in a disorderly manner, which can naturally ease the pain points of CNN-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework</head><p>The proposed 3D volume Transformer model consists of a 2D-view encoder and a 3D-volume decoder. The 2D-view encoder encodes the relevant information amongst different views via view attention layers. The 3D-volume decoder learns global correlations of different spatial locations in volume attention layers and predicts the final 3D volume output. We uniformly split the 3D space into a set of tokens and the predicted volumes for each token are finally stitched into the final 3D reconstruction output. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an overview of the proposed framework.</p><p>In this paper, we implement three different versions of method based on the proposed framework: Vanilla 3D Volume Transformer (VolT), Vanilla 3D Volume Transformer+ (VolT+), and view-divergence-Enhance 3D Volume Transformer (EVolT).</p><p>? VolT: A baseline implementation of the proposed method using vanilla Transformer model as our baseline and using standard VGG16 features as our initial view embeddings. ? VolT+: Using 2D-view embeddings obtained from an advanced pretrained CNN compared with VolT. We use it to testify the impact of 2D-view embeddings on our Transformer-based framework for multi-view 3D reconstruction. ? EVolT: A full implementation of our method adopting the proposed view-divergence enhancing function in the proposed 3D Volume Transformer framework. Here, to obtain 2D-view initial embeddings, we use a pretrained CNN that is shared among multi-views. Note that we can also build "EVolT+" with an advanced CNN for 2Dview embedding learning in VolT+ to further improve the performance of EVolT. However, we prefer to keep the small parameter size of the EVolT and emphasize the advantage of the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Divergence-enhanced 2D-view Encoder</head><formula xml:id="formula_0">Suppose I = {I 1 , I 2 , ? ? ? , I M },</formula><p>where M denote the multiview image set of an object to be reconstructed. For each I m , we first use a pretrained view-shared CNN to obtain a set of initial view embedding x m ? R 1?d , where d is the feature dimension. Then, the 2D-view encoder takes in the initial view embeddings X 0 = [x 1 ; x 2 ; ? ? ? ; x M ] ? R M?d and refines the multi-view representations by exploring a global relationship amongst multiple views using a series of self-attention layers. Here, to keep permutation invariant for the view sequence X , the positional encodings of a standard Transformer are removed. We build our divergence-enhanced 2D-view encoder based on DETR <ref type="bibr" target="#b16">[17]</ref> by stacking N = 6 basic blocks. Each basic block consists of a multi-head divergence-enhanced view attention layer (denoted as MH-DEAtt, Eq. <ref type="formula">(2)</ref>) and a position-wise feed-forward network (FFN, Eq. (4)). The 2Dview encoder is formulated as follows:</p><formula xml:id="formula_1">X 0 = [x 1 ; x 2 ; ? ? ? ; x M ] (1) X l = MH-DEAtt(X l?1 , X 0 ) (2) X l = Norm(X l + X l?1 ) (3) X l = FFN(X l )<label>(4)</label></formula><formula xml:id="formula_2">X l = Norm(X l +X l )<label>(5)</label></formula><p>where "Norm" denotes layer normalization and l is the index of a basic block (l = 1, ? ? ? , L). The embeddings of the layer L are used as the output of our 2D-view encoder. As shown in the right side of <ref type="figure" target="#fig_0">Figure 1</ref>, the scaled dotproduct attention (denoted as Attn) aggregates the feature representations amongst multiple views by learning view-toview relationships. Meantime, we propose a view-divergence enhancing function (DiView) to ease the discrepancy degradation of the multi-view representations in deeper layers. Specifically, DiView introduces skip connections and concatenates the internal view features with the input view embeddings in the feature dimension. The MH-DEAtt layer is defined as follows</p><formula xml:id="formula_3">MH-DEAtt(X, X 0 ) = DiView(A, X 0 )W view where A = cat(A 1 , ? ? ? , A H ) (6) A h = Attn(Q h , K h , V h )</formula><p>where "cat" denotes the concatenation operation and h is the number of head in MH-DEAtt layer. W view ? R (Hd k +d)?d denotes the parameter matrix of the linear function, and d k is the feature dimension in each head. In the h-th head, M queries stacked in Q h ? R M?d k are projected from M view embeddings stacked in X with the parameter matrix W h Q ? R d?d k . Similarly, the keys and values stacked in</p><formula xml:id="formula_4">K h ? R M?d k and V h ? R M?d k are obtained with parameter matrices W h K ? R d?d k , W h V ? R d?d k , respectively: Q h = XW h Q ; K h = XW h K ; V h = XW h V .<label>(7)</label></formula><p>Specifically, in the Attention function "Attn", the output for a query is represented as an attention-score weighted sum of the values V. Therefore, the Attn function is formulated as</p><formula xml:id="formula_5">Attn(Q, K, V) = softmax( QK T ? d k )V,<label>(8)</label></formula><p>where d k is a scalar for normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D-volume Decoder</head><p>The 3D-volume decoder in our framework learns the global correlation amongst different spatial locations and explore the relationship between the view and spatial domains. Given an object, we denote Y 0 = [y 1 ; y 2 ; ? ? ? ; y N ] as a sequence learnable 3D-volume queries at the input end of the decoder, where y n ? R 1?d corresponds to the n-th 3D-volume. Here, positional encodings E pos are added to 3D-volume embeddings to keep the position information in the spatial domain. In the decoder, a basic block contains a volume attention layer, a view-volume attention layer, and a FFN. The decoder can be formulated as follows:</p><formula xml:id="formula_6">Y 0 = [y 1 ; y 2 ; ? ? ? ; y N ] + E pos (9) Y l = MH-VolAttn(Y l?1 ) (10) Y l = Norm(Y l + Y l?1 ) (11) Y l = MH-ViewVolAttn(? l , X L )<label>(12)</label></formula><formula xml:id="formula_7">Y l = Norm(Y l +? l )<label>(13)</label></formula><formula xml:id="formula_8">Y l = FFN(? l )<label>(14)</label></formula><formula xml:id="formula_9">Y l = Norm(Y l +? l )<label>(15)</label></formula><p>where MH-VolAttn (in Eq. <ref type="formula">(10)</ref>) and MH-ViewVolAttn (in Eq. <ref type="formula" target="#formula_6">(12)</ref>) denote the multi-head volume attention layer and the multi-head view-volume attention layer, respectively. We use the embeddings at the layer L as the output of the decoder. In our decoder, the MH-VolAttn layer learns global dependencies amongst different 3D-volumes, and is calculated as follows:</p><formula xml:id="formula_10">MH-VolAttn(Y) = cat(A 1 , ? ? ? , A H )W vol where A h = Attn(YW h Q , YW h K , YW h V .<label>(16)</label></formula><p>The MH-ViewVolAttn layer integrates the relevant information across the view and spatial domains, and is calculated as follows:</p><formula xml:id="formula_11">MH-ViewVolAttn(Y, X L ) = cat(A 1 , ? ? ? , A H )W where A h = Attn(YW h Q , X L W h K , X L W h V )f i<label>(17)</label></formula><p>where W vol ? R Hd k ?d and W ? R Hd k ?d are the parameter matrices of the corresponding linear functions. Finally, after the 3D-volume decoder, we use a linear function to project the output embeddings of each 3D volume to their 3D output space. Then the predicted 3D volumes are reshaped and grouped to the final reconstruction output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pix2Vox-A [1]</head><p>VGG16 <ref type="bibr" target="#b19">[20]</ref> 114.24 Pix2Vox++/A <ref type="bibr" target="#b2">[3]</ref> ResNet50 <ref type="bibr" target="#b20">[21]</ref> 96.31 VolT VGG16 <ref type="bibr" target="#b19">[20]</ref> 28.63 VolT+ 2D-CNN+3D-DCNN 96.76 EVolT VGG16 <ref type="bibr" target="#b19">[20]</ref> 29.03</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We utilize the ShapeNet dataset <ref type="bibr" target="#b18">[19]</ref> to evaluate the proposed methods and other comparison methods. We follow 3D-R2N2 <ref type="bibr" target="#b3">[4]</ref> and use the same setting for a fair comparison. Specifically, we use a subset of ShapeNet which consists of 13 categories and 43,783 common 3D objects. For each 3D object, 24 2D-images are rendered from different viewing angles circling around the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics 1) IoU:</head><p>The mean Intersection-over-Union (IoU) calculates the matching degree between predicted 3D voxel grids and their ground-truth grids. A higher IoU value means a better reconstruction result. For each voxel grid, the IoU is defined as:</p><formula xml:id="formula_12">IoU = (i,j,k) I(y(i, j, k) &gt; t)I(?(i, j, k)) (i,j,k) I[I(y(i, j, k) &gt; t) + I(?(i, j, k))] ,<label>(18)</label></formula><p>where y(i, j, k) denotes the predicted occupancy probability which is binarized with an optimal fixed voxelization-threshold t for compared methods.?(i, j, k) is the ground truth at (i, j, k). I(?) is an indicator function.</p><p>2) F-Score: Compared with IoU, F-score <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b2">[3]</ref> explicitly evaluates the distance between object surfaces, which is more interpretable. F-score is formally defined as the harmonic mean between precision P(d) and recall R(d) with a distance threshold d:</p><formula xml:id="formula_13">F-Score(d) = 2P(d)R(d) P(d) + R(d)<label>(19)</label></formula><p>A higher F-score with a stringent distance threshold indicates a better reconstruction result. In F-Score, P(d) estimates the reconstruction accuracy by counting the portion of reconstructed points lying within the distance d = 1% to the ground truth. R(d) quantifies the reconstruction completeness by counting the percentage of ground-truth points lying within the distance d to the reconstruction. These two metrics are defined as follows:</p><formula xml:id="formula_14">P(d) = 1 |R| r?R [e r?G &lt; d], e r?G = min r?G r ? g<label>(20)</label></formula><formula xml:id="formula_15">R(d) = 1 |G| g?G [e g?R &lt; d], e g?R = min r?R g ? r ,<label>(21)</label></formula><p>where [?] is the Iverson bracket. G is the ground-truth point set and R is the reconstructed point set being evaluated. We apply F-Score with the same setting in <ref type="bibr" target="#b2">[3]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Divergence measurement for multi-view representations:</head><p>We also define a metric to explore the convergence of multiview representations in different layers. Since the convergence has a positive correlation with the divergence decay of multiview attentions, we utilize a similarity measure based on multiview attentions to evaluate the divergence enhancing ability in our method.</p><p>In each view attention layer, an attention-score matrix S = softmax( QK T ? d k ) contains view-to-view attention vectors. The m-th row of S, denoted as s m , is an attention-score vector where each element represents its attention weight to another view. For 3D reconstruction of a specific object, the Euclidean distance measuring the similarity of multi-view attentions, is defined as</p><formula xml:id="formula_16">D = 1 N view m s m ?s 2 wheres = 1 N view m s m .<label>(22)</label></formula><p>Here, a small D means a more considerable similarity and the convergence of multi-view representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We set the batch size to 64 and the view image size to 224 ? 224 for training. The 3D spatial size of the voxelized output is set to 32 ? 32 ? 32. The VolT and its two variants VolT+, and EVolT are trained by an AdamW optimizer <ref type="bibr" target="#b22">[23]</ref> with a ? 1 of 0.9 and a ? 2 of 0.999. <ref type="table" target="#tab_0">Table I</ref> shows the parameter sizes of competing methods and pretrained CNNs for the initial view embeddings used in different competing methods. Compared with Pix2Vox-A <ref type="bibr" target="#b0">[1]</ref> and Pix2Vox++/A <ref type="bibr" target="#b2">[3]</ref>, the parameter size of EVolT is only around 30% of them. To obtain the reported best results, Pix2Vox-A and Pix2Vox++/A both adopt an additional 3D-CNN-based refiner containing another 3D-CNN and 3D-DCNN. In contrast, our proposed end-to-end methods do not need additional refiner and can also achieve the best results. To testify the effect of the transformer architecture, in VolT+, we apply an advanced CNN feature extraction model for 2Dview embeddings from the 2D-CNN and 3D-DCNN without the last layer in Pix2Vox-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-view 3D Object Reconstruction</head><p>1) Quantitative results: Here, we show the quantitative results of compared methods on ShapeNet using different evaluation metrics. <ref type="table" target="#tab_0">Table II</ref> shows the comparison of 24-view object reconstruction on ShapeNet using IoU and F-Score metrics. The highest value for each category is highlighted in bold. This table shows that EVolT reaches the highest IoU and F-score amongst the compared methods. VolT gets moderate  results between Pix2Vox-A and Pix2Vox++/A. VolT+ works better than VolT because it uses better initial features. How-ever, VolT+ still falls behind EVolT even the EVolT is simply based on the plain VGG features. These observations indicate that the view-divergence enhancing function in EVolT plays an indispensable role in increasing its performance against the compared methods. <ref type="table" target="#tab_0">Table III</ref> shows the multi-view object reconstruction results on ShapeNet. The best score for each number of views is highlighted in bold. This table shows that the performances of our methods increase appreciably as the number of views increases. In comparison, other compared methods increase slightly when the view number enlarges. For example, the mean IoU of EVolT increases by 0.04 from 8 views to 24 views, which is eight times the improvement of Pix2Vox++/A. This observation indicates that the proposed Transformerbased methods has better scaling ability and can learn a more comprehensive 3D representation with the increase of view number. We can also see from this table that our proposed methods get the best F-Score when the view number is larger than 6 and get the best IoU when the view number is higher  2) Qualitative results: In <ref type="figure" target="#fig_1">Figure 2</ref>, we show the qualitative results of 3D object reconstruction of different methods on ShapeNet. In each object sample, we provide object reconstruction results from different number of input views, i.e., 12 views, 18 views, and 24 views. The first two rows on the left part of <ref type="figure" target="#fig_1">Figure 2</ref> show the 12 input views of an object, and the corresponding reconstruction results of competing methods are shown at the second row on the right. Similarly, the first three rows on the left part are the 18 input views corresponding to the results on the right. <ref type="figure" target="#fig_1">Figure 2</ref> shows that EVolT can obtain more accurate and complete 3D reconstruction against compared methods. For example, the EVolT results in the last column successfully recover chair legs and monitor stand while other methods only show incomplete parts. More qualitative results can be found in our Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>The following ablation experiments are made to verify the effectiveness of the proposed view-divergence enhancing function.</p><p>1) Effect on 3D reconstruction accuracy: In <ref type="figure" target="#fig_2">Figure 3</ref>, we quantitatively evaluate the influence of the view-divergence enhancing function on 3D reconstruction results by comparing EVolT with VolT and Pix2Vox-A. From <ref type="figure" target="#fig_2">Figure 3</ref>, we can observe that EVolT significantly outperforms VolT that achieves better results than Pix2Vox-A. This indicates the positive effect of the view-divergence enhancing function on 3D reconstruction results.</p><p>2) Effect on the view divergence: In <ref type="figure" target="#fig_3">Figure 4</ref>, we visualize the view-to-view attention matrix in different layers by VolT and EVolT. We set the input view number to 24 in this experiment. In the attention matrix at each layer, the m-th row shows an attention vector where each element is the attention weight of the m-th view to another view. From the top of <ref type="figure" target="#fig_3">Figure 4</ref>, we can observe that rows become more similar in a standard transformer as the attention layers go deeper. As a comparison, in EVolT, we can see the diversity of multiview attention still keeps in deep layers, which means that the divergence enhancement function in EVolT can effectively slow down the convergence degradation of multi-views in deeper layers.</p><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, the similarity measurement score D in Eq. 22 is also recorded to analyze the convergence amongst multiview representations in each layer. For 100 randomly chosen objects, we plot D in different layers displayed in the left of <ref type="figure" target="#fig_4">Figure 5</ref>, and the right shows the average values of D. A small D suggests a significant convergence amongst multiview representations. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the value of D obtained by VolT declines gradually with deepening the 2D-view encoder layer while the value of D of EVolT at the same layer keeps higher than that of VolT.</p><p>The ablation studies indicate that the view-divergence enhancing function plays an essential role in improving the proposed EVolT performance and relieving the convergence amongst multi-view representations in different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORKS</head><p>In this paper, we propose a Transformer-based framework for multi-view 3D reconstruction and achieves state-of-theart accuracy on ShapeNet with fewer parameters than other CNN-based methods. We propose three versions of the method (VolT, VolT+ and EVolT) to explore view and spatial domain relationships for multi-view 3D reconstruction. Meantime, we explore the problem of divergence decay for the multi-view information in deeper layers and proposed view-divergence enhancing function to ease such a problem. In our future work, we will work on exploring the interpretability of the proposed framework and give its explanation for 3D reconstruction. We also plan to build an interpretable way to visualize and understand the correspondence between the latent representation and multiple input views.  <ref type="table">Table 6</ref> gives a comparison between the proposed transformer-based 3D reconstruction methods and existing CNN-based methods from the perspective of components and structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ADDITIONAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. View Divergence</head><p>In <ref type="figure" target="#fig_7">Figure 7</ref>, we plot the estimated probability density of the D value at different attention layers for VolT and EVolT. We use kernel density estimation (KDE) to compute the probability density and explore the convergence of multi-view representations in different attention layers. A small D means a more considerable convergence of multi-view representations.</p><p>In each view attention layer, the probability density functionp(D) of D is estimated a? </p><p>where s i m is the attention vector of the m-th view for the i-th object. The number of random objects is set to N object = 100. The input view number is set to N view = 24. Here, we used the Gaussian kernel K(x) = 1 ? 2? exp(? x <ref type="bibr" target="#b1">2</ref> 2 ). h is computed by the rule of thumb of Scott. It is shown in <ref type="figure" target="#fig_7">Figure 7</ref> that the density of EVolT has a much larger variance than that of the VolT. Also, as the attention layers go deeper, the D value of the VolT gradually moves closer to 0 while the EVolT can still cover a larger range of D values. This indicates that the divergence enhancement function in EVolT can effectively slow down the convergence degradation of multi-views in deeper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head><p>We provide more object reconstruction results of competing methods, as shown in <ref type="figure" target="#fig_8">Figure 8</ref>, 9, 10, and 11. In each object sample, we provide object reconstruction results from different number of input views, i.e., 12 views, 18 views, and 24 views. The first two rows on the left part of <ref type="figure" target="#fig_8">Figure 8</ref> show the 12 input views of an object, and the corresponding reconstruction results of competing methods are shown at the second row on the right. Similarly, the first three rows on the left part are the 18 input views corresponding to the results on the right. The qualitative comparison suggests the superiority of the proposed method in terms of the reconstruction topology and details.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of EVolT for Multi-view 3D Object Reconstruction (left). The proposed view-divergence enhancing function in our EVolT (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative 3D object reconstruction results on ShapeNet based on different number of input 2D-view images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Effect of the view-divergence enhancing function on 3D reconstruction results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Multi-view attention-matrix visualization in VolT and EVolT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Discrepancy amongst multi-view representations in VolT and EVolT. than 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of CNN-based methods and the proposed Transformer-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Kernel density estimation of D value in different attention layers for VolT and EVolT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative reconstruction results of competing methods for bench (top), aeroplane (middle), and sofa (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 . 13 Fig. 10 . 14 Fig. 11 .</head><label>913101411</label><figDesc>Qualitative reconstruction results of competing methods for table (top), lamp (middle), and chair (bottom). Qualitative reconstruction results of competing methods for aeroplane (top), display (middle), and chair (bottom). Qualitative reconstruction results of competing methods for sofa (top), aeroplane (middle), and bench (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PARAMETER</head><label>I</label><figDesc>SIZES OF COMPETING METHODS AND PRETRAINED CNNS FOR 2D-VIEW EMBEDDINGS IN COMPETING METHODS.</figDesc><table><row><cell>Pretrained CNN used for 2D-view embeddings</cell><cell>Param. (M)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF 24-VIEW RECONSTRUCTION ON SHAPENET USING IOU AND F-SCORE. THE BEST SCORE FOR EACH CATEGORY IS IN BOLD.</figDesc><table><row><cell></cell><cell>24-view IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">24-view F-Score@1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>Pix2Vox-A</cell><cell>Pix2Vox++/A</cell><cell>VolT</cell><cell>VolT+</cell><cell>EVolT</cell><cell>Pix2Vox-A</cell><cell>Pix2Vox++/A</cell><cell>VolT</cell><cell>VolT+</cell><cell>EVolT</cell></row><row><cell>airplane</cell><cell>0.731</cell><cell>0.729</cell><cell>0.719</cell><cell>0.725</cell><cell>0.741</cell><cell>0.635</cell><cell>0.614</cell><cell>0.604</cell><cell>0.618</cell><cell>0.636</cell></row><row><cell>bench</cell><cell>0.679</cell><cell>0.686</cell><cell>0.678</cell><cell>0.682</cell><cell>0.707</cell><cell>0.525</cell><cell>0.522</cell><cell>0.513</cell><cell>0.525</cell><cell>0.548</cell></row><row><cell>cabinet</cell><cell>0.822</cell><cell>0.829</cell><cell>0.825</cell><cell>0.825</cell><cell>0.832</cell><cell>0.448</cell><cell>0.456</cell><cell>0.452</cell><cell>0.455</cell><cell>0.464</cell></row><row><cell>car</cell><cell>0.880</cell><cell>0.883</cell><cell>0.884</cell><cell>0.885</cell><cell>0.894</cell><cell>0.598</cell><cell>0.598</cell><cell>0.604</cell><cell>0.609</cell><cell>0.624</cell></row><row><cell>chair</cell><cell>0.620</cell><cell>0.647</cell><cell>0.645</cell><cell>0.641</cell><cell>0.681</cell><cell>0.318</cell><cell>0.341</cell><cell>0.339</cell><cell>0.340</cell><cell>0.373</cell></row><row><cell>display</cell><cell>0.599</cell><cell>0.613</cell><cell>0.635</cell><cell>0.613</cell><cell>0.674</cell><cell>0.320</cell><cell>0.335</cell><cell>0.366</cell><cell>0.339</cell><cell>0.403</cell></row><row><cell>lamp</cell><cell>0.475</cell><cell>0.493</cell><cell>0.478</cell><cell>0.481</cell><cell>0.520</cell><cell>0.335</cell><cell>0.351</cell><cell>0.320</cell><cell>0.338</cell><cell>0.366</cell></row><row><cell>speaker</cell><cell>0.751</cell><cell>0.762</cell><cell>0.762</cell><cell>0.753</cell><cell>0.772</cell><cell>0.309</cell><cell>0.326</cell><cell>0.327</cell><cell>0.317</cell><cell>0.339</cell></row><row><cell>rifle</cell><cell>0.676</cell><cell>0.686</cell><cell>0.663</cell><cell>0.693</cell><cell>0.711</cell><cell>0.615</cell><cell>0.624</cell><cell>0.597</cell><cell>0.634</cell><cell>0.653</cell></row><row><cell>sofa</cell><cell>0.764</cell><cell>0.782</cell><cell>0.781</cell><cell>0.776</cell><cell>0.800</cell><cell>0.427</cell><cell>0.454</cell><cell>0.449</cell><cell>0.448</cell><cell>0.478</cell></row><row><cell>table</cell><cell>0.644</cell><cell>0.666</cell><cell>0.649</cell><cell>0.658</cell><cell>0.675</cell><cell>0.398</cell><cell>0.419</cell><cell>0.407</cell><cell>0.418</cell><cell>0.431</cell></row><row><cell>telephone</cell><cell>0.837</cell><cell>0.849</cell><cell>0.857</cell><cell>0.850</cell><cell>0.867</cell><cell>0.659</cell><cell>0.666</cell><cell>0.678</cell><cell>0.675</cell><cell>0.687</cell></row><row><cell>watercraft</cell><cell>0.655</cell><cell>0.668</cell><cell>0.670</cell><cell>0.670</cell><cell>0.693</cell><cell>0.441</cell><cell>0.460</cell><cell>0.456</cell><cell>0.470</cell><cell>0.494</cell></row><row><cell>Overall</cell><cell>0.706</cell><cell>0.720</cell><cell>0.714</cell><cell>0.716</cell><cell>0.738</cell><cell>0.462</cell><cell>0.473</cell><cell>0.468</cell><cell>0.475</cell><cell>0.497</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF MULTI-VIEW RECONSTRUCTION ON SHAPENET USING IOU AND F-SCORE. THE BEST SCORE FOR EACH VIEW NUMBER IS IN BOLD.</figDesc><table><row><cell>F-Score@1%</cell><cell>24</cell><cell>23</cell><cell>22</cell><cell>21</cell><cell>20</cell><cell>18</cell><cell>16</cell><cell>14</cell><cell>12</cell><cell>8</cell><cell>6</cell><cell>4</cell></row><row><cell>3D-R2N2 [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.383</cell><cell>-</cell><cell>0.382</cell><cell>-</cell><cell>0.382</cell><cell>0.383</cell><cell>-</cell><cell>0.378</cell></row><row><cell>AttSets [2]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.448</cell><cell>-</cell><cell>0.447</cell><cell>-</cell><cell>0.445</cell><cell>0.444</cell><cell>-</cell><cell>0.430</cell></row><row><cell>Pix2Vox-A [1]</cell><cell>0.462</cell><cell>0.462</cell><cell>0.462</cell><cell>0.462</cell><cell>0.462</cell><cell>0.461</cell><cell>0.461</cell><cell>0.461</cell><cell>0.460</cell><cell>0.458</cell><cell>0.456</cell><cell>0.452</cell></row><row><cell>Pix2Vox++/A [3]</cell><cell>0.473</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.462</cell><cell>-</cell><cell>0.461</cell><cell>-</cell><cell>0.460</cell><cell>0.459</cell><cell>-</cell><cell>0.457</cell></row><row><cell>VolT</cell><cell>0.468</cell><cell>0.467</cell><cell>0.467</cell><cell>0.465</cell><cell>0.464</cell><cell>0.461</cell><cell>0.459</cell><cell>0.456</cell><cell>0.450</cell><cell>0.430</cell><cell>0.410</cell><cell>0.356</cell></row><row><cell>VolT+</cell><cell>0.475</cell><cell>0.475</cell><cell>0.474</cell><cell>0.474</cell><cell>0.474</cell><cell>0.473</cell><cell>0.472</cell><cell>0.471</cell><cell>0.469</cell><cell>0.464</cell><cell>0.460</cell><cell>0.451</cell></row><row><cell>EVolT</cell><cell>0.497</cell><cell>0.496</cell><cell>0.495</cell><cell>0.494</cell><cell>0.492</cell><cell>0.489</cell><cell>0.486</cell><cell>0.481</cell><cell>0.475</cell><cell>0.448</cell><cell>0.423</cell><cell>0.358</cell></row><row><cell>IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D-R2N2 [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.636</cell><cell>-</cell><cell>0.636</cell><cell>-</cell><cell>0.636</cell><cell>0.635</cell><cell>-</cell><cell>0.625</cell></row><row><cell>AttSets [2]</cell><cell>0.694</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.693</cell><cell>-</cell><cell>0.692</cell><cell>-</cell><cell>0.688</cell><cell>0.685</cell><cell>-</cell><cell>0.675</cell></row><row><cell>Pix2Vox-A [1]</cell><cell>0.706</cell><cell>0.706</cell><cell>0.706</cell><cell>0.706</cell><cell>0.706</cell><cell>0.705</cell><cell>0.705</cell><cell>0.705</cell><cell>0.704</cell><cell>0.702</cell><cell>0.700</cell><cell>0.697</cell></row><row><cell>Pix2Vox++/A [3]</cell><cell>0.720</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.719</cell><cell>-</cell><cell>0.718</cell><cell>-</cell><cell>0.717</cell><cell>0.715</cell><cell>-</cell><cell>0.708</cell></row><row><cell>VolT</cell><cell>0.714</cell><cell>0.713</cell><cell>0.712</cell><cell>0.711</cell><cell>0.711</cell><cell>0.708</cell><cell>0.706</cell><cell>0.703</cell><cell>0.699</cell><cell>0.681</cell><cell>0.662</cell><cell>0.605</cell></row><row><cell>VolT+</cell><cell>0.716</cell><cell>0.716</cell><cell>0.716</cell><cell>0.715</cell><cell>0.715</cell><cell>0.714</cell><cell>0.714</cell><cell>0.713</cell><cell>0.711</cell><cell>0.707</cell><cell>0.704</cell><cell>0.695</cell></row><row><cell>EVolT</cell><cell>0.738</cell><cell>0.738</cell><cell>0.737</cell><cell>0.735</cell><cell>0.735</cell><cell>0.732</cell><cell>0.729</cell><cell>0.726</cell><cell>0.720</cell><cell>0.698</cell><cell>0.675</cell><cell>0.609</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Supplementary Material: Multi-view 3D Reconstruction with Transformer I. COMPARISON OF STRUCTURES IN COMPETING METHODS</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pix2vox: Context-aware 3d reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2690" to="2698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust attentional aggregation of deep feature sets for multi-view 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="73" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pix2vox++: multiscale context-aware 3d object reconstruction from single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2919" to="2935" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>?zye?il</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305" to="364" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual simultaneous localization and mapping: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fuentes-Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Ascencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rend?n-Mancha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep-MVS: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Raynet: Learning volumetric 3d reconstruction with ray potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What do single-view 3d reconstruction networks learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
