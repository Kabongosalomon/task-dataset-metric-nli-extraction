<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain specific BERT representation for Named Entity Recognition of lab protocol</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Vaidhya</surname></persName>
							<email>iamtejasvaidhya@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kharagpur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Kaushal</surname></persName>
							<email>ayushk4@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kharagpur</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain specific BERT representation for Named Entity Recognition of lab protocol</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised models trained to predict properties from representations, have been achieving high accuracy on a variety of tasks. For instance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistic tasks. But the vocabulary used in the medical field contains a lot of different tokens used only in the medical industry such as the name of different diseases, devices, organisms, medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the System for Named Entity Tagging based on Bio-Bert. Experimental results show that our model gives substantial improvements over the baseline and stood the fourth runner up in terms of F1 score, and first runner up in terms of Recall with just 2.21 F1 score behind the best one. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A large amount of data is generated every year in the medical field. One of the most important generated data is the documentation of protocols. It provides individual sets of instructions that allow scientists to recreate experiments in their own laboratory. Most of them are written in Natural language which reduces its machine readability. The protocol gives a concise overview of the project which reduces its pre-processing needs but also make it less informative syntactically that eventually results in less accuracy.</p><p>Recent progress in Named Entity Recognition was made possible by the advancements of deep learning techniques used in natural language processing (NLP). For instance, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997)</ref> and Conditional Random Field (CRF) <ref type="bibr" target="#b8">(Namikoshi et al., 2017)</ref> have greatly improved performance in biomedical named entity recognition (NER) over the last few years. Bio-BERT  outperform all the other previous approaches with the help of BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> architecture pre-trained on Bio-medical texts <ref type="bibr" target="#b2">(Giorgi and Bader, 2018;</ref><ref type="bibr" target="#b3">Habibi et al., 2017;</ref><ref type="bibr" target="#b11">Wang et al., 2018;</ref>.</p><p>In this paper, we are introducing our system for the NER tagging on the WLP dataset <ref type="bibr" target="#b6">(Kulkarni et al., 2018)</ref>. We use a variant of Bio-Bert . The primary motivation to use the model is it's medical vocabulary and features encoded in the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description and Data Set</head><p>Formally, the WNUT 2020 Shared Task-1 Named Entity Recognition, organized within, the 6th Workshop on Noisy User-generated Text (WNUT), 2020 <ref type="bibr" target="#b10">(Tabassum et al., 2020</ref>) is a NER prediction task. It can be expressed as 'tokens-level' classification task mathematically as:</p><p>Let the sentence S be defined as: S = {s 1 , s 2 , ..., s n } n is the number words in the sentences can be classified into the following label set y = {l 1 , l 2 , l 3 , ..., l m } where m is labels. Given named entity of type XXX. Whenever two entities of type XXX are immediately next to each other, the first word of the second entity will be tagged B-XXX in order to show that it starts another entity and the entities inside B-XXX will be represented as I-XXX. For example, the sentence {nCoV-2019, sequencing, protocol} have the following labels {B-Reagent, B-Method, I-Method}.</p><p>DataSet: All of the protocols <ref type="bibr" target="#b6">(Kulkarni et al., 2018)</ref> were collected from protocols.io using their public APIs by organising team. For the shared arXiv:2012.11145v1 [cs.CL] 21 Dec 2020 Task-1 W-NUT 2020: Named Entity Extraction, the annotation of 615 protocols are re-annotated using BART styled annotated protocols by 3 annotators with 0.75 inter-annotator agreement, measured by span-level Cohen's Kappa. The re-annotators incorporate missing entity-relations and also corrected the inconsistencies.</p><p>The task aims to create the system for Protocol-Named Entities Recognition (NER). The main difference that makes it difficult for traditional NER taggers is the vast vocabulary in medical filed and use of limited syntactic information. For instances "QIAprep Spin Miniprep" is device used in medical industry, but not present in our regular vocabulary that also makes it difficult for traditional NER tagger to learn. <ref type="figure" target="#fig_0">Figure 1</ref> provides the visualization of annotated datasets provided by WNUT 2020 Shared Task-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In the section 3.1 we are going to define our proposed architecture and in section 3.2 briefly review the Bio-BERT  used for final submission and also different Domain specific BERT based model used for experiment as shown in the <ref type="table">Table 1</ref>.</p><p>Baseline The organiser provided a simple Linear CRF model 3 . It utilized simple gazetteers and handcrafted feature to predict the entities from the test data. We replaced it with our proposed BERT based Architecture as describe in the section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>As described in figure 2, we first sub-word tokenize each token of sentences, using BERT's wordpiece tokenizer of Huggingface library and pass it through different domain specific BERT models or BERT Transformer stacks (scibert, biobert, bertbased, bert-large etc) to extract contextualised representation <ref type="bibr">(Beltagy et al., 2019;</ref><ref type="bibr" target="#b1">Devlin et al., 2018)</ref>. We then select the representation of first sub-word token for each word and use simple Linear or Dense layer with the softmax activation function as classifier to get probability on the labels from the contextualised representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bio-BERT</head><p>Bio-BERT ) is a contextualized language representation model, based on BERT, a pre-trained model that is trained on different combinations of general &amp; biomedical domain corpora. According to , just like its parental model BERT, it is also capable of capturing contextualized bidirectional representations. Thus it has outperformed existing architectures in most of the Named Entity Recognition tasks within the biomedical domain by using a limited amount of dataset. We hypothesize that such domain-specific bidirectional representations are also critical for our task. Bio-BERT are pre-trained on the following different datasets {Wiki + Books, Wiki + Books + PubMed, Wiki + Books + PMC, Wiki + Books + PubMed + PMC}.</p><p>we again hypothesize to achieved best performance in PubMed(comprises more than 30 million citations for biomedical literature from MEDLINE, life science journals, and online books.) trained dataset because of its linguistic similarity with protocols. For instance both of them contains medical procedure to reproduce medical experiment.</p><p>Other Models used in Experiment We also used other domain specific BERT for experimentation using the same architecture as discussed in section 3.1 with replacement of BERT Models in place of BioBERT (as shown in figure 2)  Language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. Hence, we try fine-tuning BERT to obtain better results on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparison and Discussion</head><p>We experimented with different BERT models as shown in <ref type="table">Table 1</ref>. We avoid any preprocessing other than the Bert specific tokenization, as it may result in loss of crucial semantic information in text.</p><p>We also assumed protocols are relatively less noisy compare to other crowd source data with compact sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>We keep maximum length of input sentence to 512 to consider Long sentences in protocols. For all the base models (12-layer, 768-hidden, 12-heads, 110M parameters) including our Bio-BERT model, we train all for 8 epoch with batch size 16. Large models(24-layer, 1024-hidden, 16-heads, 340M parameters.) are trained for 4 epochs with batch size 16. We early stop the models using the valid set. The dropout probability was set to 0.1 for all layers. Optimization is done using Adam <ref type="bibr" target="#b5">(Kingma and Ba, 2014)</ref> with a learning rate of 1e-5. The remaining hyperparameters were kept same as <ref type="bibr" target="#b1">Devlin et al. (2018)</ref>. We used the PyTorch <ref type="bibr" target="#b9">(Paszke et al., 2019)</ref> implementation of BERT from Huggingfaces tranformers <ref type="bibr" target="#b12">(Wolf et al., 2019)</ref> library. For selecting best models in experimental phase (i.e. before release of test set) we use split of 60/20/20 for train, dev and test respectively. Our final submission, used a 70/30 split for train/valid set of initial data and Bio-BERT  model and split sentence with more than 512 tokens to two more sentence to get desired model's input sentence length. To evaluate the performance of the system, an evaluation script along with the dataset was provided by the organizers 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Inferences</head><p>Our Bio-Bert  based model performed best of all the models because of domain specific knowledge. Our model performed extremely well on final test set as shown in <ref type="table" target="#tab_2">Table  2</ref> and stood 4th runner up in term of F1 score and   1st runner up in term of recall out of 13 teams participated in the competition.</p><p>Inferences: Use of uncased large BERT results in significant loss of about 2.29 F1 score in comparison of cased-large BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, clearly shows importance of syntactic nature of protocols. We observed better performance, after increase in both training and validation set for our final submissions indicating inability of model to fully capture the representation due to fine tuning on limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Error Analysis</head><p>1. BERT tokenizer is not efficient on Bio-Medical text as illustrated in <ref type="table" target="#tab_4">Table 4</ref> . Its vocabulary does not consists of Bio-medical words and it is not trained on domain specific setting. Hence makes it difficult for BERT model to learn the encoding based on poorly sub tokenized word. The possible solution will be training tokenizers on both biomedical and general text sets.</p><p>2. Treatment of task as token level classification problem results in incorrect detection of intermediate Entity as illustrated in <ref type="table" target="#tab_3">Table 3</ref>. Hence evidenced in decrease of F1 score in complete match compare to partial match in our submission provided by organisers.</p><p>3. Use of Nomenclature, Scientific formula, abbreviations makes it difficult for Pre-trained language models to generalised with limited fine tuning data. Though with the help of contextual details it is observed that the BERT was able to correctly predict scientific formula Bio-Med Terms subword-tokenized acetyltransferase ['ace','ty','lt','ran','s', 'fer','ase'] Hematoxylin <ref type="bibr">['He','mat','ox','yl','</ref>in'] sulfanilamide <ref type="bibr">['su','lf','ani','lam','</ref>ide'] ddH2O <ref type="bibr">['d','d','H','2','</ref>O'] lBiotin-16-UTP <ref type="bibr">['l',</ref><ref type="bibr">'B',</ref><ref type="bibr">'iot',</ref><ref type="bibr">'in',</ref><ref type="bibr">'16',</ref><ref type="bibr">'U',</ref><ref type="bibr">'TP']</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we present our system for the Named Entity recognition for Bio-medical protocols a for Shared Task at W-NUT Task-1 2020. We built upon the recent success of Pre-trained language models and apply them for protocols. Our System achieves close to state-of-art performance on this task. As future work, we will try to experiment with XLNet  and different ensembling between the models and would like to extend the work of <ref type="bibr" target="#b0">Clark et al. (2019)</ref> by performing layer by layer Analysis of BERT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualisation of annotated dataset 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The TOK X represents the token of sentence where X ? N and N is the length of sentence. T-I.J represents the I th tokens and J th subtokens and we used WordPiece tokenizer for all of our models.? Sci-BERT (Beltagy et al., 2019) : A pretrained contextualized embedding model based on BERT to address the lack of highquality, large-scale labeled scientific data. ? BERT {large/base,cased/uncased} :BERT (Devlin et al., 2018) It is designed to train deep bidirectional representations by jointly conditioning on both left and right context in all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the held-out test set provided by shared task organisers on final submission</figDesc><table><row><cell cols="3">Tokens correct labels predicted labels</cell></row><row><cell>standard</cell><cell>B-Reagent</cell><cell>B-Reagent</cell></row><row><cell>T4</cell><cell>I-Reagent</cell><cell>B-Device</cell></row><row><cell>DNA</cell><cell>I-Reagent</cell><cell>I-Reagent</cell></row><row><cell>Ligase</cell><cell>I-Reagent</cell><cell>I-Reagent</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Error arises due to consideration of token level classification</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Illustration of inefficient sub-tokenization of Bio-Med words at some places. For instance, in the sentence {Add, 5gm, SDS}, SDS was correctly labelled as Reagent by our model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tejasvaidhyadev/ NER_Lab_Protocols</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Image Source: Github repository of WNUT 2020 Shared Task-1 NER. 3 https://github.com/jeniyat/WNUT_2020_ NER/tree/master/code/baseline_CRF</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/jeniyat/WNUT_2020_ NER/tree/master/code/eval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.linkedin.com/in/ santosh-t-y-s-s-39227b116/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the Computer Science and Engineering Department of Indian Institute of Technology, Kharagpur for providing us the computational resources required for performing various experiments.We are very grateful for the invaluable suggestions given by T.Y.S.S. santosh 5 and Aarushi Gupta. We also thank the organizers of the Shared Task-1 at WNUT, EMNLP-2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: Pretrained language model for scientific text. In EMNLP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>Black-BoxNLP@ACL</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arxiv:1810.04805Comment: 13 pages</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transfer learning for biomedical named entity recognition with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary D</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty449</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4087" to="4094" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning with word embeddings improves biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Habibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Luis</forename><surname>Wiegandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx228</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An annotated corpus for machine reading of instructions in wet lab protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Machiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BioBERT: a pretrained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crf-based bibliography extraction from reference strings using a small amount of training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Namikoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Takasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Twelfth International Conference on Digital Information Management (ICDIM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WNUT-2020 Task 1: Extracting Entities and Relations from Wet Lab Protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeniya</forename><surname>Tabassum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2020 Workshop on Noisy User-generated Text (WNUT)</title>
		<meeting>EMNLP 2020 Workshop on Noisy User-generated Text (WNUT)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-type biomedical named entity recognition with deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty869</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1745" to="1752" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collabonet: collaboration of deep neural networks for biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-2813-6</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">249</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
