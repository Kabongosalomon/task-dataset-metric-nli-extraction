<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">See Clearer at Night: Towards Robust Nighttime Semantic Segmentation through Day-Night Image Conversion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaite</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">See Clearer at Night: Towards Robust Nighttime Semantic Segmentation through Day-Night Image Conversion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Networks</term>
					<term>Semantic Segmentation</term>
					<term>Generative Adversarial Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, intelligent driving navigation and security monitoring have made considerable progress with the help of deep Convolutional Neural Networks (CNNs). As one of the state-of-the-art perception approaches, semantic segmentation unifies distinct detection tasks widely desired by both autonomous driving and security monitoring. Currently, semantic segmentation shows remarkable efficiency and reliability in standard scenarios such as daytime scenes with favorable illumination conditions. However, in face of adverse conditions such as the nighttime, semantic segmentation loses its accuracy significantly. One of the main causes of the problem is the lack of sufficient annotated segmentation datasets of nighttime scenes. In this paper, we propose a framework to alleviate the accuracy decline when semantic segmentation is taken to adverse conditions by using Generative Adversarial Networks (GANs). To bridge the daytime and nighttime image domains, we made key observation that compared to datasets in adverse conditions, there are considerable amount of segmentation datasets in standard conditions such as BDD and our collected ZJU datasets. Our GAN-based nighttime semantic segmentation framework includes two methods. In the first method, GANs were used to translate nighttime images to the daytime, thus semantic segmentation can be performed using robust models already trained on daytime datasets. In another method, we use GANs to translate different ratio of daytime images in the dataset to the nighttime but still with their labels. In this sense, synthetic nighttime segmentation datasets can be generated to yield models prepared to operate at nighttime conditions robustly. In our experiment, the later method significantly boosts the performance at the nighttime evidenced by quantitative results using Intersection over Union (IoU) and Pixel Accuracy (Acc). We show that the performance varies with respect to the proportion of synthetic nighttime images in the dataset, where the sweet spot corresponds to most robust performance across the day and night. The proposed framework not only makes contribution to the optimization of visual perception in intelligent vehicles, but also can be applied to diverse navigational assistance systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Vision tasks like object detection and semantic segmentation (i.e. pixel-wise scene classification) are always the key points in security monitoring and autonomous driving. Convolutional Neural Networks(CNNs) have fueled the development of these methods thanks to the availability of larger datasets and computationallypowerful machines that have emerged in recent years. Semantic segmentation, which unifies distinct detection tasks with a single consumer camera 1-3 makes RADAR and LiDAR sensors become the second choices, freeing scene perception from complex multi-sensor fusion. <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> Some state-of-the-art CNN methods like PSPNet, <ref type="bibr" target="#b6">7</ref> Re-fineNet, <ref type="bibr" target="#b7">8</ref> DeepLab, <ref type="bibr" target="#b8">9</ref> and ACNet 10 perform semantic segmentation with very high accuracies. In order to apply semantic segmentation to autonomous driving and security monitoring, we have proposed ERF-PSPNet, 1, 11 a high-accuracy real-time semantic segmentation method in previous work, which is more computationally efficient than most of the state-of-the-art methods.</p><p>All these perception algorithms are designed to operate on images taken at daytime under good illumination conditions. <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> However, outdoor applications can hardly escape from challenging weather and illumination conditions. One of the reasons why computer vision system based on semantic segmentation have not been widely applied yet is because that it can not deal with adverse conditions. For example, semantic segmentation <ref type="figure">Figure 1</ref>. The figure shows the main frame of our work. On the left, a day-night converter is trained using unpaired datytime and nighttime images. The first row shows our first method: training a day-model and converting night domain images to the day domain before inference. Bottom row shows our second method: converting different ratios of images in the training set to nighttime images to train a night model. The ratio of synthetic nighttime images determines the model's accuracy in testing sets.</p><p>using visible light camera performs unsatisfactorily in the nighttime for the reason that when under extremely weak illuminance, the structure, texture and color features of objects change drastically. These features can either disappear because of the lack of the illuminance or being highly disturbed by artificial light. Thus, how to enhance the robustness of semantic segmentation has been an important issue in the computer vision domain. <ref type="bibr" target="#b14">15</ref> In this work, we focus on improving nighttime semantic segmentation performance.</p><p>There are some researchers that have proposed to use Far-Infrared (FIR) camera instead of visible light camera. <ref type="bibr" target="#b15">16</ref> FIR camera is a feasible measure, but they are expansive and only provide low-resolution images. In addition, there are rare FIR semantic segmentation datasets. In the other way, visible light camera is much cheaper, and there are a large number of datasets of daytime images. For these reasons, we choose visible light camera as the image acquisition device. On the other hand, high-accuracy semantic segmentation models are trained from large scale of annotated images. But annotating nighttime images requires extensive time and human effort, and it is impossible to annotate images at the pixel wise for all the other adverse conditions.</p><p>In this paper, we mainly propose a main frame (see <ref type="figure">Fig. 1</ref>) to overcome the problem of large accuracy downgrade from daytime to nighttime in semantic segmentation. Inspired by the idea of Generative Adversarial Networks (GANs), 17 nighttime images are converted on-the-fly during inference to the daytime domain as a pre-processing step of the first proposed method. In the other way, we augment an original large-scale semantic segmentation dataset such as the BDD database 18 by translating part of the daytime images to nighttime images. Among the experiments, the feasibility of improving robustness of semantic segmentation is validated. In addition, we record a dataset in the Yuquan Campus of Zhejiang university with both day and night images as well as GPS information by using our Multi-Modal Stereo Vision Sensor 19 (see <ref type="figure" target="#fig_0">Fig.2</ref>), which has been made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Understanding of the Road Scene</head><p>Semantic segmentation is important in understanding the content of images and finding target objects, and this technique is vital for the field of automatic driving. <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref> Currently, most of state-of-the-art semantic segmentation models are based on fully convolutional end-to-end networks. <ref type="bibr" target="#b21">22</ref> Inspired by SegNet, 23 semantic segmentation models usually follow an encoder-decoder network architecture. The encoder is a vanilla CNN which is trained to classify the input, and the decoder is used to upsample the output of the encoder to the same size as the input image. <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> Further, more efficient networks were proposed to achieve the goal of real-time semantic segmentation. <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28</ref> Our works are based on ERF-PSPNet, 1, 11 a state-of-the-art semantic segmentation network designed for navigation assistance systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Adaption</head><p>Generally, CNNs only learn features from the domain of training datasets, and may perform much worse in a different domain. This is also the reason why semantic segmentation model trained in daytime domain drops accuracy in the nighttime domain. To improve the generalization of convolutional neural network, many methods were proposed. Most commonly, data augmentation techniques like random cropping, random rotation and flipping are used to make networks perform stably in unfamiliar domains. <ref type="bibr" target="#b11">12</ref> Effective use of synthetic data has been preliminarily studied in. <ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30</ref> There is another domain adaptation-based approach that was proposed to adapt semantic segmentation models from synthetic images to real environments. <ref type="bibr" target="#b30">31</ref> The other attempts that are most close to our work, which also improve the model robustness at the nighttime, make use of twilight images to transfer knowledge from standard daytime conditions to nighttime images. <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33</ref> Similar efforts were also made to address robust foggy scene parsing <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> and rainy scene semantic segmentation. <ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37</ref> Unsupervised learning has also been frequently leveraged to pre-process input images, in order to prevent performance from degrading catastrophically when the input domain differs significantly from previously seen domains. <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38</ref> Specifically, this research line is also highly related to topological localization, <ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39</ref> where modern visual localizers like <ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41</ref> can also benefit from the input adaptation to perform more reliably against variation challenges. More recently, model distillation/imitation were applied to make model behave stable in unseen domains. <ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Stylization</head><p>Since I. Goodfellow et al. proposed Generative Adversarial Networks (GANs), <ref type="bibr" target="#b16">17</ref> GANs have become the most promising method for image stylization. Formally, GANs simultaneously contain two models: a generative model G that captures the critical distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than generated by G. Although state-of-the-art GANs like Pix2Pix 44 perform impressively for style transfer, the training data in both domains have to be pre-formatted into a single X/Y image pair that holds tight pixel-wise correlation. A recently proposed CycleGAN 45 is designed to perform a full translation cycle, and make it possible to make use of images in two different domains without paring images, which is suitable for our work to translate image across daytime and nighttime domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In our work, two methods are proposed to narrow the gap between daytime and nighttime images in semantic segmentation. These methods respectively correspond to converting nighttime images to daytime images and the vice versa. <ref type="figure">Fig. 1</ref> shows our framework. In both methods, we train a CycleGAN to perform domain converting. In the first method, we convert nighttime images to daytime, in order to shift the images to the suitable domain. Next, the ERF-PSPNet 1, 11 trained on daytime images predict semantic maps in the inference. In the second method, CycleGAN converts parts of daytime images in the training set to nighttime images to extend the domain coverage of the datasets. After that, the adapted training dataset with a certain percent of nighttime images is used to train ERF-PSPNet, in order to improve its performance at the nighttime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training CycleGAN for Night-day Domain Converting</head><p>In this subsection, our work is training a GAN to translate nighttime images to daytime or reverse. Image-toimage translation is a class of vision problems in which the goal is to learn the mapping between input images and output images by using a training set of aligned image pairs. But for our task, large scale of paired training data is not available. Although collecting paired datasets by ourselves is theoretically possible, it is impractical to collect datasets for every different styles of scenes. What we need is an universal night-day domain converter that can be utilized at the dataset level.</p><p>CycleGAN 45 is an approach for learning to translate an image from a source domain to a target domain in the absence of paired examples, which suits our needs. CycleGAN contains two sets of GANs. Each GAN contains a generator and a discriminator. Generator and discriminator make translator, to translate image from domain X to domain Y or vice versa. Two GANs represent two generators: F and G, and they are inverses of each other. we have trained both the mapping G and F simultaneously and adding a cycle consistency loss that encourages</p><formula xml:id="formula_0">F (G(x)) ? x<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">G(F (y)) ? y<label>(2)</label></formula><p>This loss makes the unpaired image-to-image translation possible.</p><p>In our work, we select 6000 daytime images and 6000 nighttime images from the BDD100K dataset, <ref type="bibr" target="#b17">18</ref> as two image domains to train CycleGAN. Limited to GPU memory, we resize images to 480?270 to train our CycleGAN. In this way, we obtain our day-night and night-day converters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Converting Images to the Daytime During Inference</head><p>The first option is to convert nighttime images to daytime images. More specifically, nighttime images acquired by the camera are converted to synthetic daytime images, which is the suitable domain for semantic segmentation.</p><p>This method does not need to train the semantic segmentation model again. In other words, the advantage of the method is that we can make use of the original weights in the trained ERF-PSPNet, which has already been demonstrated to be stable in most datasets 20 and actual scenarios. <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> Additionally, the night-day conversion and semantic segmentation inference are separated, which makes it easier to adjust.</p><p>But the computational cost of the inference process is increased, which is a disadvantage for real-time semantic segmentation. For each inference process, the converted images from CycleGAN are fed into ERF-PSPNet. While efficient segmentation network is readily available for predicting accurate semantic maps, the forward pass of CycleGAN costs nearly 1 second for each 480?270 image. This is too slow for a real-time semantic segmentation model like ERF-PSPNet and the system losses its real-time performance. Another disadvantage is that compared to original images, the synthetic image produced by GAN may be biased. For example, GAN may convert faraway buildings to trees etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating Nighttime Images to Expand the Training Set</head><p>The second option is to convert daytime images in the BDD10K training set with segmentation labels to nighttime images. Then, the training set with part of synthetic nighttime images is fed into ERF-PSPNet, with focal loss as the loss function. <ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47</ref> The idea comes from the lack of the nighttime datasets with precise segmentation labels.</p><p>The advantage of the method is that for a trained model, no extra calculation is introduced in the inference process. For this reason, ERF-PSPNet can keep its real-time property. In our experiment, we conduct experiment to explore how the ratio of synthetic nighttime images influences the accuracy of the semantic segmentation model. Based on the experiment, we perform the discussion on the necessity of adding nighttime images with precise labels in the training dataset.</p><p>However, the disadvantage of the method is the time-consuming process of re-training model, and the model may not always be robust for all kinds of environments. In addition, because of the large scale of parameters in CycleGAN, we have to resize images in BDD100K to 480?270 to train the GAN. In this way, GAN can only produce images with the size of 480?270, which is below 1280?720, the resolution of images in BDD100K. So we have to upsample the synthetic images to 1280?720 before feeding into the segmentation model. Such operation makes unavoidable influence on the accuracy of the final prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Training</head><p>The first step of our experiment is to train a GAN converting nighttime images to daytime images and vice versa. As describe above, paired images are not necessary for CycleGAN, <ref type="bibr" target="#b44">45</ref> but the datasets must contain diverse driving images at both daytime and nighttime. As nighttime images are absent in most mainstream datasets for semantic segmentation like Cityscapes 13 and Mapillary Vistas, <ref type="bibr" target="#b13">14</ref> finally we utilize the lately released BDD datasets. <ref type="bibr" target="#b17">18</ref> BDD datasets contain two parts: BDD100K and BDD10K. The former contains 10,0000 driving images captured in diverse conditions and different time with GPS information, object detection annotation, lane and drivable area annotation. The latter one contains 9,000 pixel-wise semantically annotated images and 1,000 test images with 19 labeled classes.</p><p>To qualitatively verify our methods for real-world applications, we also collect daytime and nighttime images in the Yuquan Campus of Zhejiang University by using our Multi-Modal Stereo Vision Sensor 19 fixed on the top of an instrumented car. More than 1,500 images were acquired for our research. Furthermore, we utilize Nighttime Driving Test dataset provided by D. Dai et al., <ref type="bibr" target="#b31">32</ref> which contains 50 nighttime images with precise segmentation annotation. <ref type="table">Table.</ref> 1 and <ref type="figure" target="#fig_1">Fig. 3</ref> show details about these datasets and all three datasets are again itemized below: BDD Dataset. <ref type="bibr" target="#b17">18</ref> In total, BDD dataset have 100,000 driving images collected from more than 50,000 rides, covering New York, San Francisco Bay Area, and Berkeley. The dataset contains diverse scene scenarios such as city streets, residential areas, and highways. BDD100K contains plenty of nighttime images, making it possible to train a day-night converter. However BDD10K contains only 32 nighttime images with pixel-wise labels.</p><p>ZJU Dataset. The dataset was captured in Zhejiang University, Yuquan Campus (Hangzhou, China) with our Multi-Modal Stereo Vision Sensor. <ref type="bibr" target="#b18">19</ref> During the nighttime collection, we capture the images under two settings: with headlight-illumination and without headlight-illumination. The most significant feature of this dataset is that it has more trees and pedestrians than the others, and the road is very narrow. ZJU Dataset has also been used in our preliminary work 15 that only uses the headlight-illuminated nighttime image sequence. In this work, we use the image sequence without headlight-illumination which is more challenging. Both image sequences including daytime, nighttime with/without headlight-illumination of the ZJU dataset have been made publicly available at https://github.com/elnino9ykl/ZJU-Dataset. Nighttime Driving Dataset. <ref type="bibr" target="#b31">32</ref> The dataset was collected during 5 rides with a car inside multiple Swiss cities and their suburbs using a GoPro Hero 5 camera, consisting of images of real driving scenes at nighttime and twilight time, with 35,000 unlabeled and 50 densely annotated images. In this paper, we only utilize the 50 annotated nighttime images for evaluation as well as comparison by taking the method proposed by D. Dai et al. <ref type="bibr" target="#b31">32</ref> as a baseline. In general, the streetscape in this dataset is very similar to BDD dataset. Because of the huge style differences between the BDD datasets and ZJU datasets, we trained the GANs respectively for two datsets. In BDD100K, we select 6,000 daytime and 6,000 nighttime images, both in clear weather. These images are fed into CycleGAN to train a day-night converter named BDD-GAN. Because of the massive computation cost of CycleGAN, images are resized to 480?270. Similarly, 850 day-time image pairs in ZJU datasets are used to train a ZJU-GAN.</p><p>Like most semantic segmentation models, ERF-PSPNet 1, 11 is composed of two parts: encoder and decoder. The encoder part has been trained on ImageNet 48 already, and all the training tasks for ERF-PSPNet lie in the training of the decoder part of the model. In the first method, ERF-PSPNet is trained on BDD10K. Nighttime images during inference are converted on-the-fly to daytime domain by CycleGAN. In the second method, different ratios of images in training set of BDD10K are used to train ERF-PSPNet. To quantitatively validate our method, we use the 32 nighttime images with segmentation annotation in the validation set of BDD10K and 50 nighttime images with precise segmentation annotations in the Nighttime Driving Test datasets. The style of images in Nighttime Driving Test dataset is similar to BDD10K, which makes it reasonable to apply BDD-trained semantic segmentation models on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Results</head><p>In the first method, we use night-to-day converter to generate synthetic daytime images, which is the comfortable domain for ERF-PSPnet. <ref type="figure" target="#fig_3">Fig. 4</ref> and <ref type="figure" target="#fig_4">Fig. 5</ref> shows some representative results of our experiment. The first row and second row show the daytime images and nighttime images respectively, and the bottom row shows the synthetic daytime image converted form the images in second row. <ref type="figure" target="#fig_3">Fig. 4</ref> shows our results in BDD Dataset. First rows of both subfigures show the stably-behaving performance of ERF-PSPNet in daytime driving images. Nearly all the classes are labeled precisely. In the second row, subfigure (a) is worse illuminated than subfigure (b). Our daytime-trained model fails to detect the sky in both images. Cars in the left are ignored by the model in subfigure (a) and the whole gas station is missed in subfigure (b). In the bottom row, the model recognizes the sky, more cars and the whole gas station in the synthetic images successfully. In this way, GANs help the model improve performance at night. However, GANs also bring some problems: textures of objects like roads and cars in synthetic images are different from the ones in the real world, causing confusion to the model. As we can see in the bottom row of subfigure (a), the left part of the road is not labeled completely. <ref type="figure" target="#fig_4">Fig. 5</ref> shows our results in ZJU dataset. Because our semantic segmentation model is trained on BDD dataset, in which the streetscape significantly varies from ZJU datset, semantic segmentation outputs in the first row are not as ideal as ones in BDD dataset due to the geographical location-related domain gap between BDD (Berkeley) and ZJU (Hangzhou). In the second row we can find that all of the bushes and motorcycles beside the road are recognized as road, which is extremely dangerous for autonomous driving. In the bottom row, synthetic daytime images perform much better than nighttime images in the second row in general (night-to-day converter helps the model to correctly label the trees and bushes, which are very dark at night), but the lost details of the synthetic images make part of the road and the traffic sign missed in the labeled images.  In the second method, different ratio of images with annotated labels in training set are converted to nighttime images to improve the robustness of semantic segmentation model in face of nighttime images. <ref type="figure">Fig. 6</ref> shows performance of the model on the validation set of the BDD dataset and the testing set of the Nighttime Driving Dataset. In this example, our method relies on training ERF-PSPNet with 2,000 synthetic nighttime images and 5,000 original images in the training set. Different ratios of synthetic nighttime images in training set will be discussed in the next subsection. As shown in <ref type="figure">Fig. 6</ref>, the traffic signs and sky are significantly better labeled with our method. ERF-PSPNet yielded with the original training set performs decently for nighttime images with good illuminance (bottom row), but much worse under bad illuminance conditions. However, our method shows enhanced robustness in all conditions. <ref type="table">Table.</ref> 2 shows the quantitative results of our two methods and contrast method. In the table, the first three rows are results from contrast method: training ERF-PSPNet with BDD10K dataset and testing with 968 daytime images and 32 nighttime images in the BDD10K test set. The contrast method represents the baseline for our proposed two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results and Discussion</head><p>The results of first method (converting nighttime images on-the-fly to synthetic daytime images during inference), are shown in the forth and fifth rows. As we can see, the results are below the baseline. In general, this method performs worse than no methods applied, but in some classes such as sky, car and trucks, accuracies improve remarkably. The main cause is that the textures of synthetic images from GANs are not as detailed as those of natural images. Semantic segmentation model is trained on natural daytime images, causing low accuracy in labeling synthetic daytime images from the test sets. Another possible reason is that nighttime images in BDD dataset and Nighttime Driving dataset are illuminated decently, so our method may not be much better than the baseline. Nighttime Driving test set, even though our ERF-PSPNet is much smaller than RefineNet 8 adopted by them. In general, our method dramatically improves the performance of ERF-PSPNet in face of nighttime images.</p><p>We perform an important experiment to explore how the ratio of synthetic nighttime images in the training set influences the result. <ref type="figure">Fig.7</ref> shows the mean IoU of our day-to-night method with respect to different ratios of synthetic nighttime images in the training dataset. When varying the number of synthetic nighttime images in the training set, we find that in the range of 0 -2,000, as the ratio of synthetic nighttime images gets higher, the model performs increasingly better for nighttime images. The model learns well the illumination of scene elements in synthetic nighttime images and textures of real objects in daytime images. But as the synthetic nighttime images gets more, IoU gets down on the contrary. Additionally, at 5,000, the curve reaches another peak. The reason may be that 5,000 is a symmetrical number to 2,000 (7,000 in total), and the model learns the texture from daytime images and the illumination from synthetic nighttime images in a complementary way, but the daytime performance has already degraded to a low level. When all images in train set are converted to nighttime images, the IoU gets even lower than 30% for the same reason as the low IoU of the first method: the textures in synthetic images are different from that in real images. In the end, it turns out that the sweet spot is to use 2,000 synthetic nighttime images and 5,000 real daytime images as training set, as this ratio reaches the best accuracy for nighttime semantic segmentation, while daytime semantic segmentation also remains robust. <ref type="figure">Figure 7</ref>. Number of synthetic nighttime images in training set -Mean IoU curve. IoU value peak appears at 2,000 synthetic nighttime images in the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we investigate the problem of semantic image segmentation of nighttime scenes. To improve the performance, two methods are proposed by training CycleGAN as a two-way day-night converter. In the first method, nighttime images are converted to daytime domain on-the-fly during inference as a pre-processing step. In the second method, a critical part of images of BDD training set are converted to synthetic nighttime images, improving the robustness of the segmentation model in the training process.</p><p>To validate our methods, three datasets are leveraged to obtain qualitative and quantitative results. Our comprehensive set of experiments indicates the path to follow, and the sweet spot to determine the training strategy, in order to attain the best robustness across the day and night. Overall, these results demonstrate that our methods improve the model performance observably, making state-of-the-art efficient networks like ERF-PSPNet work robustly at night.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Our Multi-Modal Stereo Vision Sensor on the top of an instrumented vehicle used to capture the ZJU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples from three datasets. The streetscape of images in ZJU dataset varies from that in BDD dataset and Nighttime Driving dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples from BDD dataset (Top: day input, Mid: night input, Bottom: night-to-day converted input). Right image is better illuminated than left image. The model labeled the whole sky more precisely for synthetic daytime images than nighttime images. The night-to-day converter helps ERF-PSPNet perform better at night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples from ZJU dataset (Top: day input, Mid: night input, Bottom: night-to-day converted input). In the second row, all the most of the trees and some cars are missed in labeled images due to the darkness. In the bottom row, the segmentation of cars and vegetation has improved. But model performs not so well around the corner of the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Main information of the three datasets.</figDesc><table><row><cell>Dataset</cell><cell>Resolution</cell><cell cols="3">Number of Images Comment Day Night</cell></row><row><cell>BDD100K</cell><cell>1280?720</cell><cell cols="2">52511 39986</cell><cell>No semantic segmentation labels</cell></row><row><cell>BDD10K</cell><cell>1280?720</cell><cell>7691</cell><cell>309</cell><cell>Precise semantic segmentation labels</cell></row><row><cell>ZJU</cell><cell>1920?1080</cell><cell>1700</cell><cell>1700</cell><cell>Different streetscape from other datasets</cell></row><row><cell cols="2">Nighttime Driving test 1920?1080</cell><cell>0</cell><cell>50</cell><cell>Good illumination by street lamp</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The results of second method: converting parts of the daytime images in training set to nighttime images on the stage of training, are shown in last four rows, together with a baseline method DarkModelAdaptation proposed by D. Dai et al.<ref type="bibr" target="#b31">32</ref> validated on their dataset. Mean IoU increases remarkably for the nighttime images in the BDD testing set and Nighttime Driving test set, and keeps the same level of accuracy as the baseline for daytime images. Compared to the method proposed by D. Dai et al.,<ref type="bibr" target="#b31">32</ref> our method rises nearly 4% on the same Examples from BDD Dataset and Nighttime Driving Dataset (Top two rows: BDD Dataset, Bottom two rows: Nighttime Driving Dataset). In general, our method (Converting 2,000 images to synthtic nighttime images in training) performs better than ordinary ERF-PSPNet(trained in original BDD10K training set). All the classes especially sky are recognized betterTable 2. Results of our two methods in BDD val set and Nighttime Driving test set.</figDesc><table><row><cell>(a) Input</cell><cell>(b) Ground Truth</cell><cell cols="2">(c) Ordinary ERF-PSPNet</cell><cell>(d) Our Method</cell></row><row><cell>Figure 6. Train/Method</cell><cell>Test</cell><cell></cell><cell cols="2">Mean IoU Mean Acc</cell></row><row><cell>BDD dataset</cell><cell>Day in BDD test set</cell><cell></cell><cell>52.10%</cell><cell>75.52%</cell></row><row><cell>BDD dataset</cell><cell cols="2">Night in BDD test set</cell><cell>32.72%</cell><cell>75.67%</cell></row><row><cell>BDD dataset</cell><cell cols="2">Nighttime Driving test set</cell><cell>36.73%</cell><cell>72.38%</cell></row><row><cell>BDD dataset</cell><cell cols="2">Night2day in BDD test set</cell><cell>29.94%</cell><cell>56.87%</cell></row><row><cell>BDD dataset</cell><cell cols="3">Night2day in Nighttime Driving test set 32.74%</cell><cell>66.46%</cell></row><row><cell cols="2">Day2night BDD dataset Day in BDD test set</cell><cell></cell><cell>53.03%</cell><cell>75.96%</cell></row><row><cell cols="3">Day2night BDD dataset Night in BDD test set</cell><cell>43.14%</cell><cell>68.93%</cell></row><row><cell cols="3">DarkModelAdaptation 32 Nighttime Driving test set</cell><cell>41.60%</cell><cell>NA</cell></row><row><cell cols="3">Day2night BDD dataset Nighttime Driving test set</cell><cell>45.09%</cell><cell>72.82%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness for the visually impaired through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1506</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intersection perception through real-time semantic segmentation to assist navigation of visually impaired pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<date type="published" when="2018-12" />
			<biblScope unit="page" from="1034" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fusion of millimeter wave radar and rgb-depth sensors for assisted navigation of the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Millimetre Wave and Terahertz Sensors and Technology XI], 10800, 1080006</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Robust semantic segmentation in adverse weather conditions by means of sensor data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10117</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expanding the detection of traversable area with realsense for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1954</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10089</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can we pass beyond the field of view? panoramic annular semantic segmentation for real-world surrounding perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
	<note>IV). IEEE</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robustifying semantic cognition of traversability across wearable rgb-depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3141" to="3155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bridging the day and night domain gap for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1184" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Segmenting objects in day and night: Edge-conditioned cnn for thermal image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10303</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multimodal vision sensor for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Artificial Intelligence and Machine Learning in Defense Applications</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Importance-aware semantic segmentation with efficient pyramidal context network for navigational assistant systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A comparative study of high-recall real-time semantic segmentation based on swift factorized network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11394</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
	<note>in [Proceedings of the European conference on computer vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="84" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic segmentation of panoramic images using a synthetic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Artificial Intelligence and Machine Learning in Defense Applications</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3752" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3819" to="3824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semantic nighttime image segmentation with synthetic stylized data, gradual adaptation and uncertainty-aware evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="687" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">I can see clearly now: Image restoration via de-raining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Porav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bruls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth-attentional features for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8022" to="8031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Don&apos;t worry about the weather: Unsupervised condition-dependent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Porav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bruls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11004</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A cross-season correspondence dataset for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9532" to="9542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual localizer: Outdoor localization based on convnet descriptor and global optimization for visually impaired pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2476</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Panoramic annular localizer: Tackling the variation challenges of outdoor localization using panoramic annular images and active deep descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05425</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Predicting polarization beyond semantics for wearable robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
