<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUPERVISED ONLINE DIARIZATION WITH SAMPLE MEAN LOSS FOR MULTI-DOMAIN DATA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Fini</surname></persName>
							<email>enrico.fini@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PerVoice Spa</orgName>
								<address>
									<addrLine>2 Fondazione Bruno Kessler</addrLine>
									<settlement>Trento, Trento</settlement>
									<region>Italy</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Brutti</surname></persName>
							<email>brutti@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PerVoice Spa</orgName>
								<address>
									<addrLine>2 Fondazione Bruno Kessler</addrLine>
									<settlement>Trento, Trento</settlement>
									<region>Italy</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUPERVISED ONLINE DIARIZATION WITH SAMPLE MEAN LOSS FOR MULTI-DOMAIN DATA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speaker diarization</term>
					<term>x-vectors</term>
					<term>clustering</term>
					<term>su- pervised learning</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a fully supervised speaker diarization approach was proposed (UIS-RNN) which models speakers using multiple instances of a parameter-sharing recurrent neural network. In this paper we propose qualitative modifications to the model that significantly improve the learning efficiency and the overall diarization performance. In particular, we introduce a novel loss function, we called Sample Mean Loss and we present a better modelling of the speaker turn behaviour, by devising an analytical expression to compute the probability of a new speaker joining the conversation. In addition, we demonstrate that our model can be trained on fixed-length speech segments, removing the need for speaker change information in inference. Using x-vectors as input features, we evaluate our proposed approach on the multi-domain dataset employed in the DIHARD II challenge: our online method improves with respect to the original UIS-RNN and achieves similar performance to an offline agglomerative clustering baseline using PLDA scoring.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The speaker diarization task consists in establishing "who spoke when" in a given audio recording <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Despite having been investigated for decades, diarization is still an unsolved problem in many real scenarios, as highlighted by the recent DIHARD I and DIHARD II challenges <ref type="bibr" target="#b2">[3]</ref>.</p><p>Typically, speaker diarization is addressed integrating several different components: voice activity detection, speaker change detection, feature extraction and clustering. Most of the research works in literature focus on extracting highly discriminative feature vectors. The first example in this direction are i-vectors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, which represent a given utterance with a single fixed-dimensional feature vector. The recent rise of neural paradigms has led to the introduction of a variety of approaches to extract the so-called speaker embeddings. These are, typically, derived from the outputs of the inner layers of a neural network trained on a speaker classification task <ref type="bibr" target="#b5">[6]</ref>. The most popular embeddings are d-vectors <ref type="bibr" target="#b6">[7]</ref> and x-vectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Conversely, not much progress has been done with regard to clustering. In most of the approaches, this stage is still based on the Agglomerative Hierachical Clustering (AHC) <ref type="bibr" target="#b9">[10]</ref> in combination with Probabilistic Linear Discriminat Analysis (PLDA) scoring <ref type="bibr" target="#b10">[11]</ref>. Recently, spectral clustering <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b12">[13]</ref>, and variatioanl bayesian clustering <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> have been introduced, showing promising result. Also, alternatives to the PLDA scoring have been introduced using neural networks that learn how to score two speech segments <ref type="bibr" target="#b15">[16]</ref>, using siamese networks <ref type="bibr" target="#b16">[17]</ref> or Bi-LSTMs <ref type="bibr" target="#b17">[18]</ref>. Nev-ertheless, clustering remains unsupervised and heavily dependent on fine-tuned hyperparameters (e.g. thresholds to stop clustering).</p><p>Recently, efforts have been made to formulate clustering in a supervised learning framework <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Supervised clustering is attractive because it can be optimized on the diarization metrics directly, or learning context dependent parameters. Additionally, supervision allows to improve performance by learning from the increasing amount of data at our disposal. For example, <ref type="bibr" target="#b18">[19]</ref> tackles the diarization problem as a classification task, while <ref type="bibr" target="#b19">[20]</ref> uses a permutation invariant loss and a clustering loss to dynamically identify speakers. Both <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b19">[20]</ref> assume that the number of speaker is known apriori or at least bounded. This assumption is removed in the UIS-RNN <ref type="bibr" target="#b20">[21]</ref>: a fully supervised approach which handles an unbound number of speakers using an online generative process. Speaker distributions are modelled with multiple instances of a parameter-sharing Recurrent Neural Network (RNN). A further, strong advantage of <ref type="bibr" target="#b20">[21]</ref> over traditional clustering algorithms is the fact that decoding is online using beam search <ref type="bibr" target="#b21">[22]</ref>. Though online diarization had already been explored, using both unsupervised <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and supervised <ref type="bibr" target="#b18">[19]</ref> paradigms, the UIS-RNN stands out in terms of performance, outperforming the previous offline state of the art on telephone data.</p><p>Although these are very interesting results, an online system that works well across multiple domains still remains an open problem. As a matter of fact, diarization systems presented in the literature appear to work relatively well on domains with a low number of speakers and no overlapping speech, like telephone data, while performance tends to deteriorate in more challenging contexts such as meetings or dinner parties.</p><p>In this paper we present an evolution of the UIS-RNN <ref type="bibr" target="#b20">[21]</ref>, which substantially improves the performance. First of all, we introduce a new loss function for training the RNN that models speakers, which provides faster convergence, encouraging the network to find deeper minima, and generalizes better on the evaluation set. Secondly, we propose a semantically grounded formulation for the unseen speaker intervention probability that is easy to calculate and improves performance in inference. In addition we train on fixed-length speech segments, and let the neural network aggregate embeddings, removing the constraint on speaker change information in inference. Finally, we shed light on the performance of the proposed method with respect to the original UIS-RNN in a multi-domain scenario through extensive testing on the DIHARD datasets. We also make our results reproducible, since we use a publicly available embedding extractor and fully disclose our code 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED APPROACH</head><p>Given a set of embeddings X = (x1, . . . , xT ) and the related speaker labels Y = (y1, . . . , yT ), where T is the total number of observations, we can cast the diarization problem in a probabilistic framework, looking for the sequence of speaker labels that maximizes the joint probability:</p><formula xml:id="formula_0">Y = arg max Y P (X, Y),<label>(1)</label></formula><p>If we model eq. 1 as an online generative problem as in <ref type="bibr" target="#b20">[21]</ref>, we can rewrite the joint probability at time t as:</p><formula xml:id="formula_1">p(xt, yt, zt|x [t?1] , y [t?1] , z [t?1] ) = p(xt|x [t?1] , yt) sequence generation ? p(yt|y [t?1] , zt) assignment ? p(zt|z [t?1] ) speaker change ,<label>(2)</label></formula><p>where zt = 1 (yt = yt?1) is a hidden binary indicator of speaker change and [t] denotes all observations up to t included. In the original definition of the UIS-RNN <ref type="bibr" target="#b20">[21]</ref>, the speaker change term of eq. 2 is modelled by a coin flipping process where the only parameter is p0, the transition probability. The speaker assignment term is implemented as a distance dependent Chinese Restaurant Processes (ddCRP) <ref type="bibr" target="#b25">[26]</ref>, a Bayesian nonparametric process that guides how speakers interleave in the time domain. Finally, the sequence generation part of eq. 2 is modelled using an RNN, specifically a Gated Recurrent Unit (GRU), that parametrizes the distribution of embeddings assuming a Gaussian distribution as follows:</p><formula xml:id="formula_2">xt|x [t?1] , y [t] ? N ? GRU ? x t ? x [t?1] y t = yt , ? 2 I , (3) where ? (GRU ? (?))</formula><p>is the averaged output of the neural network with parameters ? instantiated for speaker yt. Given a dataset D = {(X1, . . . , XM ) , (Y1, . . . , YM )}, including M sequences of embeddings and related label, the optimal set of network parameters ? * can be obtained minimizing the following negative log likelihood <ref type="bibr" target="#b20">[21]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Original UIS-RNN training</head><formula xml:id="formula_3">Mean ? Loss (MSE) ? 1 ? 1 Sequence A ( ( ) ) [ ? 1 ]</formula><formula xml:id="formula_4">L = |D| m=1 ? ln p(Xm|Ym; ?).<label>(4)</label></formula><p>Using the model in eq. 3, eq. 4 can be reformulated in a Mean Squared Error (MSE) fashion <ref type="bibr" target="#b26">[27]</ref>:</p><formula xml:id="formula_5">LMSE = |D A | i=1 |A i | j=1 ai,j ? ? GRU ? a i,[j?1] 2 .<label>(5)</label></formula><p>Given S speakers and P permutations applied to the data for augmentation purposes, DA = (A1, . . . , AS?P ) is a set of single speaker sequences, where each sequence Ai = (ai,1, . . . , ai,L i ) ? DA is obtained by concatenating a random permutation of the embeddings generated by the i-th speaker. Li and ai,j are respectively the length and the j-th embedding of sequence Ai. Note that, since the sequences are shuffled, the network can not learn any causal relationship between observations and how to predict the next embedding. Basically, the network is trained to generate samples ?t from an auxiliary distribution q with expectation E[?t] equal to E[xt]. Therefore, the network will learn to predict the mean of the distribution of the embeddings. <ref type="figure" target="#fig_0">Figure 1</ref> graphically describes the training presented in <ref type="bibr" target="#b20">[21]</ref> and implemented in <ref type="bibr" target="#b26">[27]</ref>. In this section we propose a modified loss that relies on more accurate targets for the network output. Rather than adjusting the network by comparing the mean of its outputs with the next observed embedding, we define a MSE loss with respect to the actual mean of the speaker embeddings of a given speaker. This results in defining a predictor of the mean of the embedding distribution, having seen only a small sample of it. More formally, we replace the MSE loss in eq. 5 with:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sample Mean Loss training: UIS-RNN-SML</head><formula xml:id="formula_6">Mean ? Loss (SML) ? 1 ? 1 Sampling Mean Sequence A ( ) ? [ , ] ( ( ) ) [ ? 1 ]</formula><formula xml:id="formula_7">L = |D A | i=1 |A i | j=1 E [s (i)] ? ? GRU ? a i,[j?1] 2 ,<label>(6)</label></formula><p>where s (i) is a function that maps each index i to the embedding distribution of the speaker who generated the observations in Ai.</p><p>In practice, the actual probability distribution of the embeddings is not available. In addition, given the limited amount of labelled data, using the bare mean over the sequence would lead to overfitting. Therefore, we build the ground truth for the network by estimating the mean over a collection of unseen samples we draw randomly with replacement from the permuted sequence itself. In formulas, given a generic sequence Ai and a subset H = (h1, . . . , hN ) ? Ai of N randomly sampled embeddings, we estimate the mean of the embeddings as:? N (Ai) = N i hi /N Eq. 6 is then rewritten leading to our Sample Mean Loss (SML) definition:</p><formula xml:id="formula_8">LSML = |D A | i=1 |A i | j=1 ? N a i,[j,L i ] ? ? GRU ? a i,[j?1] 2 ,<label>(7)</label></formula><p>where we denote the ordered set (j, . . . , Li) as [j, Li]. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the proposed training approach for a generic sequence A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">New speaker probability</head><p>One of the most interesting advantages of the UIS-RNN <ref type="bibr" target="#b20">[21]</ref> over other supervised methods, like <ref type="bibr" target="#b19">[20]</ref>, is its ability to model an unbounded number of speakers. This is achieved using a ddCRP model <ref type="bibr" target="#b25">[26]</ref> that provides the probability of switching back to a previously seen speaker proportionally to the number of turns of that speaker and accounts for the probability of a new speaker joining the conversation. Assuming speakers are numerated in order of appearance starting from 1, we let:</p><formula xml:id="formula_9">p yt = k|zt = 1, y [t?1] ? N k,t?1 (8) p yt = max y [t?1] + 1|zt = 1, y [t?1] ? ?,<label>(9)</label></formula><p>where N k,t?1 is the number of blocks of contiguous utterances of speaker k. The probability of switching to a new speaker is controlled by the parameter ? which is critical for the correct functioning of the whole framework: large values of ? force the model to over estimate the number of speakers, instantiating several networks; conversely small values result in limiting the number of speakers by merging clusters. With respect to the estimation performed in <ref type="bibr" target="#b20">[21]</ref>, we propose the following analytical formulation for ?:</p><formula xml:id="formula_10">? = |D| m=1 (max (Ym) ? 1) |D| m=1 |Ym| t=1 1 (ym,t = ym,t+1) .<label>(10)</label></formula><p>This formulation has the advantage that it can be derived from eq. 9, and therefore it is semantically coherent with the role of the parameter. In addition, the value of the parameter is estimated straight from the data, independently of any the error metric or heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We train and evaluate our method on the data used in the DIHARD-II challenge <ref type="bibr" target="#b2">[3]</ref>. The challenge features two audio input conditions: single channel and multi-channel. We focus on single channel data with reference Speech Activity Detection (SAD), as per the track 1 of the competition. The dataset is divided into two subsets, development and evaluation, each consisting of selections of 5-10 minute audio files sampled from 11 different conversational domains for a total of approximately 2 hours of audio.</p><p>Using stratified holdout, we further split the development set into training set (80%) and validation set (20%). Also, we randomize the holdout procedure, such that for every experiment we get a different data partitioning. Stratification is performed over the set of domains, according to their frequency in the whole development set.</p><p>Although the proposed approach does not handle cases where multiple speakers are active simultaneously, we do not exclude overlapping speech segment from the training material. In fact, we observed that considering multi-speaker segments as a separate speaker slightly improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental setup</head><p>We use as speaker embeddings of our supervised diarization system x-vectors <ref type="bibr" target="#b8">[9]</ref> using the pre-trained models available in the Kaldi diarization recipe <ref type="bibr" target="#b27">[28]</ref>. X-vectors with dimension 512 are extracted from non-overlapped 1 second speech segments and are subsequently reduced to dimension 200 with Principal Component Analysis (PCA) before feeding them to the model.</p><p>For what concerns the sequence generation component, our network resembles the architecture presented in <ref type="bibr" target="#b20">[21]</ref>. However, since we are using different features, x-vectors on fixed-length segments instead of d-vectors extracted from ground truth speaker segments, we explored several configurations varying the sizes of the layers. We found that reasonable results are obtained using one recurrent and one fully connected layer with 200 units each.</p><p>The other two parameters, p0 and ? for the speaker change and the speaker assignment components respectively, are estimated using their analytical formulations. For the transition probability p0 we apply the same formula as in <ref type="bibr" target="#b20">[21]</ref>, while for ? we use eq. 10. We also explored some search based techniques for hyperparameter optimization, like grid search and line search, but we found they do not provide noticeable improvements in performance. Furthermore, the value for the variance of the observations ? 2 is optimized during training using Adam, as in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Apart from the SML loss, two more regularization losses help the model to converge <ref type="bibr" target="#b26">[27]</ref>. The first one is a simple L2 loss on the parameters of the GRU, the second one uses an inverse gamma distribution to regularize the value of ? 2 that would otherwise diverge to very large values.</p><p>In inference we use beam search with beam size ? = 15. Unlike in <ref type="bibr" target="#b20">[21]</ref>, in our dataset we can not consider the number of speakers to be bounded. This makes inference expensive.</p><p>Networks are trained several times using Adam optimizer and the best model is selection by measuring the Diarization Error Rate (DER) on the validation set, using a smaller beam width (? = 2) to reduce the computational cost.</p><p>DER is measured using dscore <ref type="bibr" target="#b28">[29]</ref>, the official scoring tool of DIHARD-II competition which does not account for any forgiveness collar, considering also overlapped speech segment. However, since none of the methods under evaluation handle overlapped speech we also report performance without overlap.  <ref type="table" target="#tab_0">Table 1</ref> reports the performance of our proposed UIS-RNN-SML, based on SML and ? estimation, in comparison against two online baselines. The first one is a na?ve implementation in which the GRU is replaced by a simple cumulative mean of the embeddings (Cum. mean + beam search in <ref type="table" target="#tab_0">Table 1</ref>). This na?ve baseline helps highlighting the contribution of the neural network, disentangling it from the other components of the framework. The second is the original UIS-RNN <ref type="bibr" target="#b20">[21]</ref>, using the implementation provided in <ref type="bibr" target="#b26">[27]</ref>. To give an idea of how difficult the task is, we also report the offline baseline provided in the DIHARD-II challenge <ref type="bibr" target="#b2">[3]</ref>, which performs  diarization by scoring the x-vectors with PLDA <ref type="bibr" target="#b10">[11]</ref>, and clustering using AHC <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The na?ve implemenation based on cumulative mean with beam search is outperformed by the UIS-RNN by a large margin, with and without overlapping speech segments. This confirms that the simple mean of a partial sequence of embeddings does not properly model the speaker and that the neural network makes an active contribution. A further small but significative DER reduction, both with and without overlap, with respect to the original implementation is provided by estimating ? with eq. 10 (third row in <ref type="table" target="#tab_0">Table 1</ref>).</p><p>Finally, a larger leap in performance is achieved by replacing the original loss function with the SML we proposed. Note that the UIS-RNN-SML achieves similar performance to the offline baseline used in DIHARD-II <ref type="bibr" target="#b2">[3]</ref>, although online unsupervised clustering algorithms usually perform significantly worse than offline clustering algorithms. The performance improvement is due the regularizing effect introduced by the SML in training. We observed that, keeping learning rate and batch size fixed, training with SML is much less noisy than the original one: using the more accurate supervision given by the sample mean results in better gradients, which in turn helps convergence to deeper minima. The stabilizing effect of the SML is evident in <ref type="figure" target="#fig_4">Fig. 4</ref> where we report the variance of the means of the speaker clusters generated by the network during training. Models trained with eq. 7 exhibit less output variance compared to those trained with eq. 5. This behaviour turns out to be very beneficial in the decoding phase when the means of the clusters should not change dramatically while the sequence unfolds.  For a better understanding of the behaviour of our proposed method, <ref type="figure" target="#fig_2">Fig. 3</ref> reports the DER for each context in the dataset. Our method is better than the original UIS-RNN in all the most challenging contexts, except for "socio field" and "child", where our performance is basically aligned to the other methods. We observe a small performance deterioration in "Audiobooks". This occurs because the UIS-RNN-SML, predicting the mean more accurately, produces slightly smaller values for the cluster variance ? 2 . Although this is beneficial in most cases, it can marginally reduce performance in contexts with very low number of speakers. This disadvantage can be partially alleviated by defining context dependent ? and p0.</p><p>Finally we evaluate the impact of the number of samples N used to estimate the mean of the distribution. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the DER on the whole evaluation set for different values of N . On these data, N = 2 provides the lower DER, but values from 2 to 4 produce very similar results. Unsurprisingly, performance degrades using larger values for N , due to overfitting, because the sample mean approximates the real mean too tightly. Note that the case N = 1 would be equivalent to the UIS-RNN except for the fact that observations are sampled with replacement. This gives a considerable improvement (27.83% against 30.3%) because outliers of the speaker clusters are less likely to be observed by the network as targets during training, reducing the overall variance of the output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper we presented an evolution of a supervised speaker diarization system where the clustering module is replaced by a trainable model called unbounded interleaved-state RNN. Specifically, we proposed a modified loss function that stimulates the neural network to model speakers more accurately. In addition, we introduced a semantically grounded formulation for the estimation of the parameter that controls the speaker assignment probability. We evaluated the proposed online diariaztion approach on the DIHARD-II multidomain data, showing, through extensive experiments, that it outperforms the original UIS-RNN formulation. Finally, we fully disclose our code and trained models to make our results reproducible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Block diagram of the original UIS-RNN training strategy for a generic sequence A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Block diagram of the proposed UIS-RNN-SML training approach for a generic sequence A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>DER for each domain in track 1 of the DIHARD-II test set. Domains are displayed in ascending order of difficulty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Cluster mean variance over sequences during the first 20K training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>DER on track 1 of DIHARD II test data, varying number of samples N in SML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>DER on track 1 of DIHARD II test data, with and without overlapping speech. PLDA+AHC refers to the off-line baseline provided with the challenge. N = 2 in UIS-RNN-SML.</figDesc><table><row><cell>DER DER -no overlap</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The first author performed this work as an intern at PerVoice and Fondazione Bruno Kessler. The implementation of this paper is available at: https://github.com/DonkeyShot21/uis-rnn-sml</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An overview of automatic speaker diarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Tranter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1557" to="1565" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speaker diarization: A review of recent research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bozonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cristia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
		<title level="m">The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="978" to="982" />
		</imprint>
	</monogr>
	<note>INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study of interspeaker variability in speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="980" to="988" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for small footprint text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4052" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaker diarization using deep neural network embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mc-Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4930" to="4934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DI-HARD challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="2808" to="2812" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Strategies to improve the robustness of agglomerative hierarchical clustering under data source variation for speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1590" to="1601" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speaker diarization with PLDA i-vector scoring and unsupervised calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="413" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A spectral clustering approach to speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speaker diarization with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5239" to="5243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Speaker diarization based on bayesian hmm with eigenvoice priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
	<note>in Speaker Odissey, 06 2018</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian HMM Based x-Vector Clustering for Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohdin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="346" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-attentive speaker embeddings for text-independent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="3573" to="3577" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingjian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Barras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning approaches for online speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Asawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bhattasali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end neural speaker diarization with permutationfree objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nagamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully supervised speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6301" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speech understanding systems: Report of a steering committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Medress</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Forgie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Klatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>O&amp;apos;malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Neuburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ritea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Shoup-Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GMM-UBM based open-set online speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wallhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Links: A high-dimensional online clustering method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Developing on-line speaker diarization system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fousek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distance dependent chinese restaurant processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2488" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Official library for the Unbounded Interleaved-State Recurrent Neural Network (UIS-RNN) algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/google/uis-rnn" />
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">dscore, official scoring tool for DIHARD-II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryant</surname></persName>
		</author>
		<ptr target="https://github.com/nryant/dscore" />
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
