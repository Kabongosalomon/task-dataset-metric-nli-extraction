<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher>WWW &apos;22 Companion</publisher>
				<availability status="unknown"><p>Copyright WWW &apos;22 Companion</p>
				</availability>
				<date>April 25-29, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
							<email>zhoubo.li@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
							<email>feiyu.xfy@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
							<email>chenmosha.cms@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang University &amp; AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Zhejiang University &amp; AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Alibaba Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Zhejiang University &amp; AZFT Joint Lab for Knowledge Engine Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Virtual Event</title>
						<meeting> <address><addrLine>Lyon, France</addrLine></address>
						</meeting>
						<imprint>
							<publisher>WWW &apos;22 Companion</publisher>
							<date type="published">April 25-29, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3487553.3524238</idno>
					<note>ACM Reference Format: Mosha Chen, and Huajun Chen. 2022. From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. In Companion Proceedings of the Web Conference 2022 (WWW &apos;22 Companion), April 25-29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA, 4 pages. ACM ISBN 978-1-4503-9130-6/22/04. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Knowledge representation and reasoning KEYWORDS Knowledge Graph Completion</term>
					<term>Generation</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset OpenBG500 for research purpose 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge Graphs (KGs) treat the knowledge in the real world as fact triples in the form of &lt;subject, predicate, object&gt;, abridged as ( , , ), where and denote entities and are the relations between entities, which can benefit a wide range of knowledgeintensive tasks. Knowledge graph completion (KGC) aims to complete the knowledge graph by predicting the missing triples. In this paper, we mainly target link prediction task for KGC based on the powerful pre-trained language models.</p><p>Most previous KG completion methods, such as TransE <ref type="bibr" target="#b1">[2]</ref>, Com-plEx <ref type="bibr" target="#b10">[11]</ref>, and RotatE <ref type="bibr" target="#b8">[9]</ref>, are knowledge embedding techniques that embed the entities and relations into a vector space and then obtain the predicted triples by leveraging pre-defined scoring functions to those vectors. Recently, some textual encoding methods (e.g., KG-BERT <ref type="bibr" target="#b13">[14]</ref>) have been proposed, which utilize the pretrained language model to encode triples and output the score for each candidate. Obviously, most previous approaches leverage the discrimination paradigm with a pre-defined scoring function to knowledge embeddings. However, such a discrimination strategy has to costly scoring of all possible triples in inference and suffer from the instability of negative sampling. Moreover, those dense knowledge embedding approaches (e.g., TransE) ignore the finegrained interactions between entities and relations and have to allocate a large memory footprint for the large-scale real-world knowledge graph. Therefore, it is intuitive to find a new technical solution for knowledge graph completion.</p><p>In this paper, we take the first step to model the knowledge graph completion with sequence to sequence generation and propose a novel approach GenKGC. To be specific, we represent entities and relations as input sequences and utilize the pre-trained language model to generate target entities. Following GPT-3's naive "in-context learning" paradigm, in which the model can learn correct output answers by concatenating the selected samples relevant to the input, we propose relation-guided demonstration by adding triples of the same relation. Moreover, during generation, we propose entity-aware hierarchical decoding to reduce the time complexity of generation. Experimental results on two datasets WN18RR, FB15k-237 and a newly released large-scale Chinese KG dataset OpenBG500 demonstrate the effectiveness of the proposed approach. The contributions of our work are as follows:  <ref type="figure">Figure 1</ref>: Architecture of GenKGC. We augment the input text of entity and relation with demonstrations, and introduce entity-aware hierarchical decoding for fast inference.</p><p>? We convert link prediction to sequence to sequence generation and propose GenKGC, which can reduce the inference time while maintaining the performance. ? We propose relation-guided demonstration and entity-aware hierarchical decoding, which can better represent entities and relations and reduce the time complexity of generation. ? We report results on two datasets and release a new largescale KG dataset, OpenBG500, for research purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD 2.1 Link Prediction as Seq2Seq Generation</head><p>Knowledge graph is defined with entity categories and entity descriptions as a tuple G = (E, R, T , C, D), where E represents a set of entities, R represents relation types, T refers to a set of triples , C refers to the entity categories and D refers to the entity descriptions. For each triple ? T , there is the form ( , , ) where , ? E is the head and tail entity respectively. For each entity ? E, there exists category to define the and a text to describe . To complete missing triples in KGs, link prediction aims at predicting the tail entity given the head entity and the query relation, denoted by ( , , ?).</p><p>In this paper, we utilize the standard encoder-decoder architecture for sequence-to-sequence generation. Note that we regard each entity and relation as the sequence of tokens. Concretely, we follow <ref type="bibr" target="#b13">[14]</ref> to use plain text to represent the entities and relations instead of unique embedding to bridge the gap between the triples in the knowledge graph and pre-trained language models. To be specific, given a triple with tail entity missing ( , , ?), we obtain the description of and respectively, and concatenate them together. For example, as shown in <ref type="figure">Figure 1</ref>, "Michael Chabon studied at" (head entity, relation) is the major part of the input sequence, and "UC, Irvine" (target entity) is part of the output sequence. Thus, we have the input sequence of &lt; , &gt; pair and generate the output sequence of . We leverage BART for model training and inference. </p><formula xml:id="formula_0">( + 1) 0.08ms KG-BERT (| | 2 ? ( + 1)) 72ms GenKGC (| | 2 ) 2.35ms Inference TransE (|E |) 0.02s KG-BERT (| | 2 ? |E |) 10100s GenKGC (| | 2 ? | | ) 0.96s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relation-guided Demonstration</head><p>Inspired by prompt-tuning <ref type="bibr" target="#b17">[18]</ref>, we propose relation-guided demonstration for the encoder. Note that there exist long-tailed distributions in the KGs, for example, there exist only 37 instances of the relation film/type_of_appearance in the FB15k-237 dataset.</p><p>Previous study <ref type="bibr" target="#b17">[18]</ref> illustrates that concatenating randomly sampled instances as demonstrations can yield better few-shot performance. Thus, we construct relation-guided demonstration examples { in , train }. To be specific, we sample those demonstrations with the guidance of relation , which consists of several triples with the same relation of input from training set. Formally, we have the final input sequence as:</p><p>= &lt;bos&gt; demonstration( ) &lt;sep&gt; &lt;sep&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity-aware Hierarchical Decoding</head><p>In the vanilla decoding setting, we have to enumerate all entities in the E and then sort them by the score function. However, this process can be time-consuming, as shown in <ref type="table" target="#tab_1">Table 1</ref> when E is very large, it is costly scoring of all possible triples. In our approach, we follow <ref type="bibr" target="#b2">[3]</ref> to use the Beam Search to obtain top-entities in the E ( is the hyperparameter of beam size). Intuitively, there is no need for negative sampling as we directly optimize by predicting the correct To be more specific, given with a triple with tail or head entity missing ( , , ?), GenKGC rank each ? E by computing a score with an autoregressive formulation:</p><formula xml:id="formula_1">( | ) = | | =1 ( | &lt; , ) =| |+1 ( | &lt; , ) ,<label>(1)</label></formula><p>where is the set of | | tokens in the category , is the set of tokens in the textual representation of .</p><p>Since KG contains rich semantic information such as entity types, it is intuitive to constrain the decoding for fast inference. We sample the type category with the lowest frequency of occurrence in the training set to constrain the entity decoding since it is challenging to discriminate those low-frequent entities. Then, we add special tokens as types in the vocabulary of pre-trained language model to constrain the decoding. To ensure that the generated entities are inside the entity candidate set, we construct a prefix tree (trie tree) to decode our entity name as shown in <ref type="figure">Figure 1</ref>. Similar to the ordinary sequence-to-sequence model, we optimize GenKGC using a standard seq2seq objective function:</p><formula xml:id="formula_2">L = ? log ( | )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT</head><p>Datasets. We evaluate our method on FB15k-237 <ref type="bibr" target="#b9">[10]</ref>, WN18RR <ref type="bibr" target="#b4">[5]</ref>, which are widely used in the link prediction literature, and a new real-world large-scale Chinese KG dataset OpenBG500 2 . In FB15k-237, descriptions of entities are obtained from the introduction section of the Wikipedia page of each entity. In WN18RR, each entity corresponds to a word sense, and description is the word definition. In OpenBG500, the descriptions of entities and relations come from the e-commerce description page. More details about datasets are listed in <ref type="table" target="#tab_3">Table 3</ref>.  Setting. We adopt BART-base as our backbone for comparison with other BERT-based KGC methods like KG-BERT. Following StAR <ref type="bibr" target="#b11">[12]</ref>, we choose the two kinds of KGE methods as our baseline models, which can be classified as graph embedding approach and textual encoding approach. Grid search is used over the three datasets and the results are reported on the test set with hyperparameters of the best performance determined by the dev set.</p><p>Metrics. We evaluate the test set of triples T ? disjoint from the set of training triples T . For inference on a test triple ( , , ), we make sure predict the entity in the candidate set E. We use the metrics of hits@1, hits@3 and hits@10.</p><p>Comparison with Discrimination Method. From <ref type="table" target="#tab_2">Table 2</ref>, we notice that GenKGC achieves better performance than KG-BERT <ref type="bibr" target="#b13">[14]</ref> across all datasets and maintains high speed during inference. The translation-based method like TransE, which treats entities and relations as dense vectors in the same space, will face the memory explosion problem. For the memory cost, TransE has to consume 260M parameters to store the entities and relations for OpenBG500 with more than 260k entities, while pre-trained models (BERT or BART) only utilize 110M parameters, which demonstrates the memory efficiency of our approach. Note that this problem will be more severe when the entities become more numerous because the space complexity is ( ). For the inference speed, KG-BERT encodes the relational triples with the pre-trained language model and ranks all candidate triples with correct and wrong entities for inference. When the candidate entities set is huge, it is time-consuming for inference; for example, as shown in <ref type="table" target="#tab_1">Table 1</ref>, KG-BERT takes about 100100s to the scoring of all possible triples given a single (entity, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Case Study</head><p>For different decoding strategies we conduct case study to analyze the result. For GenKGC w/o hierarchical decoding, we utilize the normal beam search to decode the text name of the missing entity. From <ref type="table" target="#tab_4">Table 4</ref>, we observe that GenKGC obtain better entity generation results while in normal beam search, the model may stop early at correct but not precise enough answer. We argue that this is caused by the bias of the pre-trained language model (e.g., common token bias) since high-frequent tokens will lead the pre-trained language model to be biased toward certain answers. Our entityaware hierarchical decoding can constrain the decoding process and mitigate the bias effect caused by pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Knowledge Graph Embedding Models. There are lots of methods of KGC are based on knowledge graph embeddings (KGE), which generally leverage an embedding vector in the continuous embedding space to represent the entity and the relation in KG <ref type="bibr" target="#b16">[17]</ref>. One kind of KGE methods is translation-based, such as TransE <ref type="bibr" target="#b1">[2]</ref>, ConE <ref type="bibr" target="#b19">[20]</ref>, TotatE <ref type="bibr" target="#b8">[9]</ref>, which consider relations as the mapping function between entities.The other kind of KGE method is semantic matching models, where they get the semantic similarity by using the multi-linear or bilinear product.</p><p>Pre-trained Language Models for KGC. Recently, since pre-trained language models, such as BERT <ref type="bibr" target="#b5">[6]</ref>, have shown significant improvement on several natural language processing tasks, several works use the transformer-based models to tackle the KGC problems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. KG-BERT <ref type="bibr" target="#b13">[14]</ref> first propose to use BERT for KGC by seeing a triple as a sequence and converts KGC into a sequence classification problem with the binary cross-entropy object. <ref type="bibr" target="#b6">[7]</ref> proposes to use a transformer encoder-decoder model that takes plain text as input and output a structured triple of the information hide in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose GenKGC, which can reach comparable results while reducing inference and training cost in link prediction with pre-trained models. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach, especially in inference time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label></label><figDesc>OpenBG500 is a subset of open business KG from https://kg.alibaba.com/.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pre-trained Encoder Pre-trained Decoder</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">&lt;bos&gt;</cell><cell></cell></row><row><cell>D</cell><cell>Yau studied at, UC Berkeley. Obama studied at Harvard.</cell><cell>[Univ]</cell><cell>UC</cell><cell>Irvine</cell><cell>&lt;eos&gt;</cell><cell>[LOC]</cell><cell>[Univ]</cell><cell>[ORG]</cell><cell>Type Decode</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>UC</cell><cell></cell><cell>Harvard University</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Irvine</cell><cell>Davis</cell><cell>San</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell>Entity</cell></row><row><cell></cell><cell>Michael Chabon studied at</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decode</cell></row><row><cell></cell><cell></cell><cell>&lt;bos&gt;</cell><cell>[Univ]</cell><cell>UC</cell><cell>Irvine</cell><cell cols="3">&lt;eos&gt; &lt;eos&gt; &lt;eos&gt;</cell><cell></cell></row><row><cell cols="2">demonstration</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>arXiv:2202.02113v6 [cs.CL] 29 Mar 2022</cell><cell cols="2">Triple:&lt;Michael Chabon,study at,?&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Inference and training efficiency comparison. | | is the length of the entity description. |E | is the numbers of all unique entities in the KG. is the negative samples for KG-BERT and the beam size for our GenKGC . The time under RTX 3090 is used to estimate the speed of training and inference given a single (entity,relation) pair on OpenBG500.</figDesc><table><row><cell>For One Triple Method</cell><cell>Complexity</cell><cell>Time under RTX 3090</cell></row><row><cell>TransE</cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experiment results on WN18RR, FB15k-237 and OpenBG500. ?Resulting numbers are reported by<ref type="bibr" target="#b7">[8]</ref>, we reproduce the model result on OpenBG500 and take other results from the original papers.</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15k-237</cell><cell></cell><cell></cell><cell>OpenBG500</cell><cell></cell></row><row><cell>Method</cell><cell cols="9">Hits@1 Hits@3 Hits@10 Hits@1 Hits@3 Hits@10 Hits@1 Hits@3 Hits@10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Graph embedding approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransE [2] ?</cell><cell>0.043</cell><cell>0.441</cell><cell>0.532</cell><cell>0.198</cell><cell>0.376</cell><cell>0.441</cell><cell>0.207</cell><cell>0.340</cell><cell>0.513</cell></row><row><cell>DistMult [13] ?</cell><cell>0.412</cell><cell>0.470</cell><cell>0.504</cell><cell>0.199</cell><cell>0.301</cell><cell>0.446</cell><cell>0.049</cell><cell>0.088</cell><cell>0.216</cell></row><row><cell>ComplEx [11] ?</cell><cell>0.409</cell><cell>0.469</cell><cell>0.530</cell><cell>0.194</cell><cell>0.297</cell><cell>0.450</cell><cell>0.053</cell><cell>0.120</cell><cell>0.266</cell></row><row><cell>RotatE [9]</cell><cell>0.428</cell><cell>0.492</cell><cell>0.571</cell><cell>0.241</cell><cell>0.375</cell><cell>0.533</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TuckER [1]</cell><cell>0.443</cell><cell>0.482</cell><cell>0.526</cell><cell>0.226</cell><cell>0.394</cell><cell>0.544</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ATTH [4]</cell><cell>0.443</cell><cell>0.499</cell><cell>0.486</cell><cell>0.252</cell><cell>0.384</cell><cell>0.549</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Textual encoding approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KG-BERT [14]</cell><cell>0.041</cell><cell>0.302</cell><cell>0.524</cell><cell>-</cell><cell>-</cell><cell>0.420</cell><cell>0.023</cell><cell>0.049</cell><cell>0.241</cell></row><row><cell>StAR [12]</cell><cell>0.243</cell><cell>0.491</cell><cell>0.709</cell><cell>0.205</cell><cell>0.322</cell><cell>0.482</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GenKGC</cell><cell>0.287</cell><cell>0.403</cell><cell>0.535</cell><cell>0.192</cell><cell>0.355</cell><cell>0.439</cell><cell>0.203</cell><cell>0.280</cell><cell>0.351</cell></row><row><cell>entity in decoding.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary statistics of benchmark datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="5"># Ent # Rel # Train # Dev # Test</cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell>FB15k-237</cell><cell>14,541</cell><cell>237</cell><cell cols="3">272,115 17,535 20,466</cell></row><row><cell cols="5">OpenBG500 269,658 500 1,242,550 5,000</cell><cell>5,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>We list a query and first five entities with their probability predicted by GenKGC w/o entity-aware decoding, and its reranking with GenKGC.</figDesc><table><row><cell cols="2">Query:(?,student,Michael Chabon)</cell></row><row><cell cols="2">Rank GenKGC w/o hierarchical decoding</cell><cell>Probability</cell></row><row><cell>1</cell><cell>University of California</cell></row><row><cell>2</cell><cell>University of California, Irvine</cell></row><row><cell>3</cell><cell>University of California, San Francisco</cell></row><row><cell>4</cell><cell>University of California, Davis</cell></row><row><cell>5</cell><cell>University of California, Santa Cruz</cell></row><row><cell cols="2">Rank GenKGC</cell><cell>Probability</cell></row><row><cell>1</cell><cell>University of California, Irvine</cell></row><row><cell>2</cell><cell>University of California, San Francisco</cell></row><row><cell>3</cell><cell>University of California, Davis</cell></row><row><cell>4</cell><cell>University of California, Santa Cruz</cell></row><row><cell>5</cell><cell>University of Calgary</cell></row><row><cell cols="3">relation) pair. While our method only needs to generate the top-</cell></row><row><cell cols="3">entities with entity-aware hierarchical decoding, which reduces</cell></row><row><cell cols="2">lots of computing resources.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoregressive Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-Dimensional Hyperbolic Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">GenIE: Generative Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ontology-enhanced Prompt-tuning for Fewshot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OntoProtein: Protein Pretraining With Gene Ontology Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haosen</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhang</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation Adversarial Network for Low resource Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05575</idno>
		<ptr target="https://arxiv.org/abs/2201.05575" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
