<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mesh Graphormer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Microsoft</roleName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
						</author>
						<title level="a" type="main">Mesh Graphormer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a graph-convolution-reinforced transformer, named Mesh Graphormer, for 3D human pose and mesh reconstruction from a single image. Recently both transformers and graph convolutional neural networks (GC-NNs) have shown promising progress in human mesh reconstruction. Transformer-based approaches are effective in modeling non-local interactions among 3D mesh vertices and body joints, whereas GCNNs are good at exploiting neighborhood vertex interactions based on a prespecified mesh topology. In this paper, we study how to combine graph convolutions and self-attentions in a transformer to model both local and global interactions. Experimental results show that our proposed method, Mesh Graphormer, significantly outperforms the previous stateof-the-art methods on multiple benchmarks, including Hu-man3.6M, 3DPW, and FreiHAND datasets. Code and pre-trained models are available at https://github. com/microsoft/MeshGraphormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose and mesh reconstruction from a single image is a popular research topic as it offers a wide range of applications for human-computer interactions. However, due to the complex body articulation, it is a challenging task.</p><p>Recently, Transformers and Graph Convolutional Neural Networks (GCNNs) have shown promising advances in human mesh reconstruction. For example, recent studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref> have suggested using GCNNs to directly regress 3D positions of mesh vertices by taking into account local interactions among neighbor vertices. In a recent study <ref type="bibr" target="#b21">[22]</ref>, a transformer encoder was used with self-attention to capture global interactions among body joints and mesh vertices, which lead to further improvement.</p><p>However, as discussed in the literature <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>, Transformers and Convolution Neural Networks (CNN) each have their own limitations. Transformers are good at modeling long-range dependencies on the input tokens, but <ref type="figure">Figure 1</ref>: Summary. We study how to combine selfattentions and graph convolutions in a transformer for human mesh reconstruction. The proposed Graphormer outperforms existing graph convolution networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> and transformer-based method <ref type="bibr" target="#b21">[22]</ref> by a clear margin. The numbers are the reconstruction error (PA-MPJPE) in the unit of millimeter. The lower the better. they are less efficient at capturing fine-grained local information. Convolution layers, on the other hand, are useful for extracting local features, but many layers are required to capture global context. In natural language processing and speech recognition, Conformer <ref type="bibr" target="#b7">[8]</ref> is a recently proposed technique that leverages the complementarity of selfattention and convolutions to learn representations. This motivates us to combine self-attention and graph convolution for the 3D reconstruction of human mesh <ref type="figure">(Figure 1</ref>).</p><p>We present a graph-convolution-reinforced transformer called Mesh Graphformer for reconstructing human pose and mesh from a single image. We inject graph convolutions into transformer blocks to improve the local interactions among neighboring vertices and joints. In order to leverage the power of graph convolutions, Graphormer is free to attend to all image grid features that contain more detailed local information and are helpful in refining the 3D coordinate prediction. Consequently, Graphormer and image grid features are mutually reinforced to achieve better performance in human pose and mesh reconstruction.</p><p>Extensive experiments show that the proposed Mesh Graphormer models both local and global interactions effectively and clearly outperforms previous state-of-the-art methods for reconstructing human mesh in several datasets. Additionally, we offer ablation studies on various model design options to incorporate graph convolutions and selfattention into a transformer encoder.</p><p>The main contributions of this paper include ? We present a graph-convolution-reinforced transformer called Mesh Graphormer to model both local and global interactions for the 3D reconstruction of human pose and mesh.</p><p>? Mesh Graphormer allows joints and mesh vertices to freely attend to image grid features to refine the 3D coordinate prediction.</p><p>? Mesh Graphormer outperforms previous state-of-theart methods on Human3.6M, 3DPW, and FreiHAND datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Human mesh reconstruction can generally be divided into parametric and non-parametric approaches. The vast majority of previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> uses a parametric human model such as SMPL <ref type="bibr" target="#b23">[24]</ref> and focuses on using the SMPL parameter space as a regression target. Given the pose and shape coefficients, SMPL is robust and practical for creating human mesh. However, as discussed in the literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref>, it remains difficult to estimate accurate coefficients from a single image, and researchers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> are trying to predict 3D posture, learning with more visual evidence, or adopting the dense correspondence maps <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b8">9]</ref> to improve the reconstruction.</p><p>Instead of regressing the parametric coefficients, nonparametric approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref> regress vertices directly from an image. Among the previous studies, the Graph Convolutional Neural Network (GCNN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref> is one of the most popular options as it is able to model the local interactions between neighboring vertices based on a given adjacency matrix <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref>. However, it is less efficient to capture global interactions between the vertices and body joints. To overcome this limitation, transformer-based methods <ref type="bibr" target="#b21">[22]</ref> use a self-attention mechanism to freely attend vertices and body joints in the mesh and thereby encode non-local relationships of a human mesh. However, it is less convenient to model local interactions than GNNbased methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Among the works mentioned above, METRO <ref type="bibr" target="#b21">[22]</ref> is the most relevant study to our proposed method. The main difference between METRO and our proposed model is that we are designing a graph-convolution-reinforced transformer encoder for reconstructing human mesh. In addition, we add image grid features as input tokens to the transformer and allow the joints and mesh vertices to attend to grid features.</p><p>Transformer architecture is developing rapidly for different applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10]</ref>. One important direction is to improve the expressiveness of a transformer network for better context modeling. Recent studies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8]</ref> show that the combination of convolution and self-attention in a transformer encoder is helpful to improve representation learning. However, previous work has mainly focused on language modeling and speech recognition. When it comes to complex data structures such as 3D human mesh, this remains an open problem.</p><p>To address these challenges, we investigate how to inject graph convolutions into the transformer encoder blocks to better model both local and global interactions among 3D mesh vertices and body joints. <ref type="figure" target="#fig_0">Figure 2</ref> shows the architecture of our proposed Graphormer encoder. It consists of a stack of N = 4 identical blocks. Each block consists of five sub-modules, including a Layer Norm, a Multi-Head Self-Attention module, a Graph Residual Block, a second Layer Norm, and a Multi-Layer Perceptron (MLP) at the end. Our Graphormer encoder has a similar architecture to the traditional transformer encoder <ref type="bibr" target="#b34">[35]</ref>, but we introduce graph convolution into the network to model fine-grained local interactions. In the following sections, we describe Multi-Head Self-Attention (MHSA) module and Graph Residual Block. Our end-to-end mesh regression framework takes an image as input and predicts 3D joints and mesh vertices at the same time. We extract both the grid features and a global feature vector using a pre-trained CNN. The features are then tokenized and fed to a multi-layer Graphormer encoder. Grid features and multi-layer Graphormer encoder are mutually reinforced to reconstruct an accurate human mesh. (b) Our multi-layer Graphormer encoder consists of three Graphormer encoder blocks. All encoders have the same number of input tokens. We gradually reduce the hidden sizes, and output the 3D coordinates of body joints and mesh vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graphormer Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Head Self-Attention</head><p>We employ the Multi-Head Self-Attention (MHSA) module proposed by Vaswani et al. <ref type="bibr" target="#b34">[35]</ref>, which uses several self-attention functions in parallel to learn contextual representation. Given an input sequence X = {x 1 , x 2 , . . . , x n } ? R n?d , where d is the hidden size. It first projects the input sequence to queries Q, keys K, and values V by using trainable parameters</p><formula xml:id="formula_0">{W Q , W K , W V } ? R d?d . That is written as Q, K, V = XW Q , XW K , XW V ? R n?d .<label>(1)</label></formula><p>The three feature representations are split into h different subspaces, e.g.,</p><formula xml:id="formula_1">Q = [Q 1 , Q 2 , . . . , Q h ] where Q i ? R n? d</formula><p>h , so that we can perform self-attention for each subspace individually. For each subspace, the output Y h = {y h 1 , y h 2 , . . . , y h n } are computed by:</p><formula xml:id="formula_2">y h i = Att(q h i , K h )V h ? R d h ,<label>(2)</label></formula><p>where Att(?) denotes the attention function <ref type="bibr" target="#b34">[35]</ref> that quantifies how semantically relevant a query q h i is to keys K h by scaled dot-product and softmax. The output Y h ? R n? d h from each subsapce are later concatenated to form the final output Y ? R n?d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Residual Block</head><p>While MHSA is useful for extracting long-range dependencies, it is less efficient at capturing fine-grained local information in complex data structures such as 3D mesh. The Graph Residual Block in our proposed network aims to address this challenge.</p><p>Given the contextualized features Y ? R n?d generated by MHSA, we improve the local interactions with the help of graph convolution:</p><formula xml:id="formula_3">Y = GraphConv(?, Y; W G ) = ?(?YW G ). (3)</formula><p>A ? R n?n denotes the adjacency matrix of a graph and W G the trainable parameters. ?(?) is the activation function that gives the network non-linearity. According to BERT <ref type="bibr" target="#b5">[6]</ref>, we use the Gaussian Error Linear Unit (GeLU) <ref type="bibr" target="#b10">[11]</ref> in this work.</p><p>To train with multiple graph convolution layers, we follow the design principle in GraphCMR <ref type="bibr" target="#b19">[20]</ref> to build our Graph Residual Block. The network architecture is spiritually similar to <ref type="bibr" target="#b19">[20]</ref>, but we replace the group normalization <ref type="bibr" target="#b38">[39]</ref> with the layer normalization <ref type="bibr" target="#b1">[2]</ref> and replace ReLU <ref type="bibr" target="#b27">[28]</ref> with GeLU <ref type="bibr" target="#b10">[11]</ref>. This is to bring the type of layers in line with the transformer.</p><p>Our Graph Residual Block makes it possible to explicitly encode the graph structure within the network and thereby improve spatial locality in the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graphormer for Mesh Reconstruction</head><p>In this section, we describe how the Graphormer encoder is applied to human pose and mesh reconstruction from a single image. <ref type="figure" target="#fig_1">Figure 3</ref>(a) gives an overview of our end-to-end mesh regression framework. An image with a size of 224x224 is used as input and image grid features are extracted. The image features are tokenized as input for a multi-layer Graphormer encoder. In the end, our end-to-end framework predicts 3D coordinates of the mesh vertices and body joints at the same time.</p><p>In the following, we describe the image-based architecture with which we extract image grid features. Next, we describe the Multi-Layer Graphormer Encoder for regression of the 3D vertices and body joints. Finally, we discuss important training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNN and Image Grid Features</head><p>In the first part of our model, we use a pre-trained imagebased CNN for feature extraction. Previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> extracts a global 2048-Dim image feature vector as a model input. The disadvantage is that a global feature vector does not contain fine-grained local details. This motivates us to add grid features <ref type="bibr" target="#b12">[13]</ref> as input tokens and allow joints and mesh vertices to freely attend to all the grid features. As we will show in our experiments, the local information provided by the grid features are effectively leveraged by the graph convolutions in the Graphormer to refine 3D positions of mesh vertices and body joints.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a), we extract the grid features from the last convolution block in CNN. The grid features are typically 7 ? 7 ? 1024 in size. They are tokenized to 49 tokens, and each token is a 1024-Dim vector. Similar to <ref type="bibr" target="#b21">[22]</ref>, we also extract the 2048-Dim image feature vector from the last hidden layer of the CNN and perform positional encoding using the 3D coordinates of each vertex and body joint in a human template mesh. Finally, we apply MLP to make the size of all input tokens consistent. After that, all input tokens have 2051-Dim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Layer Graphormer Encoder</head><p>Given the grid features, joint queries, and vertex queries, our multi-layer Graphormer encoder sequentially reduces the dimensions to map the inputs to 3D body joints and mesh vertices at the same time.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b), our multi-layer Graphormer encoder consists of three encoder blocks. The three encoder blocks have the same number of tokens including 49 grid feature tokens, 14 joint queries, and 431 vertex queries. However, the three encoders have different hidden dimensions. In this work, the hidden dimensions of the three encoders are 1024, 256 and 64, respectively.</p><p>Similar to <ref type="bibr" target="#b21">[22]</ref>, our multi-layer Graphormer encoder processes a coarse mesh with 431 vertices. We use a coarse template mesh for positional encoding, and our Graphormer encoder outputs a coarse mesh. Then, we use a linear pro-jection to sample the coarse mesh up to the original resolution (with 6K vertices in SMPL mesh topology). As explained in the literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, learning a coarse mesh followed by upsampling is helpful to avoid redundancies in the original mesh and makes training more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Details</head><p>Following METRO <ref type="bibr" target="#b21">[22]</ref>, we train our model by applying a loss of L 1 in addition to the Graphormer outputs. In our training, we also use Masked Vertex Modeling <ref type="bibr" target="#b21">[22]</ref> to improve the robustness of our model. To be specific, we apply L 1 loss to 3D mesh vertices and body joints. We also apply L 1 loss to 2D projected body joints to improve the alignment between the image and the reconstructed mesh. In addition, we apply intermediate supervision on the coarse mesh to accelerate convergence.</p><p>We train Graphormers with the Adam optimizer <ref type="bibr" target="#b15">[16]</ref>. We use an initial learning rate of 1 ? 10 ?4 for both Graphormer and CNN backbones. We train our model for 200 epochs and lower the learning rate by a factor of 10 after 100 epochs. All Graphormer weights are randomly initialized, and the CNN backbone is initialized with ImageNet pre-trained weights. Following <ref type="bibr" target="#b21">[22]</ref>, we report on results with the HRNet backbone <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first discuss the datasets we use in our training. We then compare our method with previous stateof-the-art methods. Finally, we provide in-depth ablation studies on our model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We conduct extensive training using publicly available datasets, including Human3.6M <ref type="bibr" target="#b11">[12]</ref>, MuCo-3DHP <ref type="bibr" target="#b24">[25]</ref>, UP-3D <ref type="bibr" target="#b20">[21]</ref>, COCO <ref type="bibr" target="#b22">[23]</ref>, MPII <ref type="bibr" target="#b0">[1]</ref>. Please note that the 3D mesh training data from Human3.6M <ref type="bibr" target="#b11">[12]</ref> is not available due to licensing issues. Therefore, we use the pseudo 3D mesh training data from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>. We follow the general setting where subjects S1, S5, S6, S7 and S8 are used in training, and test subjects S9 and S11. We present all results using the P2 protocol <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. To get a fair comparison with earlier state-of-the-art <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>, we also use 3DPW <ref type="bibr" target="#b35">[36]</ref> training data for 3DPW experiments.</p><p>We also apply our method to reconstructing 3D hand. We conduct training on FreiHAND <ref type="bibr" target="#b43">[44]</ref> and evaluate using its online server. Please note that we use test-time augmentation for FreiHAND experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Main Results</head><p>We compare our method with previous approaches to reconstruct human mesh on Human3.6M and 3DPW datasets. In <ref type="table" target="#tab_0">Table 1</ref>, we can see that our method outperforms the previous state-of-the-art for both datasets. This includes both   GCNN-based models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref> and transformer-based methods <ref type="bibr" target="#b21">[22]</ref>. It suggests that Graphormer models both local and global interactions better and achieves more accurate mesh reconstruction. <ref type="figure" target="#fig_2">Figure 4</ref> shows the qualitative results of our method on Human3.6M and 3DPW datasets. We see that our method is robust to challenging poses and noisy backgrounds.</p><p>We also apply our method to 3D hand reconstruction and compare it with other modern approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44]</ref> on FreiHAND dataset. In <ref type="table" target="#tab_2">Table 2</ref>, we see that our method works much better than the prior art. This demonstrates the generalization capability of our method for other objects. <ref type="figure" target="#fig_4">Figure 5</ref> shows the qualitative results of our method on FreiHAND test set. We can see that Graphormer can be   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>We conduct a large-scale ablation study on Hu-man3.6M <ref type="bibr" target="#b11">[12]</ref> to investigate our model capability. We evaluate our model using the 3D pose accuracy, and report the accuracy using PA-MPJPE metric <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Effectiveness of Grid Features: In our first ablation study, we are interested in the effect of adding image grid features to the transformer encoder. We have implemented a baseline model <ref type="bibr" target="#b21">[22]</ref> that uses the image grid features as inputs. <ref type="table" target="#tab_4">Table 3</ref> shows that the addition of image grid features improves the reconstruction performance by a clear margin. This indicates that using a single global feature vector is one of the performance bottlenecks in existing techniques.    analysis for a variety of feature maps in an HRNet <ref type="bibr" target="#b36">[37]</ref>. <ref type="figure">Figure 6</ref> shows the six feature maps we have selected for experiments, and <ref type="table" target="#tab_5">Table 4</ref> shows the performance comparison. Our finding is that the latest feature map (labeled F in <ref type="figure">Figure 6</ref>) works better than others. The results suggest that the feature pyramid is helpful for improving the performance of a transformer encoder.   transformer can be combined. Since our proposed framework contains three encoder blocks, we examine the effect of graph convolution by gradually adding a graph convolution layer to each encoder and comparing the performance. In <ref type="table" target="#tab_6">Table 5</ref>, the first row corresponds to the baseline that does not use any graph convolution in the network. The rest of the rows show the results of adding a graph convolution layer to different encoder blocks. The results show some interesting observations: (i) Adding a graph convolution to Encoder1 or Encoder2 does not improve performance. (ii) Adding the graph convolution to Encoder3 improves 0.9 PA-MPJPE. The results suggest that the lower layers focus on the global interactions of the mesh vertices to model the posture of the human body, while the upper layers pay more attention to the local interactions for better shape reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Adding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Encoder Architecture:</head><p>We further examine the network architecture in an encoder block. That is, we investigate the relationship between the MHSA module (Multi-Head Self-Attention) and the Graph Convolution module by using three different designs: (i) We use a graph convolution layer and MHSA in parallel, similar to <ref type="bibr" target="#b39">[40]</ref>. (ii) We first use a graph convolution layer and then MHSA. (iii) We first use MHSA and then a graph convolution layer. Please refer to the supplementary material for a graphical illustration of the three architecture designs. In <ref type="table" target="#tab_8">Table 6</ref>, we can see that adding a graph convolution layer directly after MHSA works much better than other design options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Adding Graph Residual Block:</head><p>In the previous sections, we explored the use of a basic graph convolution layer in a transformer encoder. Here we extend our experiments to Graph Residual Block. In particular, we replace a graph convolution layer with a Graph Residual Block to improve model capacity. In <ref type="table" target="#tab_9">Table 7</ref>, we see that a <ref type="figure">Figure 7</ref>: Qualitative comparison. The top row (with pink mesh) shows the results of a transformer baseline <ref type="bibr" target="#b21">[22]</ref> which has no graph convolutions. The bottom row (with blue mesh) shows our results with Graphormer. The attention visualization shows the interactions between the left knee and all other vertices, and the brighter color indicates stronger interactions. As can be seen, Graphormer is helpful to model both global and local interactions among vertices and body joints, and achieves a more accurate reconstruction. Please note the color of each attention map is normalized with respect to the maximum attention. <ref type="figure" target="#fig_1">Figure 13</ref> in the supplementary material shows the attention maps without color normalization.  Graph Residual Block works better than a basic graph convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship between Grid Features and Graph Convolution:</head><p>An interesting question is what happens when we use Graphormer Encoder, but not grid features. To answer the question, <ref type="table" target="#tab_11">Table 8</ref> shows the comparisons. First of all, the first row of <ref type="table" target="#tab_11">Table 8</ref> corresponds to the baseline which has no grid features or graph convolution in the transformer. Compared to the fourth row of <ref type="table" target="#tab_11">Table 8</ref>, we see that we can achieve an improvement of 0.8 PA-MPJPE if we only enable grid features. Next, if we enable graph convolution alone, as shown in the third row of <ref type="table" target="#tab_11">Table 8</ref>, only 0.1 PA-MPJPE is improved. Finally, as shown in the bottom row of <ref type="table" target="#tab_11">Table 8</ref>, when we enable both grid features and graph convolution, it eventually improves PA-MPJPE by 2.2 which is much greater than the sum of the two individual improvements (0.1 + 0.8). It shows that grid features and graph convolutions mutually reinforce each other, which leads to an further increase in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Local Interactions:</head><p>We further study the effect of Graphormer in learning both global and local interactions among body joints and mesh vertices. We extract the attention maps from the last layer of our last encoder (i.e., Encoder3), and compute the average attentions of all the attention heads. We compare the self-attentions of Graphormer with that of the existing approach <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure">Figure 7</ref> shows the qualitative comparison. We can see in the top row of <ref type="figure">Figure 7</ref>, previous approach <ref type="bibr" target="#b21">[22]</ref> fails to model the interactions between the left knee and left toes. In contrast, as shown in the bottom row of <ref type="figure">Figure 7</ref>, Graphormer is able to model both global and local interactions, especially those between the left knee and the left toes. As a result, Graphormer reconstructs a more favorable shape compared to prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced Mesh Graphormer, a new transformer architecture that incorporates graph convolutions and selfattentions for reconstructing human pose and mesh from a single image. We explored various model design options and demonstrated that both graph convolutions and grid features are helpful for improving the performance of the transformer. Experimental results show that our method generates new state-of-the-art performance on Human3.6M, 3DPW, and FreiHAND datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Input METRO Graphormer Input METRO Graphormer A. Qualitative Comparison <ref type="figure" target="#fig_5">Figure 8</ref> shows qualitative results of Graphormer compared with METRO <ref type="bibr" target="#b21">[22]</ref> in the scenario of heavy occlusions. We can see that both methods are quite robust against occlusions, but Graphormer generates better head and body poses. At the top right of <ref type="figure" target="#fig_5">Figure 8</ref>, almost half of the subject is occluded. Graphormer reconstructs a human mesh with more accurate head/body pose compared to METRO. At the bottom right of <ref type="figure" target="#fig_5">Figure 8</ref>, the subject is occluded by the car door. We see Graphormer reconstructs a more reasonable body shape. At the bottom left, the subject is standing behind the fence. Our method reconstructs a human mesh with the two legs better aligned with the image. The results demonstrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Qualitative Results</head><p>Further, to demonstrate the robustness and generalization capability of our model to challenging scenarios, we test our model on the hand images that are collected from the Internet. The images have severe occlusions with different objects.</p><p>To make the task even more difficult, we create artificial occlusions including black vertical stripes to cover one or two fingers, or part of the palm of the hand in the test images. Please note that the artificial occlusions are only used in the inference stage. We do not use any artificial occlusions in training.</p><p>In <ref type="figure">Figure 9</ref>, <ref type="figure">Figure 10</ref>, <ref type="figure">Figure 11</ref>, and <ref type="figure" target="#fig_0">Figure 12</ref>, we show the input images and our reconstructed hand meshes. For each figure, the top row shows the occlusion scenario with narrow black stripes. From the second row to the bottom, we gradually increase the width of the stripes to occlude more fingers or more parts of the hand. <ref type="figure">Figure 9</ref> shows a hand with an orange. Our model is able to reconstruct a reasonable hand mesh, even if the hand is severely obscured by the vertical stripes. The results show that Graphormer is to some extent robust to the artificial occlusion patterns.</p><p>In <ref type="figure">Figure 10</ref>, there is a hand grasping a banana. Although the banana is a novel object unseen in training and a large portion of the fingers are occluded by the banana, Graphormer successfully reconstructs the hand mesh under various occlusion scenarios. This demonstrates the generalization ability of our proposed Graphormer. <ref type="figure">Figure 11</ref> shows a hand with an ice cream cone. Please note that the ice cream cone is unseen during training, and the interaction between the hand and the ice cream cone is complex. However, our model generalizes well to the test images. Even though the occlusions by the ice cream cone are severe and sometimes most of the fingers are occluded by the black vertical stripes, Graphormer still reconstructs a reasonable hand shape with object-specific grasp. <ref type="figure" target="#fig_0">Figure 12</ref> shows a hand with half an orange. Most of the fingers are invisible in this image. However, our model creates a hand mesh with the correct hand pose.</p><p>We further present the video results of <ref type="figure">Figure 9</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison between MLP and GCN</head><p>In this section, we replace graph residual block with MLPs, and study the performance of the use of MLPs with a similar or larger model size.</p><p>In <ref type="table" target="#tab_12">Table 9</ref>, the first row corresponds to the baseline transformer that uses image grid features. In the second and the third rows, we gradually increase the hidden size of the MLP module in the transformer, but we do not achieve any gain in performance. The bottom row of <ref type="table" target="#tab_12">Table 9</ref> shows the results of our Graphormer. As can be seen, adding graph convolutions has a slight increase of 0.04M parameters, but it improves performance significantly from 35.9 to 34.5 PA-MPJPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Design Options of Graphormer Encoder</head><p>In <ref type="figure" target="#fig_2">Figure 14</ref>, we graphically illustrate three design options of the Graphormer encoder we have studied in the paper. Please refer to <ref type="table" target="#tab_8">Table 6</ref> in our main paper for the performance comparisons between the design options. We observe that placing graph convolutions after MHSA works better than other design options for the reconstruction of human mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion of Attention Map</head><p>Please note that the attention colors in the paper's diagrams are normalized based on the maximum attention value. Because the maximum attention value for Graphormer is smaller, so the overall colors are lighter. We attach the two diagrams without color normalization in <ref type="figure" target="#fig_1">Figure 13</ref>. We see that both methods pay similar attention on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Input Output Input Output <ref type="figure" target="#fig_0">Figure 12</ref>: Qualitative results of our method. It is a hand holding half an orange. Graphormer is able to reconstruct a reasonable hand mesh even though most of the fingers are invisible. Please see Fig4.gif for more detailed video results. the left arm and right foot, while Graphormer also attends to the left lower leg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion of Camera Parameters</head><p>We learn camera parameters for a weak perspective camera model. Following <ref type="bibr" target="#b19">[20]</ref>, we predict a scaling factor s and a 2D translation vector t. Please note that the model prediction is already in the camera coordinate system, thus we don't have to compute global camera rotation. The camera parameters are learned via 2D pose re-projection optimization. It doesn't require any GT camera parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Training Time</head><p>We conducted experiments on a machine with 8 NVIDIA V100 GPUs. We use a batch size of 32. For each epoch, our training takes about 35 minutes. We train the proposed model for 200 epochs. The overall training takes 5 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Computational Costs</head><p>Since we inject graph convolutions into the transformer, one may wonder about the computational costs of the proposed Graphormer. To answer the question, we report the number of parameters and the computational complexity in terms of GFLOPs. <ref type="table" target="#tab_0">Table 10</ref> shows the comparison between the conventional transformer and the proposed Graphormer. We also report the computational cost of the HRNet CNN backbone for reference. As we can see, adding graph convolutions has a slight increase of 0.04M parameters and <ref type="figure" target="#fig_2">Figure 14</ref>: Three design options we have studied for building our proposed Graphormer Encoder. The designs are inspired by language and speech literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>0.01 GFLOPs compared to the conventional transformer. The results suggest that little complexity has been added to the transformer architecture. However, Graphormer significantly improves the state-of-the-art performance across multiple benchmarks. This verifies the effectiveness of the proposed method.</p><p>Please note that the total parameters of our end-to-end pipeline is the sum of HRNet and Graphormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Limitation</head><p>We observed that our method may not work well if the reconstruction target is out of the view. For example, as shown in <ref type="figure" target="#fig_4">Figure 15</ref>(a), when the majority of the human body is not in the input image, our method fails to estimate a correct human mesh. This is probably due to the lack of out-of-the-view 3D training data in our training set. In <ref type="figure" target="#fig_4">Figure 15</ref>(b), only two hands are visible and the rest of the human body is out of the view. Our method does not work well in this case. We plan to address this issue in our future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of a Graphormer Encoder. We propose a graph-convolution-reinforced transformer encoder to capture both global and local interactions for 3D human mesh reconstruction. The encoder consists of a stack of N = 4 identical blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Graphormer for Human Mesh Reconstruction. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of our method on different datasets. The top two rows are the results on Human3.6M, and the bottom two rows are the results on 3DPW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3DPW</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of our method on FrieHAND test set. Graphormer generalizes to 3D hand reconstruction even if the gestures are novel and complicated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results when there are heavy occlusions. For each example, we show results from METRO<ref type="bibr" target="#b21">[22]</ref> and Graphormer. We can see that both METRO and Graphormer are quite robust against occlusions, but Graphormer generates more favorable head pose and body pose. Blue: METRO. Silver: Graphormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, Figure 10, Figure 11, and Figure 12. Please find the video Input Output Input Output Input Output</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Attention map without color normalization. Graphormer pays more attentions to the lower left leg compared to METRO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 15 :</head><label>15</label><figDesc>Failure cases. Mesh Graphormer may not work well if the reconstruction target is out of the view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with the previous state-of-the-art methods on 3DPW and Human3.6M datasets.</figDesc><table><row><cell>Human3.6M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with the previous state-of-the-art methods on FreiHAND dataset.</figDesc><table><row><cell>used to reconstruct a 3D hand even if the target gesture is</cell></row><row><cell>novel and complicated.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of the effective of using the image grid features in a transformer framework.</figDesc><table><row><cell>Grid Features</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell></row><row><cell>PA-MPJPE</cell><cell cols="6">37.1 37.7 36.3 38.1 36.0 35.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of the use of different image grid features from the HRNet.</figDesc><table><row><cell>Figure 6: Illustration of different grid features we extracted</cell></row><row><cell>from HRNet [37] for ablation study.</cell></row><row><cell>Encoder1 Encoder2 Encoder3 PA-MPJPE</cell></row><row><cell>35.9</cell></row><row><cell>36.7</cell></row><row><cell>36.2</cell></row><row><cell>35.1</cell></row><row><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Ablation study of Graphormer Block for three en- coders used for human mesh regression. : Add graph convolution into the specified transformer encoder; : No graph convolution in the specified transformer encoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison between different design options of the architecture of Graphormer Block.</figDesc><table><row><cell>Design Choices</cell><cell>PA-MPJPE</cell></row><row><cell>Grid Features</cell><cell>35.9</cell></row><row><cell>Grid Features + Graph Conv.</cell><cell>35.1</cell></row><row><cell>Grid Features + Graph Res. Block</cell><cell>34.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of the basic graph convolution and Graph Residual Block.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of different combinations of image grid features and graph convolutions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results of our method. There is a hand with an orange. We demonstrate the robustness of our model by adding artificial occlusions including black vertical stripes to the images. We can see that Graphormer reconstructs plausible hand mesh under the occlusion scenarios. Please see Fig1.gif for more detailed video results. Qualitative results of our method. There is a hand holding a banana. We do not have any banana training images. However, Graphormer generalizes to the novel object, and creates the hand mesh with the correct pose. Please see Fig2.gif for more detailed video results. Qualitative results of our method. There is a hand holding an ice cream cone. The ice cream cone is a novel object unseen in training, and the hand pose is also object-specific. We can see that Graphormer works reasonably well for the test images. Please see Fig3.gif for more detailed video results.</figDesc><table><row><cell>Input Figure 11: Method Output Output Graphormer -GRB Graphormer -GRB + MLP1 Input Input Graphormer -GRB + MLP2 Figure 10: Input Graphormer</cell><cell>Output Output Para (M) ? Para (M) PAMPJPE ? Input Input 98.39 ? 35.9 98.43 0.04 35.9 98.92 0.53 36.0 98.43 0.04 34.5</cell><cell>Output Output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison between the use of MLP and GRB.</figDesc><table /><note>results in the attached GIF files.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Number of parameters and computational complexity in terms of GFLOPs.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11579</idno>
		<title level="m">A2-nets: Double attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond static features for temporally consistent 3d human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pose2pose: 3d positional pose-guided 3d rotational pose prediction for expressive 3d human pose and mesh estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11534</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Human mesh recovery from multiple shots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09843</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating 3d faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Qanet: Combining local convolution with global selfattention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning 3d human shape and pose from dense body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
