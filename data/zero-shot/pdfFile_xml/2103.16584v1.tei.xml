<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameterized Hypercomplex Graph Neural Networks for Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Research</orgName>
								<orgName type="department" key="dep2">Digital Technologies</orgName>
								<orgName type="institution">Bayer AG</orgName>
								<address>
									<postCode>13353</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Freie Universit?t Berlin</orgName>
								<address>
									<postCode>14195</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bertolini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Research</orgName>
								<orgName type="department" key="dep2">Digital Technologies</orgName>
								<orgName type="institution">Bayer AG</orgName>
								<address>
									<postCode>13353</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>No?</surname></persName>
							<email>frank.noe@fu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Freie Universit?t Berlin</orgName>
								<address>
									<postCode>14195</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
							<email>djork-arne.clevert@bayer.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Research</orgName>
								<orgName type="department" key="dep2">Digital Technologies</orgName>
								<orgName type="institution">Bayer AG</orgName>
								<address>
									<postCode>13353</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parameterized Hypercomplex Graph Neural Networks for Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graph Neural Networks ? Graph Representation Learning ? Graph Classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent advances in representation learning in hypercomplex (HC) space, this subject is still vastly unexplored in the context of graphs. Motivated by the complex and quaternion algebras, which have been found in several contexts to enable effective representation learning that inherently incorporates a weight-sharing mechanism, we develop graph neural networks that leverage the properties of hypercomplex feature transformation. In particular, in our proposed class of models, the multiplication rule specifying the algebra itself is inferred from the data during training. Given a fixed model architecture, we present empirical evidence that our proposed model incorporates a regularization effect, alleviating the risk of overfitting. We also show that for fixed model capacity, our proposed method outperforms its corresponding real-formulated GNN, providing additional confirmation for the enhanced expressivity of HC embeddings. Finally, we test our proposed hypercomplex GNN on several open graph benchmark datasets and show that our models reach state-of-the-art performance while consuming a much lower memory footprint with 70% fewer parameters. Our implementations are available at https://github.com/bayer-science-for-a-better-life/phc-gnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Geometric deep learning, broadly considered, refers to deep learning methods in the non-Euclidean domain. Although being just in its infancy, the field has already achieved quite remarkable success <ref type="bibr" target="#b2">[3]</ref>. The prime example is perhaps constituted by data naturally represented as graphs, which poses significant challenges to Euclidean-based learning <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b5">5]</ref>. This difficulty resides in the topological properties of a graph, which is defined by a pair of sets G = (V, E), where V is the set of vertices and E is the set of edges between vertices, that is, e ij = (v i , v j ) ? E is the edge between nodes v i , v j ? V . This structure is inherently discrete and does not possess a natural continuous metric, both fundamental properties for defining the Euclidean topology. These issues have cost the machine learning practitioner time and effort to develop feature engineering techniques to represent the data suitably for Euclidean-based learning methods. For instance, circular fingerprints encode a molecule's graph-like structure through a one-hot-encoding of certain pre-established chemical substructures. The steadily increasing number of applications in which graphs naturally represent the data of interest has driven the development of proper graph-based learning <ref type="bibr" target="#b44">[44]</ref>. A prime source of applications stems from chemistry, where predictive models for bioactivity or physicochemical properties of a molecule are rapidly gaining relevance in the drug discovery process <ref type="bibr" target="#b30">[30]</ref>. Other applications arise in the context of social and biological networks, knowledge graphs, e-commerce, among others <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b44">44]</ref>.</p><p>A crucial step for defining graph-based learning is to extend the above definition of a graph by considering each element of the sets V, E as feature vectors, that is, x v , e ij ? M, where M is a suitable manifold. In this context, the field of graph representation learning (GRL) is often divided into spectral <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25]</ref> and non-spectral/spatial approaches <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b41">41]</ref>. The latter class, to which this works belong, is based on the idea of local message passing, where vector messages between connected nodes are exchanged and updated using neural networks <ref type="bibr" target="#b12">[12]</ref>. Most of the literature on GNNs has focused on M = R n , that is, vertex and edge embeddings are into Euclidean space. Therefore, it is natural to ask whether this choice is again a restriction imposed by history or simplicity and to which extent GRL could benefit from greater freedom in choosing the manifold M. As first step we consider M = R n as a topological space, but generalize its algebra structure, that is, a vector space equipped with a bilinear product, beyond the real numbers. Example of these are the familiar complex and quaternion algebras, and these and more general arXiv:2103.16584v1 <ref type="bibr">[cs.</ref>LG] 30 Mar 2021 algebras are often referred to as hypercomplex number systems. In mathematics, a hypercomplex algebra is defined as a finite-dimensional unital algebra over the field of real numbers <ref type="bibr" target="#b23">[23]</ref>, where unital refers to the existence of an identity element e I such that e I ? q = q ? e I = q for all elements q in the algebra. Such property imposes strong constraints on the algebra structure and dimensionality, which turns out to be 2 n for n ? Z &gt;0 . Although hypercomplex number systems crucially inspired our proposed framework, the algebras learned by our models do not satisfy, in general, such constraints. While we are fully aware of this distinction, we will often loosely refer in the following to our models/embeddings as "hypercomplex" to better align with existing literature and avoid introducing additional unnecessary terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The present work lies at the intersection of three active research areas: (1) geometric approaches to GNNs, <ref type="bibr" target="#b1">(2)</ref> hypercomplex/algebraic generalizations of deep learning methods, and (3) regularization/parameter efficiency techniques. This section illustrates how our work relates to these areas and which new aspects we introduce or generalize.</p><p>Geometric deep learning is the discipline that comprises the formalization of learning of data embeddings as functions defined on non-Euclidean domains <ref type="bibr" target="#b2">[3]</ref>. Hyperbolic manifolds, for example, constitute an important class of non-Euclidean spaces which has been successfully deployed in deep learning. Here, basic manifold-preserving operations such as addition and multiplication in the context of neural networks have been extended to hyperbolic geometry <ref type="bibr" target="#b10">[10]</ref>. Such advances led to the development of hyperbolic GNNs. The works <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b6">6]</ref> have empirically shown that the hyperbolic setting is better suited for representation learning on real-world graphs with scale-free or hierarchical structure. As mentioned above, another defining property of the embedding function learned by a neural network is its underlying vector space structure. Complex-and hypercomplex-based neural networks have received increasing attention in several applications, from computer vision to natural language processing (NLP) tasks <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b39">39]</ref>. Hypercomplex representation learning offers promising theoretical properties and practical advantages. It has been argued that, as in the complex case <ref type="bibr" target="#b0">[1]</ref>, networks possess a richer representational capacity, resulting in more expressive embeddings. Hypercomplex models encompass greater freedom in the choice of the product between the algebra elements: in the case of the quaternion algebra, the Hamilton product naturally incorporates a weight-sharing within the component of the quaternion representation, yielding an additional form of regularization. This approach is, however, virtually unexplored in the graph setting. <ref type="bibr" target="#b31">[31]</ref> recently introduced a quaternion based graph neural network, where they showed promising results for node and graph prediction tasks. The characteristic of the hypercomplex product just mentioned, responsible for heavily reducing the number of parameters (for fixed model depth and width), can be generalized to yield more generic algebras. As a consequence, it is possible to train deeper models, avoiding to overfit the data while supplying more expressive embeddings. The crucial adaption in complex-and hypercomplex-valued neural networks, compared to their real-valued counterpart, lies in the reformulation of the linear transformation, i.e., of the fully-connected (FC) layer. Recent work in the realm of NLP by <ref type="bibr" target="#b47">[47]</ref> introduces the PHM-layer, an elegant way to parameterize hypercomplex multiplications (PHM) that also generalizes the product to n-dimensional hypercomplex spaces. The model benefits from a greater architectural flexibility when replacing fully-connected layers with their alternative that includes the interaction of the constituents of a hypercomplex number. Due to the pervasive application of FC layers in deep learning research, there exists rich literature of methods that aim to modify such transformation in neural networks with the goal to obtain improved parameter efficiency as well as generalization performance. Some examples include low-rank matrix factorization <ref type="bibr" target="#b35">[35]</ref>, knowledge distillation of large models into smaller models <ref type="bibr" target="#b18">[18]</ref>, or some other form of parameter sharing <ref type="bibr" target="#b36">[36]</ref>. In this work, we embark on the first extensive exploration of hypercomplex graph neural networks. We benchmark our models in graph property prediction tasks in the OGB and Benchmarking-GNNs datasets <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b9">9]</ref>. The reader might wonder whether our models' performance gain is sufficient to justify the additional formalism of (parametrized) hypercomplex algebras. The answer is that, under many aspects, our models are less involuted than several of the current best-performing graph learning algorithms. For instance, we do not employ any unusually sophisticated aggregation method or message passing function. Moreover, we did not need any extensive amount of hyperparameter engineering/tuning for reaching state-of-the-art (SOTA) performance in the benchmark datasets. This motives our conclusion that the hypercomplex representations are "easier to learn" and expressive enough to be adaptive to various types of graph data.</p><p>In summary, we make the following contributions:</p><p>-We propose Parameterized Hypercomplex Graph Neural Networks (PHC-GNNs), a class of graph representation learning models that combine the expressiveness of GNNs and hypercomplex algebras to learn improved node and graph representations. -We study the learning behavior of the hypercomplex product as a function of the algebra dimensions n. We introduce novel initialization and regularization techniques for the PHM-layer, based on our theoretical analyses, and provide empirical evidence for optimal learning at large n. -We demonstrate the effectiveness of our PHC-GNNs, reaching SOTA performance compared to other GNNs with a much lower memory footprint, making it appealing for further research in GRL to develop even more powerful GNNs with sophisticated aggregation schemes that use the idea of hypercomplex multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hypercomplex Neural Networks</head><p>In this section, we introduce a few elemental concepts and useful terminology in hypercomplex representation learning for our upcoming generalization effort on graphs. We begin by reviewing basic facts about representation learning in complex and quaternion space from literature. We then turn to describe the building blocks and the key features of our class of GNNs in the next Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Review of Complex and Quaternion NNs</head><p>Complex numbers define an algebraic extension of the real numbers by an imaginary unit i, which satisfies the algebraic relation i 2 = ?1. Since a complex number z = a + bi ? C is specified by two real components a, b ? R, complex numbers are a bi-dimensional algebra over the real numbers. The real components a, b ? R are called real and imaginary part, respectively. An extension of the same procedure of adding additional (but distinct!) imaginary units, give rise to higher dimensional algebras, known as hypercomplex number algebras. With three imaginary units we recover the most famous hypercomplex algebra, the quaternion algebra: quaternion numbers assume the form q = a + bi + cj + dk ? H with a, b, c, d ? R and they define a four-dimensional associative algebra over the real numbers. The quaternion algebra is defined by the relations i 2 = j 2 = k 2 = ijk = ?1, which determine the non-commutative Hamilton product, named after Sir Rowan Williams <ref type="bibr" target="#b15">[15]</ref>, who first discovered the quaternions in 1843. Crucial for neural network applications is the representation of the quaternions in terms of 4 ? 4 real matrices, given by</p><formula xml:id="formula_0">Q r = ? ? ? ? a ?b ?c ?d b a ?d c c d a ?b d ?c b a ? ? ? ? .<label>(1)</label></formula><p>This representation 3 , although not unique, has several advantages. First, the quaternion algebra operations correspond to the addition and multiplication of the corresponding matrices. Second, the first column of Q r encodes the real and imaginary units' coefficients, simplifying the extraction of the underlying component-based quaternion representation. Finally, (1) is directly generalised as follows to represent linear combinations in higher-dimensional quaternion space H d . Given a quaternion vector q = q a +</p><formula xml:id="formula_1">q b i + q c j + q d k ? H d , where q a , q b , q c , q d ? R d , the quaternion linear transformation associated to the quaternion-valued matrix W = W a + W b i + W c j + W d k ? H k?d is defined as W ? q = ? ? ? ? 1 i j k ? ? ? ? ? ? ? ? W a ?W b ?W c ?W d W b W a ?W d W c W c W d W a ?W b W d ?W c W b W a ? ? ? ? ? ? ? ? q a q b q c q d ? ? ? ? .<label>(2)</label></formula><p>3 The same reasoning holds for the complex case, which is recovered by setting the j, k components to zero. However, although H includes C, the quaternions are not an associative algebra over the complex numbers.</p><p>This constitute one of the main building blocks for quaternion-valued neural networks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b33">33]</ref>. The matrix version of the Hamilton product, denoted by the symbol ? in Equation <ref type="formula" target="#formula_1">(2)</ref>, encodes the interaction between the real part and three imaginary parts and introduces weight-sharing when performing the matrix-vector multiplication. A simple dimension counting shows this: the embedding vector q of (real) dimension dim R H d = dim R R 4d = 4d is assigned to a weight matrix of dimension dim R H k?d = 4kd, instead of 16kd as we would expect from the usual matrix product. Thus, the Hamilton product benefits from a parameter saving of 1/4 learnable weights compared to the real-valued matrix-vector multiplication.</p><p>Since the largest body of work in complex-and quaternion-valued neural networks merely utilizes the weight-sharing property just described, it is natural to ask whether the product (2) can be extended beyond the quaternion algebra, thereby allowing us to consider algebras of arbitrary dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameterized Hypercomplex Layer</head><p>The parameterized hypercomplex multiplication (PHM) layer introduced by <ref type="bibr" target="#b47">[47]</ref> aims to learn the multiplication rules defining the underlying algebra, i.e., the interaction between the real and imaginary components of the algebra's product. One obvious advantage of the PHM layer is lifting the restriction on the (predefined) algebra dimensionality, otherwise being limited to n = {2, 4, 8, 16, ...} as in the case of the algebra of complex, quaternion, octonion, and sedenion numbers, respectively. The PHM layer takes the same form as a standard affine transformation, that is,</p><formula xml:id="formula_2">y = PHM(x) = Ux + b .<label>(3)</label></formula><p>The key idea is to construct U as a block-matrix, as in <ref type="formula" target="#formula_1">(2)</ref>, through the sum of Kronecker products. The Kronecker product generalizes the vector outer product to matrices: for any matrix X ? R m?n and Y ? R p?q , the Kronecker product X ? Y is the block matrix</p><formula xml:id="formula_3">X ? Y = ? ? ? x 11 Y . . . x 1n Y . . . . . . . . . x m1 Y . . . x mn Y ? ? ? ? R mp?nq ,</formula><p>where x ij = (X) i,j . Now, let n be the dimension of the hypercomplex algebra, and let us suppose that k and d are both divisible by a user-defined hyperparameter m ? Z &gt;0 such that m ? n. Then, the block-matrix U in Equation <ref type="formula" target="#formula_2">(3)</ref> is given by a sum of n Kronecker products</p><formula xml:id="formula_4">U = n i=1 C i ? W i ,<label>(4)</label></formula><p>where C i ? R m?m are denoted contribution matrices and W i ? R k m ? d m are the component weight matrices. In the rest of our discussion, we make the simplifying assumption that m = n, for which (4) yields n( kd n 2 + n 2 ) = kd n + n 3 degrees of freedom. Since k and d correspond to the output-and input-size for a linear transformation, and n determines the user-defined PHM-dimension, the overall complexity of the matrix U is O( kd n ) under the mild assumption that kd n 4 . This shows that the PHM-layer enjoys a parameter saving factor of up to 1 n compared to a standard fully-connected layer <ref type="bibr" target="#b47">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hypercomplex Graph Neural Network</head><p>In this section, we introduce our novel hypercomplex graph representation learning model with its fundamental building blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initialization of Linear Independent Contributions</head><p>While the authors <ref type="bibr" target="#b47">[47]</ref> introduced the PHM layer, no further details on the initialization of the contribution matrices {C i } n i=1 have been elucitated. In the case of known hypercomplex algebras, these matrices can be chosen to be of full rank -that is, the rows/columns are linearly independent -, and whose elements belong to the set {?1, 0, 1}. Following the same logic, let? n be a diagonal matrix with alternating signs on the diagonal elements? n = diag(1, ?1, 1, ?1, . . . ) .</p><p>In our work, we initialize each contribution matrix C i as a product between the matrix? n and a power of the cyclic permutation matrix P n that right-shifts the columns of? n , that is,</p><formula xml:id="formula_6">C i =? n P i?1 n ,<label>(6)</label></formula><p>where (P n ) i,j = 0 except for j ? i = 1 and i = n, j = 1 where it has value 1. It is immediate to verify that the columns of the constructed C i 's are linearly independent, as desired. Note that the above construction is not the only one yielding contribution matrices with such properties. In fact, for n ? {2, 4} we do not implement <ref type="bibr" target="#b6">(6)</ref>, but instead we initialize the contribution matrices as in the complex and quaternion algebra.</p><p>Learning Dynamics for Larger n With the initialization scheme defined above, each C i in (6) contains n non-zero elements versus n(n ? 1) zero entries. Hence, the sparsity for each contribution matrix scales quadratically as a function of n, while the number of non-zero entries only linearly. While it is still possible for our model to adjust the parameters of the contribution matrices during training, it is conceivable that initializing too sparsely the fundamental operation of algebra, will deteriorate training. To overcome this issue, we also implement a different initialization scheme</p><formula xml:id="formula_7">C i ? U (?1, 1) ,<label>(7)</label></formula><p>by sampling the elements from the contributions matrices uniformly from U (?1, 1). We will show in Section 5 that this initialization strategy greatly benefits the training and test performance for models with larger n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tensor Representation</head><p>In our work we make heavy use of the reshaping operation to flatten/vectorize the hypercomplex embeddings. This enables us to apply operations such as the PHM-layer, batch-normalization or the computation of "real" logits. Explicitly, let H ? R b?k be a real embedding matrix, where the two axes correspond to the batch and feature dimension, respectively. In terms of hypercomplex embeddings, the second dimension has the size of k = nm, that is, where each component of the m-dimensional algebra embedding is concatenated as shown for the Hamilton product in <ref type="bibr" target="#b1">(2)</ref>. The reshape operation reverts the vectorization of the second axis, i.e., we reshape the embedding matrix as 3D tensor to H ? R b?n?m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Batch Normalization</head><p>Batch normalization <ref type="bibr" target="#b22">[22]</ref> is employed almost ubiquitously in modern deep networks to stabilize training by keeping activations of the network at zero mean and unit variance. Prior work <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b11">11]</ref> introduced complex and quaternion batch normalization, which uses a general whitening approach to obtain equal variance among the n = {2, 4} number constituents. The whitening approach, however, is computationally expensive compared to the common batch normalization, as it involves computing the inverse of a (n ? n) covariance matrix through the Cholesky decomposition, which has a complexity of O(mn 3 ) for an embedding of size m.</p><p>In our experiments, we found that applying the standard batch normalization for each algebra-component after 2D?3D reshaping is faster and achieves better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Regularization</head><p>Regularization is one of the crucial elements of deep learning to prevent the model from overfitting the training data and increase its ability to generalize well on unseen, possibly out-of-distribution test data <ref type="bibr" target="#b27">[27]</ref>. In what follows, we introduce a few concepts adapted from real-valued neural networks and we extend their applicability to our models. Let A ? R l?n?m be a reshaped 3D tensor, where A could be a hidden embedding or a weight tensor of our model. We employ further regularization techniques on the second axis, which refers to the algebra dimension n. This is motivated by the idea to decouple the interaction between algebra components when performing the multiplication.</p><p>Weight Regularization Recall that an element of a n-dimensional algebra is specified by n real components, that is, Let us recall that the l p -norm of an element w = w 1 + w 2 i 1 + ? ? ? + w n i n?1 of a n-dimensional algebra is defined as</p><formula xml:id="formula_8">l p (w) = n i=1 |w i | p 1/p .<label>(8)</label></formula><p>Now, given the set of weight matrices for a PHM-layer, i.e., {W 1 , . . . , W n }, where each W i ? R k?d , we compute the L p norm on the stacked matrix W ? R k?n?d along the second dimension resulting to:</p><formula xml:id="formula_9">L p (W) = 1 kd k a=1 d b=1 l p (W [a,? ? ?,b] ) .<label>(9)</label></formula><p>This regularization differs from the commonly known regularization of weight tensors, where the l p norm is applied to each element, such as the Frobenius-norm for p = 2:</p><formula xml:id="formula_10">||W|| F = ( a,b,c |W [a,b,c] | 2 ) 1 2 .</formula><p>Sparsity Regularization on Contribution Matrices. In our model implementation, we enable further regularization on the set of contribution matrices C = {C i } n i=1 by applying the l 1 -norm on each flattened matrix:</p><formula xml:id="formula_11">L(C) = 1 n 3 n i=1 ? ? a,b |C i,[a,b] | ? ? .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Computation of Real Logits</head><p>Given an embedding matrix H ? R b?k = R b?n?m there are several options to convert a hypercomplex number (along the second axis i = 1, . . . , n) to a real number, such that the result lies in R b?m . In our work, we utilize a fully-connected layer (FC) that maps from R b?nm to R b?m , i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-Transformer</head><formula xml:id="formula_12">(H) : R b?nm ? ? R b?m , Real-Transformer(H) = HA r + b r .<label>(11)</label></formula><p>Other possible choices of conversion are the sum or norm operations along the second axis of the 3D-tensor representation of H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Hypercomplex Graph Neural Network</head><p>Input Featurization. We implement hypercomplex input-feature initialization in GNNs by applying an encoder on the real-valued input features. For continuous features, we apply a standard linear layer to real-valued features x v ? R F to obtain the hypercomplex zero-th hidden embedding h</p><formula xml:id="formula_13">(0) v ? R k = R nm for each node v.</formula><p>According on the algebra dimension n, we then split the k-dimensional vector h (0) v into n sub-vectors, each of size m, yielding m dimensional hypercomplex features. Consequently, each hypercomplex (embedding) vector can be reshaped into size (n, m). The same procedure is applied to continuous raw edge-features e uv ? R B for every connected pair of nodes (u, v) ? E. In case of molecular property prediction datasets, raw node-and edge-features are often categorical variables, e.g., indicating atomic number, chirality and formal charges of atoms. Categorical edge-features identify instead the bond type between two connected atoms. Categorical input node-and edge-features are transformed using an learnable embedding lookup table <ref type="bibr" target="#b21">[21]</ref> that is commonly used in natural language processing. This lookup table maps word entities of a dictionary to continuous vector representations in e (0)</p><formula xml:id="formula_14">uv ? R nm .</formula><p>Message Passing We build our PHC message passing layer based on the graph isomorphism network (GIN-0) introduced by <ref type="bibr" target="#b45">[45]</ref> with the integration of edge features <ref type="bibr" target="#b21">[21]</ref>. The GIN model is a simple, yet powerful architecture that employs injective functions within each message passing layer, obtaining representational power as expressive as the Weisfeiler-Lehman (WL) test <ref type="bibr" target="#b42">[42]</ref>. Before any transformations on the embeddings are made, neighboring node representations are aggregated,</p><formula xml:id="formula_15">m (l) v = u?N (u) ? uv (h (l?1) u + e (l) uv ) ,<label>(12)</label></formula><p>where the edge-embeddings e (l) uv are obtained through the same encoding procedure as for the l = 0 representations described above. The aggregation weights ? uv can be computed using different mechanisms <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b28">28]</ref>. The GIN model, for instance, utilizes the sum-aggregator, i.e., all weights ? uv = 1. In our class of models we implement several common aggregation strategies, namely, ? uv ?{sum, mean, min, max, softmax}. Such flexibility is crucial for our models, as different aggregators learn different statistical property of a graph <ref type="bibr" target="#b45">[45]</ref>. Often, datasets differ regarding the topological properties of graphs, such as density and size, and as a consequence, optimal embeddings for a given dataset are notably sensitive to the choice of message passing aggregation strategy <ref type="bibr" target="#b7">[7]</ref>. The interpretation of Equation <ref type="formula" target="#formula_0">(12)</ref> is that the message received by node v is a variable aggregation of the sum of the neighboring node embeddings and its corresponding edge-embeddings. This message is then the key ingredient in the update strategy of the node v embedding through a Multi-Layer-Perceptron (MLP)</p><formula xml:id="formula_16">h (l) v = MLP (l) h (l?1) v + m (l) v .<label>(13)</label></formula><p>It is in this step that the PHM-layer from Equation <ref type="formula" target="#formula_2">(3)</ref> is implemented. Our model differs from complex-and quaternion-based models by the fact that the multiplication rule to construct the final weight-matrix for the linear transformation is learned through the data, see. Eq. (4). Note that the multiplication rule for the quaternion-based model is fixed as shown in Equation <ref type="formula" target="#formula_1">(2)</ref>. The iterative application of the aggregation function (12) (when ? ? sum) on hidden node embeddings updated through (13) turns out to define an injective function. Our proposed message passing layer is therefore a simple generalization of the GIN module, but uses the parameterized hypercomplex multiplication layer from Equation <ref type="formula" target="#formula_2">(3)</ref>. For the case we set the hyperparameter n = 1, our model reduces to a modified version of GIN-0, where the (block) weight-matrix for each affine transformation consists of the sum of Kronecker products from only one matrix, as shown in Equation <ref type="formula" target="#formula_4">(4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip Connections</head><p>We apply skip-connection (SC) after every message passing layer by including either the initial h (0) v or the previous layer h</p><formula xml:id="formula_17">(l?1) v embedding information of node v, h (l) v = SC(h (a) v , h (l) v ) = h (a) v + h (l) v , a = 0, l ? 1<label>(14)</label></formula><p>Graph Pooling The graph-level representation h G is obtained by soft-averaging the node embeddings from the final message passing layer, i.e.,</p><formula xml:id="formula_18">h G = v?G w v h (L) v ,<label>(15)</label></formula><p>where w v is a soft-attention weight-vector and denotes element-wise multiplication. We follow the proposal of Jumping-Knowledge GNNs <ref type="bibr" target="#b46">[46]</ref> and assign attention scores to each hidden node embedding from the last embedding layer. Let H (L) ? R |V |?k L denote the node embedding matrix, where k L = n ? m L is the size of the final message passing layer L. We compute the soft-attention weights in <ref type="bibr" target="#b15">(15)</ref> by calculating the real logits as defined in (11), followed by a sigmoidal activation function ?(?), that is,</p><formula xml:id="formula_19">W sa = ?(Real-Transformer(H (L) )) .<label>(16)</label></formula><p>The rows of the soft-attention matrix W sa ? (0, 1) |V |?m L are the (broadcasted) vectors entering in the graph pooling <ref type="bibr" target="#b15">(15)</ref>.  <ref type="bibr" target="#b15">(15)</ref> are further passed to a task-based downstream predictor, which can (but does not have to) be a Neural Network. For example, a 3-layer MLP is applied in the Benchmarking-GNNs framework <ref type="bibr" target="#b9">[9]</ref>, while the baseline models from OGB [20] deploy a simple 1-layer MLP. In our work, we implement a 2-layer MLP that processes the graph embeddings through the PHM-layer <ref type="formula" target="#formula_2">(3)</ref>, followed by an additional linear layer to compute the logits as described in <ref type="formula" target="#formula_0">(11)</ref>.</p><p>Although we define our GNN as graph classification model, the model can in fact also be utilized for node classification tasks. Such a model can be obtained, by not applying the graph pooling as described in Equation <ref type="formula" target="#formula_0">(15)</ref>, and instead use the last hidden layer nodes embedding H L to compute the real logits with (11) before applying the Softmax activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the effectiveness of parameterized hypercomplex GNNs on six datasets from two recent graph benchmark frameworks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20]</ref>. We discuss our fundings by displaying results for three datasets in the main text, and we refer to the Supplementary Information (SI) for further evidence. The two recent graph benchmark frameworks address the inadequacy of past benchmark datasets, which are rather small in size, and thus not suitable for proper model evaluation. These issues become even more relevant for real-life graph-based learning applications, where often the datasets are fairly extensive and the issue of out-of-distribution samples is key in assessing the true predictive performance of the algorithm. To demonstrate the architectural advantage and effectiveness of the hypercomplex multiplication, we evaluate the performance of our GNNs for increasing algebra dimension n. We recall that in our framework, this hyperparameter controls the amount of parameter sharing in the PHM layer <ref type="bibr" target="#b2">(3)</ref>. In all our experiments, we report the test performance evaluated on the model saved from the epoch with the best validation performance.</p><p>Increasing n for a Fixed Network Architecture <ref type="table" target="#tab_0">Table 1</ref> shows results on two molecular property prediction datasets from OGB <ref type="bibr" target="#b20">[20]</ref>, where all the models share the same fixed network architecture. Note that, due to the inherent weight-sharing mechanism, the number of parameters decreases as n increases. We observe an improved performance of our GNN when we adopt the (parameterized) hypercomplex multiplication layers. In fact, all models that were trained with PHM-layer, with the exception of the PHC-5 model, performed better than the "real" baseline PHC-1. This result supports our hypothesis that the employment of hypercomplex multiplication acts as regularizer and aids to better generalization performance on the test set.</p><p>For the medium-scale ogbg-molpcba dataset, our models include 7 message passing layers as stated in <ref type="bibr" target="#b13">(13)</ref>, each of a fixed size of 512. We refer to the SI for further details regarding architecture and hyperparameters. In deep learning, it is often observed that parameter-heavy models tend to outperform parameter-scarce models, but incur the risk of overfitting the training data, as the high number of degrees of freedom tempts the model to simply "memorize" the training data, with the consequential detrimental effect of poor generalization on unseen test data. Consequently, significant effort needs to be invested in regularizing the model, often in an ad-hoc manner. This experiment showed that HC-based models offer an elegant and universally-applicable approach to regularization that does not require any extensive hyperparameter tuning. As our baseline PHC-1 model in the ogbg-molpcba benchmark seems <ref type="table">Table 2</ref>. Results of the PHC-GNNs on the ZINC graph property prediction dataset. Our model can increase its embedding size for a fixed-length network through the inherent weight-sharing component. All shown models are constraint to a capacity budget of approximately 100K (L=4) and 400K (L=16) parameters and the performances are averaged over 4 runs <ref type="bibr" target="#b9">[9]</ref>. Models with ?-suffix are initialized with <ref type="bibr" target="#b7">(7)</ref>. to overfit the training data (see SI for learning curves), having more parameter efficient models with the same architecture led to overall better performance. To further study the relation between n and model regularization, we trained the same model-architecture but with a much smaller embedding sizes of 64. Within this setting of under-parameterized models, the GNN with n = 1 performs best on the train/val/test dataset, followed by the model with increasing PHM-dim. This shows that, in a heavily underfitting setting, merely increasing the HC algebra dimension proves to be detrimental. Additionally, we empirically observe that models that can learn the multiplication rule from the training data (PHC-2 and PHC-4) outperform the complex-and quaternion-valued models (PHC-2-C and PHC-4-Q) in the OGB benchmarks.</p><p>Increasing n for a Fixed Parameter Budget For our next experiment we examine the test performance of our models with increasing hyperparameter n, while constraining the parameter budget to approximately 100K and 400K parameters <ref type="bibr" target="#b9">[9]</ref>. We design a fixed parameter-budget experiment to explore the expressiveness of the hypercomplex embeddings independently of the regularization effect investigated above, as all models possess the same overfitting capacity. This also constitutes a realistic scenario on the production level, where a constrained and low model memory footprint is crucial <ref type="bibr" target="#b37">[37]</ref>. A feature of our proposed PHC-GNN is the ability to increase the embedding size of hidden layers for larger hyperparameter n without increasing the parameter count. <ref type="table">Table 2</ref> shows the results of experiments conducted on the ZINC dataset for a fixed-length hypercomplex GNN architecture, with L={4,16} message passing-and 2 downstream-layers. The models differ merely in the embedding sizes, which are chosen so that the total parameter count respects the fixed budget. We observe that the models making use of the PHM-layer outperform the "real"-valued baseline, that uses standard FC layers. Particularly, being able to increase the embedding size seems to strengthen the performance of PHC-models on the test dataset. Nevertheless, we discover that above a certain value for the PHM-dimension n the performance deteriorates. One possible explanation for this behaviour lies in the learning dynamics between the set of contribution and weight matrices {C i , W i } n i=1 through the sum of Kronecker products in each PHM-layer (4). With increasing n, the initialization rule in (6) returns n (increasingly) sparse contribution matrices, which seem to negatively affect the learning behaviour for the PHC-{8, 10, 16} models.</p><p>For example, in the n = 16 scenario, exactly 16 elements from each C i matrix are non-zero, in comparison to the remaining 16 ? 15 = 240 zero elements. This aggravates the learning process as the weight-sharing achieved by the i th Kronecker product in (4) is not fully exploiting the interaction between all "algebra components". Using the initialization described in <ref type="bibr" target="#b7">(7)</ref> enhanced the performance as shown in the undermost part of <ref type="table">Table 2</ref> and displayed in <ref type="figure">Figure 1</ref> across the datasplits. Moreover, we are able to further improve performance of large-n models by adopting the sparse weight-decay regularization described in <ref type="bibr" target="#b10">(10)</ref>. We refer to the SI for a more thorough discussion. Another reason for the performance decline of models with larger n, even when utilizing the different initialization scheme, is related to the complexity ratio of the PHM weight matrix U i in <ref type="bibr" target="#b4">(4)</ref>. Recall that . Models- ? that utilize the initialization strategy from <ref type="formula" target="#formula_7">(7)</ref> obtain better performance on the splits. Outliers are marked as a red points. <ref type="table">Table 3</ref>. Performance results of our model on molecular property prediction datasets against: DGN <ref type="bibr" target="#b1">[2]</ref>, PNA <ref type="bibr" target="#b7">[7]</ref>, GIN, GCN, and DeeperGCN <ref type="bibr" target="#b28">[28]</ref> and DeeperGCN/GIN-FLAG <ref type="bibr" target="#b26">[26]</ref>. The results for GIN and GCN are reported from <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b9">9]</ref>. Model performances marked with * include a virtual-node <ref type="bibr" target="#b12">[12]</ref> in their underlying method. U i consists of kd n + n 3 = k 2 n + n 3 trainable parameters, where we assume k = d, the two contributions reflecting the trainable weight and contribution matrices, respectively. Now, given a fixed parameter budget, the allocation for the contribution matrices grows on a cubic scale with n, limiting the ability to increase the embedding size k, which plays a crucial role in the feature transformation through the weight matrices. As n grows, an increasingly higher share of parameters are allocated to the contribution matrices, and for large enough n, this negatively affects the learning behaviour of our models, as in the 100K case for n = {8, 6, 10} in <ref type="table">Table 2</ref>.</p><p>Finally, we compare our models with the current best performing algorithms on the datasets analyzed above. <ref type="table">Table 3</ref> shows that our GNNs are among the top-3 models in all datasets, and it defines a new state-of-the-art on ogbg-molpcba. Particularly significant is the comparison with the GIN+FLAG model. FLAG <ref type="bibr" target="#b26">[26]</ref> is an adversarial data augmentation strategy which accomplishes a data-dependent regularization. Since, as we remarked in Section 4.6, our models can be considered as a generalization of GIN, we observe that our GNNs outperform the FLAG regularization strategy applied on the same underlying learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced a model class we named Parameterized Hypercomplex Graph Neural Networks. Our class of models extends to the graph setting the expressive power and the flexibility of (generalized) hypercomplex algebras. Our experiments showed that our models implement a powerful and flexible approach to regularization with a minimal amount of hyperparameter tuning needed. We have empirically shown that increasing the dimension of the underlying algebra leads to memory-efficient models (in terms of parameter-saving) and performance benefits, consistently outperforming the corresponding real-valued model, both for fixed architecture and fixed parameter budget. We have studied the learning behaviour for increasing algebra-dimension n, and addressed the sparsity phenomenon that manifests itself for large n, by introducing a different initialization strategy and an additional regularization scheme. Finally, we have shown that our models reach state-of-the-art performance on all graph-prediction benchmark datasets.</p><p>In this work, we have undertaken the first thorough study on the applicability of higher dimensional algebras in the realm of GNNs. Given the very promising results we have obtained with a relatively simple base architecture, it would be worthwhile to extend to the hypercomplex domain the recent progresses that have been achieved in "real" graph representation learning. For example, it would be interesting to improve the expressivity of our model by learning the aggregation function for the local message passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Availability</head><p>Source code of the proposed method is openly available on GitHub at https://github.com/bayer-science-for-a-better-life/phc-gnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>There are no conflicts to declare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Additional Model Implementation Details</head><p>In this section we report further details regarding the implementation of our class of graph neural network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dropout</head><p>Dropout <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b38">38]</ref> is a simple, yet very effective regularization technique to prevent hidden neuron units from excessively co-adapting. Randomly excluding certain units in a neural network during training leads to more robust features, which are meaningful in conjunction with several random subsets of neurons. In our work we implement two dropout strategies. First, in the spirit of the hypercomplex approach, we randomly zero-out entire hidden units with their n algebra components. Explicitly, given an embedding matrix H b?n?m , we randomly sample (during training) a Bernoulli-mask B of shape (b, 1, m) with probability (1 ? p) and multiply the mask element-wise with H using broadcasting. The dropped tensor is further multiplied element-wise by a factor of 1 1?p to maintain the expected output values when dropout is turned off at inference time. As an alternative, we also implemented the commonly used dropout by randomly sampling the dropout mask from the flattened embedding 3D tensor, i.e., H ? R b?nm . In our experiments, for fixed probability p, we did not observe a performance difference between the two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Parameter Initialization for Weight Matrices</head><p>We initialized the component weight matrices as initially described in complex-and quaternion Neural Networks <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b33">33]</ref>. Both works start with the decomposition of a variance term as</p><formula xml:id="formula_20">Var(W ) = E(|W | 2 ) ? [E(|W |)] 2 ,<label>(17)</label></formula><p>where [E(|W |)] 2 = 0 since the weight distribution is symmetric around 0. As derived in <ref type="bibr" target="#b33">[33]</ref>, W follows a Chi-distribution with n = 4 degrees of freedom in the quaternion case, and n = 2 degrees of freedom in the complex case. To adapt the initialization procedure from <ref type="bibr" target="#b13">[13]</ref>, the standard deviation in our cases is defined as ? = 2 n ? (n in + n out )</p><p>.</p><p>We follow Algorithm 1 described in <ref type="bibr" target="#b33">[33]</ref> with our defined standard deviation ? to initialize the weight matrices {W i } n i=1 of a PHM-layer. As an alternative, our implementations also include the initialization of each weight matrix W i seperately using the Glorot or He initialization scheme <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b16">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Loss Function</head><p>Given a dataset of N graphs, specified by node and edge features, with their corresponding target labels</p><formula xml:id="formula_22">D = {(X i , E i , y i )} N i=1</formula><p>, we define the loss function as</p><formula xml:id="formula_23">L total = 1 N N i=1 [L task (y i , f ? (X i , E i )) + ? 1 L 2 (? W ) + ? 2 L(? C )] .<label>(19)</label></formula><p>Here, L task is the task-dependent loss function, f ? is the output function learned by our GNN, y represent the target label and L(? C ) is defined in Equation <ref type="formula" target="#formula_0">(10)</ref> in the main text, which controls the sparsity regularization on all the contribution matrices [{C i } n i=1 ]. Finally, L(? W ) is the regularization term applied to all weight matrices [{W i } n i=1 ] of our GNN and defined in Equation <ref type="formula" target="#formula_9">(9)</ref>.  <ref type="formula" target="#formula_0">(14)</ref> in the main text). "Previous" means that the skip-connection is done with the embedding from the previous layer, i.e., (l ? 1) and "initial" refers to the embedding from hidden layer 0. Abbreviations are as follows: MP = message passing, DN = downstream network. The column MP-MLP describes whether a 2-layer MLP is used in the message passing layer, as described in Eq. (13) in the main text. If MP-MLP=False, only a 1-layer MLP is used for feature transformation. We recall that the PHM-layer is used throughout the network (both MP and DN). The tuple (?, p) deschribe the decay factor for adjusting the learning rate after p patience epochs if the validation performance has not improved. The tuple (?1, ?2) refers to the regularization coefficients for the weight and contribution matrices, respectively, as described in <ref type="bibr" target="#b19">(19)</ref>. The (maximum) number of epochs for ZINC, MNIST, CIFAR10 was set to 1000, but the training would be interrupted if the minimal learning rate of 10 ?6 was reached or if the execution time exceeded 72 hours.  <ref type="table" target="#tab_3">Table 4</ref> lists the architectures and the hyperparameter setting for all the experiments presented in the main text and in the Supplementary Information (SI). In the remainder of this section we report some additional training strategy details specific to each dataset. In all our experiments we used ReLU as an activation function and ADAM optimizer <ref type="bibr" target="#b24">[24]</ref> with decreasing learning-rate based on a plateau-scheduler or step-scheduler. In each dataset, we execute t runs, where the first run starts with the random seed 0. Subsequent runs are then executed with an increasing random seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Open Graph Benchmark (OGB)</head><p>Network Architectures ogbg-molpcba For the medium-scale ogbg-molpcba dataset, our default models include 7 message passing layers.In our experiments, we found that employing a MLP with 2 layers, as deployed in <ref type="bibr" target="#b45">[45]</ref>, results in inferior validation performance. Therefore, we only included 1 PHM-layer, but set a larger embedding size of 512. After the softattention graph-pooling , we employ a 2-layer MLP with 786 and 256 units, respectively. In each of the 5 runs, we trained for 150 epochs with an initial learning rate of 0.005 and weight-decay regularization ? 1 = 10 ?5 but no sparsity regularization on the contribution matrices, i.e., ? 2 = 0 . We used ADAM optimizer <ref type="bibr" target="#b24">[24]</ref> and multiplied the learning rate with 0.75 after 5 epochs of patience if the validation performance did not improve. Additionally, we used gradient clipping with maximum l 2 -norm of value 2.0. The ?-function, i.e., the aggregation schema, such as {min, max, mean, sum, softmax} was set to "sum". The models in <ref type="table">Table 5</ref> were trained with the same setting, but the embedding sizes for the 7 message passing layers were set to 64, and the 2 layers in the downstream network consists of 64 and 32 units, respectively. Additionally, the shallow models utilize a 2-layer MLP in each message passing layer, as opposed to the default models reported in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table">Table 5</ref>. Results of the GNN on the ogb-molpcba graph property prediction dataset for a shallow model with embedding size 64. The number of message passing and downstream layers is set to 7 and 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model # Params</head><p>Precision </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Benchmarking GNNs</head><p>In this case, we strove to meet the parameter budgets of 100K for the two Computer Vision datasets, as well as of 100/400K for the molecular property prediction dataset. We did not perform any extensive hyperparameter search, but we limited ourselves to adapt the default configurations provided by <ref type="bibr" target="#b9">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Details about Experiment Section</head><p>In this section we present further details concerning the experiments presented in Section 5 in the main text. <ref type="figure" target="#fig_3">Figure 2</ref> reports the train/validation curves for the experiment described at the beginning of Section 5, in the setting of "increasing n for a fixed network architecture". <ref type="figure" target="#fig_3">Figure 2</ref>(a) shows that in a heavily underparameterized setting increasing the algebra dimension n without correspondingly increasing the embedding size leads to a worse perfomance, as the corresponding models are even more parameter-scarce. We list the train/validation/test performance of the models in <ref type="table">Table 5</ref>. On the contrary, we observe in 2(b) that increasing the algebra dimension n prevents overfitting, which can be observed for the PHC-1 model, and yields a performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Further Experiments</head><p>In this section we present results of further experiments that, for space constraint, were not included in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 PHC is a Better Regularizer than Dropout</head><p>In this work, we provided evidence that our graph implementation of the PHM-layer acts as an effective and versatile regularizer. From this perspective, it is then natural to ask how the PHM-layer performs in comparison to other regularization techniques. Although we do not fully answer this question -which probably would require a dedicated work -we begin addressing it here. Namely, we examine whether a PHC-1 model regularized with a higher dropout value will outperform the PHC-(n &gt; 2) models. <ref type="table" target="#tab_5">Table 6</ref> and <ref type="figure" target="#fig_6">Figure 3</ref>(a) show that indeed increasing dropout does regularize model. In fact, the validation loss for the regularized PHC-1- * model reaches a lower value as compared to PHC-1 (see bottom-left plot in <ref type="figure" target="#fig_6">Figure  3(a)</ref>). However, the overall test performance does not improve, and as a consequence PHC-1- * cannot match the performance of the hypercomplex models. We increased the dropout in the message passing layers, i.e., MP-dropout=[0.4] * 14 and in the downstream layers, i.e., DN-dropout=[0.5, 0.2], following the scheme presented in <ref type="table" target="#tab_3">Table 4</ref>. (a) Learning curves of the shallow PHC models (one run, seed= 0). The embedding size for every hidden layer is set to 64. The PHC-1 model performs best in the underparamterized setting (see also <ref type="table">Table 5</ref>).     <ref type="table" target="#tab_6">Table 7</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Initialization Strategy for Large n</head><p>As mentioned in the main text, we observed that the PHC-n model performance deteriorates when n is above a certain value. This value highly depends on the dataset and the task at hand, but from our experience, the performance loss occurs already when n is at least 8. We address this phenomenon through a different initialization strategy (in Eq. <ref type="formula" target="#formula_7">(7)</ref>) for the algebra's multiplication, which aims at compensating the sparsity in the hypercomplex product introduced by the standard initialization strategy (Equation <ref type="formula" target="#formula_6">(6)</ref> in the main text). <ref type="table" target="#tab_6">Table 7</ref> shows the performance improvement of model PHC-8- ? (non-sparse initialization) compared to model PHC-8 (sparse initialization) on the ogbg-molpcba dataset. Since this dataset is  <ref type="table">Table 8</ref>. Sample sizes for the splits in each dataset used in our experiments. For more details about split strategy and graph statistics, we refer to <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b9">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Training Validation Test Domain  <ref type="table">Table 8</ref>, and we only reported performances for the random initialization strategy on the small ZINC dataset with 400K model parameters, we provide further evidence that leveraging the random initialization scheme is also beneficial for models with more than 400K (learnable) parameters and trained on larger datasets. Furthermore, we show the learning curves for the PHC-10 and PHC-16 models trained on the (smaller) ZINC dataset in <ref type="figure">Figure 4</ref>. The learning curves display better learning ability for the PHC-GNN when the contribution matrices are uniform randomly initialized, leading to denser contribution matrices, which subsequently allow more interaction between the hypercomplex components in the sum of Kronecker products. <ref type="table">Table 9</ref>. Results of the PHC-10- ? and PHC-16- ? on the ZINC graph regression (400K parameters). With increasing sparse regularization factor ?2, the performance across the datasplits improve. In total, 4 runs are executed and the mean performance measures are displayed. The entries in row 1 st to 6 th , correspond to the PHC-10- ? model. The entries afterwards refer to the PHC-16- ? model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Additional Regularization Strategy for Large n</head><p>The large-n initialization addresses the issue of sparsity in the algebra's multiplication that arises for large n. As briefly noted in the main body of the work, the resulting model might suffer from some  <ref type="figure">4</ref>. Model performance increases if the random initialization is used (PHC-n- ?) compared to the default initialization (PHC-n). The improved performance difference due to the random (dense) initialization of contribution matrices is especially noticeable in the case of n = 16, where denser contribution matrices alleviate training by allowing more weight-sharing in the final weight matrix that will be used for the affine transformations. (See also <ref type="figure" target="#fig_10">Figure 6</ref>). degree of overfitting. We address this by including a new regularization term (Equation <ref type="formula" target="#formula_0">(10)</ref> in the main text). The effect of this term is to control the sparsity of the contribution matrices defining the algebra's product, allowing us to find the optimal amount of product sparsity for a given task. <ref type="table">Table 9</ref> lists the performance on the ZINC dataset of PHC-n models for n = 10, 16, for several values of the coefficient ? 2 of the additional regularization term. For n = 10, we observe an essentially constant performance of the differently regularized models. For n = 16, instead, we note a performance improvement of the regularized models. In <ref type="figure" target="#fig_9">Figure 5</ref> we report the learning curves for these experiments.</p><p>For the PHC-16 model, we visualize the effect of the sparsity regularization coefficient ? 2 on the matrix defining the algebra's product (defined in Equation <ref type="formula" target="#formula_4">(4)</ref> in the main text). <ref type="figure" target="#fig_10">Figure 6</ref> depicts a heat map for the coefficients of the 304 ? 304 matrix defining the weight matrix U obtained after applying the sum of Kronecker products. Recall that this weight matrix U is utilized for the affine transformation in the PHM-layer. Here, we define the sparsity s of the matrix U ? R k?d as follows:</p><formula xml:id="formula_24">s(U) = 1 ? 1 kd k i=1 d j=1 U [i,j] .<label>(20)</label></formula><p>Values of s closer to 1 indicate that the weight-matrix U is more sparse. We see that when the contribution matrices of the model are sparsily initialized, only the diagonal terms are activated. That is, the model collapses to a quasi-real-valued network, in which the hypercomplex components do not mix with each other. When the contribution matrices of the model are instead uniformely initiated, the distribution of activated values is less concentrated, and more interaction between the hypercomplex components is present. Finally, turning on ? 2 causes the model to zero-out some matrix coefficients, while preserving the component-mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Computing Performance</head><p>We report in <ref type="table" target="#tab_0">Table 10</ref> the execution time in s/epoch for several PHC-n models in three datasets. We observe an increasing computational time cost when n increases. This is due to the linearly increasing number of Kronecker products necessary to be computed (n for PHC-n). Our models were implemented in PyTorch version 1.7.1 <ref type="bibr" target="#b34">[34]</ref> which does not provide a CUDA implementation of the Kronecker product. We used our customized PyTorch implementation of the Kronecker product.</p><p>For the above reason, it is also important to mention that although our method is memory-efficient and enables us to reduce model parameters by using the PHM-layer, it involves more floating point operations (FLOPS) by computing n Kronecker products, which is reflected in an increase of the execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Computer Vision Graph Datasets</head><p>For completeness, we report in <ref type="table" target="#tab_0">Table 11</ref> the performance of our PHC-n models on the computer vision graph datasets MNIST and CIFAR10. All the models satisfy a budget constraint of approximatively 100K parameters, and are trained following the guidelines of <ref type="bibr" target="#b9">[9]</ref>. Once again, we observe that hypercomplex models achieve a higher accuracy in the classification tasks than the real-valued PHC-1 model. <ref type="table" target="#tab_0">Table 12</ref>. Result of our PHC-GNN on the ogb-molppa graph property prediction dataset. We only conducted one run on this dataset and we reported the model on the validation set (PHC-6). We compare our model against other methods from the literature. The results for GIN, GIN+VN and GCN are taken from <ref type="bibr" target="#b20">[20]</ref>. The FLAG method <ref type="bibr" target="#b26">[26]</ref> is applied to GIN and DeeperGCN <ref type="bibr" target="#b28">[28]</ref>. Finally, we also trained our PHC-GNN on the ogbg-ppa dataset, which consists of undirected association neighborhoods. The nodes in each association graph represent proteins, and the prediction task is to classify each association graph into 37 taxonomic groups <ref type="bibr" target="#b20">[20]</ref>. In <ref type="table" target="#tab_0">Table 12</ref> we compare our method against models reported in the literature. Also for this dataset, our model shows a strong performance with a lower parameter budget. As the association graphs are densely connected (avg. # edges=2, 266 and avg. node degree=18.3), choosing an appropriate aggregation function ? in the message passing layer is crucial for training and generalization performance of the model. We selected the aggregation schema based on the best performing models, in this case, DeeperGCN <ref type="bibr" target="#b28">[28]</ref>, which adopted the softmax-aggregation function with learnable temperature factor ? . The softmax aggregation function can be regarded as a combination between the "max" and "mean" aggregation functions, depending on the value of ? . In the ogbg-ppa dataset, an aggregation function that tends to select the maximum value of connected neighboring nodes seems to boost the training and generalization performance. We tested the standard aggregation functions, such as "sum" and "mean", but have found that "max" and "softmax" perform better on the validation sets.</p><p>For the above reason, models like GCN or GIN, which utilize the "mean" and "sum" aggregators, cannot reach a validation accuracy of 70% even with the inclusion of a virtual node. Such aggregators seem to be non-optimal in very dense association graphs, where most likely only representative mode values of neighboring nodes are required to propagate messages to contribute to the final prediction task <ref type="bibr" target="#b45">[45]</ref>. The "max" aggregator, on the other hand, is designed to achieve exactly this type of selected node propagation. The choice of (appropriate) aggregation function ?, as illustrated in <ref type="table" target="#tab_3">Table 4</ref>, is a deciding component in every GNN and can vary for each dataset/task. In our work, we focused on the development of a GNN that utilizes feature transformations motivated by the idea of hypercomplex multiplications. For future research, it would be exciting to combine the benefits of hypercomplex feature transformations with learnable aggregation functions to develop even more powerful GNNs, suitable for a larger variety of graph datasets.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>PHC- 8 Fig. 1 .</head><label>81</label><figDesc>PHC-10 PHC-16 PHC-8-PHC-10-PHC-16Boxplot distribution from the datasplits over 4 runs of the 400K PHC-models with n = 8, 10, 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Learning curves of the reported (in the main text) PHC models (one run, seed=0). The embedding sizes are set to 512 for every hidden layer. The PHC-1 model is overfitting the training data in this overparamezerized setting, while the PHC?n models with n &gt; 1 obtain better generalization performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Learning curves for the PHC-models trained on the ogbg-molcpba dataset. In the overparameterized setting, using hypercomplex multiplication aids in better generalization and prevents from overfitting the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>PHC-1-larger-dropout (a) Learning curves for PHC-1 models trained with default settings, and one with larger dropout. The model with larger dropout is not overfitting the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Learning curves for PHC-8 models trained with default initialization of contribution matrices (PHC-8) and random initialization (PHC-8- ?). The PHC-8- ? model outperforms the default model on the validation set, as well as test dataset (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Learning curves for the PHC-1 and PHC-8 trained on the ogbg-molcpba dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>? 0.009 0.193 ? 0.010 0.165 ? 0.005 10 ?5 0.155 ? 0.004 0.187 ? 0.015 0.165 ? 0.002 10 ?4 0.152 ? 0.005 0.186 ? 0.012 0.164 ? 0.004 10 ?3 0.154 ? 0.003 0.188 ? 0.015 0.163 ? 0.002 10 ?2 0.148 ? 0.010 0.184 ? 0.009 0.163 ? 0.001 10 ?1 0.156 ? 0.008 0.193 ? 0.012 0.166 ? 0.007 0 0.160 ? 0.011 0.192 ? 0.013 0.172 ? 0.003 10 ?5 0.163 ? 0.015 0.191 ? 0.013 0.165 ? 0.005 10 ?4 0.162 ? 0.010 0.197 ? 0.012 0.172 ? 0.007 10 ?3 0.158 ? 0.004 0.197 ? 0.005 0.170 ? 0.002 10 ?2 0.151 ? 0.012 0.181 ? 0.007 0.164 ? 0.006 10 ?1 0.168 ? 0.006 0.200 ? 0.006 0.174 ? 0.007</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Learning curves of the models from the 1 st run, i.e., seed=0. ZINC performance of the PHC-16- ? model over 4 runs. Learning curves of the models from the 1 st run, i.e., seed=0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Model performance for PHC-10- ? (a-b) and PHC-16- ? (c-d) models (400K parameter budget) with varying sparsity regularization factor ?2.(a) Weight matrix in an initial layer of of the PHC-16 and PHC-16- ? networks. (b) Weight matrix in a deeper layer of the PHC-16 and PHC-16- ? networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Weight matrix U = n i=1 Ci ? Wi used in the affine transformation. The default initialization for the contribution matrices Ci in the PHC-16 model (first column) leads to a sparser weight-matrix, due to a too restrictive interactions between the matrices Wi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on the OGB graph classification datasets. The PHC-GNN can reduce the total number of model parameters and obtains improved averaged test performance over 10 (ogbg-molhiv) and 5 runs (ogbg-molpcba).</figDesc><table><row><cell>Model</cell><cell cols="4">ogbg-molhiv # Params ROC-AUC (%) ? # Params PR (%) ? ogbg-molpcba</cell></row><row><cell>PHC-1</cell><cell>313K</cell><cell>78.18 ? 0.94</cell><cell>3.15M</cell><cell>29.17 ? 0.16</cell></row><row><cell>PHC-2</cell><cell>178K</cell><cell>79.25 ? 1.07</cell><cell cols="2">1.69M 29.47 ? 0.26</cell></row><row><cell>PHC-2-C</cell><cell>178K</cell><cell>79.13 ? 0.87</cell><cell>1.69M</cell><cell>29.41 ? 0.15</cell></row><row><cell>PHC-3</cell><cell>135K</cell><cell>79.07 ? 1.16</cell><cell>1.19M</cell><cell>29.35 ? 0.28</cell></row><row><cell>PHC-4</cell><cell>111K</cell><cell>79.34 ? 1.16</cell><cell>0.99M</cell><cell>29.30 ? 0.16</cell></row><row><cell>PHC-4-Q</cell><cell>111K</cell><cell>79.04 ? 1.89</cell><cell>0.99M</cell><cell>29.21 ? 0.23</cell></row><row><cell>PHC-5</cell><cell>101K</cell><cell>78.34 ? 1.64</cell><cell>0.87M</cell><cell>29.13 ? 0.24</cell></row><row><cell cols="3">Downstream Predictor The graph-level representations</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Network architectures and hyperparameters for different models/datasets. The ? column describes the aggregation method for gathering neighboring node embeddings. The sc-type column indicates which embedding are used for the skip-connection (see Eq.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results of the PHC-1 on the ogbg-molpcba graph classification. The model with larger dropout (PHC-1- * ) is prevented from overfitting the training data but also obtains lower test performance metric. Nonetheless, the best performing model is the PHC-2 model (as reported in the main article) without additional regularization. 15M 39.09 ? 0.34 29.99 ? 0.13 29.12 ? 0.26 PHC-2 1.69M 50.08 ? 0.29 30.68 ? 0.25 29.47 ? 0.26</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-Recall (%) ?</cell></row><row><cell></cell><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell cols="4">PHC-1 112K 20.47 ? 0.18 21.56 ? 0.18 20.88 ? 0.18</cell></row><row><cell>PHC-2</cell><cell>92K</cell><cell cols="2">18.08 ? 0.32 20.26 ? 0.31 19.80 ? 0.29</cell></row><row><cell>PHC-3</cell><cell cols="3">100K 17.35 ? 0.22 19.72 ? 0.32 19.30 ? 0.20</cell></row><row><cell>PHC-4</cell><cell cols="3">109K 16.08 ? 0.15 18.74 ? 0.12 18.31 ? 0.14</cell></row><row><cell>PHC-5</cell><cell cols="3">124K 16.17 ? 0.42 18.55 ? 0.40 18.28 ? 0.34</cell></row><row><cell cols="2">Model # Params</cell><cell cols="2">Precision-Recall (%) Training Validation</cell><cell>Test</cell></row><row><cell>PHC-1</cell><cell cols="3">3.15M 56.01 ? 0.76 30.38 ? 0.28 29.17 ? 0.16</cell></row><row><cell cols="2">PHC-1- *  3.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Results of the PHC-8 on the ogbg-molpcba graph classification. Displayed are the model performances with default (PHC-8) and random initialiaztion strategy (PHC-8- ?) for the contribution matrices. 689K 39.42 ? 0.45 29.59 ? 0.13 28.73 ? 0.39</figDesc><table><row><cell>Model</cell><cell># Params</cell><cell>Precision-Recall (%) Training Validation</cell><cell>Test</cell></row><row><cell>PHC-8</cell><cell cols="3">689K 41.89 ? 0.34 28.10 ? 0.18 27.00 ? 0.14</cell></row><row><cell>PHC-8- ?</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Execution time [ seconds epoch ] from the models reported in the main article. The models for hiv,pcba were trained on a single NVIDIA Tesla V100-32GB GPU and the models for ZINC (400K) were trained on a single NVIDIA Tesla V100-16GB GPU.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell></cell><cell cols="3">ogbg-molhiv ogbg-molpcba ZINC</cell></row><row><cell>PHC-1</cell><cell>15.64</cell><cell>111.60</cell><cell>8.07</cell></row><row><cell>PHC-2</cell><cell>21.34</cell><cell>150.24</cell><cell>17.55</cell></row><row><cell>PHC-3</cell><cell>27.42</cell><cell>196.85</cell><cell>22.16</cell></row><row><cell>PHC-4</cell><cell>24.62</cell><cell>239.49</cell><cell>31.82</cell></row><row><cell>PHC-5</cell><cell>37.92</cell><cell>312.75</cell><cell>33.03</cell></row><row><cell>PHC-8</cell><cell>60.30</cell><cell>388.93</cell><cell>62.00</cell></row><row><cell>PHC-10</cell><cell>?</cell><cell>?</cell><cell>66.97</cell></row><row><cell>PHC-16</cell><cell>?</cell><cell>?</cell><cell>117.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>PHC-GNNs results on computer vision graph datasets. The performance measure is the accuracy on the test dataset. 2K 97.32 ? 0.05 111.6K 66.80 ? 0.23 PHC-4 106.8K 97.36 ? 0.06 107.3K 66.47 ? 0.46 PHC-5 104.4K 97.24 ? 0.17 104.9K 66.42 ? 0.27</figDesc><table><row><cell>Model</cell><cell cols="4">MNIST # Params Acc. (%) ? # Params Acc. (%) ? CIFAR10</cell></row><row><cell cols="5">PHC-1 101.3K 97.08 ? 0.10 101.5K 66.32 ? 0.15</cell></row><row><cell cols="2">PHC-2 99.4K</cell><cell>97.32 ? 0.08</cell><cell>99.7K</cell><cell>66.79 ? 0.10</cell></row><row><cell cols="2">PHC-3 111.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>DeeperGCN+Flag 2.34M 74.84 ? 0.52 77.52 ? 0.69 DeeperGCN 2.34M 73.13 ? 0.78 77.12 ? 0.71 GIN+VN+Flag 3.29M 67.89 ? 0.79 72.45 ? 1.14 GIN+VN 3.29M 66.78 ? 1.05 70.37 ? 1.07 GIN 1.84M 65.62 ? 1.07 68.92 ? 1.00 GCN 480K 64.97 ? 0.34 68.39 ? 0.84</figDesc><table><row><cell>Model</cell><cell># Params</cell><cell cols="2">Acc. (%) ? Validation Test</cell></row><row><cell>PHC-GNN (ours)</cell><cell>1.84M</cell><cell>71.35</cell><cell>75.61</cell></row></table><note>D.6 Experiments on Protein-Protein Association Networks</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<title level="m">Directional graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/MSP.2017.2693418</idno>
		<ptr target="http://dx.doi.org/10.1109/MSP.2017.2693418" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4868" to="4879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<title level="m">Principal neighbourhood aggregation for graph nets</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<title level="m">Convolutional networks on graphs for learning molecular fingerprints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/dbab2adc8f9d078009ee3fa810bea142-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Maida</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2018.8489651</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2018.8489651" />
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/gilmer17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Precup, D., Teh, Y.W.</editor>
		<meeting>the 34th International Conference on Machine Learning. Machine Learning Research<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/jmlr/jmlrp9.html#GlorotB10" />
	</analytic>
	<monogr>
		<title level="m">AISTATS. JMLR Proceedings</title>
		<editor>Teh, Y.W., Titterington, D.M.</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Theory of quaternions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Hamilton</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/20489494" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Irish Academy</title>
		<meeting>the Royal Irish Academy</meeting>
		<imprint>
			<date type="published" when="1844" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). p. 1026-1034. ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV). p. 1026-1034. ICCV &apos;15<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<ptr target="http://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Open graph benchmark: Datasets for machine learning on graphs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJlWWJSFDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15, JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solodovnikov</surname></persName>
		</author>
		<ptr target="https://www.springer.com/gp/book/9781461281917" />
		<title level="m">Hypercomplex Numbers</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations. ICLR &apos;17</title>
		<meeting>the 5th International Conference on Learning Representations. ICLR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<title level="m">Flag: Adversarial data augmentation for graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuka?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<title level="m">Regularization for deep learning: A taxonomy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<title level="m">Deepergcn: All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8230" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling physico-chemical admet endpoints with multitask graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ter Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clevert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecules</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<title level="m">Quaternion graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quaternion convolutional neural networks for heterogeneous image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8518" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quaternion recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByMHvs0cFQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://bit.ly/3q9mtLS" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2013.6638949</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2013.6638949" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6655" to="6659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning implicitly recurrent CNNs through parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJgYxn09Fm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<title level="m">Low-memory neural network training: A technical report</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<title level="m">Lightweight and efficient neural natural language processing with quaternion networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1T2hmZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A L A</forename><surname>Weisfeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep octonion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Senhadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2020.02.053</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0925231220302435" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">397</biblScope>
			<biblScope unit="page" from="179" to="191" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
		<ptr target="http://dx.doi.org/10.1109/TNNLS.2020.2978386" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/xu18c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Parameterization of hypercomplex multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rcQdycl0zyk" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
