<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangbin</forename><surname>Bae</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ignas Budvytis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>BAE, BUDVYTIS, CIPOLLA: IRONDEPTH 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image surface normal estimation and depth estimation are closely related problems as the former can be calculated from the latter. However, the surface normals computed from the output of depth estimation methods are significantly less accurate than the surface normals directly estimated by networks. To reduce such discrepancy, we introduce a novel framework that uses surface normal and its uncertainty to recurrently refine the predicted depth-map. The depth of each pixel can be propagated to a query pixel, using the predicted surface normal as guidance. We thus formulate depth refinement as a classification of choosing the neighboring pixel to propagate from. Then, by propagating to sub-pixel points, we upsample the refined, low-resolution output. The proposed method shows state-of-the-art performance on NYUv2 [32] and iBims-1 [18] -both in terms of depth and normal. Our refinement module can also be attached to the existing depth estimation methods to improve their accuracy. We also show that our framework, only trained for depth estimation, can also be used for depth completion. The code is available at https://github.com/baegwangbin/IronDepth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular 3D reconstruction is one of the fundamental problems in computer vision, with a wide variety of applications including autonomous driving <ref type="bibr" target="#b14">[15]</ref>, augmented reality <ref type="bibr" target="#b15">[16]</ref>, and 3D photography <ref type="bibr" target="#b18">[19]</ref>. In this paper, we focus on two popular approaches in single image 3D reconstruction -surface normal estimation and depth estimation. The two problems are closely related as surface normal can also be computed from the predicted depth-map.</p><p>For both tasks, deep learning-based methods have shown impressive performance. However, if we calculate the surface normal from the output of depth estimation methods, its accuracy is significantly worse than that of surface normal estimation methods. For NYUv2 <ref type="bibr" target="#b31">[32]</ref> dataset, the surface normal calculated from the depth-map predicted by AdaBins <ref type="bibr" target="#b2">[3]</ref> has mean angular error of 28.8 ? , which is nearly twice as large as 14.9 ? achieved by the direct While the predicted depth-maps look similar, the point cloud comparison shows that the characteristics in the scene geometry (e.g., orientation of the walls) are better preserved in the refined output.</p><p>estimation of Bae et al. <ref type="bibr" target="#b0">[1]</ref>. This suggests that, while depth estimation methods show low per-pixel depth errors, the recovered surface does not faithfully capture the characteristics of the scene geometry (e.g., the walls and floors are not flat).</p><p>Poor surface normal accuracy of depth estimation methods is mainly caused by two problems. First is the imbalance in the training data. <ref type="figure">Fig. 2-(left)</ref> shows that most of the pixels in NYUv2 <ref type="bibr" target="#b31">[32]</ref> have ground truth depth of 1-4m. If depth estimation is solved as regression, the network is biased to predict those intermediate depth values, leading to poor surface normal accuracy. Secondly, estimating a depth-map with high surface normal accuracy requires view-dependent inference. <ref type="figure">Fig. 2-(right)</ref> shows that, for perspective camera, the depth-map corresponding to a flat surface is not linear. Unlike surface normal, the depth gradient is not constant within the surface and is dependent on the viewing direction. Depth estimation is thus difficult to solve using convolutional neural networks, which are designed to be translation-equivariant.</p><p>In this paper, we propose IronDepth, a novel framework that uses surface normal and its uncertainty to recurrently refine the initial depth-map (iron: iterative refinement of depth using normal). Given the estimated depth and surface normal of a pixel, we can define a plane. Then, for a query pixel, we can calculate how its depth should be updated in order for it to belong to the same plane. We call this the normal-guided depth propagation. We then formulate depth refinement as classification of choosing the neighboring pixel to propagate from. After refining the initial depth-map in a coarse resolution, we apply the same normalguided depth propagation to sub-pixel points to upsample the refined output. <ref type="figure" target="#fig_0">Fig. 1</ref> shows how the proposed normal-guided refinement improves the quality of the 3D reconstruction.</p><p>Our method achieves state-of-the-art performance on NYUv2 <ref type="bibr" target="#b31">[32]</ref>. We also outperform other methods in cross-dataset evaluation on iBims-1 <ref type="bibr" target="#b17">[18]</ref>. While the improvement in depth accuracy is small, the surface normal calculated from our depth prediction is significantly more accurate than those obtained by the competing methods.</p><p>We also run additional experiments to further investigate the usefulness of the proposed surface normal-guided depth refinement module. Firstly, the initial depth prediction can be replaced with the output of the existing depth estimation methods to improve their accuracy. We confirmed this by applying our framework to five state-of-the-art depth estimation methods. Secondly, our framework can seamlessly be applied to depth completion. Given a sparse depth measurement, we can fix the depth for the pixels with measurement. This allows the information (i.e. sparse depth measurements) to be propagated to the neighboring pixels, improving the overall accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Single image depth estimation. The goal of the problem is to estimate the per-pixel metric depth. Owing to the advances in deep neural networks (DNNs), state-of-the-art approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> use DNNs to extract features and predict the per-pixel metric depth. While most methods solve depth estimation as regression, other methods recast the problem as classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> or ordinal regression (i.e. classification on ordered thresholds) <ref type="bibr" target="#b13">[14]</ref> by discretizing the output depth. We propose a hybrid approach where the initial depth-map is obtained via regression and then refined by solving classification of selecting the neighboring pixel to propagate from. Single image surface normal estimation. The goal of the problem is to estimate the perpixel surface normal vector, defined in the camera-centered coordinates. Similar to depth estimation, this problem is solved via direct regression using DNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. Notable contributions have been made by using a spatial rectifier to improve the performance on tilted images <ref type="bibr" target="#b7">[8]</ref>, and using vision transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> to encode the global context <ref type="bibr" target="#b36">[37]</ref>. While most methods only estimate the normal, recent work by Bae et al. <ref type="bibr" target="#b0">[1]</ref> also estimates the associated uncertainty. They also proposed to apply the training loss on a subset of pixels selected based on the estimated uncertainty, thereby improving the quality of prediction on small structures and near object boundaries. Improving depth estimation using surface normal. Many attempts have been made to exploit the relationship between depth and surface normal. Yin et al. <ref type="bibr" target="#b37">[38]</ref> proposed virtual normal loss, where triplets of pixels are sampled during training and the surface normal of the triangle is computed from the predicted depth and the ground truth. They applied L1  is estimated from h t+1 using a lightweight CNN (Pr-Net). Lastly, we apply the same principle (normal-guided depth propagation ? weighted sum) for sub-pixel points to obtain the fullresolution output. loss between the computed normals. Long et al. <ref type="bibr" target="#b24">[25]</ref> improved upon this work by adaptively combining the normals computed for different triplets. Our normal-guided depth propagation is inspired by GeoNet++ <ref type="bibr" target="#b28">[29]</ref>, which iterates between depth-to-normal and normal-to-depth modules. The difference is three-fold. Firstly, the normal-to-depth module in <ref type="bibr" target="#b28">[29]</ref> is deterministic (i.e. no learnable parameter). The propagation weight between pixel i and j is determined by their surface normal similarity, n i n j . This can fail if i and j belong to disconnected planes with similar surface normals. Instead, we learn the propagation weights in a recurrent framework. Secondly, we use surface normal uncertainty to avoid propagating from the pixels with high uncertainty. Lastly, we extend the normal-guided depth propagation to depth upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal-guided depth propagation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The proposed pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. It takes a single RGB image with known camera intrinsics as input. Firstly, we use an off-the-shelf network <ref type="bibr" target="#b0">[1]</ref> to estimate the pixel-wise surface normal and its uncertainty. Secondly, D-Net estimates an initial low-resolution depthmap, which is refined iteratively using the predicted surface normal as guidance (Sec. 3.1). Lastly, we propose normal-guided upsampling to recover the full resolution output (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Iterative depth refinement using surface normal</head><p>While the predicted surface normal cannot give us the metric depth of a pixel, it tells us how the depth should change around each pixel. Our goal is to exploit such geometric constraint to improve the initial, unconstrained depth prediction. Firstly, we estimate an initial depthmap d t=0 using a convolutional encoder-decoder (D-Net). Then, for each pixel, the depths of the neighboring pixels are propagated towards the central pixel, using the surface normal as guidance. The weighted sum of the propagated depths then gives us the updated depth-map d t+1 . To ensure computational efficiency, the refinement is performed in a coarse resolution. Initial depth prediction. The initial depth-map, d t=0 , is estimated with D-Net, a lightweight convolutional encoder-decoder with EfficientNet B5 <ref type="bibr" target="#b33">[34]</ref> backbone. The architecture is same as the one used in <ref type="bibr" target="#b2">[3]</ref>, except that we only decode until H/8 ?W /8 resolution, where H and W are the input height and width. Using the decoded feature-map as input, three sets of convolutional layers estimate (1) the initial depth-map d t=0 , (2) context feature f and (3) the initial hidden state h t=0 , all in H/8 ?W /8 resolution.</p><p>Hidden state update. The hidden state h t is updated recurrently using a Convolutional Gated Recurrent Unit <ref type="bibr" target="#b5">[6]</ref> (ConvGRU). We use the architecture of <ref type="bibr" target="#b34">[35]</ref>. The input to the ConvGRU cell is the concatenation of the context feature f and the surface normal confidence ?. Since higher value of ? means that the predicted surface normal has lower uncertainty, the network can learn to propagate from the neighboring pixel with high ?. Recurrent depth refinement. Consider a pixel i with pixel coordinates (u i , v i ). Assuming a pinhole camera, its camera-centered coordinates X c t can be given as</p><formula xml:id="formula_0">X c t (u i , v i ) = ? ? u i ?u 0 ? u v i ?v 0 ? v 1 ? ? ? d t (u i , v i ) = r(u i , v i ) ? d t (u i , v i ) , K = ? ? ? u 0 u 0 0 ? v v 0 0 0 1 ? ? ,<label>(1)</label></formula><p>where r(u i , v i ) represents a ray with unit depth, K is the camera calibration matrix, and t indexes the iteratively updated depth-map. Now, consider a local neighborhood of pixel i, which can be defined as N i = { j : |u i ? u j | ? ? and |v i ? v j | ? ? }. We use ? = 2 in all experiments (i.e. 5 ? 5 neighborhood) as it led to a good balance between accuracy and computational efficiency. If the pixel i belongs to the same plane as a neighboring pixel j with depth d t (u j , v j ) and surface normal n(u j , v j ), its depth should be</p><formula xml:id="formula_1">d prop t+1 (u i , v i , j) = n (u j , v j )r(u j , v j ) n (u j , v j )r(u i , v i ) d t (u j , v j ).<label>(2)</label></formula><p>We call this the normal-guided depth propagation. In order for pixel i to belong to the same plane as its neighboring pixel j, its depth should be updated to d</p><formula xml:id="formula_2">prop t+1 (u i , v i , j). The values of d prop t+1 (u i , v i , j)</formula><p>, computed for j ? N i , can be considered as the per-pixel candidates for the refined depth-map. The updated depth-map can thus be given as</p><formula xml:id="formula_3">d t+1 (u i , v i ) = ? j?N i w prop t+1 (u i , v i , j) ? d prop t+1 (u i , v i , j),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">w prop t+1 (u i , v i ) = {w prop t+1 (u i , v i , j)</formula><p>} is estimated from the hidden state h t+1 , using a lightweight CNN (see Appendix for the architecture). Eq. 3 shows that depth refinement can be formulated as a K-class classification, where K is the number of pixels in the neighborhood N i . Note that the neighborhood N i also includes the pixel i itself, in which case</p><formula xml:id="formula_5">d prop t+1 (u i , v i , i) = d t (u i , v i ).</formula><p>The network can thus choose not to update depth for certain pixels.</p><p>Normal-guided depth propagation (Eq. 2) is a view-dependent operation, which depends on the pixels coordinates (u i , v i ) and (u j , v j ). However, once the depth candidates d prop t+1 (u i , v i , j) are computed, choosing from them requires view-independent inference, making the problem easier for the network to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Normal-guided depth upsampling</head><p>For computational efficiency, the depth-map is refined in a coarse resolution (H/8 ? W /8).</p><p>After refinement, the depth-map should be upsampled to match the input resolution. However, linearly upsampling the depth-map does not preserve the surface normal (see <ref type="figure">Fig. 2</ref>). To this end, we introduce normal-guided upsampling. Normal-guided depth upsampling. For each pixel in high-resolution depth-map, we can propagate the depths of its 3 ? 3 neighbors in the coarse depth-map. Then, a lightweight CNN (i.e. Up-Net in <ref type="figure" target="#fig_1">Fig. 3</ref>) solves 9-class classification of choosing the coarse resolution neighbor to propagate from. The weighted sum of the propagated depth candidates gives us the upsampled depth-map d up t . We show in the experiments that using the proposed normalguided upsampling leads to better surface normal accuracy than using bilinear upsampling. Network training. The initial depth-map d 0 , estimated by D-Net, is recurrently refined and upsampled for N iter times, producing {d up t } where t ? {0, 1, ..., N iter }. The loss is computed as a weighted sum of their L1 losses,</p><formula xml:id="formula_6">L depth = N iter ? i=0 ? N iter ?i ||d gt ? d up t || 1 ,<label>(4)</label></formula><p>where 0 &lt; ? &lt; 1 puts a bigger emphasis on the final output. Following <ref type="bibr" target="#b34">[35]</ref>, we set ? = 0.8. N iter is set to 3 during training and 20 at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets. Our method is trained and tested on NYUv2 <ref type="bibr" target="#b31">[32]</ref>, which consists of RGB-D frames covering 464 indoor scenes. After training, we also evaluate the network on iBims-1 <ref type="bibr" target="#b17">[18]</ref> (contains 100 RGB-D frames) without fine-tuning to test its generalization ability. The ground truth surface normal for <ref type="bibr" target="#b17">[18]</ref> is obtained by running PCA with 7 ? 7 neighborhood. Evaluation protocol. We evaluate the refined depth-map both in terms of depth and normal. Depth accuracy is evaluated using the metrics defined in <ref type="bibr" target="#b11">[12]</ref>. We also compute surface normals from the predicted depth-map, by running per-pixel PCA with 7 ? 7 neighborhood. Then, the angular error between the computed surface normal and the ground truth is measured. Following <ref type="bibr" target="#b12">[13]</ref>, we report the mean, median and root-mean-squared error (lower is better). We also report the percentage of pixels with error less than [11.25 ? , 22.5 ? , 30 ? ] (higher is better). Implementation details. The proposed pipeline is implemented with PyTorch <ref type="bibr" target="#b26">[27]</ref>. We use the AdamW optimizer <ref type="bibr" target="#b25">[26]</ref> and schedule the learning rate using <ref type="bibr" target="#b32">[33]</ref> with lr max = 3.5?10 ?4 . We train N-Net for 5 epochs with a batch size of 16. The other components are trained for 10 epochs with a batch size of 4. The EfficientNet <ref type="bibr" target="#b33">[34]</ref> backbone of D-Net is fixed with the weights from <ref type="bibr" target="#b2">[3]</ref>.  <ref type="table">Table 1</ref>: Quantitative evaluation on NYUv2 <ref type="bibr" target="#b31">[32]</ref>. We show state-of-the-art performance both in terms of depth and normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input / GT Ours -Pred Depth Ours -Point Cloud AdaBins -Pred Depth</head><p>AdaBins -Point Cloud <ref type="figure">Figure 4</ref>: This figure provides a qualitative comparison between AdaBins <ref type="bibr" target="#b2">[3]</ref> and our method. While the predicted depth-maps look similar, the point cloud comparison shows that our method is better at capturing the orientation of the surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In Sec. 5.1, we evaluate our method on NYUv2 <ref type="bibr" target="#b31">[32]</ref> and iBims-1 <ref type="bibr" target="#b17">[18]</ref>. We make quantitative and qualitative comparison against the state-of-the-art methods and run ablation study experiments. In Sec. 5.2, we explore the usefulness of the proposed normal-guided depth propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main results</head><p>NYUv2. Tab. 1 shows that our method achieves state-of-the-art performance on NYUv2 <ref type="bibr" target="#b31">[32]</ref>. While the differences in the depth metrics are small, the surface normals computed from our depth-maps are significantly more accurate than those obtained by the other methods. For example, the mean angular error (20.8 ? ) is 22.4% smaller than the second best method (26.8 ? achieved by <ref type="bibr" target="#b37">[38]</ref>). <ref type="figure">Fig. 4</ref> provides a qualitative comparison against <ref type="bibr" target="#b2">[3]</ref>. The point cloud comparison shows that our method faithfully captures the surface layout of the scene.    <ref type="table">Table 2</ref>: Cross-dataset evaluation on iBims-1 <ref type="bibr" target="#b17">[18]</ref>. Our method shows significantly higher surface normal accuracy. We also outperform other methods in terms of planarity (quantifies how planar the prediction is for walls, table surfaces and floors).</p><p>shows how the accuracy improves during the iterative refinement. Before refinement, the accuracy is similar to that of <ref type="bibr" target="#b2">[3]</ref>. The accuracy improves quickly in the first few iterations and converges after about 10 iterations. iBims-1. Tab. 2 evaluates the generalization ability on iBims-1 <ref type="bibr" target="#b17">[18]</ref>. Similar to the results on NYUv2, we show a small improvement in depth accuracy, but a large improvement in surface normal accuracy. We also report ? plan and ? orie (defined in <ref type="bibr" target="#b17">[18]</ref>), which quantify the planarity of the pixels belonging to walls, table surfaces and floors. We achieve 47.4% reduction in ? plan and 51.6% reduction in ? orie , compared to AdaBins <ref type="bibr" target="#b2">[3]</ref>. Generalization. In <ref type="figure">Fig. 6</ref>, we further demonstrate the generalization ability of our method. As highlighted in <ref type="bibr" target="#b0">[1]</ref>, surface normal estimation networks generalize well across different datasets as they rely on low-level cues (e.g., texture gradients, shading). Since IronDepth uses the predicted surface normal to refine the initial depth-map, it can generalize well even when the domain gap is large (e.g., train on indoor scenes ? test on outdoor scenes).</p><p>Ablation study. Tab. 3 provides the results of the ablation study experiments. Iterative depth refinement with normal-guided depth propagation (Sec. 3.1) significantly improves the accuracy, both in terms of depth and normal. Compared to bilinear upsampling, the proposed normal-guided upsampling (Sec. 3.2) leads to better surface normal accuracy. Inference speed. The inference time of the full pipeline is 66.26 ms, when measured on a single 2080Ti GPU. The proposed normal-guided depth refinement only takes 0.57ms per iteration. This is because the refinement is performed in a coarse resolution (H/8 ?W /8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal</head><p>Depth ScanNet Middlebury2014 KITTI <ref type="figure">Figure 6</ref>: This figure shows predictions made by our method trained on NYUv2 <ref type="bibr" target="#b31">[32]</ref>. The network generalizes well to ScanNet <ref type="bibr" target="#b6">[7]</ref>, Middlebury <ref type="bibr" target="#b30">[31]</ref> and KITTI <ref type="bibr" target="#b14">[15]</ref>.  <ref type="table">Table 3</ref>: Ablation study experiments. We train the pipeline with and without the iterative refinement, and also try different methods of depth upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Applications</head><p>Lastly, we discuss the possible applications of the proposed framework. Application to existing depth estimation methods. The normal-guided depth refinement can be applied to the predictions made by the existing depth estimation methods. Specifically, we can replace the d 0 estimated by D-Net with the predictions made by other methods (the network is not fine-tuned for each method). Tab. 4 shows that the accuracy is improved across all metrics. Significant improvement in the surface normal accuracy suggests that our framework can be used as a post-processing tool to improve the surface normal accuracy of the existing monocular depth estimation methods. This also suggests that replacing our D-Net (i.e. lightweight convolutional encoder-decoder) with a more sophisticated architecture can further improve the accuracy. Application to depth completion. Suppose that a network is trained to estimate depth from a single RGB image. If a new piece of information (e.g., sparse depth measurement from a LiDAR sensor) is available at test time, the network should be able to adapt to that information and the prediction should be more accurate. However, such ability to adapt is not possessed by most depth estimation methods. Since we refine the depth map by propagating information between the pixels, we can seamlessly apply our method to a scenario where sparse depth measurements are available (i.e. depth completion setup). Given a sparse depth measurement, we can add anchor points by fixing the depth for the pixels with measurement. We simulate this by providing the ground truth for a small number of pixels. Tab. 5 shows how the accuracy can be improved by adding such anchor points. The information provided   <ref type="table">Table 5</ref>: We provide the ground truth for a small number of pixels and fix their values. The depths of those pixels are propagated to the neighboring pixels, improving the overall accuracy. We can also multiply the initial prediction by a factor that minimizes the error for the anchor points (before performing the refinement). Columns <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> show that applying such scale-matching leads to a bigger improvement.</p><p>for the anchor points (i.e. the measured depth) can be propagated to the neighboring pixels, making the overall prediction more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we proposed IronDepth, a novel framework that uses surface normal and its uncertainty to recurrently refine the predicted depth-map. We used normal-guided depth propagation to formulate depth refinement as classification of choosing the neighboring pixel to propagate from. Our method achieves state-of-the-art performance on NYUv2 <ref type="bibr" target="#b31">[32]</ref> and iBims-1 <ref type="bibr" target="#b17">[18]</ref>, both in terms of depth and surface normal. Point cloud comparison shows that our method is better at capturing the surface layout of the scene. The proposed framework can also be used as a post-processing tool for the existing depth estimation methods, or to propagate a sparse depth measurement to improve the overall accuracy. Acknowledgement. This research was sponsored by Toshiba Europe's Cambridge Research Laboratory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network Architecture</head><p>Tab. 6 shows the architecture of D-Net, which estimates the initial depth-map d t=0 , context feature f and the initial hidden state h t=0 . Tab. 7 shows the architecture of Pr-Net, which estimates the propagation weights w Conv2D(ks=3, C out =128, padding=1), ReLU(), Conv2D(ks=1, C out =128, padding=0), ReLU(), Conv2D(ks=1, C out =64, padding=0)</p><p>h t=0 H/8 ?W /8 ? 64 <ref type="table">Table 6</ref>: D-Net architecture. In each convolutional layer, "ks" means the kernel size and C out is the number of output channels. F N represents the feature-map of resolution H/N ?W /N. X +Y means that the two tensors are concatenated, and up(?) is bilinear upsampling.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>This figure shows how the proposed normal-guided depth refinement improves the quality of the 3D reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>This figure illustrates the proposed IronDepth pipeline. Given the input image, N-Net estimates the pixel-wise surface normal and its uncertainty. D-Net estimates the initial low-resolution depth-map d t=0 . It also produces the context features f and hidden state h t=0 , which are passed through a ConvGRU [6] cell to produce h t+1 . With n and d t , we can propagate the depth of the neighboring pixels to generate a set of depth candidates d prop t+1 . The updated depth-map d t+1 is then given as the weighted sum ? d prop t+1 w prop t+1 , where w prop t+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>This figure shows how the accuracy on NYUv2<ref type="bibr" target="#b31">[32]</ref> improves during the normalguided iterative refinement. The accuracy converges after about 10 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>H/ 8 ?W / 8 ?</head><label>88</label><figDesc>, C out = 128, padding=1), ReLU(), Conv2D(ks=1, C out = 128, padding=0), ReLU(), Conv2D(ks=1, C out = 5 ? 5, padding=0) w prop t (5 ? 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>mean median rmse 11.25 ? 22.5 ? 30 ? GeoNet [28] 0.142 0.499 0.062 0.801 0.963 0.992 41.5 35.5 50.2 11.7 30.5 42.2 DORN [14] 0.106 0.397 0.046 0.877 0.970 0.990 44.7 39.3 53.3 9.2 26.7 38.0 VNL [38] 0.100 0.368 0.043 0.895 0.980 0.996 26.8 17.0 37.9 36.3 59.4 68.6 BTS [22] 0.110 0.392 0.047 0.886 0.978 0.994 32.4 24.7 42.1 22.7 46.1 58.3</figDesc><table><row><cell cols="2">Depth error abs rel rmse log 10 ? 1 Depth accuracy ? 2 ? 3 AdaBins [3] Method 0.103 0.364 0.044 0.902 0.983 0.997 28.8 20.7 38.6 28.3 53.2 64.7 Normal error Normal accuracy</cell></row><row><cell cols="2">TransDepth [37] 0.106 0.365 0.045 0.900 0.983 0.996 30.0 22.4 39.7 25.6 50.2 62.4</cell></row><row><cell>Ours</cell><cell>0.101 0.352 0.043 0.910 0.985 0.997 20.8 11.3 31.9 49.7 70.5 77.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>10 ? 1 ? 2 ? 3 mean median rmse 11.25 ? 22.5 ? 30 ? ? plan ? orie ] 0.22 1.06 0.11 0.55 0.86 0.95 37.1 29.6 46.9 18.0 38.7 50.6 6.25 17.51 Ours 0.21 1.03 0.11 0.59 0.87 0.95 25.3 14.2 37.4 43.1 63.9 71.6 3.29 8.48</figDesc><table><row><cell cols="4">Depth error Depth accuracy Normal error rel rmse log SharpNet [30] 0.26 1.07 0.11 0.59 0.84 0.94 -Method --</cell><cell cols="3">Normal accuracy Planarity ---9.95 25.67</cell></row><row><cell>VNL [38]</cell><cell cols="6">0.24 1.07 0.11 0.55 0.85 0.94 39.8 30.4 51.0 17.9 38.6 49.4 6.49 18.72</cell></row><row><cell>BTS [22]</cell><cell cols="6">0.24 1.08 0.12 0.53 0.84 0.94 44.0 37.8 53.5 13.0 29.5 40.0 7.25 20.52</cell></row><row><cell>DAV [17]</cell><cell>0.24 1.06 0.10 0.59 0.84 0.94 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-7.21 18.45</cell></row><row><cell>AdaBins [3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>median rmse 11.25 ? 22.5 ? 30 ? DORN [14] 0.106 0.397 0.046 0.877 0.970 0.990 44.7 39.3 53.3 9.2 26.7 38.0 DORN + Ours 0.099 0.359 0.042 0.898 0.978 0.993 21.3 11.8 32.5 48.5 69.6 77.1 VNL [38] 0.100 0.368 0.043 0.895 0.980 0.996 26.8 17.0 37.9 36.3 59.4 68.6 VNL + Ours 0.097 0.353 0.042 0.902 0.983 0.996 20.5 11.0 31.7 50.6 71.0 78.2</figDesc><table><row><cell cols="2">Depth error abs rel rmse log 10 ? 1 Depth accuracy ? 2 ? 3 mean BTS [22] Normal error Method 0.110 0.392 0.047 0.886 0.978 0.994 32.4 24.7 42.1 22.7 46.1 58.3 Normal accuracy</cell></row><row><cell>BTS + Ours</cell><cell>0.104 0.368 0.044 0.899 0.981 0.995 21.0 11.5 32.1 49.4 70.2 77.6</cell></row><row><cell>AdaBins [3]</cell><cell>0.103 0.364 0.044 0.902 0.983 0.997 28.8 20.7 38.6 28.3 53.2 64.7</cell></row><row><cell>AdaBins + Ours</cell><cell>0.100 0.351 0.042 0.911 0.985 0.997 20.7 11.3 31.8 49.9 70.6 78.0</cell></row><row><cell>TransDepth [37]</cell><cell>0.106 0.365 0.045 0.900 0.983 0.996 30.0 22.4 39.7 25.6 50.2 62.4</cell></row><row><cell cols="2">TransDepth + Ours 0.103 0.352 0.043 0.906 0.984 0.997 20.6 11.1 31.7 50.3 70.9 78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Normal-guided depth refinement applied to the existing depth estimation methods. The accuracy is improved across all metrics.</figDesc><table><row><cell># Measurements</cell><cell>Depth metrics (w/o scale-match) abs rel rmse log 10 ? 1 ? 2 ? 3 abs rel rmse log 10 ? 1 Depth metrics (w/ scale-match) ? 2 ? 3</cell></row><row><cell>0</cell><cell>0.101 0.352 0.043 0.910 0.985 0.997 0.101 0.352 0.043 0.910 0.985 0.997</cell></row><row><cell>10</cell><cell>0.097 0.341 0.041 0.917 0.986 0.997 0.076 0.300 0.033 0.944 0.991 0.998</cell></row><row><cell>50</cell><cell>0.084 0.304 0.035 0.938 0.990 0.998 0.063 0.260 0.027 0.962 0.994 0.999</cell></row><row><cell>100</cell><cell>0.070 0.266 0.030 0.957 0.993 0.999 0.053 0.231 0.023 0.972 0.995 0.999</cell></row><row><cell>200</cell><cell>0.051 0.212 0.021 0.976 0.996 0.999 0.041 0.191 0.018 0.983 0.997 0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>prop t for each iteration. Tab. 8 shows the architecture of Up-Net, which estimates the upsampling weights w up t .</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell>Layer</cell><cell></cell><cell cols="2">Output Output Dimension</cell></row><row><cell>image</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>H ?W ? 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F 8</cell><cell>H/8 ?W /8 ? 64</cell></row><row><cell>image</cell><cell></cell><cell></cell><cell>EfficientNet B5</cell><cell></cell><cell>F 16</cell><cell>H/16 ?W /16 ? 176</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F 32</cell><cell>H/32 ?W /32 ? 2048</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell></cell><cell></cell></row><row><cell>F 32</cell><cell></cell><cell></cell><cell cols="2">Conv2D(ks=1, C out =2048, padding=0)</cell><cell>x 0</cell><cell>H/32 ?W /32 ? 2048</cell></row><row><cell></cell><cell>?</cell><cell cols="3">Conv2D(ks=3, C out =1024, padding=1), ?</cell><cell></cell></row><row><cell>up(x 0 ) + F 16</cell><cell>?</cell><cell></cell><cell>GroupNorm(n groups = 8),</cell><cell>? ? 2</cell><cell>x 1</cell><cell>H/16 ?W /16 ? 1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LeakyReLU()</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">?</cell><cell cols="2">Conv2D(ks=3, C out =512, padding=1), ?</cell><cell></cell></row><row><cell>up(x 1 ) + F 8</cell><cell cols="2">?</cell><cell>GroupNorm(n groups = 8),</cell><cell>? ? 2</cell><cell>x 2</cell><cell>H/8 ?W /8 ? 512</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LeakyReLU()</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction Heads</cell><cell></cell></row><row><cell></cell><cell cols="4">Conv2D(ks=3, C out =128, padding=1), ReLU(),</cell><cell></cell></row><row><cell>x 2</cell><cell cols="4">Conv2D(ks=1, C out =128, padding=0), ReLU(),</cell><cell>d t=0</cell><cell>H/8 ?W /8 ? 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Conv2D(ks=1, C out =1, padding=0)</cell><cell></cell></row><row><cell></cell><cell cols="4">Conv2D(ks=3, C out =128, padding=1), ReLU(),</cell><cell></cell></row><row><cell>x 2</cell><cell cols="4">Conv2D(ks=1, C out =128, padding=0), ReLU(),</cell><cell>f</cell><cell>H/8 ?W /8 ? 64</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Conv2D(ks=1, C out =64, padding=0)</cell><cell></cell></row><row><cell>x 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Architecture of Pr-Net. Pr-Net estimates the propagation weights w prop t .</figDesc><table><row><cell>Input</cell><cell>Layer</cell><cell>Output</cell><cell>Output Dimension</cell></row><row><cell>h t</cell><cell>Conv2D(ks=3, C out =128, padding=1), ReLU(), Conv2D(ks=1, C out =128, padding=0), ReLU(),</cell><cell>up w t</cell><cell></cell></row><row><cell></cell><cell>Conv2D(ks=1, C out =8 ? 8 ? 9, padding=0)</cell><cell></cell><cell></cell></row></table><note>H/8 ?W /8 ? (8 ? 8 ? 9)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Architecture of Up-Net. Up-Net estimates the upsampling weights w up t .</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2210.03676v1 [cs.CV] 7 Oct 2022</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating and exploiting the aleatoric uncertainty in surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangbin</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3174" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Surface normal estimation of tilted images via spatial rectifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khiem</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stergios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Roumeliotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>David F Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Framenet: Learning local canonical frames of 3d surfaces from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation of cnn-based single-image depth estimation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Korner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision Workshops</title>
		<meeting>of European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One shot 3d photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ocean</forename><surname>Suhib Alsisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="76" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spherical regression: Learning viewpoints, surface normals and 3d rotations on n-spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive surface normal constraint for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geonet++: Iterative geometric neural network with edge-aware refinement for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of residual networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07120</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transformers solve the limited receptive field for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>of IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
