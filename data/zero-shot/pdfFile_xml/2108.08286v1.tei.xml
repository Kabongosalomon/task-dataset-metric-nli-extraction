<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reparametrization of Multi-Frame Super-Resolution and Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhat</forename><surname>Goutam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danelljan</forename><surname>Martin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Fisher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reparametrization of Multi-Frame Super-Resolution and Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Burst Denoising RAW Burst SR Noisy Burst Input BPN Ours Ground Truth RAW LR Burst Input DBSR Ours Ground Truth Figure 1. We propose a deep reparametrization of the classical MAP objective (1) for multi-frame image restoration. Our general formulation minimizes a learned reconstruction error in a deep latent space. The proposed approach outperforms previous state-of-the-art methods DBSR [2] and BPN [64] on the RAW burst super-resolution (top) and burst denoising (bottom) tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by introducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the latent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also benefiting from the principled multi-frame fusion provided by the classical MAP formulation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-frame image restoration (MFIR) is a fundamental computer vision problem with a wide range of important applications, including burst photography <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b64">63]</ref> and remote sensing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b47">46]</ref>. Given multiple degraded and noisy images of a scene, MFIR aims to reconstruct a clean, sharp, and often higher-resolution output image. By effectively leveraging the information contained in different input images, MFIR approaches are able to reconstruct richer details that cannot be recovered from a single image.</p><p>As a widely embraced paradigm <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b48">47]</ref>, MFIR is addressed by first modelling the image formation process as, x i = H ? mi (y) + ? i . In this model, the original image y is affected by the scene motion ? mi , image degradation H, and noise ? i , resulting in the observed image x i . Assuming the noise ? i follows an i.i.d. Gaussian distribution, the original image y is reconstructed from the set of noisy observations {x i } N 1 by finding the maximum a posteriori (MAP) estimate,</p><formula xml:id="formula_0">y = arg min y N i=1 x i ? H (? mi (y)) 2 2 + R(y) ,<label>(1)</label></formula><p>where R(y) is the imposed prior regularization. While the MAP formulation (1) has enjoyed much popularity, there are several challenges when employing it in real-world settings. The formulation <ref type="bibr" target="#b0">(1)</ref> assumes that the degradation operator H is known, which is not often the case. Moreover, it requires manually tuning the regular-izer R(y) for good performance <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21]</ref>. Despite these shortcomings, the MAP formulation <ref type="bibr" target="#b0">(1)</ref> provides an elegant modelling of the MFIR problem, and a principled way of fusing information from multiple frames. This inspires us to formulate a deep MFIR method that leverages the compelling advantages of (1), while also benefiting from the end-to-end learning of the degradation operator H and the regularizer R.</p><p>We propose a deep reparametrization of the classical MAP objective <ref type="bibr" target="#b0">(1)</ref>. Our approach is derived as a generalisation of the image space reconstruction problem <ref type="bibr" target="#b0">(1)</ref>, by transforming the MAP objective to a deep feature space. This is achieved by first introducing an encoder network that replaces the L 2 norm in (1) with a learnable error metric, providing greater flexibility. We then reparametrize the target image y with a decoder network, allowing us to solve the optimization problem in a learned latent space. The decoder integrates strong learned image priors into the prediction, effectively removing the need of a manually designed regularizer R. Our deep reparametrization also allows us to directly learn the effects of complex degradation operator H in the deep latent space of our formulation. To further improve the robustness of our model to e.g. varying noise levels and alignment errors, we introduce a network component that estimates the certainty weights of all observations in the objective.</p><p>We validate the proposed approach through extensive experiments on two multi-frame image restoration tasks, namely RAW burst super-resolution, and burst denoising. Our approach sets a new state-of-the-art on both tasks by outperforming recent deep learning based approaches (see <ref type="figure" target="#fig_1">Fig. 1</ref>). We further perform extensive ablative experiments, carefully analysing the impact of each of our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Frame Super-Resolution: MFSR is a well-studied problem, with more than three decades of active research. Tsai and Huang <ref type="bibr" target="#b61">[60]</ref> were the first to propose a frequencydomain based solution for MFSR. Peleg et al. <ref type="bibr" target="#b48">[47]</ref> and Irani and Peleg <ref type="bibr" target="#b31">[31]</ref> proposed an iterative approach based on an image formation model. Here, an initial guess of the SR image is obtained and then refined by minimizing a reconstruction error. Several works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b52">51]</ref> extended the objective in <ref type="bibr" target="#b31">[31]</ref> with a regularization term to obtain a maximum a posteriori (MAP) estimate of the HR image. Robustness to outliers or varying noise levels were further addressed in <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b71">70]</ref>.</p><p>The aforementioned approaches assume that the image formation model, as well as the motion between input frames can be reliably estimated. Several works address this limitation by jointly estimating these unknown parameters <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b69">68]</ref>, or marginalizing over them <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b59">58]</ref>. Alternatively, a number of approaches directly predict the HR image without simulating the image formation process. Chiang and Boult <ref type="bibr" target="#b8">[9]</ref> upsample and warp the input images to a common reference, before fusing them. Farsiu et al. <ref type="bibr" target="#b18">[18]</ref> extend this approach with a robust regularization term. Takeda et al. <ref type="bibr" target="#b56">[55,</ref><ref type="bibr" target="#b57">56]</ref> proposed a kernel regression based approach for super-resolution. Wronski et al. <ref type="bibr" target="#b64">[63]</ref> used the kernel regression technique to perform joint demosaicking and super-resolution. A few deep learning based solutions have also been proposed recently for MFSR, mainly focused on remote sensing applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b47">46]</ref>. Bhat et al. <ref type="bibr" target="#b1">[2]</ref> propose a learned attention-based fusion approach for hand held burst superresolution. Haris et al. <ref type="bibr" target="#b23">[23]</ref> propose a recurrent backprojection network for video super-resolution.</p><p>Multi-Frame Denoising: In addition to the MFSR approaches discussed previously, a number of specialized multi-frame denoising approaches have also been proposed in the literature. Tico <ref type="bibr" target="#b58">[57]</ref> performs block matching both within an image, as well as across the input images to perform denoising. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b43">42]</ref> extend the popular image denoising algorithm BM3D <ref type="bibr" target="#b10">[11]</ref> to video. Buades et al. <ref type="bibr" target="#b6">[7]</ref> estimate the noise level from the aligned images, and use a combination of pixel-wise mean and BM3D to denoise. Hasinoff et al. <ref type="bibr" target="#b24">[24]</ref> used a hybrid 2D/3D Wiener filter to denoise and merge burst images for HDR and low-light photography applications. Godard et al. <ref type="bibr" target="#b20">[20]</ref> extend a single frame denoising network for multiple frames using a recurrent neural network. Mildenhall et al. <ref type="bibr" target="#b46">[45]</ref> employed a kernel prediction network (KPN) to obtain per-pixel kernels which are used to merge input images. The KPN approach was then extended by <ref type="bibr" target="#b44">[43]</ref> to predict multiple kernels, while <ref type="bibr" target="#b65">[64]</ref> introduced basis prediction networks to enable the use of larger kernels.</p><p>Deep Optimization-based image restoration: A number of deep learning based approaches <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b66">65,</ref><ref type="bibr" target="#b67">66]</ref> have posed image restoration tasks as an explicit optimization problem. The P 3 <ref type="bibr" target="#b62">[61]</ref> and RED <ref type="bibr" target="#b51">[50]</ref> approaches provide a general framework for utilizing standard denoising methods as regularizers in optimization-based image restoration methods. Zhang et al. <ref type="bibr" target="#b67">[66]</ref> used the half quadratic splitting method to plug a deep neural-network based denoiser prior into model-based optimization methods. Kokkinos et al. <ref type="bibr" target="#b36">[36]</ref> used a proximal gradient descent based framework to learn a regularizer network for burst photography applications. These prior works mainly focus on only learning the regularizer, while assuming that the data term (image formation process) is known and simple. Furthermore, the reconstruction error computation, as well as the error minimization are restricted to be in the image space. In contrast, our deep reparametrization approach allows jointly learning the imaging process as well as the priors, without restricting the image formation model to be simple or linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>In this work, we tackle the multi-frame (MF) image restoration problem. Given multiple images {x i } N i=1 , x i ? R h?w?cin of a scene, the goal is to merge information from these input images to generate a higher quality output? ? R sh?sw?cout . Here, c in and c out are the number of image channels, while s is the super-resolution factor. We consider a general scenario where the input images are either captured using a stationary or a hand held camera. The input and output images can either be in RAW or RGB format, depending on the end application.</p><p>One of the most successful paradigms to MF restoration and super-resolution in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b48">47]</ref> is to first model the image formation process,</p><formula xml:id="formula_1">x i = H ? mi (y) + ? i<label>(2)</label></formula><p>Here, y is the underlying image and ? mi is the warping operation which accounts for scene motion m i . The image degradation operator H models e.g. camera blur, and the sampling process in camera. The observation noise ? i ? p ? is assumed to follow a given distribution p ? . The degradation operator H and the scene motion m i take different forms depending on the addressed task. For example, in the super-resolution task, H acts as the downsampling kernel. Similarly, the scene motion m i can denote the parameters of an affine transformation, or represent a per-pixel optical flow in case of dynamic scenes. Note that the degradation operator H as well as the scene motion m i are unknown in general and need to be estimated. Given the imaging model <ref type="bibr" target="#b1">(2)</ref>, the original image y is generally estimated by minimizing the error between each observed image x i , and its simulated counterpartx i = H (? mi (y)), using the maximum a posteriori (MAP) estimation technique. If the observation noise follows an i.i.d. Gaussian distribution, the MAP estimate? is obtained as,</p><formula xml:id="formula_2">y = arg min y N i=1 x i ? H (? mi (y)) 2 2 + R(y) (3)</formula><p>Here, R(y) is the regularization term that integrates prior knowledge about the original image y.</p><p>The formulation (3) provides a principled way of integrating information from multiple frames, leading to its popularity. However, it requires manually tuning the degradation operator H and the regularizer R, while also lacking the flexibility to generalize to more complex noise distributions. In this work, we propose a deep reparametrization of (3) to address the aforementioned issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Reparametrization</head><p>We introduce a deep reparametrization that transforms the optimization problem (3) into a learned deep feature space (see <ref type="figure">Figure 2</ref>). In this section, we will first derive our approach based on the reconstruction loss (3), and then discuss its advantages over the original image-space formulation. Our generalized deep image reconstruction objective is derived from (3) in three steps, detailed next.</p><p>Step 1: We note that the first term in problem (3) minimizes a L 2 distance x i ?x i 2 between the observed image x i and the simulated imagex i = H (? mi (y)). Instead of limiting the objective to the squared error x i ?x i 2 2 in image space, we learn a more general distance measure d(x i ,x i ). We parametrize the metric d by an encoder network E, to obtain image embeddings E(x i ) ? Rh ?w?ce . The error d(x i ,x i ) is then computed as the L 2 distance between the embeddings of the input image x i and the simulated imag?</p><formula xml:id="formula_3">x i , as d(x i ,x i ) = E(x i ) ? E(x i ) 2 .</formula><p>Thanks to the depth and non-linearity of the encoder E, the distance measure d can represent highly flexible error metrics, more suitable for complex noise and error distributions.</p><p>Step 2: While the encoder E maps the error computation to a deep feature space, the resulting objective is still minimized in the output image space y. As a second step, we therefore reparametrize the objective (3) in terms of a latent deep representation z ? Rsh ?sw?cz of the image y. To this end, we introduce a decoder network D that maps the latent representation z to the estimated image y = D(z). Since z is a direct parametrization of the target image y, we can optimize the objective w.r.t. z and predict the final image a? y = D(?) once the optimal latent representation? is found. The resulting objective is thus expressed as,</p><formula xml:id="formula_4">L(z) = N i=1 E(x i ) ? E ? H ? ? mi ? D(z) 2 2 + R (D (z)) y = D(?) ,? = arg min z L(z) .<label>(4)</label></formula><p>Here, '?' denotes composition f ? g(?) = f (g(?)) of two functions f , g.</p><p>Next, we assume the decoder D to be equivariant w.r.t. the warping operation ? mi . That is, the decoder and warping operation commute as ? mi ? D = D ? ? mi . In fact, if ? mi is solely composed of a translation, this condition is readily ensured by the translational equivariance of the CNN decoder D. For more complex motions, the equivariance condition still holds to a good approximation if the motion m i locally resembles a translation. This is generally the case for the considered burst photography settings, where the motion between frames are small to moderate. Furthermore, as for optical flow networks that also employ feature warping <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b55">54]</ref>, our decoder D can learn to accommodate the desired warping equivariance through end-to-end training. By using the equivariance condition ? mi ?D = D?? mi  <ref type="figure">Figure 2</ref>. Left: Classical multi-frame image restoration approaches minimize a reconstruction error (3) between the observed images xi and the simulated images H (?m i (?)) to obtain the output image?. Right: In contrast, we employ an encoder E to compute the reconstruction error (6) in a learned feature space. The reconstruction error is minimized w.r.t. a latent representation z, which is then passed through the decoder D to obtain the prediction?. in (4), we obtain the objective,</p><formula xml:id="formula_5">L(z) = N i=1 E(x i ) ? E ? H ? D G ? ? mi (z) 2 2 + R (D (z)) ,<label>(5)</label></formula><p>which allows us to directly apply the warping ? mi on the latent representation z.</p><p>Step 3: As the final step, we focus on the degradation operator H. In general, H is unknown and thus needs to be estimated or learned. Although it could be directly parametrized as a separate neural network, we propose a different strategy. By directly comparing (3) and (5), we interestingly find the role of H in (3) replaced by the composition G = E ?H ?D in <ref type="bibr" target="#b4">(5)</ref>. Instead of learning the image space degradation map H, we can thus directly parametrize its resulting deep feature space operator G. Here, G can be seen as the feature space degradation operator, which is used to directly obtain the simulated image embedding G (? mi (z)). We thereby obtain the following objective,</p><formula xml:id="formula_6">L(z) = N i=1 E(x i ) ? G ? mi (z) 2 2 + Q(z) . (6)</formula><p>In (6), we have also introduced the latent space regularizer Q = R ? D, which can similarly be parametrized directly in order to avoid invoking the decoder D during the optimization process. Next, we will discuss the advantages of our deep reformulation (6) of (3), brought by each of the neural network modules E, D, and G. Encoder E: The encoder maps the input images x i to an embedding space E(x i ), where the reconstruction error is defined. It can thus learn to transform complex noise distributions p ? and other error sources, stemming from e.g. in-accurate motion estimation m i , to a feature space where it is better approximated as independent Gaussian noise. Our approach thus avoids strict assumptions imposed by the L 2 loss in image space through the flexibility of the encoder E. Decoder D: The minimization problem (3) is often solved using iterative numerical methods, such as the conjugate gradient method. The convergence rate of such methods strongly depend on the conditioning of the objective. Since we optimize (5) w.r.t. a latent representation z instead of the output image y, our decoder D serves as a preconditioner, leading to faster convergence. Furthermore, while effective image space regularizers R(y) are often complex <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21]</ref>, our latent parametrization z allows for trivial regularizers Q(z). Similar to CNN-based single-image superresolution approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b70">69]</ref>, the decoder D also learns strong image priors which are applied during the prediction step? = D(?). Thanks to the regularizing effect of our decoder, we found it sufficient to simply set Q(z) = ? z 2 2 where ? is a learnable scalar. Feature degradation G: The image degradation operator H can be complex and non-linear in general, making it hard to solve the minimization problem (3). In our deep reformulation (6) of (3), the image degradation H is replaced by its</p><formula xml:id="formula_7">feature space counterpart G = E ? H ? D.</formula><p>Here, the encoder E and decoder D are deep neural networks, capable of learning highly non-linear mappings. These can therefore learn a latent space where the degradation operation is approximately linear. That is, for a given image degradation H, we can learn appropriate G, E and D such that G ? E ?H ?D even in the case when G is constrained to be linear. Consequently, we constrain G to be a linear convolution filter, which is accommodated by the end-to-end learning of suitable E and D where such a linear relation holds.</p><p>As a result, our optimization problem (6) is convex and can be easily optimized using efficient quadratic solvers.</p><p>We model the encoder E, decoder D, and degradation G as convolutional neural networks. As detailed in Sec. 3.5, these networks are learned directly from data. But first, we propose a further generalization to our objective (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Certainty Predictor</head><p>In our formulation (6), the reconstruction error for each frame, location, and feature channel are weighted equally. This is the correct model if the errors, often seen as observation noise, are identically distributed. In practice however, images are affected by heteroscedastic noise <ref type="bibr" target="#b27">[27]</ref>, which varies spatially depending on the image intensity value. Furthermore, the reconstruction errors in <ref type="formula">(3)</ref> and <ref type="formula">(6)</ref> are affected by the quality of the motion estimation m i . In practical applications, the scene motion m i is unknown and needs to be estimated using e.g. optical flow. As a result, the estimated m i may contain significant errors for certain regions, leading to sub-optimal results. In order to model these effects, we further introduce a certainty predictor module W .</p><p>Our certainty predictor aims to determine element-wise certainty values v i ? Rh ?w?ce for each element in the residual E(x i ) ? G (? mi (z)). Intuitively, image regions with higher noise or unreliable motion estimate m i should be given lower certainty weights, effectively reducing their impact in the MAP objective <ref type="bibr" target="#b5">(6)</ref>. The certainty values v i are computed using the image embeddings {E(x j )} N j=1 , motion estimate m i , and the noise level n i (if available) as input. Our final optimization problem, including the certainty weights v i is then expressed as,</p><formula xml:id="formula_8">L(z) = N i=1 v i ? (E(x i ) ? G (? mi (z))) 2 2 + ? z 2 2 where v i = W {E(x j )} N j=1 , m i , n i .<label>(7)</label></formula><p>In relation to the MAP estimation (3), the certainty weights correspond to an estimate of the inverse standard deviation v i = 1 ?i of the encoded observations E(x i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization</head><p>To ensure practical inference and training, it is crucial that our objective <ref type="bibr" target="#b6">(7)</ref> can be minimized efficiently. Furthermore, in order to learn our network components end-to-end, the optimization solver itself needs to be differentiable. Due to the linearity of warp operator ? mi and the choice of linear feature degradation G, our objective L(z) is a linear leastsquares problem, which can be addressed with standardized techniques. In particular, we employ the steepest-descent algorithm, which can be seen as a simplification of the Conjugate Gradient <ref type="bibr" target="#b53">[52]</ref>. Both algorithms have been previously employed in classical MFIR approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">22]</ref>, and more recently in deep optimization-based few-shot learning approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b60">59]</ref>.</p><p>The steepest-descent algorithm performs an optimal line search ? j = arg min ? L(z j ? ?g j ) in the gradient g j = ?L(z j ) direction to update the iterate z j+1 = z j ? ? j g j . Since the problem is quadratic, simple closed-form expressions can be derived for both the gradient g j and step length ? j . For our model <ref type="formula" target="#formula_8">(7)</ref>, the complete algorithm is given by,</p><formula xml:id="formula_9">g j = ?2 N i=1 ? T mi G * T v 2 i ? E(x i ) ? G * ? mi (z j ) + 2?z j ? j = g j 2 2 N i=1 2 v i ? (G * ? mi (g i )) 2 2 + 2? g j 2 2 (8) z j+1 = z j ? ? j g j .</formula><p>Here, * , * T , and ? denote the convolution, transposed convolution, and element-wise product, respectively. Further, ? T mi is the transposed warp operator. A detailed derivation is provided in the supplementary material. Note that both the gradient g j and step length ? j can be implemented using standard differentiable neural network operations.</p><p>To further improve convergence speed, we learn an initializer P which predicts the initial latent encoding z 0 = P (E(x 1 )) using the embedding of the first image x 1 . Our approach then proceeds by iteratively applying K SD steepest-descent iterations <ref type="bibr" target="#b7">(8)</ref>. Due to the fast convergence provided by the steepest-descent steps, we found it sufficient to only use K SD = 3 iterations. By unrolling the iterations, our optimization module can be represented as a feedforward network A G,W,P that predicts the optimal encodin? z. Our complete inference procedure is then expressed as,</p><formula xml:id="formula_10">y = D A G,W,P E(x i ), m i N i=1<label>(9)</label></formula><p>In the next section, we will describe how all the components in our architecture can be directly learned end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>Our entire MFIR network is trained end-to-end from data in a straightforward manner, without enforcing any additional constraints on the individual components. We use a</p><formula xml:id="formula_11">training dataset D = {({x k i } N i=1 , y k )} consisting of input- target pairs. For each input {x k i } N i=1</formula><p>, we obtain the prediction? k using <ref type="bibr" target="#b8">(9)</ref>. The network parameters for each of our components E, G, W , P , and D are then learned by minimizing a prediction error (y k ,? k ) over the training dataset D using e.g. stochastic gradient descent. In this work, we use the popular L 1 loss (y,?) = y ?? 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>We describe the application of our approach to RAW burst super-resolution, and burst denoising tasks. A detailed description is provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">RAW Burst Super-resolution</head><p>Here, the method is given a set of RAW bayer images captured successively from a hand held camera. The task is to exploit these multiple shifted observations to generate a denoised, demosaicked, higher-resolution output. In this setting, the image degradation H can be seen as a composition of camera blur, decimation, sampling, and mosaicking operations. Next, we briefly describe our architecture. Encoder E: The encoder packs each 2 ? 2 block in the input RAW image along the channel dimension to obtain a 4 channel input. This is then passed through an initial conv. layer followed by a series of residual blocks <ref type="bibr" target="#b25">[25]</ref> with ReLU activations and without BatchNorm <ref type="bibr" target="#b30">[30]</ref>. A final conv. layer predicts a 256-dimensional encoding of the input image. Operator G: We use a conv. layer with strides as our feature-space degradation G. The strides corresponds to the downsampling factor of G. Note that this downsampling need not be the same as the downsampling factor s of the image degradation H. Our latent representation z can encode higher-resolution information in the channel dimension, enabling use of a smallers for efficiency. We empirically observed that it is sufficient to sets = 2 and perform the remaining upsampling by factor s/s in our decoder D. Decoder D: Our decoder consists of a series of residual blocks (same type as in E), followed by upsampling by a factor of s/s using sub-pixel convolution <ref type="bibr" target="#b54">[53]</ref>. The upsampled feature map is passed through additional residual blocks, followed by a final conv. layer to obtain?. Motion Estimation: We compute the motion m i between each input image x i and a reference image x 1 as pixel-wise optical flow in order to be robust to small object motions in the scene. Specifically, we use a PWCNet <ref type="bibr" target="#b55">[54]</ref> trained by the authors on the synthetic FlyingChairs <ref type="bibr" target="#b13">[14]</ref>, FlyingTh-ings3D <ref type="bibr" target="#b45">[44]</ref>, and MPI Sintel <ref type="bibr" target="#b7">[8]</ref> datasets. Certainty predictor W : We uses three sources of information in order to predict the certainty v i : i) The encoding E(x i ) which provides information about local image structure e.g. presence of an edge, texture etc. ii) The residual E(x i ) ? ? mi (E(x 1 )) between the encoding of i-th image x i and the reference image encoding E(x 1 ) warped to i-th image, which can indicate possible alignment failures, and iii) The sub-pixel sampling location m i mod 1 of the pixels in the i-th image. These three entities are passed through a residual network to obtain the certainty v i for image x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Burst Denoising</head><p>Given a burst of noisy images, the aim of burst denoising is to generate a clean output image. In general, burst denoising requires filtering over both the temporal and spatial dimensions. While the classical MAP formulation (3) accommodates the latter by specially designed regularizers, our approach can learn spatial filtering through two mechanisms. First, the encoder E and decoder D networks allow effective spatial aggregation. Second, our certainty predictor can predict both frame-wise and spatial (through channel-dimension encodings) aggregation weights. Following <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b65">64]</ref>, we consider a burst denoising scenario where an estimate of per-pixel noise variance n i is available. In practice, such an estimate is available from the exposure parameters reported by the camera. Next, we briefly detail our network architecture employed for this task. Encoder E: We concatenate the image x i and the noise estimate n i and pass it through a residual network to obtain the noise conditioned image encodings E(x i , n i ) Operator G: We use a conv. layer as our operator G. Decoder D: Our decoder consists of a series of residual blocks, followed by a final conv. layer which outputs?. Motion Estimation: We use a similar strategy as employed in Sec. 4.1 to estimate the motion between images. Certainty predictor W : We use a similar certainty predictor as employed in Sec. 4.1, with a minor modification. We input the noise estimate n i directly to W as to condition our minimization problem <ref type="formula" target="#formula_8">(7)</ref> on the input noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We perform comprehensive evaluation of our approach on RAW burst super-resolution and burst denoising tasks. Detailed results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">RAW Burst Super-Resolution</head><p>Here, we evaluate our approach on the RAW burst superresolution task. Our experiments are performed on the Syn-theticBurst dataset, and the BurstSR dataset, both introduced in <ref type="bibr" target="#b1">[2]</ref>. The SyntheticBurst dataset consists of synthetically generated RAW bursts, each containing 14 images. The bursts are generated by applying random translations and rotations to a sRGB image, and converting the shifted images to RAW format using an inverse camera pipeline <ref type="bibr" target="#b4">[5]</ref>. The BurstSR dataset, on the other hand, contains real-world bursts captured using a hand held smartphone camera, along with a high-resolution ground truth captured using a DSLR camera. Since the input bursts and HR ground truth are captured using different cameras, there are spatial and color mis-alignments between the two, posing additional challenges for both training and evaluation. We perform superresolution by a factor s = 4 in all our experiments.</p><p>Training details: For evaluation on the SyntheticBurst dataset, we train our model on synthetic bursts generated using sRGB images from the Zurich RAW to RGB <ref type="bibr" target="#b28">[28]</ref> training set. We use a fixed burst size N = 14 during our training. Our model is trained for 500k iterations, with a batch size of 16, using the ADAM <ref type="bibr" target="#b33">[33]</ref>  evaluation on the BurstSR val set. In order to handle the mis-alignments between the input and the ground truth in BurstSR dataset, we perform a spatial and color alignment of the network prediction to the ground truth, using the strategy employed in <ref type="bibr" target="#b1">[2]</ref>, before computing the prediction error.</p><p>Results: We compare our approach with the recently introduced DBSR <ref type="bibr" target="#b1">[2]</ref> that employs a deep network with an attention-based fusion of input images. Our approach employs the same optical flow estimation network as DBSR.</p><p>We also compare with HighResNet <ref type="bibr" target="#b11">[12]</ref>, and a CNN-based single-image baseline consisting of only our encoder and decoder modules. All models are trained using the same training settings as our approach, and evaluated using all available burst images (N = 14). The results on the Syn-theticBurst dataset containing 300 bursts, in terms of PSNR, SSIM <ref type="bibr" target="#b63">[62]</ref>, and LPIPS <ref type="bibr" target="#b68">[67]</ref> are shown in Tab. 1. All metrics are computed in linear image space. Our approach, minimizing a feature-space reconstruction error, obtains the best results, outperforming DBSR by +0.80 dB in PSNR. We also report results on the real-world BurstSR val set containing 882 bursts, using the evaluation strategy described in <ref type="bibr" target="#b1">[2]</ref> to handle the spatial and color mis-alignments. Our approach obtains promising results, outperforming DBSR by +0.28 dB in PSNR. These results demonstrate that our deep reparametrization of the classical MAP formulation generalizes to real-world degradation and noise. The computation time required to process a burst containing 14 RAW images to generate a 1896 ? 1080 RGB output is also reported in Tab. 1. A qualitative comparison is provided in <ref type="figure">Fig. 3</ref>.  <ref type="table">Table 2</ref>. Comparison of our method with prior approaches on the grayscale burst denoising set <ref type="bibr" target="#b46">[45]</ref> in terms of PSNR. Results for the first four methods are from <ref type="bibr" target="#b46">[45]</ref>, while the results for MKPN are from <ref type="bibr" target="#b65">[64]</ref>. Our approach obtains the best results, outperforming the previous state-of-the-art method BPN on all noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Burst Denoising</head><p>We evaluate our approach on the grayscale and color burst denoising datasets introduced in <ref type="bibr" target="#b46">[45]</ref> and <ref type="bibr" target="#b65">[64]</ref>, respectively. Both datasets are generated synthetically by applying random translations to a base image. The shifted images are then corrupted by adding heteroscedastic Gaussian noise <ref type="bibr" target="#b27">[27]</ref> with variance ? 2 r + ? s x. Here x is the clean pixel value, while ? r and ? s denote the read and shot noise parameters, respectively. During training, the noise parameters (log(? r ), log(? s )) are sampled uniformly in the log-domain from the range log(? r ) ?  <ref type="table">Table 3</ref>. Comparison with previous methods on the color burst denoising set <ref type="bibr" target="#b65">[64]</ref> in terms of PSNR. The results for KPN are from <ref type="bibr" target="#b65">[64]</ref>. Our approach outperforms BPN on all four noise levels.</p><p>Training details: Following <ref type="bibr" target="#b46">[45]</ref>, we use the images from the Open Images <ref type="bibr" target="#b38">[37]</ref> training set to generate synthetic bursts. We train on bursts containing N = 8 images with resolution 128 ? 128. Our networks are trained using the ADAM <ref type="bibr" target="#b33">[33]</ref> optimizer for 150k and 300k iterations for the grayscale and color denoising tasks, respectively. The entire training takes less than 40h on a single Nvidia V100 GPU. Results: We compare our approach with the recent kernel prediction based approaches KPN <ref type="bibr" target="#b46">[45]</ref>, MKPN <ref type="bibr" target="#b44">[43]</ref>, and BPN <ref type="bibr" target="#b65">[64]</ref>. Since our motion estimation network (PWCNet) is trained on external synthetic data, we include a variant of our approach, denoted as Ours ? , using a custom optical flow network. Our flow network is jointly trained with the rest of the architecture using a photometric loss, without any extra supervision or data. We also include results for the popular denoising algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">42]</ref> based on non-local filtering, the multi-frame HDR+ method <ref type="bibr" target="#b24">[24]</ref>, as well as a single image baseline consisting of only our encoder and decoder. The results over the 73 bursts from the grayscale burst denoising dataset <ref type="bibr" target="#b46">[45]</ref>, are shown in Tab. 2. Our approach sets a new state-of-the-art, outperforming the previous best method BPN <ref type="bibr" target="#b65">[64]</ref> on all four noise levels. Ours ? employing a custom flow network also obtains promising results, outperforming BPN on three out of four noise levels.</p><p>We also evaluate our approach on the recently introduced color burst denoising dataset <ref type="bibr" target="#b65">[64]</ref> containing 100 bursts. The results, along with the computation time for processing a 1024 ? 768 resolution burst, are shown in Tab. 3. Further qualitative comparison is provided in <ref type="figure">Fig. 3</ref>. As in the grayscale set, our approach obtains the best results, significantly outperforming the previous best method BPN. Ours ? employing a custom flow network also outperforms BPN by over 1.5 dB in average PSNR, while operating at a significantly higher speed. Furthermore, note that unlike BPN and KPN that are restricted to operate on fixed-size bursts, our approach can operate with bursts of any size, providing additional flexibility for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>Here, we analyse the impact of key components in our formulation. The experiments are performed on the Syn-theticBurst super-resolution dataset <ref type="bibr" target="#b1">[2]</ref> and the grayscale burst denoising dataset <ref type="bibr" target="#b46">[45]</ref>. We train different variants of our approach, with and without the encoder E, decoder D, and the certainty predictor W . This is achieved by replac-  <ref type="formula">(3)</ref>) leads to poor results on both super-resolution and denoising tasks (a). Note that unlike in the classical MAP based approaches, the degradation operator H is still learned in this case. The performance of the image space formulation is improved by employing our certainty predictor (b). The improvement is more prominent in the burst denoising task, where the certainty values allow handling varying noise levels. Our variants employing only the encoder (c), or the decoder (d) module obtain better performance, thanks to the increased modelling capability provided by the use of deep networks. The certainty predictor W provides additional improvements, even when employed together with the encoder (f) or the decoder (g). Removing either of the three components E, D, or W (g)-(e) from our final version leads to a decrease in performance, demonstrating that each of these components are crucial. The performance decrease is much larger in the super-resolution task due to the more complex image degradation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a deep reparametrization of the classical MAP formulation for multi-frame image restoration. Our approach minimizes the MAP objective in a learned deep feature-space, w.r.t. a latent representation of the output image. Crucially, our deep reparametrization allows learning complex image formation processes directly in latent space, while also integrating learned image priors into the prediction. We further introduce a certainty predictor module to provide robustness to e.g. alignment errors. Our approach obtains state-of-the-art results on RAW burst superresolution as well as burst denoising tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We provide additional details and analysis of our approach in this supplementary material. Section A provides a derivation of the closed-form expressions for the steepest-descent steps <ref type="bibr" target="#b7">(8)</ref>. The linearity of the warping operator is discussed in Section B. Our entire inference pipeline is detailed in Section C. The network architectures employed for the RAW burst super-resolution and burst denoising tasks are described in detail in Section D. An analysis of our certainty predictor W is provided in Section E. Section F contains an additional ablative analysis of our approach. Further qualitative comparison on the burst super-resolution and denoising datasets are provided in Section G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of Steepest-Descent Steps</head><p>In this section, we derive the closed-form expressions for the gradient g j = ?L(z j ) of loss <ref type="bibr" target="#b6">(7)</ref>, as well as the steepestdescent step lengths ? j . Our optimization objective <ref type="formula" target="#formula_8">(7)</ref> is here restated as,</p><formula xml:id="formula_12">L(z) = N i=1 r i 2 2 + ? z 2 2 (10) where r i = v i ? (E(x i ) ? G * ? mi (z)) .</formula><p>In the following derivation, we will interchangeably treat the entities in (10) either as 3D feature maps or as corresponding vectors. By using the chain rule, the gradient of (10) is computed as,</p><formula xml:id="formula_13">g = ?L(z) = N i=1 2 ?r i ?z T r i + 2?z (11) = N i=1 ?2 ?G * ? mi (z) ?z T (v i ? r i ) + 2?z (12) = N i=1 ?2 ?G * ? mi (z) ?? mi (z) ?? mi (z) ?z T (v i ? r i ) + 2?z (13) = N i=1 ?2 ?? mi (z) ?z T ?G * ? mi (z) ?? mi (z) T (v i ? r i ) + 2?z (14) = N i=1 ?2? T mi G * T (v i ? r i ) + 2?z (15) = ?2 N i=1 ? T mi G * T v 2 i ? (E(x i ) ? G * ? mi (z)) + 2?z<label>(16)</label></formula><p>Here, G * T denotes the transpose of the convolution operator u ? G * u, which is the same as the transpose of the Jacobian ?G * u ?u . Similarly, ? T mi denotes the transpose of the linear warp operator z ? ? mi (z), which corresponds to the transpose of the Jacobian</p><formula xml:id="formula_14">??m i (z) ?z .</formula><p>Our step length ? j is computed by performing an optimal line search ? j = arg min ? L(z j ??g j ) in the gradient direction g j . Since our loss (10) is convex, it has a unique global minima, which is be obtained by solving for the stationary point dL(z j ??g j ) d? = 0. By setting u = z j ? ?g j and applying chain rule, we get</p><formula xml:id="formula_15">0 = dL(u) d? = du d? T ?L(u) (17) = [?g j ] T ? 2 N i=1 ? T mi G * T v 2 i ? (E(x i ) ? G * ? mi (u)) + 2?u (18) = 2[g j ] T N i=1 ? T mi G * T v 2 i ? E(x i ) ? G * ? mi z j ? ?g j + 2?[?g j ] T (z j ? ?g j )<label>(19)</label></formula><formula xml:id="formula_16">= [?g j ] T g j + 2 N i=1 ?[g j ] T ? T mi G * T v 2 i ? G * ? mi g j + 2??[g j ] T g j (20) = ? g j 2 + 2? N i=1 v i ? G * ? mi g j 2 + 2?? g j 2<label>(21)</label></formula><p>In equation <ref type="formula" target="#formula_1">(20)</ref>, we have utilized the closed-form expression <ref type="bibr" target="#b16">(16)</ref> for g j , while also exploiting the linearity of the warp operator ?. Using <ref type="formula" target="#formula_0">(21)</ref>, the step length ? is obtained as,</p><formula xml:id="formula_17">? j = g j 2 N i=1 2 v i ? G * ? mi (g j ) 2 + 2? g j 2<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Linearity of the Warping Operator</head><p>In our work, we assume that the warping operation ? m (x) is linear. Note that this assumption holds even in the most general case, i.e. where the scene motion is given by a pixel-wise optical flow, represented as a pixel-to-pixel mapping m : </p><formula xml:id="formula_18">R 2 ? R 2 . Let x : R 2 ? R d be</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inference Pipeline</head><p>Here, we detail the inference pipeline used by our multi-frame image restoration approach. Our approach minimizes the feature space reconstruction loss <ref type="bibr" target="#b6">(7)</ref> in the main paper to fuse information from the input images. The entire pipeline is outlined in Algorithm 1. Given the set of input images {x i } N i=1 , we first pass each image x i through the encoder network e i = E(x i ) to obtain deep image embeddings {e i } N i=1 . For each image, we also compute the scene motion m i w.r.t. to first image x 1 , and the certainty values v i used to weigh our feature space reconstruction loss (Sec. 3.3). Next, we estimate the optimal latent encoding? of the output image y which minimizes our reconstruction loss L(z). This is achieved by using the iterative steepest-descent algorithm (Sec. 3.4). First, we obtain an initial latent encoding z 0 using an initializer network P . The initial encoding is then refined by applying K SD steepest-descent steps (Equation 8) to obtain z KSD . The latent encoding z KSD is finally passed through the decoder network? = D(K SD ) to obtain the prediction?. Note that each step in our inference pipeline is differentiable w.r.t. the parameters of the encoder E, certainty predictor W , initializer P , feature degradation G, and decoder D networks. This allows us to learn each of these modules end-to-end from data, as described in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. RAW Burst Super-Resolution</head><p>Here, we provide more details about the network architecture employed for the burst super-resolution task in Section 5.1 and 5.3. Encoder E: The encoder packs each 2 ? 2 block in the input RAW image along the channel dimension to obtain a 4 channel input. This ensures translation invariance, while also reducing the memory usage. The packed input is passed 6: z 0 ? P (e 1 )) # Obtain initial latent encoding 7: for j = 0, . . . , K SD ? 1 do # For every steepest-descent iteration 8:</p><formula xml:id="formula_19">g j ? ?2 N i=1 ? T mi G * T v 2 i ? e i ? G * ? mi (z j ) + 2?z j</formula><p># Obtain gradient of loss <ref type="bibr" target="#b6">(7)</ref> in main paper w.r.t. z 9:</p><formula xml:id="formula_20">? j ? g j 2 2 N i=1 2 vi?(G * ?m i (g i )) 2 2 +2? g j 2 2</formula><p># Calculate optimal step-length along gradient direction g j 10:</p><p>z j+1 ? z j ? ? j g j # Update latent encoding using the estimated step-length ? j 11:? ? D(z KSD ) # Decode latent encoding to obtain the output image through a convolution+ReLU block to obtain a 64 dimensional feature map. This feature map is processed by 9 Residual blocks <ref type="bibr" target="#b25">[25]</ref>, before being passed through a final convolution+ReLU block to obtain a 256 dimensional embedding E(x i ) of the input x i . Operator G: We use a convolution layer with stride 2 as our feature degradation operator G. The operator takes a 64 dimensional embedding as input to generate a 256 dimensional output, which is compared with the embedding E(x i ) of the input image to compute the feature space reconstruction error. Initializer P : We use the sub-pixel convolution layer <ref type="bibr" target="#b54">[53]</ref> to generate the initial latent encoding z 0 of the output image?. The initializer takes the embedding E(x 1 ) of the first burst image x 1 as input and upsamples it by a factor of 2 via sub-pixel convolution to output z 0 . Certainty Predictor W : The certainty predictor takes the embedding of the input images {E(x j )} N j=1 , along with the motion estimation m i as input. Each image embedding E(x i ) is passed through a convolution+ReLU block to obtain the 64dimensional feature map e i . The features e 1 extracted from the reference image x 1 are then warped to i-th image to compute the residual e i ? ? mi (e 1 ), which indicates possible alignment errors. Additionally, we pass the modulo 1 of scene motion m i mod 1 through a convolution+ReLU block followed by a residual block to obtain the 64-dimensional motion featuresm i . The encoding e i , the residual e i ? ? mi (e 1 ), and the motion featurem i are then concatenated along the channel dimension and projected to 128 dimensional feature map using a convolution+ReLU block. The resulting feature map is passed through 3 residual blocks, followed by a final convolution layer to obtain the output? i . Certainty value v i for the i-th image is then obtained as the absolute value |? i | to ensure positive certainties. Decoder D: The decoder passes the encoding? of the output image? through a convolution+ReLU block to obtain a 64dimensional feature map. This feature map is passed through 5 residual blocks, followed by an upsampling by a factor of s/s = 4 using the sub-pixel convolution layer <ref type="bibr" target="#b54">[53]</ref>. The upsampled 32-dimensional feature map is then passed through 5 additional residual blocks, followed by a final convolution layer which predicts the output RGB image?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Burst Denoising</head><p>Here, we provide more details about the network architecture employed for the burst denoising task in Section 5.2 and 5.3 in the main paper. Encoder E: The encoder concatenates the input image x i and the per-pixel noise estimate n i along the channel dimension, and passes it through a convolution+ReLU block. The resulting 32-dimensional output is processed by 4 residual blocks, followed by a convolution+ReLU block to obtain a 64-dimensional encoding E(x i , n i ) of the input image x i . Operator G: We use a convolution layer as our feature degradation operator G. The operator takes a 16 dimensional encoding z of the output image y as input to generate a 64 dimensional output, which is compared with the embedding E(x i , n i ) of the input image to compute the feature space reconstruction error. Initializer P : We pass the embedding E(x 1 ) of the first burst image x 1 through a convolution layer to obtain the initial output image encoding z 0 . Certainty Predictor W : The certainty predictor takes the embedding of the input images {E(x j , n j )} N j=1 , the noise estimate n i , along with the motion estimation m i as input. Each image embedding E(x i , n i ) is passed through a convolution+ReLU block to obtain the 16-dimensional feature map e i . The features e 1 extracted from the reference image x 1 are then warped to i-th image to compute the residual e i ? ? mi (e 1 ), which indicates possible alignment errors. In parallel, the per-pixel noise estimate n i is passed through a convolution+ReLU block, followed by a residual block to obtain 32-dimensional noise features? i . Additionally, we pass the modulo 1 of scene motion m i mod 1 through a convolution+ReLU block to obtain the 8-dimensional motion featuresm i . The encoding e i , the residual e i ? ? mi (e 1 ), the noise features? i , and the motion featuresm i are then concatenated along the channel dimension and projected to 32 dimensional feature map using a convolution+ReLU block. The resulting feature map is passed through 1 residual blocks, followed by a final convolution layer to obtain the output? i . Certainty value v i for the i-th image is then obtained as the absolute value |? i | to ensure positive certainties. Decoder D: The decoder passes the encoding? of the output image? through a convolution+ReLU block to obtain a 64dimensional feature map. This feature map is passed through 9 residual blocks, followed by a final convolution layer which predicts the denoised image?. Motion Estimation in Ours ? : In Section 5.2, we report results for a variant of our approach Ours ? which employs a custom optical flow network to find the relative motion between image x i and the reference image x 1 . We use a pyramidal approach with cost volume, commonly employed in state-of-the-art optical flow networks <ref type="bibr" target="#b55">[54,</ref><ref type="bibr" target="#b29">29]</ref>. The architecture of our optical flow network is described here. We first pass both x i and x 1 through a convolution+ReLU block to obtain 32-dimensional feature maps. These are then passed through 6 residual blocks. Before each of the first two residual blocks, we downsample the input feature maps by a factor of 2 using a convolution+ReLU block with stride 2 for computational efficiency. Next, we construct a feature pyramid with 2 scales, which is used to compute the optical flow. We pass the output of the last residual block through a convolution+ReLU block to obtain 64-dimensional feature maps f 1 i and f 1 1 . These feature maps are then passed through another convolution+ReLU block with stride 2 to obtain lower resolution feature maps f 2 i and f 2 1 . Next, we construct a partial cost volume containing pairwise matching scores between pixels in f 2 i and f 2 1 using the correlation layer. For efficiency, we only compute matching scores of a pixel in f 2 i with spatially nearby pixels in f 2 1 within a 7 ? 7 window. The cost volume is concatenated with the feature map f 2 i and passed through two convolution+ReLU blocks with 128 and 64 output dimensions. The output feature map is passed through a final convolution layer to obtain a coarse optical flow m 2 i . This initial estimate is upsampled by a factor of 2 and used to warp the feature map f 1 1 to the i-th image. We then compute the matching scores between f 1 i and ? m 2 i (f 1 1 ) using a 5 ? 5 spatial window. A refined optical flow m 1 i is then obtained using the same architecture as employed for pyramid level 2, without weight-sharing. The estimate m 1 i is then upsampled by a factor of 4 to obtain the final motion estimate m i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analysis of Certainty Predictor</head><p>In this section, we analyse the behaviour of our certainty predictor W . The certainty predictor computes the certainty values v i for each element in our residual E(x i ) ? G(? mi (z)). This allows us to reduce the impact of e.g. errors in motion estimate m i , by assigning a lower weight for such regions in our MAP objective <ref type="bibr" target="#b6">(7)</ref>. We analyse the behaviour of W by manually corrupting the motion estimate m i . <ref type="figure">Figure 4b</ref> shows the channel-wise mean of the predicted certainty values v i for an input image <ref type="figure">(Figure 4a</ref>), using the estimated scene motion m i . We observe that the mean certainty values are approximately uniform over the image, with some slight variations according to the image intensity values. Next, we corrupt the motion estimate m i for the left half of the image by adding a fixed offset of 16 pixels in both directions. The certainty values predicted using these corrupted motion m i is shown in <ref type="figure">Figure 4c</ref>. As desired, our certainty predictor detects the alignment errors and assign a lower certainty values to the corresponding image regions (left half of the image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Detailed Ablative Study</head><p>In this section, we provide a detailed ablative study analysing the impact different components in our architecture. Our analysis is performed on the SyntheticBurst super-resolution dataset <ref type="bibr" target="#b1">[2]</ref>, as well as the grayscale burst denoising dataset <ref type="bibr" target="#b46">[45]</ref>. Impact of number of iterations K SD : We analyse the impact of the number of steepest-descent iterations K SD by training and evaluating our approach with different values of K SD . The results on the SyntheticBurst super-resolution dataset <ref type="bibr" target="#b1">[2]</ref> are provided in <ref type="table">Table 5a</ref>, while the results on the grayscale burst denoising dataset <ref type="bibr" target="#b46">[45]</ref> is shown in <ref type="table">Table 5b</ref>. Additionally, a convergence analyses of the steepest-descent steps is provided in <ref type="figure">Figure 5</ref>. The entries with K SD = 0 directly output the initial encoding z 0 predicted by our initializer P to the decoder. Since the initializer only utilizes the first burst image, the entry K SD = 0 corresponds to the SingleImage baseline included in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. Performing just a single steepest-descent step already provides a large improvement over the SingleImage baseline with a PSNR of 39.60 on the SyntheticBurst dataset. This demonstrates the fast convergence of the steepest-descent iterations. Both the super-resolution  <ref type="figure">Figure 4</ref>. Analysis of our certainty predictor W . The channel-wise mean of the certainty values predicted by W for the input image (a), using the estimated scene motion mi is shown in (b). Next, we corrupt the motion estimate mi for the left half of the image by adding a fixed offset of 16 pixels in both directions. The mean certainty values predicted using the corrupted motion estimate mi is shown in (c). Our certainty predictor can detect the errors in motion estimate mi and assign lower certainty values to the corresponding image regions.  <ref type="table">Table 5</ref>. Impact of initializer P and number of steepest-descent iterations KSD on the SyntheticBurst super-resolution (a) and grayscale denoising (b) datasets. <ref type="figure">Figure 5</ref>. Convergence analysis of the steepest-descent iterations. We plot our loss <ref type="bibr" target="#b6">(7)</ref>, w.r.t. the number of steepest-descent iterations. The loss is averaged over the 300 burst sequences from the SyntheticBurst dataset. and denoising performance improves gradually with an increase in K SD , and the best results are obtained with K SD = 3 iterations. Performing more iterations K SD &gt; 3 results in a small decrease in performance, indicating that early-stopping can act as a regularizer, leading to improved performance. Impact of initializer P : Here, we analyse the impact of our initializer module P , which computes the initial encoding z 0 . We evaluate a variant of our approach which does not utilizes P , instead setting the initial encoding z 0 to zeros. The results on the SyntheticBurst dataset and the grayscale burst denoising dataset are shown in <ref type="table">Table 5a</ref> and <ref type="table">Table 5b</ref>, respectively. Thanks to the fast convergence of the steepest-descent iterations, we observe that the initializer module P only provides a small improvement in performance. Impact of downsampling factors of feature degradation G: We analyse the impact of the downsampling factors of operator G on the burst super-resolution task. Results for different values ofs on the SyntheticBurst dataset is provided in <ref type="table" target="#tab_6">Table 6</ref>. We observe that using a higher downsampling factors in operator G leads to better results. However the improvement when using a downsampling factors = 4 compared tos = 2 is relatively small (+0.07dB). Hence, we uses = 2 in our final version to obtain better computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative Results</head><p>In this section, we provide additional qualitative results. A qualitative comparison with BPN [64] on the grayscale <ref type="bibr" target="#b46">[45]</ref> and color <ref type="bibr" target="#b65">[64]</ref> burst denoising datasets are provided in <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref>, respectively. <ref type="figure">Figure 8</ref> contains a comparison of our approach to DBSR [2] on the SyntheticBurst RAW super-resolution dataset. Additional comparison on the real-world BurstSR dataset <ref type="bibr" target="#b1">[2]</ref> is provided in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BPN</head><p>Ours GT <ref type="figure">Figure 6</ref>. Qualitative comparison of our approach with BPN [64] on the grayscale burst denoising dataset <ref type="bibr" target="#b46">[45]</ref>. Compared to BPN, our approach can recover higher frequency details, without oversmoothing the image.</p><p>Input BPN Ours GT <ref type="figure">Figure 7</ref>. Qualitative comparison of our approach with BPN [64] on the color burst denoising dataset <ref type="bibr" target="#b65">[64]</ref>. Our approach can generate clean images without introducing any color artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBSR</head><p>Ours GT <ref type="figure">Figure 8</ref>. Qualitative comparison of our approach with DBSR [2] on the SyntheticBurst super-resolution dataset <ref type="bibr" target="#b1">[2]</ref>. Our approach can better recover high frequency details and generates sharper images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBSR</head><p>Ours GT <ref type="figure">Figure 9</ref>. Qualitative comparison of our approach with DBSR [2] on the real-workd BurstSR dataset <ref type="bibr" target="#b1">[2]</ref>. Note that the ground truth image and the input burst are captured using different cameras, resulting in a color shift between the network predictions and the ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a continuous representation of a d-dimensional feature map (obtained by e.g. bilinear interpolation, which is itself linear). Then the warping operator can be conveniently expressed as a function composition ? m (x) = x ? m, i.e. ? m (x)(p) = x(m(p)) for any pixel location p ? R 2 . The warping operator ? m is hence linear for any motion m since ? m (ax 1 + bx 2 ) = (ax 1 + bx 2 ) ? m = ax 1 ? m + bx 2 ? m = a? m (x 1 ) + b? m (x 2 ) for any scalars a, b ? R and feature maps x 1 , x 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Inference pipeline Input: Multiple images {x i } N i=1 , Number of steepest-descent iterations K SD 1: for i = 1, . . . , N do 2: e i ? E(x i ) # Map each input image to embedding space 3: m i ? MotionEstimator(x i , x 1 ) # Estimate scene motion for each image w.r.t. the first image 4: v i ? W (e i , e 1 , m i ) # Estimate the certainty values for each image 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Certainty Values, Original m i (c) Certainty Values, Corrupted m i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>optimizer. The model trained on the synthetic data is then additionally finetuned for 40k iterations on the BurstSR training set for</figDesc><table><row><cell>SyntheticBurst</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Grayscale Denoising</cell></row><row><cell>BurstSR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Color Denoising</cell></row><row><cell></cell><cell></cell><cell>DBSR</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>Ground Truth</cell><cell>BPN</cell><cell>Ours</cell><cell>Ground Truth</cell></row><row><cell></cell><cell></cell><cell>SyntheticBurst</cell><cell></cell><cell></cell><cell>BurstSR</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">PSNR? LPIPS ? SSIM? PSNR? LPIPS ? SSIM? Time (s)</cell></row><row><cell>SingleImage</cell><cell>36.86</cell><cell>0.113</cell><cell>0.919</cell><cell>46.60</cell><cell>0.039</cell><cell>0.979</cell><cell>0.02</cell></row><row><cell>HighResNet [12]</cell><cell>37.45</cell><cell>0.106</cell><cell>0.924</cell><cell>46.64</cell><cell>0.038</cell><cell>0.980</cell><cell>0.11</cell></row><row><cell>DBSR [2]</cell><cell>40.76</cell><cell>0.053</cell><cell>0.959</cell><cell>48.05</cell><cell>0.025</cell><cell>0.984</cell><cell>0.24</cell></row><row><cell>Ours</cell><cell>41.56</cell><cell>0.045</cell><cell>0.964</cell><cell>48.33</cell><cell>0.023</cell><cell>0.985</cell><cell>0.40</cell></row></table><note>Figure 3. Qualitative comparison of our approach with the previous state-of-the-art methods DBSR [2] and BPN [64] on RAW burst super-resolution (first four columns) and burst denoising (last four columns) tasks.. Comparison on the SyntheticBurst and real-world BurstSR validation dataset from [2].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>?1.5), respectively. Note that the noise parameters for the highest noise gain (Gain ? 8) are unseen during training. Thus, performance on this noise level can indicate the generalization of the network to unseen noise. The noise parameters (log(? r ), log(? s )) are assumed to be known both during training and testing, and can be utilized to estimate per-pixel noise variance.Gain ? 1 Gain ? 2 Gain ? 4 Gain ? 8 Average Runtime (s)</figDesc><table><row><cell>?3, ?1.5] and log(? s ) ? [?4, ?2]. The networks are then evaluated on 4 different noise gains (1, 2, 4, 8), corresponding to noise parameters (?2.2, ?2.6), (?1.8, ?2.2), (?1.4, ?1.8), and (?1.1, SingleImage 37.94 34.98 31.74 28.03 33.17 0.005 KPN [45] 38.86 35.97 32.79 30.01 34.41 -BPN [64] 40.16 37.08 33.81 31.19 35.56 0.328 Ours 42.21 39.13 35.75 32.52 37.40 0.198 Ours  ? 41.90 38.85 35.48 32.29 37.13 0.046</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Impact of our encoder E, decoder D, and certainty predictor W modules on SyntheticBurst<ref type="bibr" target="#b1">[2]</ref> and grayscale denoising<ref type="bibr" target="#b46">[45]</ref> datasets. ?PSNR denotes difference with our final model (h).</figDesc><table><row><cell></cell><cell cols="2">SyntheticBurst</cell><cell cols="2">Denoising</cell></row><row><cell></cell><cell cols="4">E D W PSNR ?PSNR PSNR ?PSNR</cell></row><row><cell>(a)</cell><cell>31.91</cell><cell>-7.91</cell><cell>28.06</cell><cell>-6.68</cell></row><row><cell>(b)</cell><cell>33.85</cell><cell>-5.97</cell><cell>33.00</cell><cell>-1.74</cell></row><row><cell>(c)</cell><cell>36.71</cell><cell>-3.11</cell><cell>33.46</cell><cell>-1.28</cell></row><row><cell>(d)</cell><cell>38.12</cell><cell>-1.70</cell><cell>32.99</cell><cell>-1.75</cell></row><row><cell>(e)</cell><cell>38.36</cell><cell>-1.46</cell><cell>34.54</cell><cell>-0.20</cell></row><row><cell>(f)</cell><cell>38.44</cell><cell>-1.38</cell><cell>33.69</cell><cell>-1.05</cell></row><row><cell>(g)</cell><cell>38.63</cell><cell>-1.19</cell><cell>34.56</cell><cell>-0.18</cell></row><row><cell>(h)</cell><cell>39.82</cell><cell></cell><cell>34.74</cell><cell></cell></row><row><cell cols="5">ing the encoder/decoder by an identity function, and setting</cell></row><row><cell cols="5">certainty weights v i to all ones, when applicable. In order</cell></row><row><cell cols="5">to ensure fairness, we employ a deeper decoder when not</cell></row><row><cell cols="5">utilizing an encoder, and vice versa. For training on the</cell></row><row><cell cols="5">SyntheticBurst dataset, we employ a shorter training sched-</cell></row></table><note>ule with 100k iterations. The mean PSNR on the Synthet- icBurst set, as well as the mean PSNR over all four noise levels in the grayscale denoising set are provided in Tab. 4. Minimizing the reconstruction error directly in the input image space (MAP estimate</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Impact of downsampling factors of feature degradation G on the SyntheticBurst dataset.</figDesc><table><row><cell cols="4">s PSNR? LPIPS? SSIM?</cell></row><row><cell>1</cell><cell>39.38</cell><cell>0.077</cell><cell>0.948</cell></row><row><cell>2</cell><cell>39.82</cell><cell>0.071</cell><cell>0.952</cell></row><row><cell>4</cell><cell>39.89</cell><cell>0.071</cell><cell>0.953</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments: This work was supported by a Huawei</head><p>Technologies Oy (Finland) project, the ETH Z?rich Fund (OK), an Amazon AWS grant, and Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion deblurring and super-resolution from an image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bascle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep burst super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">J?remo</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11028" to="11037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A note on multi-image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Local and Non-Local Approximation in Image Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient super-resolution via image warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="761" to="771" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video denoising by sparse 3d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kostadin Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="145" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kostadin Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Highres-net: Recursive fusion for multi-frame super-resolution of satellite imagery. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michel Deudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalaitzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Goytom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arefin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1646" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unified blind method for multi-image super-resolution and single/multiimage blur deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2101" to="2114" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiframe demosaicing and super-resolution from undersampled color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust shift and add approach to superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Optics + Photonics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep burst denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Direct super-resolution and registration using raw cfa images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gotoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-resolution image reconstruction from a sequence of rotated and translated frames and its application to an infrared imaging system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Bognar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3892" to="3901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A nonlinear least square technique for simultaneous image registration and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2830" to="2841" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Radiometric ccd camera calibration and noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondepudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05509</idno>
		<title level="m">Replacing mobile camera isp with a single deep learning model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eddy Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP Graph. Model. Image Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning for fast super-resolution reconstruction from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawulok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Benecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Hrynczenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kostrzewa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piechaczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nalepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smolka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Defense + Commercial Sensing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust multiframe super-resolution employing iteratively re-weighted minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schebesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="58" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep image demosaicking using a cascade of convolutional residual denoising networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Iterative residual cnns for burst photography applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5922" to="5931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multilabel and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<ptr target="https://github.com/openimages,2017.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5835" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Handheld mobile photography in very low light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Liba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Karnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiurui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video denoising using separable 4d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-kernel prediction networks for denoising of burst images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Talmaj Marinc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>G?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hellge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2404" to="2408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepsum: Deep neural network for superresolution of unregistered multitemporal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bordone Molini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3644" to="3656" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving image resolution using subpixel motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shmuel Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Overcoming registration uncertainty in image superresolution: Maximize or marginalize?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Capel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bayesian methods for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Capel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Regularization by denoising: Clarifications and new interpretations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Reehorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schniter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="67" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extraction of high-resolution frames from video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="996" to="1011" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>IEEE transactions on image</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pwcnet: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Robust kernel regression for restoration and reconstruction of images from sparse noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1257" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Kernel regression for image processing and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-frame image denoising and stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bayesian image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Few-shot classification by few-iteration meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multiframe image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advance Computer Visual and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Plug-and-play priors for model based reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singanallur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="945" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Handheld multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wronski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Garcia-Dorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Kai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Basis prediction networks for effective burst denoising with large kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep unfolding network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3214" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Commutability of blur and affine warping in super-resolution with application to joint estimation of triple-coupled variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1796" to="1808" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR</title>
		<editor>I-I</editor>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
