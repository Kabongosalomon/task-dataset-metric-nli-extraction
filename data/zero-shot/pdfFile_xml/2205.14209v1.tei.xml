<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STARGRAPH: A COARSE-TO-FINE REPRESENTATION METHOD FOR LARGE-SCALE KNOWLEDGE GRAPH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhu</forename><surname>Li</surname></persName>
							<email>lihongzhu@360.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Gao</surname></persName>
							<email>gaoxiangrui@360.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
							<email>dengyafeng@360.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Qihoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">STARGRAPH: A COARSE-TO-FINE REPRESENTATION METHOD FOR LARGE-SCALE KNOWLEDGE GRAPH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector, ignoring the rich information contained in neighbor entities. We propose a method named StarGraph, which gives a novel way to utilize the neighborhood information for largescale knowledge graphs to get better entity representations. The core idea is to divide the neighborhood information into different levels for sampling and processing, where the generalized coarse-grained information and unique fine-grained information are combined to generate an efficient subgraph for each node. In addition, a self-attention network is proposed to process the subgraphs and get the entity representations, which are used to replace the entity embeddings in conventional methods. The proposed method achieves the best results on the ogbl-wikikg2 dataset, which validates the effectiveness of it. The code is now available at https://github.com/hzli-ucas/StarGraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A Knowledge Graph (KG) is a directed graph with real-world entities as nodes and relationships between entities as edges. In this graph, each directed edge together with its head and tail entities form a triple (head entity, relation, tail entity), indicating that the head and tail entities are connected by a relation.</p><p>Knowledge graph embedding (KGE), also known as knowledge representation learning (KRL), aims to embed entities and relations into low-dimensional continuous vector spaces to characterize their latent semantic features. A scoring function is defined to measure the plausibility for triples in such spaces. The embeddings of entities and relations are then learned by maximizing the total plausibility of the observed triples. These learned embeddings can be used to implement various tasks such as knowledge graph completion [1, 2], relationship extraction [3], entity classification [4], etc. The plausibility of each triple is calculated on the embeddings of the entities and relations in it, and the embeddings are directly taken out from the embedding tables. Such a shallow lookup decides that those models are inherently transductive. Moreover, the rich information contained in the neighboring triples is not taken into account.</p><p>Compared with shallow embedding models, neighborhood neural encoders, that are able to extract structural information, usually perform much better across various KG datasets. Any generic graph neural networks (GNN), e.g, GCN [5], GraphSAGE [6]  and GAT [7], could be employed as the encoder. However, there is a problem adopting these methods to large-scale knowledge graphs, for previous graph networks takes the multi-hop subgraph of the node as input. Due to the large number of nodes and edges, multi-hop subgraphs in large-scale graphs can easily exceed the size limit, and the generation of subgraphs and network calculations can be very time-consuming.</p><p>The neighborhood can provide extra information for each node so as to obtain a better representation. In order to adopt neighborhood neural encoders in large-scale KG, an intuitive idea is to utilize partial neighborhood information instead of the full multi-hop subgraph. In this paper, we propose to divide the neighborhood information into different levels for consideration, using coarse-grained information to enhance the generalization, and fine-grained information to ensure the distinction of different entity representations. Additionally, in order to reasonably model the subgraph structure * First author and second author contribute equally to this work</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Illustration of a subgraph generated by StarGraph. Dots and lines represent nodes and edges in the graph, respectively, with larger dots indicating anchors. The nodes in different levels of the subgraph are marked with different colors. and enable sufficient interaction of node embeddings, we take the lead in using a self-attentive network to extract the neighborhood information and obtain entity representations.</p><p>The coarse-grained representation can be implemented by the anchor-based strategy <ref type="bibr" target="#b6">[8]</ref>, where a small number of nodes are selected as anchors, and the nearest anchors are sampled into the current subgraph. Finer-grained information can then be provided by part of the neighbors and the current node itself. <ref type="figure">Figure 1</ref> shows a subgraph consisting of nodes with multi-level information, where the current node is circled in blue, the neighboring nodes in green, and the nearest anchors in red. It can be seen that different nodes may share the same set of anchors, e.g. N1 and N2 in the graph, so the fine-grained nodes sampling is necessary.</p><p>The coarse-to-fine subgraph sampling is like how to locate a star in the sky, a few bright stars (anchors) are used to roughly indicate the range, and then neighboring stars are used to pinpoint the location. For subgraphs in the entire graph looks like constellations among the stars, we call the proposed method StarGraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Distance-based models consist a major branch of knowledge graph embedding methods. TransE <ref type="bibr" target="#b0">[1]</ref> embeds relations and entities into the same vector space, and the relation embedding is interpreted as a translation from the embeddings of the head entity to the tail entity. It is a simple and effective model, but cannot deal with complex and symmetry relations. RotatE <ref type="bibr" target="#b7">[9]</ref> takes inspiration from Euler's formula and uses the rotations of vectors to explain various relations, where the inverse relations can be modeled by complex embeddings. PairRE <ref type="bibr" target="#b8">[10]</ref> propose to learn two embeddings for each relation, respectively used to map head and tail entitiy embeddings into the corresponding relation space. It extends the model ability to handle sub-relations. TripleRE <ref type="bibr" target="#b9">[11]</ref> learns another embedding for each relation on the basis of PairRE, and the extra relation embedding is used as a translation vector between the mapped entity embeddings of head and tail. This minor modification significantly improves the model results.</p><p>NodePiece <ref type="bibr" target="#b6">[8]</ref> borrows the idea of subword tokenization from natural language processing (NLP) and proposes to use a small number of nodes as anchors, constructing vocabulary for each entity using anchors and known relationship types, and using a multi-layer perceptron to obtain node representations calculated from token sequences. However, there are some drawbacks in this method. It treats tokens in a sequential manner, and the order of tokens introduces noise information to the unordered graph structure. The anchors-based information is coarse and cannot provide good differentiation for nearby nodes. In addition, anchors are used to replace nodes for the purpose of reducing the number of parameters, and have not been considered for information enhancement to improve the model effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The StarGraph is a method to obtain entity representations based on coarse-to-fine neighborhood information. In terms of procedure, it can be divided into two stages, generating and encoding subgraphs. The obtained entity representations, together with the relational embeddings, are used to compute the scores of the triples by a distance-based score function, and optimized with the self-adverse negative sampling loss <ref type="bibr" target="#b7">[9]</ref>. We will describe each part in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subgraph Generation</head><p>For knowledge graph representation learning, the neighborhood information is proved helpful, and the multi-hop neighbors are often used to enrich the target node description. When the graph is very large and contain enormous nodes, the multi-hop subgraph of the target node could be extremely large and contains much redundant information. We propose to sample a smaller yet more effective subgraph, by describing a node in a coarse-to-fine way. Each subgraph contains three types of nodes as follows.</p><p>Anchors At first, a minority of nodes are selected as anchors. In order to provide common information, anchors should be well known to different nodes, therefore are chosen according to the node degrees in a decreasing order. For each node, a fixed number of anchors are sampled from its multi-hop neighbors by breadth-first-searching. The number of anchors is a constant value chosen with experience.</p><p>Neighbors Neighbors refer to a fixed number of nodes sampled from one-hop neighborhood of the target node. Compared with anchors, the neighbors are usually closer to the target node, and can be any node rather than the previously selected minority. The neighbors provide more specificity information than the anchors, but also aim to make a common description for the target node. So we sample the neighbors in a degree-decreasing order.</p><p>Center To ensure the subgraphs of different nodes being distinguishable, we take the target node itself as the center of its subgraph.</p><p>During the generation of subgraphs, anchors are picked from the predefined anchor set, while neighbors and centers could be any node in the total graph. For ease of description, we denote the anchor set as A, all nodes of the total graph as N . There are A ? N , and |A| &lt; |N |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>Transformer <ref type="bibr" target="#b10">[12]</ref> is proposed by Google in 2017, and widely used in many fields, such as NLP, CV and even graphs. The basic unit in Transformer, the self-attention module provides a novel way to deal with the inputs. It takes a set of features as inputs, update each feature with all other features within the set, and control the information flow with an attention matrix. Compared with multi-layer perceptron (MLP), self-attention module provide a more effective way for the nodes to exchange information and allow the nodes staying unordered. Therefore we adopt self-attention module to build the encoder.</p><p>The encoder takes the embeddings of nodes within the subgraph as inputs, each node treated as a token. The nodes in the subgraph are unordered, so the position embedding is not used. Type embedding is added to the token embedding before the transformer layer, indicating each node within the subgraph to be the type of anchor, neighbor or center.</p><p>As described in section 3.1, each subgraph consist of nodes from A and N , providing coarse and fine-grained information respectively for the target node. As with the entire graph, each node keeps some unique information, while only a small group of nodes are anchors, carrying much general information. Denoting the embedding sizes of A and N as D A and D N , we set a large D A to improve the information capacity, and a small D N to reduce the probability of overfitting. To get the tokens of the same embedding size, D A is used as the hidden size for transformer layer, and the node embeddings are mapped to dimension D A by a linear layer ahead.</p><p>Since the self-attention layer has a global receptive field, a single layer can adequately mix the information of all nodes. In addition, deep self-attention networks take large scale data and long time pre-training to achieve good results. Therefore, we use a single transformer block, i.e. one attention layer followed by two linear layers, to construct the network. After the transformer block, the output node features are aggregated by mean-pooling across the subgraph to get the representation of target node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Score Function</head><p>We adopt a score function that is modified upon TripleRE <ref type="bibr" target="#b9">[11]</ref>, a recently proposed distance-based method. The formula of TripleREv2 is</p><formula xml:id="formula_0">f r (h, t) = ?||h ? (r h + u ? e) ? t ? (r t + u ? e) + r||,<label>(1)</label></formula><p>where ? denotes the element-wise product, h and t correspond to the head and tail entity embeddings respectively, [r h , r t , r] comprise the relation embedding, u is a constant specified by users, set to 1 in the original work. We modify equation <ref type="formula" target="#formula_0">(1)</ref> by switching e with r h /r t , so that the coefficient u acts on the relation embedding. Now the score of triplets (h, r, t) is calculated as</p><formula xml:id="formula_1">f r (h, t) = ?||h ? (u ? r h + e) ? t ? (u ? r t + e) + r|| = ?||h ? t + r + u ? (h ? r h ? t ? r t )||,<label>(2)</label></formula><p>which can be interpreted as a boosting method based on TransE and PairRE. The new form is referred to as TripleRE' in the rest of this paper. Although TripleRE' is exactly the same as TripleREv2 when u equals 1, it consistently performs better during tuning the hyper-parameter u according to our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>We utilize the self-adversarial negative sampling loss <ref type="bibr" target="#b7">[9]</ref> as objective for training,</p><formula xml:id="formula_2">L = ? log ?(? ? f r (h, t)) ? n i=1 w i log ?(f r (h i , t i ) ? ?), w i = exp f r (h i , t i ) n j=1 exp f r (h j , t j ) ,<label>(3)</label></formula><p>where ? is a fixed margin and ? is the sigmoid function. We randomly sample n negative triples for each positive one, and (h i , r, t i ) is the i-th negative triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metric ogbl-wikikg2</head><p>To explore the effect of StarGraph on large-scale knowledge graph, we conduct experiments on the ogbl-wikikg2 dataset, which is a knowledge graph extracted from the Wikidata knowledge base <ref type="bibr" target="#b11">[13]</ref>. It contains 2,500,604 entities, 535 relation types and 17,137,181 triplet edges (head, relation, tail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Metric</head><p>The task is to predict new triplet edges given the training edges. The evaluation metric follows the standard filtered metric widely used in KG. Specifically, each test triplet edges are corrupted by replacing its head or tail with randomly-sampled 1,000 negative entities, while ensuring the resulting triplets do not appear in KG. The goal is to rank the true head (or tail) entities higher than the negative entities, which is measured by Mean Reciprocal Rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detailed Settings</head><p>We set batch-size to 512, negative-sampling-size to 64, and train the model for 500,000 steps. The learning rate is initialized to 0.0005 and decreased by 0.1 at half the maximum training step. Dropout is adopted after each linear layer, and the drop-ratio is 0.05. AdamW is used as the optimizer in our experiments.</p><p>Hyper-parameter u in equation <ref type="formula" target="#formula_1">(2)</ref> is tuned and set to 0.1 according to model performance on validation set, and ? in equation <ref type="formula" target="#formula_2">(3)</ref> is set to 6.0 following PairRE <ref type="bibr" target="#b8">[10]</ref>.</p><p>Unless otherwise stated, we adopt self-attention network as the encoder, and set D A = 256, D N = 32. The number of anchors and neighbors sampled for each subgraph is fixed as 20 and 5 respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct some comparison experiments to explore the effectiveness of the proposed method, including the network architecture and subgraph generation strategy.</p><p>Encoder To test the effectiveness of self-attention network as the encoder, we use the two-layer MLP adopted in NodePiece as a control. In this group of experiments, networks take only anchors in each subgraph as inputs, abandoning neighbors and center for simplicity. The experiment results are given in table 1. Attention-based network achieved a significant improvement in evaluation metrics over MLP, and is used as the encoder in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgraph</head><p>We carry out an ablation study on subgraph generation, where we analyze the performance when we omit neighbors, centers or both types from subgraphs. <ref type="table" target="#tab_1">Table 2</ref> shows that on the basis of anchors, additional node sampling for subgraph significantly improves the results, which validate the effectiveness of the proposed subgraph generation method. Note that anchors+center introduces only one more node into each subgraph while there are five more nodes for neighbors, but anchors+centers performs better than anchors+neighbors. This suggests the importance of subgraph containing more specific information. The combination of the three kinds of nodes achieves the best results, and removing either neighbors or center leads to a drop in results. This proves the superiority of the proposed subgraph generation method which aggregates multi-level information from coarse to fine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on ogbl-wikikg2</head><p>The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Our method achieves 0.7288 (validation set) and 0.7201 (test set) on MRR, that are state-of-the-art results on the ogbl-wikikg2 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose StarGraph, a novel node representation method that is based on neighborhood information and applicable to large knowledge graphs. The underlying idea of the method is to process the neighborhood information in a hierarchical manner, by generating a subgraph containing three types of nodes to capture the neighborhood information, and employing a self-attentive network to process the information. We achieves best results on ogbl-wikikg2 dataset, which demonstrates the effectiveness of the proposed method. Although StarGraph makes considerable progress compared to previous methods, there is still much room for improvement in it, including but not limited to exploring better subgraph generation strategies and network structures. We also consider applying StarGraph to other related tasks, such as inductive link prediction, relationship prediction and node classification. These are treated as future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The results on ogbl-wikikg2 with different encoders. Subgraphs only consist of anchors in this experiment.</figDesc><table><row><cell>Encoder</cell><cell cols="3">#Dims Test MRR Valid MRR</cell></row><row><cell>MLP</cell><cell>256</cell><cell>0.6853</cell><cell>0.6897</cell></row><row><cell>Attention</cell><cell>256</cell><cell>0.6992</cell><cell>0.7062</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results on ogbl-wikikg2 with different subgraphs. Numbers in the brackets indicate how many such type of node are contained in each subgraph. The indicates that subgraphs contain corresponding type of nodes.</figDesc><table><row><cell cols="2">anchors(20) neighbors(5) center(1) Test MRR Valid MRR</cell></row><row><cell>0.6992</cell><cell>0.7062</cell></row><row><cell>0.7150</cell><cell>0.7258</cell></row><row><cell>0.7182</cell><cell>0.7262</cell></row><row><cell>0.7205</cell><cell>0.7291</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The results on ogbl-wikikg2. Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74-84, 2013.</figDesc><table><row><cell>Model</cell><cell>#Dims</cell><cell>Test MRR</cell><cell>Valid MRR</cell></row><row><cell>TransE</cell><cell>500</cell><cell cols="2">0.4256?0.0030 0.4272?0.0030</cell></row><row><cell>RotatE</cell><cell>250</cell><cell cols="2">0.4332?0.0025 0.4353?0.0028</cell></row><row><cell>PairRE</cell><cell>200</cell><cell cols="2">0.5208?0.0027 0.5423?0.0020</cell></row><row><cell>AutoSF</cell><cell>-</cell><cell cols="2">0.5458?0.0052 0.5510?0.0063</cell></row><row><cell>ComplEx</cell><cell>250</cell><cell cols="2">0.5027?0.0027 0.3759?0.0016</cell></row><row><cell>TripleRE</cell><cell>200</cell><cell cols="2">0.5794?0.0020 0.6045?0.0024</cell></row><row><cell>TripleREv2</cell><cell>200</cell><cell cols="2">0.6045?0.0017 0.6117?0.0008</cell></row><row><cell>ComplEx-RP</cell><cell>50</cell><cell cols="2">0.6392?0.0045 0.6561?0.0070</cell></row><row><cell>AutoSF + NodePiece</cell><cell>-</cell><cell cols="2">0.5703?0.0035 0.5806?0.0047</cell></row><row><cell>TripleREv2 + NodePiece</cell><cell>200</cell><cell cols="2">0.6582?0.0020 0.6616?0.0018</cell></row><row><cell>TripleREv3 + NodePiece</cell><cell>200</cell><cell cols="2">0.6866?0.0014 0.6955?0.0008</cell></row><row><cell>InterHT + NodePiece</cell><cell>200</cell><cell cols="2">0.6779?0.0018 0.6893?0.0015</cell></row><row><cell>TranS + NodePiece</cell><cell>200</cell><cell cols="2">0.6882?0.0019 0.6988?0.0006</cell></row><row><cell>TranS(large) + NodePiece</cell><cell>-</cell><cell cols="2">0.6939?0.0011 0.7058?0.0018</cell></row><row><cell>TripleRE' + StarGraph</cell><cell cols="3">256/32 0.7201?0.0011 0.7288?0.0008</cell></row><row><cell>[3]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nodepiece: Compositional and parameterefficient representations of large knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pairre: Knowledge graph embeddings via paired relation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Triplere: Knowledge graph embeddings via triple relation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<idno>viXra:2112.0095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">viXra preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
