<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-identification via Attention Pyramid</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpei</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jiwen</forename><forename type="middle">Lu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-An</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jie</forename><forename type="middle">Zhou</forename></persName>
						</author>
						<title level="a" type="main">Person Re-identification via Attention Pyramid</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-identification</term>
					<term>Attention Learning</term>
					<term>Feature Pyramid</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an attention pyramid method for person re-identification. Unlike conventional attention-based methods which only learn a global attention map, our attention pyramid exploits the attention regions in a multi-scale manner because human attention varies with different scales. Our attention pyramid imitates the process of human visual perception which tends to notice the foreground person over the cluttered background, and further focus on the specific color of the shirt with close observation. Specifically, we describe our attention pyramid by a "split-attend-merge-stack" principle. We first split the features into multiple local parts and learn the corresponding attentions. Then, we merge local attentions and stack these merged attentions with the residual connection as an attention pyramid. The proposed attention pyramid is a lightweight plug-and-play module that can be applied to off-theshelf models. We implement our attention pyramid method in two different attention mechanisms including: channel-wise attention and spatial attention. We evaluate our method on four largescale person re-identification benchmarks including Market-1501, DukeMTMC, CUHK03, and MSMT17. Experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin with limited computational cost. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Person Re-Identification (ReID) focuses on matching the images or videos of the same person captured from nonoverlapping cameras, which is of paramount importance for many applications, such as suspect tracking and missing person retrieval. It has been significantly advanced in recent years with the aggressive improvement of deep learning. Despite the recent progress, learning a discriminative feature to identify the person from a large set of gallery candidates is still challenging due to large intra-class variance caused by pose variations, occlusions, or cluttered backgrounds.</p><p>Recently, attention mechanism [1]- <ref type="bibr" target="#b5">[6]</ref> has been widely used in the ReID system to facilitate high-performance identification and demonstrates the powerful representation ability by discovering discriminative regions and mitigating the misalignment. For example, Zhang et al. <ref type="bibr" target="#b0">[1]</ref> proposed a relationaware attention model to focus on the inter-relation in the feature map. Li et al. <ref type="bibr" target="#b4">[5]</ref> introduced harmonious attention to simultaneously learn hard region-level and soft pixel-level attention.</p><p>These methods learn to explore salient regions in the global image, which can be formulated as a salient detection task.</p><p>The authors are with the Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University, Beijing, 100084, China. Email: chen-gy16@mails.tsinghua.edu.cn; brucegu@umd.edu; lujiwen@tsinghua.edu.cn; bja1021@bupt.edu.cn; jzhou@tsinghua.edu.cn. <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/CHENGY12/APNet Pyramid Scale attend <ref type="figure">Fig. 1</ref>. The motivation of the attention pyramid network. The attentions of the human vision system are changing with the visual scale. When we look at a whole image, the upper part of the person attracts our attentions. This attention will further be turned to the red coat in the arms when we have a close look.</p><p>However, detecting the salient regions with the attention model is confronted with the dilemma to jointly capture both coarse and fine-grained clues, since the focus varies as the image scale changes. As shown in <ref type="figure">Fig. 1</ref>, we tend to focus on the upper part of the person with the given whole image, transfer our sight to the salient coat in the arms given the upper part, and further concentrate on the more discriminative regions. It is consistent with the human vision system attending salient objects sequentially, from coarse to fine.</p><p>To address the above issue, we propose effective attention pyramid networks (APNet) to jointly learn the attentions under different scales. It is motivated by the widely-used feature pyramid method (FPN) <ref type="bibr" target="#b6">[7]</ref> in the visual detection system, which captures multi-scale clues with the pyramid structure. We regard attention learning as a process of salience detection and learn the attention pyramid for multi-scale saliences. Unlike the FPN applying the feature maps of high resolution in shallow layers to detect the small objects and low resolution in deep layers to detect large objects, our APNet focuses on learning the discriminative representation covering the salient regions from coarse to fine. Therefore, we propose a "splitattend-merge-stack" principle to build our attention pyramid, which splits the feature maps into different granularities and arXiv:2108.05340v1 [cs.CV] 11 Aug 2021 aggregates the attentive clues by stacking the attention module learned from different granularities. In each level of our attention pyramid, we split the features into multiple local parts and learn the attention maps of each part. Then, we merge these local attention maps to obtain the attention of the global image. At different pyramid levels, we split the features into local parts with different granularities to capture the salient clues in multiple scales. Finally, we stack the attention maps from coarse to fine as a pyramid structure to aggregate clues from different granularities.</p><p>Compared with the traditional attention model, our APNet effectively captures the discriminative clues with different scales with the proposed "split-attend-merge-stack" principle. Compared with other feature pyramid learning methods which extract features of different scales and aggregate them, our APNet requires no extra feature extraction module by replacing it with the splitting and stacking patterns. Therefore, our APNet can be easily integrated into any baseline attention model with a lightweight computational cost (comparable with attention learning). Beyond spatial attention, our attention pyramid framework can be applied to other attention modules, such as channel-wise attention or temporal attention. To evaluate the generality of our attention pyramid framework, we implement our attention pyramid framework for channel-wise attention module and spatial attention module, which respectively explores the discriminative clues in channel-wise and spatial domains. We conduct extensive experiments to evaluate our APNet on four popular person re-identification benchmarks including Market-1501 <ref type="bibr" target="#b7">[8]</ref>, DukeMTMC-reID <ref type="bibr" target="#b8">[9]</ref>, CUHK03 <ref type="bibr" target="#b9">[10]</ref>and MSMT17 <ref type="bibr" target="#b10">[11]</ref>. Experimental results demonstrate that our APNet outperforms the state-of-the-art methods by a large margin with limited computational cost. Besides, we also conduct the cross-dataset evaluation and occlusion evaluation to evaluate the generalization ability and robustness of our method.</p><p>We summarize the contributions of this work as follows: 1) We propose the attention pyramid networks for person ReID, which jointly explores the salient clues in different scales by the proposed "split-attend-merge-stack" principle. 2) We implement our attention pyramid framework for different attention modules including channel-wise and spatial ones. 3) In the experiments, our method achieves obvious improvement with limited computational cost and we demonstrate better generalization ability and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review three related topics, including deep person re-identification, attention mechanism, and feature pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Person Re-identification</head><p>Recent Person ReID methods obtain excellent performance with the development of deep learning models, which learn the robust feature representation for the misaligned person image. Most of these methods are categorized into two research fields including capturing more prior knowledge or supervisory signals and designing more effective networks. To mine more clues as the prior knowledge, some methods utilize body structure knowledge <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> and human pose information <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> for accurate part detection or person normalization. For example, Kalayeh et al. <ref type="bibr" target="#b14">[15]</ref> propose to use the human semantic parsing as the prior knowledge to refine the person ReID model. While PN-GAN <ref type="bibr" target="#b14">[15]</ref> estimates the human pose and normalizes it by a generative adversarial network to mitigate the influence of pose variations. Besides, attribute labels <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and spatial-temporal pattern <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> are also introduced in the Person ReID system as the complementary supervisory signal to improve performance. AAnet <ref type="bibr" target="#b18">[19]</ref> integrates person attributes and attribute attention maps into a unified learning framework to guide attention learning with the attribute labels. St-ReID <ref type="bibr" target="#b19">[20]</ref> utilizes the spatial-temporal information in the camera network to filter irrelevant negative samples of gallery set and significantly improve the performance. Furthermore, some methods <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b25">[26]</ref> aim to optimize the loss function to mine the relations of instances and learn the discriminative embeddings. For example, TriNet <ref type="bibr" target="#b21">[22]</ref> first proposes to introduce the triplet loss into the deep ReID system and achieves excellent performance. While Chen et al. <ref type="bibr" target="#b23">[24]</ref> further improve it by a margin-based online hard negative mining strategy to enhance the generalization ability of the ReID model. Network designing is another important direction of person ReID. In the early research of deep person ReID, many methods are proposed to explore effective network structure such as earliest Deepreid <ref type="bibr" target="#b9">[10]</ref>, region-based SpindleNet <ref type="bibr" target="#b11">[12]</ref>, effective OSNet <ref type="bibr" target="#b26">[27]</ref> and the strong baseline BOT <ref type="bibr" target="#b27">[28]</ref>. Recently, many methods focus on designing the part-based model <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>, which split the feature maps into multiple parts to learn local features and aggregate them for recognition. Despite significant performance improvement, these methods always suffer the high computational cost. Attention model <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b38">[39]</ref> is also an important direction to design novel network architecture, which discovers salient regions and mitigates the misalignment to learn robust representation. Furthermore, some methods <ref type="bibr" target="#b39">[40]</ref> are proposed to automatically search the network architectures for person ReID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Model</head><p>Attention model <ref type="bibr" target="#b40">[41]</ref> naturally imitates the perception of humans to concentrate on what we are interested in. Recently, it has gained great success in many fields, such as visual understanding <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, natural language processing <ref type="bibr" target="#b43">[44]</ref>, and graph learning <ref type="bibr" target="#b44">[45]</ref>. The attention model also plays an irreplaceable role for the person ReID system to learn discriminative representation. Liu et al. <ref type="bibr" target="#b45">[46]</ref> introduce the attention model to locate the discriminative salient regions and model it with an RNN. Furthermore, Zhao et al. <ref type="bibr" target="#b46">[47]</ref> and Xu et al. <ref type="bibr" target="#b3">[4]</ref> apply the body part detector to employ the clues of the human body structure in the attention model. Beyond spatial attention, many methods adopt attention mechanisms on the temporal domain <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b51">[52]</ref> to explore key temporal frames. The attention model is also applied on the channel domain <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b52">[53]</ref> to discover key feature channels, and even on the instance level <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> to capture more valuable instances. Despite the widespread use and the convincing performance of the attention model, the problem of how to jointly capture the salient clues of different scales is still barely studied. SCSN <ref type="bibr" target="#b55">[56]</ref> cascades multiple attention models on the extracted features to capture different clues. However, the cascaded structure requires a complex mechanism to avoid information duplication, which is computing expensive for the attention model. In this paper, we focus on designing a basic attention norm to jointly discover the salient clues from coarse to fine, called attention pyramid networks (APNet). Compared with other attention models, the proposed APNet can achieve obvious improvement by the pyramid-structure perception with limited computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Pyramid</head><p>Feature pyramid is a widely-used method to learn multiscale feature representation for detecting objects of various scales. FPN <ref type="bibr" target="#b6">[7]</ref> proposes a top-down pathway to fuse the features with different resolutions and scales. In addition, many variants of FPN are proposed to improve the information propagation ability of FPN, such as PANet <ref type="bibr" target="#b56">[57]</ref> adds a bottom-up path, Bi-FPN <ref type="bibr" target="#b57">[58]</ref> proposes a new cross-scale connection, or connects high-level and low-level features in a nonlinear way <ref type="bibr" target="#b58">[59]</ref>. Recently, feature pyramid methods have demonstrated great success in various fields, such as semantic segmentation <ref type="bibr" target="#b59">[60]</ref> and person ReID <ref type="bibr" target="#b22">[23]</ref>. Different from the FPN-based methods in the detection task connecting the features of different resolutions, these methods first extract the features with different scales by an extra multi-scale branch and learn to aggregate them. For example, Li et al. <ref type="bibr" target="#b59">[60]</ref> extract features of different scales with multiple convolution blocks and fuse them to learn the attention, while Zheng et al. <ref type="bibr" target="#b22">[23]</ref> split the feature maps into parts with different scales and learn the multi-branch local features of each part. Despite the convincing performance, the extra multi-scale feature representation branch is computationally expensive. In this work, we propose a novel feature pyramid framework with the "split-attend-merge-stack" principle, which replaces the complex extra feature extraction module with the splitting and stacking pattern. Thanks to such principle, we learn the pyramid feature with similar cost as the attention learning. Our APNet sequentially learns the attention from coarse to fine, instead of aggregating simultaneously learned features of all scales or parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>In this section, we first introduce our attention pyramid networks (APNet) and implement them on both channel-wise and spatial attention methods. Then, optimization procedure and implementation details will be presented. Finally, we discuss different pyramid networks and explain why the proposed APNet is more effective and efficient for person ReID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attention Pyramid</head><p>Attention mechanism has been proved to be efficient in exploiting discriminative features of the image. However, how to jointly capture the salient clues of different scales with a limited cost is still challenging. To address this problem, we propose attention pyramid networks (APNet) to guide the network sequentially discover salient clues of different scales from coarse to fine, for the comprehensive and complementary perception. Different from conventional feature pyramid methods which extract features with different scales and aggregate them, our APNet captures the multi-scale clues with the attention model. We first learn the coarse attention and use it to guide the more fine-grained attention learning of the split feature maps. We name it as the "split-attendmerge-stack" principle in the proposed attention pyramid. In the following, we will introduce this principle in detail and explain why it is effective.</p><p>Given a person image I, we first extract the feature map X ? R C?H?W by a backbone network block F : I ? X, where C denotes the channel dimension and H ? W denotes the spatial domain. Then, we learn the attention pyramid P to discover multi-scale salient parts on the feature X. The attention pyramid contains attention maps with different levels as P = {P i }, where P i denotes the ith pyramid layer. In different pyramid layers, we learn the discriminative clues of different scales. The coarse attentions guide the following finegrained attention learning. It can be formulated as a sequential learning process as:</p><formula xml:id="formula_0">X i = P i (X i?1 ),<label>(1)</label></formula><p>where X i denotes the feature map in the i th level. As shown in <ref type="figure">Fig. 2</ref>, for the attention learning of each level, we first split it into more fine-grained granularities and merge these learned attentions as the attention map of the current level. Split: The split operation takes the feature map X i as input and outputs n split feature tensors X i,j , where j = {1, 2, ..., n} is the index of split feature parts. Specifically, the number of split parts is exponential growth based on a radix, such as n = 2 i when pyramid level is i and radix is 2. With the increase of pyramid level i, the number of split parts is accordingly increasing which indicates the granularities of these feature parts are more fine-grained. The split operation is applied to the corresponding domain of the attention modules. For example, we tend to slice the feature map X i into multiple parts X i,j ? R C? H n ?W along the height dimension H for the spatial attention module and apply the division on the channel domain C to obtain X i,j ? R C n ?H?W for channelwise attention.</p><p>Attend &amp; Merge: Given split feature tensors {X i,j }, we learn a set of sub-attention model {A i,j } to capture the discriminative clues of each feature tensor. Then we merge all sub-attention models as the inverse process of the split operation to obtain the whole attention map A i in the same size, which is formulated as: The architecture of Attention Pyramid Networks (APNet). Our APNet adopts the "split-attend-merge-stack" principle, which first splits the feature maps into multiple parts, obtains the attention map of each part, and the attention map for the current pyramid level is constructed by merging each attention map. Then in a deeper pyramid level, we split the features into more fine-grained parts and learn the fine-grained attention guiding by coarse attentions. Finally, attentions with different granularities are stacked as an attention pyramid and applied to the original input feature by element-wise product.</p><formula xml:id="formula_1">A i = [A i,j (X i,j )] n j=1 ,<label>(2)</label></formula><p>where [?] n j=1 refers to the concentration of n attention maps. This aggregated attention map A i indicates the salient regions in ith level.</p><p>Stack: We guide the network to focus on the significant features progressively by stacking attention from coarse to fine as a pyramid. With the learned attention A i , the network discovers more discriminative features as:</p><formula xml:id="formula_2">X i = ?(A i ) * X i?1 ,<label>(3)</label></formula><p>where * denotes the element-wise product. The features are reweighted with the normalized attention map, and fed into the next pyramid level to guide the more fine-grained attention learning. With the increase of pyramid level, the more finegrained clues are discovered based on original coarse features.</p><p>Multi-Stage Operation: Despite capturing multi-scale discriminative clues with the pyramid structure, it is still challenging to retrieve all salient features at a single step. To alleviate such issue, we apply the proposed attention pyramid at multiple convolution stages of the backbone network, which gradually guides the deep network to discover the salient clues. Specifically, we apply four attention pyramids on the bottom of each residual block of the backbone ResNet50 <ref type="bibr" target="#b60">[61]</ref> network. The multi-stage structure encourages the network to learn more discriminative representation by progressive refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Model</head><p>In this work, we implement our attention pyramid framework into two different attention models including channelwise attention and spatial attention.</p><p>Spatial Attention: Spatial attention focuses the most discriminative region of the input feature map and ignores the irrelevant region (e.g. occlusion). To highlight the contribution of the proposed attention pyramid structure directly, we adopt the relation-aware attention block (RGA-S) [1] 2 as the baseline attention model. Specifically, given a feature tensor X ? R H?W ?C from a CNN layer, we will learn a spatial attention feature map of size H ? W . In this paper, we only simply introduce the basic learning process of the RGA module. The detail architecture and parameter settings can be found in <ref type="bibr" target="#b0">[1]</ref>. As shown in part (a) of <ref type="figure" target="#fig_1">Fig. 3</ref>, RGA-S first splits X into a set of feature nodes {f p } p=1:H?W |f p ? R C . Then the pairwise relations r p,q between feature node f p and feature node f q is calculated as a dot-project affinity as</p><formula xml:id="formula_3">r p,q = ?(f p ) T ?(f q ),</formula><p>where ?(?) and ?(?) are two 1 ? 1 convolution layers. These relations help to mine semantics and thus attention. Relation vectors including horizontal r p,: and vertical r :,q are feed into 1 ? 1 convolution layers and connected as two relation-based attention maps A S RH ? R H?W ?C and A S RV ? R H?W ?C , where C = H ? W is the channel dimension of attention maps. Finally, the spatial attention A S is learned as</p><formula xml:id="formula_4">A S = ?(W s 2 ReLU (W s 1 [A S RH , A S RV , A S G ])),<label>(4)</label></formula><p>where W s 1 and W s 2 are two 1 ? 1 convolution layers, [., ., .] refers to feature connection and A S G denotes the original global spatial attention which learned by a 1 ? 1 convolution layer and a pool layer.</p><p>Channel-wise Attention: Channel-wise attention helps the model to focus on the more salient feature by assigning larger weight to channels that show a higher response. We adopt the "Squeeze and Excitation" block (SE Layer) <ref type="bibr" target="#b52">[53]</ref> as the channel-wise attention, which consists of one global average pooling layer and two consecutive fully-connected layers. As shown in part (b) of <ref type="figure" target="#fig_1">Fig. 3</ref>, given the feature map X, the channel-wise attention A C is defined as:</p><formula xml:id="formula_5">A C = ?(W c 2 ReLU (W c 1 P ool avg (X)))<label>(5)</label></formula><p>where W c 1 and W c 2 are the parameters of two fully-connected layers, and P ool avg refers to the average pooling layer. Similar to spatial attention, the final channel-wise attention A C is applied on the original feature map X by channel-wise multiplication. In our attention pyramid framework, we replace the original feature map X with the split sub-features X i,j to learn the local channel-wise sub-attentions and merge them as Equation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization</head><p>In the training stage, we use triplet loss and classification loss as identity supervisory to train our APNet following the settings in BOT <ref type="bibr" target="#b27">[28]</ref>. The triplet loss optimizes the embedding space to increase the inter-class distance and reduce the intraclass distance. The triplet loss is formulated as:</p><formula xml:id="formula_6">L tri = 1 N N i=1 ||f i ? f + i || ? ||f i ? f ? i || + m + ,<label>(6)</label></formula><p>where [?] + indicates the max function max(0, ?), and f i , f + i , f ? i denote the embedding of the anchor sample, the positive sample, and the negative sample in a batch, respectively. We adopt the hard example mining strategy in our loss function and set the distance margin m = 0.3 of positive samples to the anchor sample than negative ones. We use the Euclidean distance as the distance metric to learn the triplet loss. The classification loss is formulated by calculating the cross-entropy between the identity ground-truth and the predicted probability:</p><formula xml:id="formula_7">L cls = ? 1 N N i=1 K k=1 y i,k log(p i,k ),<label>(7)</label></formula><p>where y i,k denotes the ground-truth label whether the identity of ith image is k. To overcome the overfitting problem in the training, we transform the above loss function with a label smooth regularization <ref type="bibr" target="#b61">[62]</ref>. In practice, we use a uniform distribution ?(k) = 1/K to balance the predicted probability as:</p><formula xml:id="formula_8">L cls+lsr = ? 1 N N i=1 K k=1 log(p i,k ) (1 ? )y i,k + K ,<label>(8)</label></formula><p>where the 0 &lt; &lt; 1 is the smoothing rate. The final loss function for the training stage is thus formulated as:</p><formula xml:id="formula_9">L apn = L cls+lsr + ?L tri ,<label>(9)</label></formula><p>where ? is the balance rate of two loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>In this subsection, we discuss the proposed attention pyramid network with other feature pyramid structures and explain why our APNet is effective and efficient. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we compare three different pyramid structures including FPN <ref type="bibr" target="#b6">[7]</ref>, Pyramid ReID <ref type="bibr" target="#b22">[23]</ref>, Feature pyramid attention <ref type="bibr" target="#b59">[60]</ref> with our APNet. FPN <ref type="bibr" target="#b6">[7]</ref> is a widely-used method in the field of object detection, which applies a top-down pathway to aggregate features with different resolutions and scales. However, it is not trivial to directly transfer this structure for the person ReID task. First, FPN aims to balance the resolution and semantic abstraction, which detects small objects with the features in high-resolution and detects large objects with the features in the high semantic information level (low-resolution). While the person ReID task focuses on the identity information behind the images. Thus, the semantic information is of more importance than the resolution for the ReID feature representations. Second, the top-down pathway in the FPN is an extra top-down branch including the convolution layer and upsampling with expensive computational cost. Compared with FPN, our APNet reduces the top-down branch and adds the pyramid structure in the backbone network as the attention model.</p><p>Pyramid ReID <ref type="bibr" target="#b22">[23]</ref> splits the feature map into different parts and learns the local clues of each part with a network branch. However, the computational cost of simultaneously extracting features of all parts is very expensive. While the relation among representations with different scales is not explicitly modeled. In APNet, we reduce the redundant computing by stacking multiple attention layers, where finegrained attention is guided by the above coarse attention maps. Besides, we show this coarse-to-fine attention model is effective to discover discriminative clues by imitating the human perception process. Some methods apply the same pyramidal structures of Pyramid ReID <ref type="bibr" target="#b22">[23]</ref>, e.g. DPD <ref type="bibr" target="#b38">[39]</ref> method applies multiple pyramidal structures in each stage of backbone. Then, DPD learns an attention model to weight different pyramidal features. There are four main differences between APNet and DPD. First, the pyramidal structure and the attention model in DPD are separated. DPD uses the attention to fuse multiple pyramidal features, while APNet aims to learn pyramidal attention to mine multi-scale features. Second, our pyramid structure follows a "split-attend-mergestack" principle, where the multi-scale features are connected with the stacked attention. In DPD, the features with different scales are treated equally, while in APNet, we use the coarse attention to guide the training of the fine-grained ones. Third, to obtain the pyramidal features, APNet applies the attention model while DPD uses a pooling layer. However, the pooling model may reduce the clues. Fourth, for the features in one stage (a block in the backbone network), our APNet applies it as the input of the next stage, while DPD connects all features as the final representation.</p><p>Feature pyramid attention(FPA) <ref type="bibr" target="#b59">[60]</ref> introduces the pyramid structure in the attention model for semantic segmentation problem, which applies convolution layers with multi-scale kernels to extract clues and fuses them to learn the attention model, as shown in <ref type="figure" target="#fig_2">Fig. 4 (c)</ref>. However, there are three problems in the pyramid attention network <ref type="bibr" target="#b59">[60]</ref>. First, it is computing expensive since each attention module consists of multiple multi-scale convolution layers. Second, multi-scale convolution layers only capture the multi-scale information </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Train <ref type="table" target="#tab_0">sets  Test sets  Cam  IDs  images  IDs  images  Market  751  12936  750  19732  6  Duke  702  16522  702  19889  8  CUHK03  767  7365  700  5332  6  MSMT  4101 32621 3060 93820  15  Occluded-Duke 702  15618  519  19871  8</ref> in the spatial domain, which is difficult to transfer for other attention models. Third, feature pyramid attention <ref type="bibr" target="#b59">[60]</ref> fuses the multi-scale information to learn the attention, but not multiscale attentions. Different from FPA <ref type="bibr" target="#b59">[60]</ref>, our APNet applies the "split-attend-merge-stack" principle to learn multi-scale attentions, which captures multi-scale information with multigranularity split instead of convolution layer. APNet is more efficient due to zero extra computational cost and it can be easily integrated into different attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluated our method on four large-scale ReID benchmarks including: Market-1501 <ref type="bibr" target="#b7">[8]</ref>, DukeMTMC-reID <ref type="bibr" target="#b8">[9]</ref>, CUHK03 <ref type="bibr" target="#b9">[10]</ref>, and MSMT17 <ref type="bibr" target="#b10">[11]</ref>. We conduct extensive ablation studies to investigate the effectiveness of each component in our method and compared our method with other state-ofthe-art methods. Besides, we also evaluated the robustness of our method on the occluded person ReID dataset Occluded-DukeMTMC-reID <ref type="bibr" target="#b62">[63]</ref>, and the generalization ability by the cross-dataset experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Experimental Settings</head><p>Datasets: We conduct extensive experiments on five widely used ReID datasets: Market-1501, DukeMTMC-reID, CUHK03, MSMT17 and Occluted-DukeMTMC. The detailed information of the datasets are shown in <ref type="table" target="#tab_0">Table I</ref>.. 1) Market-1501: The Market-1501 dataset contains 751 identities with 12936 images for training and 750 identities with 19732 images for testing. All the images are captured by five high-resolution cameras and one low-resolution camera in a university. Deformable Part Model (DPM) is used as the pedestrian detector for the dataset. The author provided two kinds of query methods, and we follow the single-query method in this work.</p><p>2) DukeMTMC-reID: The DukeMTMC-reID dataset is a subset of the DukeMTMC dataset which contains 1,404 identities. 702 identities with 16522 images are selected as the training set and the remaining 702 identities with 19889 images are the testing set. The images are captured by 8 highresolution cameras in Duke Univerisity and each identity is guaranteed to be observed by two cameras.</p><p>3) CUHK03: We conducted experiments on both versions of person boxes of the CUHK03 benchmark: manually labeled and auto-detected with a pedestrian detector. We chose the CUHK03-NP split in <ref type="bibr" target="#b63">[64]</ref>, which selects 767 identities for training and the other 700 ones for testing. Compared with the 1367/100 split, the CUHK03-NP split is more realistic and challenging. 4) MSMT17: The MSMT17 dataset is the largest re-ID dataset, which contains 126,441 images of 4,101 identities captured by 15 cameras. In practice, 32,621 images of 1,041 identities are used for training and 93,820 images of 3,060 identities are used for testing. The dataset is recorded in 4 days with different weather conditions in a month using 12 outdoor cameras and three indoor cameras.</p><p>Evaluation Metrics: For all four datasets, we shared the same experiment settings with the standard person ReID experimental setups. We evaluated the ReID accuracy on four datasets by the cumulative matching characteristic (CMC) curve and mean Average Precision (mAP). CMC shows the ReID accuracy by counting the query identities among the top N results. The mAP score calculates the area under the precision-recall curve, which reflects the overall reidentification accuracy rather than only counting top N true matching. Note that, for all experiments, we directly calculate the distance with Euclidean distance, and do not employ the Re-ranking <ref type="bibr" target="#b63">[64]</ref> tricks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We adopt ResNet50 <ref type="bibr" target="#b60">[61]</ref> pre-trained on ImageNet <ref type="bibr" target="#b64">[65]</ref> as our backbone network for the experiments. The stride of the last residual block is set to 1 instead of the original stride = 2 for a larger receptive region. We implement our APNet on both channel-wise and spatial attentions. The proposed attention pyramid is added after all four residual blocks. During training, three data augmentation methods including random cropping, horizontal flipping, and erasing are considered. The margin of triplet loss and the label smoothing regularization rate were set as 0.3 and 0.1, respectively. For the channel-wise attention pyramid, we follow the settings in <ref type="bibr" target="#b1">[2]</ref> for a fair comparison. We use 384 ? 192 input image size, and train the model 160 epochs with the Adam optimizer whose initial learning rate is 0.0004 and is divided by 10 every 40 epoch. For the spatial attention pyramid, we adopt the settings in <ref type="bibr" target="#b0">[1]</ref> whose input images are resized to 256 ? 128. We found that the multi-part trick in MGN <ref type="bibr" target="#b30">[31]</ref> is very effective on the CUHK03 dataset but limited improvement on other datasets. Therefore, we only apply this multi-part trick on CUHK03. For all experiments, we randomly select 20 persons in a batch for training, where each person has 4 images. During the evaluation, we use the 2048 dimension features after the BN bottleneck for person matching. We employ the cosine distance as the evaluation metric to calculate the similarity of two images and use the average feature between the original testing image and the horizontally flipped one. All the experiments are conducted with PyTorch 1.7 with two Nvidia 2080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>To explore the effectiveness of each component of APNet and different hyper-parameters, we conducted comprehensive ablation studies on both spatial and channel-wise APNet. As shown in <ref type="table" target="#tab_0">Table II</ref>, We compare both spatial attention pyramid network (APNet-S) and channel-wise attention pyramid network (APNet-C) in different pyramid levels and multiple baselines including APNet 0 , Stacked attention, and original SE-ResNet <ref type="bibr" target="#b52">[53]</ref>. APNet-S 3 denotes the network with 3-level spatial attention pyramid P = {P 1 , P 2 , P 3 }. APNet 0 is the baseline network without any attention model. Stacked attention denotes stacking same attentions without splitting as P = {P 1 , P 1 , P 1 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Attention Pyramid Method vs. Baseline:</head><p>We first examined the effectiveness of the proposed attention pyramid method. <ref type="table" target="#tab_0">Table II</ref> shows the comparison of our APNet method with the baseline network without attention named as APNet 0 . We observed that both APNet-S and APNet-C consistently obtain significant improvement over the baseline network. We achieve +2.7%/ + 1.2% mAP/Rank1 performance improvement on the Market-1501 dataset and +4.3%/ + 2.0% for DukeMTMC-reID. On the large scale MSMT17 dataset, we still outperformed +12.3%/ + 8.4% and +7.8%/ + 5.4% mAP/Rank1 by channel-wise and spatial APNet.</p><p>2) Attention Pyramid vs. Stacked Attention: To illustrate the superiority of the pyramid structure for the attention model, we designed a stacking scheme to prove that the naive linear attention stacking method is not helping the network to learn a more discriminative feature map. Specifically, instead of following our proposed "split-attend-merge-stack" pipeline introduced in <ref type="figure">Fig 2,</ref> we just send the input feature map into three attention layers with the same architecture consecutively. The only difference between this stacked attention and our attention pyramid is the "split" mechanism. As shown in <ref type="table" target="#tab_0">Table II</ref>, we compared our APNet and stacked attention model on both spatial attention and channel-wise attention. With the 3 level attentions, our APNet-C 3 outperforms the stacked attention by a large margin on all four datasets, including +0.9%/ + 0.8% mAP/Rank1 on Market-1501, +2.6%/ + 1.8% mAP/Rank1 on DukeMTMC-reID, and +7.9%/ + 7.5% mAP/Rank1 on MSMT17. The improvement is also significant for the spatial attention pyramid APNet-S. It demonstrates the effectiveness of the proposed "split" mechanism. Although more attention layers and parameters are used, the performance of stacked attention is even lower than the original single attention layer APNet-C 1 or APNet-S 1 . It proves that more attention layers and parameters with the higher computational cost are not key factors for the network to learn a more robust feature representation, and directly proves the superiority of our proposed attention pyramid method is achieved by the effective pyramid structure but not more parameters.</p><p>3) Influence of Pyramid Level: In addition to the "split" mechanism, we also evaluated the effectiveness of the "stack" principle. For this goal, we compared the APNet with different attention map stacking schemes. Taking channel-wise attention pyramid network as an example, APNet-C 1 only has a global attention map {P 1 }, while APNet-C 2 and APNet-C 3 respectively stack 2 and 3 attentions as {P 1 }, {P 1 , P 2 }, and {P 1 , P 2 , P 3 }. From Table II, we can observe that the performance obviously increases when stacking fine-grained attention on the original global attention. On the Market-1501 and DukeMTMC-reID dataset, APNet-C 2 outperforms APNet-C 1 with +0.9%/ + 0.7% and +2.5%/ + 1.8% mAP/Rank1, while this improvement is larger for MSMT17 as +4.0%/ + 1.8%. When the attention pyramid goes deeper, the increase turns slow, which indicates adding the levels of the attention pyramid is rarely helpful when the level of the pyramid is enough. It is because the resolution is too limited to discover the discriminative semantic clues in deeper layers. Due to the trivial improvement and computational cost increment of the deeper attention pyramid, we choose the low-level attention Methods GFLOPS APNet-C0 6.23 APNet-C1 6.23 APNet-C2 6.24 APNet-C3 6.25 SE-ResNet <ref type="bibr" target="#b52">[53]</ref> 6.24 Pyramid ReID <ref type="bibr" target="#b22">[23]</ref> 9.96 SCSN <ref type="bibr" target="#b55">[56]</ref> 7.24 pyramid for the following experiments. 4) Channel-wise Attention vs. Spatial Attention: We implement our attention pyramid network with both spatial and channel-wise attention. In both Market-1501 and DukeMTMC-reID datasets, we get very similar results with these two attention modules. However, we observe that our method with channel-wise attention is better than spatial attention on the MSMT17 dataset. For channel-wise attention, the Rank-1 and mAP accuracy are 3.7%/3.7%, 4.6%/2.9% and 4.1%/2.1% greater than spatial attention for APNet at pyramid level 1, level 2 and level 3, respectively. It might be due to the larger spatial variances with indoor and outdoor images, which causes the inaccurate spatial representation for the model. 5) Influence of Split Radix: Beyond pyramid levels, we also conducted experiments to analyze the radix of the split process. The radix indicates the speed of scale reduction in our attention pyramid. In this experiment, we use attention pyramid networks at level 2 as the baseline model and apply different radixes for comparison. Specifically, we choose split radix as r = {1, 2, 4, 8, 16, 32} to respectively build our APNet, where radix R?1 denotes no splitting. The experiment results are shown in Fig5. We can observe obvious maxima of both mAP and Rank-1 accuracy at radix equals to 2, and both accuracy slightly drops when radix increases. The phenomenon that performance drops while radix increases indicates the lower granularity is detrimental to discover the discriminative semantic clues. Such phenomenon is consistent with the observation that the improvement turns to slow with a deeper attention pyramid. It motivates us to maintain an adequate scale in the splitting process.</p><p>6) Analysis of computational cost: A core advantage of our APNet is the more efficient computing than other pyramid structures. To evaluate the efficiency of the proposed attention pyramid, we compare the computational cost of APNet and other methods such as SE-ResNet <ref type="bibr" target="#b52">[53]</ref>, Pyramid ReID <ref type="bibr" target="#b22">[23]</ref> and SCSN <ref type="bibr" target="#b55">[56]</ref>. As shown in <ref type="table" target="#tab_0">Table III</ref>, we summarize the Giga floating-point operations per second (GFLOPS) to represent the computational cost. Specifically, we use a single image resized to 224 ? 224 as the input and calculate the GFLOPS by mainstream tool PyTorch-OpCounter 3 for all experiments. The stride of the last residual block of ResNet in every tested method is set to 1. As shown in <ref type="table" target="#tab_0">Table II and Table III</ref>, our AP-Net achieves higher accuracy with comparable computational cost compares with SE-ResNet <ref type="bibr" target="#b52">[53]</ref>,. Specifically, APNet-C outperforms SE-ResNet by +1.6%/ + 0.7%, +3.4%/ + 1.1% and +5.0%/+2.6% mAP/Rank1 respectively on three datasets with similar computational cost. It is because we only apply the attention pyramid on the top of each CNN block but not every convolution layer. We also compare our APNet with other feature pyramid structures such as Pyramid ReID <ref type="bibr" target="#b22">[23]</ref>. The comparison results show our APNet saves almost 40% computational cost than Pyramid ReID, by reducing multiple convolution branches and using attention instead. We did not compare APNet with FPN <ref type="bibr" target="#b6">[7]</ref> or FPA <ref type="bibr" target="#b59">[60]</ref>, since these methods focus on detection or segmentation tasks whose GFLOPS is significantly larger than APNet for the recognition task. Compared with the cascaded attention model SCSN <ref type="bibr" target="#b55">[56]</ref>, our attention pyramid is more efficient by 14.0% GFLOPS. It is mainly because APNet needs no extra salience selection and information aggregation modules. 7) Generalizability for different backbones: In most experiments of this paper, we apply the "ResNet50" as the backbone for a fair comparison with others. It is because most methods apply the "ResNet50" as their backbones in the field of person ReID. To evaluate the generalizability of our method for different backbones, we apply APNet into different deep learning architectures including "ResNet101" <ref type="bibr" target="#b60">[61]</ref>, "DesNet169" <ref type="bibr" target="#b79">[79]</ref>, and "Inception-V3" <ref type="bibr" target="#b61">[62]</ref>. In <ref type="table">Table V</ref>, we compare the experimental performance of these deep learning architectures with/without our APNet on the Market-1501 <ref type="bibr" target="#b7">[8]</ref> dataset. In this experiment, we first evaluate different deep learning architectures as baselines, and apply our APNet-C in these backbones. For both baseline backbone and our method, we employ the same loss functions, hyper-parameters, and training settings. We observe that our APNet can achieve consistent improvement on all backbone architectures. For ResNet50, our APNet achieved 2.7% improvement on mAP score and 1.2% improvement on Rank-1 accuracy. APnet also obtained excellent performance with ResNet101 backbone with 91.2% mAP score and 90.4% Rank-1 accuracy. It indicates that APNet is effective for different backbone scales. APnet also improved the Inception-V3 and DesNet169 in a large margin, i.e., 1.7% mAP score and 0.9% Rank-1 3 https://github.com/Lyken17/pytorch-OpCounter accuracy for Inception-V3 and 1.5% mAP score and 1.1% Rank-1 accuracy for DesNet169. It demonstrates that APNet is effective for different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison to State-of-the-Art Methods</head><p>To show the effectiveness of our proposed method, We compared the mAP, Rank-1, and Rank-5 accuracy of APNet with several state-of-the-art ReID methods on the popular Market-1501, DukeMTMC-reID, CUHK03, and MSMT17 datasets. <ref type="table" target="#tab_0">Table IV</ref> summarizes the results of the comparison for different methods on all four datasets. We introduce the results without attention model on the top of the <ref type="table">Table and</ref> report the performance of attention-based methods in the second part. Finally, we show both spatial and channel-wise APNet at the bottom of the <ref type="table">Table.</ref> We observe that APNet achieves superior performance over all listed methods on four benchmarks, which illustrates the effectiveness of our method.</p><p>On the Market-1501 dataset, our APNet achieves the stateof-the-art performance with the efficient attention pyramid mechanism. RGA-SC <ref type="bibr" target="#b0">[1]</ref> is the current state-of-the-art method that uses a relation-aware global attention, and we improve the performance by 1.1% and 0.1% on mAP and Rank-1 accuracy. We also reproduce the result of RGA-S which only uses the spatial attention as our method. The result shows our APNet-S with pyramid structure helps the RGA-S improves 1.0% and 0.4% on mAP and Rank-1. Pyramid ReID <ref type="bibr" target="#b22">[23]</ref> crops the input image into pieces with different scales and captures multi-scale clues with multiple convolution branches, which achieves the best performance without attention. We outperform Pyramid ReID by 2.3%, 0.5% on mAP and Rank-1, respectively. Beyond performance, we also save 37.4% computational cost than Pyramid ReID by the efficient attention-based pyramid structure.</p><p>On the DukeMTMC-reID dataset, APNet outperforms the second best method ISP [70] by 1.5%/0.5% on the mAP/Rank-1 performance. SCSN <ref type="bibr" target="#b55">[56]</ref> cascades multiple refine stages including an attention model and salience selection model, which obtains the state-of-the-art Rank-1 performance. However, this cascaded attention structure needs a complex mechanism with large computational cost to avoid information duplication such as the salience selection model. Our APNet achieves the same Rank-1 accuracy and a large improvement of +2.5% for mAP score with 14.0% less computation.</p><p>On the CUHK03 dataset, our method achieved performance improvement on both detected and labeled settings. SCSN <ref type="bibr" target="#b55">[56]</ref> method achieves the state-of-the-art performance on the CUHK03 dataset which outperforms other methods by a large margin. Compared with SCSN <ref type="bibr" target="#b55">[56]</ref>, our APNet obtains better performance. Specifically, we obtain 85.3% mAP score and 87.4% Rank-1 accuracy on the manually labeled data, and obtain 81.5% mAP score and 83.0% Rank-1 accuracy on the auto-detected one. We find that the multi-part trick in MGN <ref type="bibr" target="#b30">[31]</ref> is very effective on the CUHK03 dataset. The multi-part trick adds a new local branch which splits the feature map into two parts and learns the local features. Specifically, the feature maps are split from the third residual block along the height dimension. In the inference, we connected the original global feature with local features for final matching. However, the effectiveness of this trick is limited when we add the scale of datasets (e.g., MSMT17 <ref type="bibr" target="#b10">[11]</ref> or Market-1501 <ref type="bibr" target="#b7">[8]</ref>). Thus, we only use the mutli-part trick on the CUHK03 dataset, but not on other datasets. MSMT17 is the large-scale person ReID dataset with both indoor and outdoor images. As shown in <ref type="table" target="#tab_0">Table IV,</ref>   <ref type="figure">Fig. 6</ref>. Visualizations of the attention map with different pyramid levels. We adopt the Grad-CAM <ref type="bibr" target="#b86">[86]</ref> to visualize the learned attention maps of our attention pyramid. For each sample, from left to right, we show the input image, attention of the first level pyramid, attention of the second level pyramid. We can observe that attentions in different pyramid levels capture the salient clues of different scales. Best viewed in color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robustness Analysis</head><p>To further evaluate the robustness of APNet, we test our method on the Occluded-DukeMTMC dataset which contains occluded and corrupted inputs. Occluded-DukeMTMC is transformed from DukeMTMC-reID by re-splitting the size of each subset manually. Original DukeMTMC-reID dataset includes 14%, 15%, and 10% of occluded images in training, query, and gallery dataset, while re-split Occluded-DukeMTMC contains 9%, 100%, and 10% of occluded images in training, query, and gallery dataset, respectively. As a consequence, at least one of the features extracted by images will from occluded images and used in pairwise distance calculation at inference time. We follow the experimental settings in <ref type="bibr" target="#b62">[63]</ref> of the Occluded-DukeMTMC dataset. It is a direct metric to evaluate the robustness of the model trained on the original images for the testing of occluded images.</p><p>Recently, many methods <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b83">[83]</ref>- <ref type="bibr" target="#b85">[85]</ref> focus on the robustness of the ReID model for occlusion and achieve obvious success. We also test APNet on the Occluded-DukeMTMC dataset and compare it with other methods designed for the occlusion problem. <ref type="table" target="#tab_0">Table VI</ref> shows the comparison between our APNet and the current state-of-the-art methods on this dataset. From the result, APNet achieves 54.2% and 62.2% in mAP and Rank-1 accuracy, respectively, which outperforms the current best model by a large margin as +10.3% and +7.1%. It demonstrates the attention model helps the network to focus on the salient region and avoid the corruption by occlusion. While the large improvement of the pyramid structure APNet-C 2 over the baseline attention model APNet-C 1 shows the pyramid structure enhances the attention model to capture the multi-scale salient clues to further prevent absorbing occluded feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Generalization Ability Analysis</head><p>In real-world applications, we always need to deploy the ReID model into unseen scenes. However, it requires extensive human labor to label an overwhelming amount of data for training models on new scenes. Thus, the generalization ability of the person ReID methods becomes a key factor for deploying the real-world application. To evaluate the generalization ability of our APNet, we conducted a cross-dataset evaluation to measure the generalization ability of the ReID model for unseen persons and scenes. Specifically, we trained the model on the Market-1501 dataset then tested it on DukeMTMC-reID, and vice versa. We conducted the experiments for APNet with different levels and the baseline model and compared it with other attention-based methods such as SCAL <ref type="bibr" target="#b1">[2]</ref>. The results in <ref type="table" target="#tab_0">Table VII</ref> show our APNet achieves +5.7%/ +6.8% mAP/Rank-1 performance over the baseline methods from Market to Duke, and obtain the improvement +4.6%,+6.4% mAP/Rank-1 from Duke to Market. It shows that the pyramid structure is effective to enhance the generalization ability of the attention model. Compared with SCAL <ref type="bibr" target="#b1">[2]</ref>, our APNet achieves the comparable Duke?Market performance and significantly improved Market?Duke performance, which also shows the great potential of attention pyramid structure in terms of generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative Analysis</head><p>To validate the effectiveness of our attention pyramid networks learning method, we used Grad-CAM <ref type="bibr" target="#b86">[86]</ref> to visualize the attention map for qualitative analysis. The attention map is generated after each pyramid level, and we expect the attention mask to emphasizes more on the discriminative feature of the person at the deeper pyramid level. As shown in <ref type="figure">Fig. 6</ref>, we can observe the salient feature of the person such as the handbag, umbrella, logo on the cloth, and shirt are highlighted. When the pyramid level goes deeper, we find the attention map is more concentrated on the salient part of the person and alleviate the common misalignment issue in image retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed simple yet effective attention pyramid networks (APNet) for the person re-identification task. To capture the salient clues with different scales, we proposed a "split-attend-merge-stack" principle to build the attention pyramid. We split the feature maps into local parts and merge all learned local attentions as global attention. By stacking the attention modules with different granularities of splitting, we construct an attention pyramid to guide the finegrained attention learning with coarse ones. We implement our APNet with both spatial and channel-wise attention modules to demonstrate it can be integrated into any attention model. By extensive experiments, we also demonstrate that APNet is more effective, efficient, and robust than other pyramid structures or attention models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2. The architecture of Attention Pyramid Networks (APNet). Our APNet adopts the "split-attend-merge-stack" principle, which first splits the feature maps into multiple parts, obtains the attention map of each part, and the attention map for the current pyramid level is constructed by merging each attention map. Then in a deeper pyramid level, we split the features into more fine-grained parts and learn the fine-grained attention guiding by coarse attentions. Finally, attentions with different granularities are stacked as an attention pyramid and applied to the original input feature by element-wise product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The different basic attention modules of Attention Pyramid Networks (APNet), including spatial attention and channel-wise attention. For spatial attention, we follow the architecture of RGA-S<ref type="bibr" target="#b0">[1]</ref> which models the spatial relations. For channel-wise attention, we adopt the widely used SE attention<ref type="bibr" target="#b1">[2]</ref>,<ref type="bibr" target="#b52">[53]</ref> module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The comparison between different feature pyramid structures. (a) FPN [7] applies a top-down pathway to aggregate features with different resolutions to detect objects with different scales. (b) Pyramid ReID [23] splits the feature maps with different scales and extracts local features of each part by convolution and fully-connected layers. (c) Feature pyramid attention (FPA) [60] applies the feature pyramid in the attention model for semantic segmentation by aggregating clues captured with multi-scale convolution layers. (d) Our attention pyramid network proposes a "split-attend-merge-stack" principle to learn multi-scale attentions by feature splitting and stacking. It is effective with the coarse-to-fine attention learning and more efficient due to no extra computational cost for feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The mAP (red) and Rank-1 accuracy (blue) with different radix for APNet with level 2 pyramid on the Market-1501 dataset.TABLE III GFLOPS OF DIFFERENT PYRAMID LEVELS FOR APNET-C, SE-RESNET,PYRAMID REID AND SCSN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc></figDesc><table /><note>DETAILED INFORMATION OF DATASETS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDIES OF THE APNET-C METHOD ON THE MARKET-1051, DUKEMTMC-REID AND MSMT17 DATASET.</figDesc><table><row><cell>Model</cell><cell cols="6">Market-1501 mAP R-1 R-5 mAP R-1 DukeMTMC-reID R-5 mAP R-1 MSMT17</cell><cell>R-5</cell></row><row><cell>APN0</cell><cell>87.8</cell><cell>95.0 98.6</cell><cell>77.2</cell><cell>88.4 94.4</cell><cell>51.2</cell><cell cols="2">75.3 84.9</cell></row><row><cell>SE-ResNet</cell><cell>88.6</cell><cell>95.5 98.5</cell><cell>78.7</cell><cell>88.5 95.1</cell><cell>58.5</cell><cell cols="2">81.1 90.3</cell></row><row><cell>Stacked Attention (C)</cell><cell>89.4</cell><cell>95.3 98.7</cell><cell>78.7</cell><cell>88.4 95.1</cell><cell>58.0</cell><cell cols="2">80.5 89.5</cell></row><row><cell>APNet-C1</cell><cell>89.6</cell><cell>95.5 98.6</cell><cell>79.0</cell><cell>88.6 95.2</cell><cell>59.5</cell><cell cols="2">81.9 90.7</cell></row><row><cell>APNet-C2</cell><cell>90.5</cell><cell>96.2 98.8</cell><cell>81.5</cell><cell>90.4 95.6</cell><cell>63.5</cell><cell cols="2">83.7 91.7</cell></row><row><cell>APNet-C3</cell><cell>90.3</cell><cell>96.1 98.8</cell><cell>81.3</cell><cell>90.2 95.8</cell><cell>63.1</cell><cell cols="2">82.8 91.2</cell></row><row><cell>Stacked Attention (S)</cell><cell>87.9</cell><cell>95.5 98.5</cell><cell>77.6</cell><cell>88.6 94.7</cell><cell>56.1</cell><cell cols="2">78.7 89.0</cell></row><row><cell>APNet-S1</cell><cell>88.0</cell><cell>95.7 98.5</cell><cell>77.5</cell><cell>88.8 94.7</cell><cell>55.8</cell><cell cols="2">78.2 88.7</cell></row><row><cell>APNet-S2</cell><cell>89.0</cell><cell>96.1 98.7</cell><cell>78.8</cell><cell>89.3 95.0</cell><cell>58.9</cell><cell cols="2">80.8 89.7</cell></row><row><cell>APNet-S3</cell><cell>89.3</cell><cell>96.1 98.6</cell><cell>78.8</cell><cell>89.2 94.8</cell><cell>59.0</cell><cell cols="2">80.7 89.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV COMPARING</head><label>IV</label><figDesc>WITH THE STATE-OF-THE-ART METHODS ON THE MARKET-1501, DUKEMTMC-REID, MSMT17, AND CHUK03 DATASETS. WE APPLY THE CUHK03-NP VERSION FOR EVALUATION, WHERE (D/L) DENOTES THE PERFORMANCE ON THE DETECTED AND LABELED DATA. denotes the result is reproduced by us.</figDesc><table><row><cell>Method</cell><cell>Publication</cell><cell cols="2">Backbone</cell><cell cols="8">Market-1501 mAP R-1 R-5 mAP R-1 R-5 mAP R-1 R-5 DukeMTMC-reID MSMT17</cell><cell>CUHK03 (d/l) mAP R-1</cell></row><row><cell>PCB+RPP [29]</cell><cell>ECCV'18</cell><cell cols="2">ResNet-50</cell><cell cols="4">81.6 93.8 97.5 69.2 83.3</cell><cell>-</cell><cell cols="3">40.4 68.2 81.6</cell><cell>57.5/-</cell><cell>63.7/-</cell></row><row><cell>MGN [31]</cell><cell cols="3">ACMMM'18 ResNet-50</cell><cell>86.9 95.6</cell><cell>-</cell><cell cols="2">78.4 88.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.0/67.4 66.8/ 68.0</cell></row><row><cell>VMP [66]</cell><cell>CVPR'19</cell><cell cols="2">ResNet-50</cell><cell cols="5">80.8 93.0 97.8 72.6 83.6 91.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DG-Net [67]</cell><cell>CVPR'19</cell><cell cols="2">ResNet-50</cell><cell>86.0 94.8</cell><cell>-</cell><cell cols="2">74.8 86.6</cell><cell>-</cell><cell cols="2">52.3 77.2</cell><cell>-</cell><cell>61.1/-</cell><cell>65.6/-</cell></row><row><cell>BoT [28]</cell><cell>CVPRW'19</cell><cell cols="2">ResNet-50</cell><cell>85.9 94.5</cell><cell>-</cell><cell cols="2">76.4 86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSA [14]</cell><cell>CVPR'19</cell><cell cols="2">ResNet-50</cell><cell>87.6 95.7</cell><cell>-</cell><cell cols="2">74.3 86.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.1/75.2 78.2/78.9</cell></row><row><cell>Pyramid [23]</cell><cell>CVPR'19</cell><cell cols="6">ResNet-101 88.2 95.7 98.4 79.0 89.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.8/76.9 78.9/78.9</cell></row><row><cell>IANet [68]</cell><cell>CVPR'19</cell><cell cols="2">ResNet-50</cell><cell>83.1 94.4</cell><cell>-</cell><cell cols="2">73.4 87.1</cell><cell>-</cell><cell cols="3">46.8 75.5 85.5</cell><cell>-</cell><cell>-</cell></row><row><cell>OSNet [27]</cell><cell>ICCV'19</cell><cell cols="2">OSNet</cell><cell>84.9 94.8</cell><cell>-</cell><cell cols="2">73.5 88.6</cell><cell>-</cell><cell cols="2">52.9 78.7</cell><cell>-</cell><cell>67.8/-</cell><cell>72.3/-</cell></row><row><cell>SNR [69]</cell><cell>CVPR'20</cell><cell cols="2">ResNet-50</cell><cell>84.7 94.4</cell><cell>-</cell><cell cols="2">72.9 84.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ISP [70]</cell><cell>ECCV'20</cell><cell cols="7">HRNet-W32 88.6 95.3 98.6 80.0 89.6 95.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.4/74.1 75.2/76.5</cell></row><row><cell>CBN [71]</cell><cell>ECCV'20</cell><cell cols="2">ResNet-50</cell><cell cols="5">83.6 94.3 97.9 70.1 84.8 92.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPD [39]</cell><cell>TIP'20</cell><cell cols="2">ResNet-50</cell><cell cols="5">87.6 95.8 98.0 78.3 88.4 94.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.5/73.3 70.2/76.0</cell></row><row><cell>DPD-101 [39]</cell><cell>TIP'20</cell><cell cols="7">ResNet-101 88.2 95.9 98.6 80.2 89.4 95.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.9/77.5 78.2/79.6</cell></row><row><cell cols="2">CBDB-Net [72] TCSVT'21</cell><cell cols="2">ResNet-50</cell><cell>85.0 94.4</cell><cell>-</cell><cell cols="2">74.3 87.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.8/76.6 75.4/77.8</cell></row><row><cell>MG-CAM [73]</cell><cell>CVPR'18</cell><cell cols="2">ResNet-50</cell><cell>74.3 83.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.6/41.0 41.7/44.4</cell></row><row><cell>HA-CNN [5]</cell><cell>CVPR'18</cell><cell cols="2">HA-CNN</cell><cell>75.7 91.2</cell><cell>-</cell><cell cols="2">63.8 80.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.6/41.0 41.7/44.4</cell></row><row><cell>DuATM [33]</cell><cell>CVPR'18</cell><cell cols="2">DenseNet</cell><cell cols="5">76.6 91.4 97.1 64.6 81.8 90.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPReID [15]</cell><cell>CVPR'18</cell><cell cols="4">ResNet-152 83.4 93.7 97.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mancs [74]</cell><cell>ECCV'18</cell><cell cols="2">ResNet-50</cell><cell cols="4">82.3 93.1 97.6 71.8 84.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.5/63.9 65.5/69.0</cell></row><row><cell>AAnet [19]</cell><cell>CVPR'19</cell><cell cols="2">ResNet-50</cell><cell>82.5 93.9</cell><cell>-</cell><cell cols="2">72.6 86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SCAL-S [2]</cell><cell>ICCV'19</cell><cell cols="2">ResNet-50</cell><cell cols="5">88.9 95.4 98.5 79.6 89.0 95.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.2/71.5 70.4/74.1</cell></row><row><cell>SCAL-C [2]</cell><cell>ICCV'19</cell><cell cols="2">ResNet-50</cell><cell cols="5">89.3 95.8 98.7 79.1 88.9 95.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.6/72.3 71.1/74.8</cell></row><row><cell>CAMA [75]</cell><cell>ICCV'19</cell><cell cols="2">ResNet-50</cell><cell cols="2">84.5 94.7 98.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.2/66.5 66.6/70.1</cell></row><row><cell>BAT-net [6]</cell><cell>ICCV'19</cell><cell cols="2">ResNet-50</cell><cell cols="9">85.5 94.1 98.2 77.3 87.7 94.7 56.8 79.5 89.1 73.2/76.1 76.2/78.6</cell></row><row><cell>MHN [76]</cell><cell>ICCV'19</cell><cell cols="2">ResNet-50</cell><cell cols="5">85 95.1 98.1 77.2 89.1 94.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.4/72.4 71.7/77.2</cell></row><row><cell>ABD-Net [34]</cell><cell>ICCV'19</cell><cell cols="2">ResNet-50</cell><cell>88.3 95.6</cell><cell>-</cell><cell cols="2">78.6 89.0</cell><cell>-</cell><cell cols="3">60.8 82.3 90.6</cell><cell>-</cell><cell>-</cell></row><row><cell>LAG-Net [77]</cell><cell>TMM'20</cell><cell cols="2">ResNet-50</cell><cell cols="5">89.5 95.6 98.3 81.6 90.4 96.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.1/82.4 82.2/85.1</cell></row><row><cell>SCSN [56]</cell><cell>CVPR'20</cell><cell cols="2">ResNet-50</cell><cell>88.5 95.7</cell><cell>-</cell><cell cols="2">79.0 90.1</cell><cell>-</cell><cell cols="4">58.0 83.0 91.2 80.2/83.3 84.1/86.3</cell></row><row><cell>RGA-SC [1]</cell><cell>CVPR'20</cell><cell cols="2">ResNet-50</cell><cell>88.4 96.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">57.5 80.3</cell><cell>-</cell><cell>74.5/77.4 79.6/81.1</cell></row><row><cell>*  RGA-S [1]</cell><cell>CVPR'20</cell><cell cols="2">ResNet-50</cell><cell cols="9">88.0 95.7 98.5 77.5 88.8 94.7 55.8 78.2 88.7 72.7/75.6 78.1/79.1</cell></row><row><cell>PISNet [78]</cell><cell>ECCV'20</cell><cell cols="2">ResNet-50</cell><cell>87.1 95.6</cell><cell>-</cell><cell cols="2">78.7 88.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>APNet-S</cell><cell></cell><cell cols="2">ResNet-50</cell><cell cols="9">89.0 96.1 98.7 78.8 89.3 95.0 59.0 80.8 89.8 78.1/81.1 80.9/83.5</cell></row><row><cell>APNet-C</cell><cell></cell><cell cols="2">ResNet-50</cell><cell cols="9">90.5 96.2 98.8 81.5 90.4 95.6 63.5 83.7 91.7 81.5/85.3 83.0/87.4</cell></row><row><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDIES OF DIFFERENT BACKBONES ON THE MARKET-1501</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DATASET.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Rank@R</cell><cell>mAP</cell><cell>R-1</cell><cell>R=5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet50</cell><cell>87.8</cell><cell>95.0</cell><cell>98.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet50 + APNet-C</cell><cell>90.5</cell><cell>96.2</cell><cell>98.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet101</cell><cell>89.6</cell><cell>95.3</cell><cell>98.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet101 + APNet-C</cell><cell>91.2</cell><cell>96.4</cell><cell>98.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Inception-V3</cell><cell>86.3</cell><cell>95.1</cell><cell>98.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Inception-V3 + APNet-C</cell><cell>88.0</cell><cell>96.0</cell><cell>98.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DesNet169</cell><cell>89.0</cell><cell>95.1</cell><cell>98.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DesNet169 + APNet-C</cell><cell>90.5</cell><cell>96.2</cell><cell>98.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI THE</head><label>VI</label><figDesc>COMPARISONS WITH THE STATE-OF-THE-ART METHODS ON THE OCCLUDED-DUKEMTMC DATASET.</figDesc><table><row><cell>Input image</cell><cell>Attention at 1</cell><cell>Attention at 2</cell><cell>Input image</cell><cell>Attention at 1</cell><cell>Attention at 2</cell><cell cols="2">Input image</cell><cell cols="2">Attention at 1</cell><cell>Attention at 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell cols="2">mAP R-1</cell><cell>R-5 R-10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HA-CNN [5]</cell><cell></cell><cell>26.0</cell><cell cols="2">34.4 51.9 59.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Adver Occluded [80]</cell><cell>32.2</cell><cell>44.5</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCB [29]</cell><cell></cell><cell>42.6</cell><cell cols="2">37.3 57.7 62.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Part Bilinear [81]</cell><cell></cell><cell>-</cell><cell>36.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FD-GAN [82]</cell><cell></cell><cell>-</cell><cell>40.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DSR [83]</cell><cell></cell><cell>30.4</cell><cell cols="2">40.8 58.2 65.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SFR [84]</cell><cell></cell><cell>32.0</cell><cell cols="2">42.3 60.3 67.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ad-Occluted [80]</cell><cell></cell><cell>32.2</cell><cell>44.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PGFA [63]</cell><cell></cell><cell>37.3</cell><cell>51.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HOReID [85]</cell><cell></cell><cell>43.8</cell><cell>55.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>APNet-C1</cell><cell></cell><cell>46.6</cell><cell cols="2">53.9 68.6 74.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>APNet-C2</cell><cell></cell><cell>54.1</cell><cell cols="2">62.2 76.3 81.5</cell></row></table><note>the attention mechanism shows great effectiveness on this challenging dataset due to discovering discriminative clues. While our APNet further obtains the improvement than the best published method ABD-Net [34] by a large margin. We obtain 63.5%/83.7% Rank1/mAP, which outperforms ABD- Net [34] with 2.7%/1.4% by the effective pyramid structure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII CROSS</head><label>VII</label><figDesc>-DOMAIN EVALUATION ON MARKET-1501 AND DUKEMTMC-REID DATASETS. WE USE MARKET ? DUKE TO REPRESENT THAT THE MODEL IS TRAINED ON THE MARKET-1501 DATASET AND TESTED ON THE DUKEMTMC-REID DATASET, AND VICE VERSA. C1 16.6 30.1 43.9 50.0 23.0 50.4 65.2 71.8 APNet-C2 21.3 35.9 50.1 56.9 24.0 51.0 66.4 72.3 APNet-C3 22.8 37.7 52.4 59.0 23.7 50.9 66.6 72.6</figDesc><table><row><cell>Method</cell><cell cols="4">Market?Duke mAP R-1 R-5 R-10 mAP R-1 R-5 R-10 Duke?Market</cell></row><row><cell>Baseline</cell><cell cols="4">15.6 29.1 43.4 50.1 19.3 44.4 61.1 66.7</cell></row><row><cell>SCAL-C</cell><cell>16.4 28.6 -</cell><cell>-</cell><cell>23.8 51.7 -</cell><cell>-</cell></row><row><cell>APNet-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">we use the official implementation of RGA-S at https://github.com/ microsoft/Relation-Aware-Global-Attention-Networks</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3186" to="3195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatial-temporal attention-aware learning for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4192" to="4205" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICCV</title>
		<imprint>
			<biblScope unit="page" from="2119" to="2128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICCV</title>
		<imprint>
			<biblScope unit="page" from="2285" to="2294" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8030" to="8039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1062" to="1071" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="650" to="667" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute-driven feature disentangling and temporal aggregation for video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial-temporal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8933" to="8940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7948" to="7956" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="480" to="496" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACMMM</title>
		<imprint>
			<biblScope unit="page" from="274" to="282" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5363" to="5372" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Second-order nonlocal attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative feature learning with foreground attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4671" to="4684" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep pyramidal pooling with attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7306" to="7316" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="803" to="818" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="369" to="378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scan: Selfand-collaborative attention network for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4870" to="4882" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning recurrent 3d attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6963" to="6976" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Temporal coherence or temporal motion: Which is more critical for video-based person re-identification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
	<note>in ECCV, 2020</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="486" to="504" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep credible metric learning for unsupervised domain adaptation person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="643" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="169" to="185" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Interactionand-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Identity-guided human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13467</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rethinking the distribution gap of person re-identification with camera-based batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="140" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Incomplete descriptor mining with elastic loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TCSVT</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1179" to="1188" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Lag-net: Multi-granularity network for person re-identification via local attention system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TMM</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Do not disturb me: Person re-identification under the interference of other pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="647" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adversarially occluded samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5098" to="5107" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="402" to="419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fd-gan: Poseguided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NIPS</title>
		<imprint>
			<biblScope unit="page" from="1230" to="1241" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7073" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Recognizing partial biometric patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07399</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">High-order information matters: Learning relation and topology for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
